id,text,aspect,aspect_confidence,perplexity,token_count
0,"We introduce Probabilistic FastText , a new model for word embeddings that can capture multiple word senses , sub-word structure , and uncertainty information .",2,0.5996034,46.5618792974957,25
0,"In particular , we represent each word with a Gaussian mixture density , where the mean of a mixture component is given by the sum of n-grams .",2,0.80663425,27.859385048005,28
0,This representation allows the model to share the “ strength ” across sub-word structures ( e.g .,3,0.42144373,49.71423180886754,17
0,"Latin roots ) , producing accurate representations of rare , misspelt , or even unseen words .",0,0.7498738,192.63707746288597,17
0,"Moreover , each component of the mixture can capture a different word sense .",3,0.46669975,93.02696209629114,14
0,"Probabilistic FastText outperforms both FastText , which has no probabilistic model , and dictionary-level probabilistic embeddings , which do not incorporate subword structures , on several word-similarity benchmarks , including English RareWord and foreign language datasets .",3,0.6653462,62.89754600665361,39
0,We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings .,3,0.8358171,29.656592320962734,19
0,"Thus , our model is the first to achieve best of both the worlds : multi-sense representations while having enriched semantics on rare words .",3,0.9521568,87.89851739713316,25
1,"Motivations like domain adaptation , transfer learning , and feature learning have fueled interest in inducing embeddings for rare or unseen words , n-grams , synsets , and other textual features .",0,0.909388,56.991682187888074,32
1,"This paper introduces a la carte embedding , a simple and general alternative to the usual word2vec-based approaches for building such representations that is based upon recent theoretical results for GloVe-like embeddings .",1,0.6620662,45.765571958775496,35
1,Our method relies mainly on a linear transformation that is efficiently learnable using pretrained word vectors and linear regression .,2,0.59065163,47.27533690064944,20
1,"This transform is applicable on the fly in the future when a new text feature or rare word is encountered , even if only a single usage example is available .",3,0.69081765,61.84598245743302,31
1,We introduce a new dataset showing how the a la carte method requires fewer examples of words in context to learn high-quality embeddings and we obtain state-of-the-art results on a nonce task and some unsupervised document classification tasks .,3,0.50921685,23.562389046295838,45
2,Word embedding models such as GloVe rely on co-occurrence statistics to learn vector representations of word meaning .,0,0.82303506,20.266882339794204,18
2,"While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words , existing approaches for modeling such relationships are based on manipulating pre-trained word vectors .",0,0.7905347,29.36859676582851,36
2,"In this paper , we introduce a novel method which directly learns relation vectors from co-occurrence statistics .",1,0.8493397,21.67121140564987,18
2,"To this end , we first introduce a variant of GloVe , in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors .",2,0.64180034,36.83475158878521,28
2,We then show how relation vectors can be naturally embedded into the resulting vector space .,3,0.42558673,58.96929180002311,16
3,"Semantic specialization of distributional word vectors , referred to as retrofitting , is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation .",0,0.8229782,42.996210104997814,32
3,"Existing retrofitting models integrate linguistic constraints directly into learning objectives and , consequently , specialize only the vectors of words from the constraints .",0,0.8105147,225.1655219627653,24
3,"In this work , in contrast , we transform external lexico-semantic relations into training examples which we use to learn an explicit retrofitting model ( ER ) .",2,0.6675172,93.47633311326962,28
3,The ER model allows us to learn a global specialization function and specialize the vectors of words unobserved in the training data as well .,3,0.47864774,55.30670525866905,25
3,We report large gains over original distributional vector spaces in ( 1 ) intrinsic word similarity evaluation and on ( 2 ) two downstream tasks − lexical simplification and dialog state tracking .,3,0.89188516,75.79884373149588,33
3,"Finally , we also successfully specialize vector spaces of new languages ( i.e. , unseen in the training data ) by coupling ER with shared multilingual distributional vector spaces .",3,0.62814295,133.13309147418113,30
4,Unsupervised neural machine translation ( NMT ) is a recently proposed approach for machine translation which aims to train the model without using any labeled data .,0,0.92950016,11.450182695436517,27
4,"The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared-latent space , which is weak in keeping the unique and internal characteristics of each language , such as the style , terminology , and sentence structure .",0,0.63235086,49.55979319626363,53
4,"To address this issue , we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences .",2,0.56432,45.1717123469941,33
4,"Besides , two different generative adversarial networks ( GANs ) , namely the local GAN and global GAN , are proposed to enhance the cross-language translation .",2,0.48382443,24.495266402524233,27
4,"With this new approach , we achieve significant improvements on English-German , English-French and Chinese-to-English translation tasks .",3,0.9298648,12.10911209371053,24
5,"Neural Machine Translation ( NMT ) performs poor on the low-resource language pair ( X , Z ) , especially when Z is a rare language .",0,0.8852957,43.14324638079832,27
5,"By introducing another rich language Y , we propose a novel triangular training architecture ( TA-NMT ) to leverage bilingual data ( Y , Z ) ( may be small ) and ( X , Y ) ( can be rich ) to improve the translation performance of low-resource pairs .",2,0.70439136,78.3534791761303,53
5,"In this triangular architecture , Z is taken as the intermediate latent variable , and translation models of Z are jointly optimized with an unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of ( X , Y ) .",2,0.74699557,106.69825934857758,43
5,"Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets , and achieves even better performance combining back-translation methods .",3,0.96382415,31.22853750041316,30
6,Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation ( NMT ) .,0,0.8869946,35.780415274969336,20
6,"While sentences are usually converted into unique subword sequences , subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary .",0,0.90315396,46.348968870871886,26
6,The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT .,1,0.847155,21.861816852640068,26
6,"We present a simple regularization method , subword regularization , which trains the model with multiple subword segmentations probabilistically sampled during training .",2,0.62743896,45.6855743786458,23
6,"In addition , for better subword sampling , we propose a new subword segmentation algorithm based on a unigram language model .",2,0.6101194,38.59875176594907,22
6,We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings .,3,0.7840756,20.562817486103974,18
7,The past year has witnessed rapid advances in sequence-to-sequence ( seq2seq ) modeling for Machine Translation ( MT ) .,0,0.96407163,14.874947485226313,20
7,"The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model , which was then out-performed by the more recent Transformer model .",0,0.74026215,19.780066578788283,30
7,Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures .,0,0.48391995,31.99244046781935,29
7,"In this paper , we tease apart the new architectures and their accompanying techniques in two ways .",1,0.8432891,54.285904231544535,18
7,"First , we identify several key modeling and training techniques , and apply them to the RNN architecture , yielding a new RNMT + model that outperforms all of the three fundamental architectures on the benchmark WMT ’14 English to French and English to German tasks .",2,0.7327374,55.03710127213948,48
7,"Second , we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths .",2,0.6265994,43.669470250373955,22
7,"Our hybrid models obtain further improvements , outperforming the RNMT + model on both benchmark datasets .",3,0.93224156,146.14525447777118,17
8,"We introduce a new entity typing task : given a sentence with an entity mention , the goal is to predict a set of free-form phrases ( e.g .",2,0.6763947,36.380208222680146,29
8,"skyscraper , songwriter , or criminal ) that describe appropriate types for the target entity .",0,0.35565656,339.11579561085637,16
8,"This formulation allows us to use a new type of distant supervision at large scale : head words , which indicate the type of the noun phrases they appear in .",2,0.5684356,73.33379359544395,31
8,"We show that these ultra-fine types can be crowd-sourced , and introduce new evaluation sets that are much more diverse and fine-grained than existing benchmarks .",3,0.85844344,26.842011467076443,27
8,"We present a model that can predict ultra-fine types , and is trained using a multitask objective that pools our new head-word supervision with prior supervision from entity linking .",2,0.5246934,102.22924277025662,32
8,Experimental results demonstrate that our model is effective in predicting entity types at varying granularity ;,3,0.9803233,41.82573938348008,16
8,"it achieves state of the art performance on an existing fine-grained entity typing benchmark , and sets baselines for our newly-introduced datasets .",3,0.638795,34.83391456976999,27
9,"Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels , neglecting the rich hierarchies over types and entities contained in curated ontologies .",0,0.9109269,55.683444927391854,42
9,Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies .,0,0.90461636,51.871132255422864,17
9,"This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information , yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing , and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset .",1,0.5812333,35.64579853007011,53
9,"We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction : MedMentions , a collection of PubMed abstracts in which 246 k mentions have been mapped to the massive UMLS ontology ;",3,0.5788484,46.8535409559936,48
9,"and TypeNet , which aligns Freebase types with the WordNet hierarchy to obtain nearly 2 k entity types .",2,0.45663604,155.3959167668572,19
9,In experiments on all three datasets we show substantial gains from hierarchy-aware training .,3,0.95695335,50.43340837373293,16
10,Embedding knowledge graphs ( KGs ) into continuous vector spaces is a focus of current research .,0,0.90631187,53.52104186724785,17
10,Early works performed this task via simple models developed over KG triples .,0,0.8376022,137.71635492162986,13
10,"Recent attempts focused on either designing more complicated triple scoring models , or incorporating extra information beyond triples .",0,0.8534364,165.44988302092221,19
10,"This paper , by contrast , investigates the potential of using very simple constraints to improve KG embedding .",1,0.65088093,123.17678098686689,19
10,We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations .,2,0.70003027,47.497564420683986,15
10,The former help to learn compact and interpretable representations for entities .,0,0.6218349,66.2453382959152,12
10,The latter further encode regularities of logical entailment between relations into their distributed representations .,3,0.40718725,179.4151709223293,15
10,"These constraints impose prior beliefs upon the structure of the embedding space , without negative impacts on efficiency or scalability .",0,0.45579267,48.3252752772076,21
10,"Evaluation on WordNet , Freebase , and DBpedia shows that our approach is simple yet surprisingly effective , significantly and consistently outperforming competitive baselines .",3,0.9140816,43.97232744898273,25
10,"The constraints imposed indeed improve model interpretability , leading to a substantially increased structuring of the embedding space .",3,0.7708153,106.70034535445426,19
10,Code and data are available at https://github.com/iieir-km/ComplEx-NNE_AER .,3,0.53953636,47.923021483784716,8
11,"Knowledge Graph ( KG ) embedding has emerged as a very active area of research over the last few years , resulting in the development of several embedding methods .",0,0.9585028,17.00443443639565,30
11,These KG embedding methods represent KG entities and relations as vectors in a high-dimensional space .,0,0.46291497,19.18582370385045,17
11,"Despite this popularity and effectiveness of KG embeddings in various tasks ( e.g. , link prediction ) , geometric understanding of such embeddings ( i.e. , arrangement of entity and relation vectors in vector space ) is unexplored – we fill this gap in the paper .",0,0.42157385,46.24220097934608,47
11,We initiate a study to analyze the geometry of KG embeddings and correlate it with task performance and other hyperparameters .,1,0.63259196,26.67059657054118,21
11,"To the best of our knowledge , this is the first study of its kind .",3,0.88535523,5.742618367715704,16
11,"Through extensive experiments on real-world datasets , we discover several insights .",3,0.62238604,31.53353155967398,12
11,"For example , we find that there are sharp differences between the geometry of embeddings learnt by different classes of KG embeddings methods .",3,0.9462836,30.811960327871805,24
11,We hope that this initial study will inspire other follow-up research on this important but unexplored problem .,3,0.92897356,18.328798137950802,20
12,We propose a unified model combining the strength of extractive and abstractive summarization .,2,0.31594858,29.14771266473248,14
12,"On the one hand , a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable .",3,0.69672334,43.68774645461195,23
12,"On the other hand , a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph .",3,0.6407882,55.64238422938427,23
12,"In our model , sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated .",2,0.65434504,21.63872099085839,30
12,"Moreover , a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions .",2,0.565239,40.61382299987337,19
12,"By end-to-end training our model with the inconsistency loss and original losses of extractive and abstractive models , we achieve state-of-the-art ROUGE scores while being the most informative and readable summarization on the CNN / Daily Mail dataset in a solid human evaluation .",3,0.76398885,29.257907903308,50
13,We present a new neural sequence-to-sequence model for extractive summarization called SWAP-NET ( Sentences and Words from Alternating Pointer Networks ) .,1,0.4614112,37.887984037137116,26
13,"Extractive summaries comprising a salient subset of input sentences , often also contain important key words .",0,0.87736636,158.75321101119278,17
13,"Guided by this principle , we design SWAP-NET that models the interaction of key words and salient sentences using a new two-level pointer network based architecture .",2,0.79339594,62.286479811240056,30
13,"SWAP-NET identifies both salient sentences and key words in an input document , and then combines them to form the extractive summary .",2,0.5603004,45.694212780988806,25
13,Experiments on large scale benchmark corpora demonstrate the efficacy of SWAP-NET that outperforms state-of-the-art extractive summarizers .,3,0.8783815,15.29883628686018,24
14,"Most previous seq2seq summarization systems purely depend on the source text to generate summaries , which tends to work unstably .",0,0.8958861,40.162952998104956,21
14,"Inspired by the traditional template-based summarization approaches , this paper proposes to use existing summaries as soft templates to guide the seq2seq model .",1,0.4240773,23.31498971091447,26
14,"To this end , we use a popular IR platform to Retrieve proper summaries as candidate templates .",2,0.8222572,165.93713006763406,18
14,"Then , we extend the seq2seq framework to jointly conduct template Reranking and template-aware summary generation ( Rewriting ) .",2,0.79580337,62.229421585195205,22
14,"Experiments show that , in terms of informativeness , our model significantly outperforms the state-of-the-art methods , and even soft templates themselves demonstrate high competitiveness .",3,0.9600081,33.56722960010462,32
14,"In addition , the import of high-quality external summaries improves the stability and readability of generated summaries .",3,0.684506,31.39702707061993,18
15,Sentence splitting is a major simplification operator .,0,0.43337342,109.39861875985962,8
15,Here we present a simple and efficient splitting algorithm based on an automatic semantic parser .,1,0.78414446,28.81467097936392,16
15,"After splitting , the text is amenable for further fine-tuned simplification operations .",3,0.5774864,95.1521455227014,13
15,"In particular , we show that neural Machine Translation can be effectively used in this situation .",3,0.6234805,25.355810323769838,17
15,"Previous application of Machine Translation for simplification suffers from a considerable disadvantage in that they are over-conservative , often failing to modify the source in any way .",0,0.9344714,68.08513551454243,28
15,"Splitting based on semantic parsing , as proposed here , alleviates this issue .",3,0.6567623,95.09653561068102,14
15,Extensive automatic and human evaluation shows that the proposed method compares favorably to the state-of-the-art in combined lexical and structural simplification .,3,0.9249273,12.03837059649728,28
16,Words play a central role in language and thought .,0,0.9437655,31.930983042253114,10
16,"Factor analysis studies have shown that the primary dimensions of meaning are valence , arousal , and dominance ( VAD ) .",0,0.9278923,68.20777070962475,22
16,"We present the NRC VAD Lexicon , which has human ratings of valence , arousal , and dominance for more than 20,000 English words .",2,0.34787175,77.44643118262705,25
16,We use Best –Worst Scaling to obtain fine-grained scores and address issues of annotation consistency that plague traditional rating scale methods of annotation .,2,0.76973766,71.23895918451919,26
16,We show that the ratings obtained are vastly more reliable than those in existing lexicons .,3,0.9746422,45.08675419406462,16
16,"We also show that there exist statistically significant differences in the shared understanding of valence , arousal , and dominance across demographic variables such as age , gender , and personality .",3,0.9752508,37.126323214614736,32
17,Semantic relations are often signaled with prepositional or possessive marking — but extreme polysemy bedevils their analysis and automatic interpretation .,0,0.8829106,116.63694834427714,21
17,"We introduce a new annotation scheme , corpus , and task for the disambiguation of prepositions and possessives in English .",2,0.3704576,50.43097953138473,21
17,"Unlike previous approaches , our annotations are comprehensive with respect to types and tokens of these markers ;",3,0.84400856,167.9522374197172,18
17,use broadly applicable supersense classes rather than fine-grained dictionary definitions ;,3,0.49042535,248.416476409125,11
17,unite prepositions and possessives under the same class inventory ;,3,0.47615173,512.0986510918697,10
17,and distinguish between a marker ’s lexical contribution and the role it marks in the context of a predicate or scene .,3,0.3822556,74.98605980698359,22
17,"Strong interannotator agreement rates , as well as encouraging disambiguation results with established supervised methods , speak to the viability of the scheme and task .",3,0.94992876,101.37184550865373,26
18,"We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials .",2,0.54621935,35.02127200429048,18
18,"Annotations include demarcations of text spans that describe the Patient population enrolled , the Interventions studied and to what they were Compared , and the Outcomes measured ( the ‘ PICO ’ elements ) .",2,0.47200826,161.3574319943991,35
18,"These spans are further annotated at a more granular level , e.g. , individual interventions within them are marked and mapped onto a structured medical vocabulary .",2,0.48574904,76.69149541579486,27
18,We acquired annotations from a diverse set of workers with varying levels of expertise and cost .,2,0.86355263,31.282243083606907,17
18,We describe our data collection process and the corpus itself in detail .,3,0.36348137,30.268984870830728,13
18,We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine .,1,0.49434695,31.289202421606205,26
19,We describe a novel method for efficiently eliciting scalar annotations for dataset construction and system quality estimation by human judgments .,1,0.37758955,71.67284124822231,21
19,"We contrast direct assessment ( annotators assign scores to items directly ) , online pairwise ranking aggregation ( scores derive from annotator comparison of items ) , and a hybrid approach ( EASL : Efficient Annotation of Scalar Labels ) proposed here .",2,0.82537884,156.49115588398976,43
19,"Our proposal leads to increased correlation with ground truth , at far greater annotator efficiency , suggesting this strategy as an improved mechanism for dataset creation and manual system evaluation .",3,0.9672615,158.37176638071844,31
20,High quality arguments are essential elements for human reasoning and decision-making processes .,0,0.8524624,38.37572793482556,13
20,"However , effective argument construction is a challenging task for both human and machines .",0,0.9475134,54.57641561401032,15
20,"In this work , we study a novel task on automatically generating arguments of a different stance for a given statement .",1,0.8337307,47.25620207506354,22
20,We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia .,2,0.38894764,28.564106640075746,20
20,"Our model first generates a set of talking point phrases as intermediate representation , followed by a separate decoder producing the final argument based on both input and the keyphrases .",2,0.7552041,61.20849247927127,31
20,Experiments on a large-scale dataset collected from Reddit show that our model constructs arguments with more topic-relevant content than popular sequence-to-sequence generation models according to automatic evaluation and human assessments .,3,0.9125673,23.17106452774723,33
21,We report on a comparative style analysis of hyperpartisan ( extremely one-sided ) news and fake news .,1,0.76837903,71.16745557288888,18
21,"A corpus of 1,627 articles from 9 political publishers , three each from the mainstream , the hyperpartisan left , and the hyperpartisan right , have been fact-checked by professional journalists at BuzzFeed : 97 % of the 299 fake news articles identified are also hyperpartisan .",3,0.43055698,56.29304449381209,48
21,"We show how a style analysis can distinguish hyperpartisan news from the mainstream ( F1 = 0.78 ) , and satire from both ( F1 = 0.81 ) .",3,0.9132021,31.95712896256171,29
21,But stylometry is no silver bullet as style-based fake news detection does not work ( F1 = 0.46 ) .,3,0.9609919,86.37256920200647,22
21,We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream .,3,0.980471,36.82778805040555,20
21,"This result is robust : it has been confirmed by three different modeling approaches , one of which employs Unmasking in a novel way .",3,0.9590725,73.41734540333952,25
21,Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection .,3,0.9581537,43.858463585347614,15
22,This question implies the challenging retrieval task of finding the best counterargument .,0,0.88833547,101.89869073766155,13
22,"Since prior knowledge of a topic cannot be expected in general , we hypothesize the best counterargument to invoke the same aspects as the argument while having the opposite stance .",2,0.49155578,75.80451850563244,31
22,"To operationalize our hypothesis , we simultaneously model the similarity and dissimilarity of pairs of arguments , based on the words and embeddings of the arguments ’ premises and conclusions .",2,0.76683646,38.701156524675746,31
22,"A salient property of our model is its independence from the topic at hand , i.e. , it applies to arbitrary arguments .",2,0.42110726,51.545213000644786,23
22,We evaluate different model variations on millions of argument pairs derived from the web portal idebate.org .,2,0.810036,112.80700564402584,17
22,"For 7.6 candidates with opposing stance on average , we rank the best counterargument highest with 60 % accuracy .",3,0.89627934,126.52975103754947,20
22,"Even among all 2801 test set pairs as candidates , we still find the best one about every third time .",3,0.94955367,177.01217396538325,21
23,Knowledge graphs have emerged as an important model for studying complex multi-relational data .,0,0.9494316,28.73050668400924,14
23,This has given rise to the construction of numerous large scale but incomplete knowledge graphs encoding information extracted from various resources .,0,0.9485368,46.327326083592354,22
23,An effective and scalable approach to jointly learn over multiple graphs and eventually construct a unified graph is a crucial next step for the success of knowledge-based inference for many downstream applications .,0,0.6858103,34.23496433682635,35
23,"To this end , we propose LinkNBed , a deep relational learning framework that learns entity and relationship representations across multiple graphs .",1,0.44453767,68.78784971147472,23
23,We identify entity linkage across graphs as a vital component to achieve our goal .,3,0.8048907,119.80178722471749,15
23,We design a novel objective that leverage entity linkage and build an efficient multi-task training procedure .,2,0.5082609,75.35884724177801,17
23,Experiments on link prediction and entity linkage demonstrate substantial improvements over the state-of-the-art relational learning approaches .,3,0.9127603,17.561069806582303,23
24,"Embedding methods which enforce a partial order or lattice structure over the concept space , such as Order Embeddings ( OE ) , are a natural way to model transitive relational data ( e.g .",0,0.78533214,54.81795739115226,35
24,entailment graphs ) .,2,0.41320506,474.245464026914,4
24,"However , OE learns a deterministic knowledge base , limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning ( e.g .",0,0.6212771,85.44314474699527,27
24,learning from expectations ) .,0,0.3510617,1406.6291326969767,5
24,"Probabilistic extensions of OE have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models , but lack the ability to model the negative correlations found in real-world knowledge .",0,0.73064464,63.34864435399571,40
24,"In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation , which motivates our construction of a novel box lattice and accompanying probability measure to capture anti-correlation and even disjoint concepts , while still providing the benefits of probabilistic modeling , such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts , and both learning from and predicting calibrated uncertainty .",3,0.404929,69.63066577329319,79
24,"We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs , and investigate the power of the model .",3,0.6990196,76.62390847802493,23
25,Many NLP applications can be framed as a graph-to-sequence learning problem .,0,0.85113865,10.506469146170282,13
25,Previous work proposing neural architectures on graph-to-sequence obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and / or standard recurrent networks to achieve the best performance .,0,0.7886859,44.85623085285949,33
25,In this work propose a new model that encodes the full structural information contained in the graph .,1,0.598969,31.38187231922434,18
25,"Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations , while tackling the parameter explosion problem present in previous work .",2,0.48454764,66.12046148337772,37
25,Experimental results shows that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation .,3,0.97248286,17.655485761779676,22
26,We know very little about how neural language models ( LM ) use prior linguistic context .,0,0.92543435,40.03450116525085,17
26,"In this paper , we investigate the role of context in an LSTM LM , through ablation studies .",1,0.93702096,40.339655993260266,19
26,"Specifically , we analyze the increase in perplexity when prior context words are shuffled , replaced , or dropped .",2,0.6626666,127.25001026702284,20
26,"On two standard datasets , Penn Treebank and WikiText-2 , we find that the model is capable of using about 200 tokens of context on average , but sharply distinguishes nearby context ( recent 50 tokens ) from the distant history .",3,0.9116945,93.86595303733881,44
26,"The model is highly sensitive to the order of words within the most recent sentence , but ignores word order in the long-range context ( beyond 50 tokens ) , suggesting the distant past is modeled only as a rough semantic field or topic .",3,0.7916787,75.7078884965107,46
26,"We further find that the neural caching model ( Grave et al. , 2017 b ) especially helps the LSTM to copy words from within this distant context .",3,0.97413355,130.82341519615986,29
26,"Overall , our analysis not only provides a better understanding of how neural LMs use their context , but also sheds light on recent success from cache-based models .",3,0.9871961,31.51404294422291,31
27,Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances .,0,0.85568136,22.132863527290226,23
27,"In this paper we present SoPa , a new model that aims to bridge these two approaches .",1,0.8981201,31.17176538992028,18
27,So Pa combines neural representation learning with weighted finite-state automata ( WFSAs ) to learn a soft version of traditional surface patterns .,2,0.5648943,96.92226666895813,23
27,"We show that SoPa is an extension of a one-layer CNN , and that such CNNs are equivalent to a restricted version of SoPa , and accordingly , to a restricted form of WFSA .",3,0.6525647,62.44721810394416,36
27,"Empirically , on three text classification tasks , SoPa is comparable or better than both a BiLSTM ( RNN ) baseline and a CNN baseline , and is particularly useful in small data settings .",3,0.912884,55.49675330937525,35
28,Humans can efficiently learn new concepts using language .,0,0.9109707,60.79141497181614,9
28,We present a framework through which a set of explanations of a concept can be used to learn a classifier without access to any labeled examples .,2,0.3569155,22.268518869481632,27
28,"We use semantic parsing to map explanations to probabilistic assertions grounded in latent class labels and observed attributes of unlabeled data , and leverage the differential semantics of linguistic quantifiers ( e.g. , ‘usually ’ vs ‘ always ’ ) to drive model training .",2,0.8791311,55.87374847824575,45
28,"Experiments on three domains show that the learned classifiers outperform previous approaches for learning with limited data , and are comparable with fully supervised classifiers trained from a small number of labeled examples .",3,0.90176123,21.04017963521942,34
29,Bi-directional LSTMs are a powerful tool for text representation .,0,0.8686836,20.615641644252072,10
29,"On the other hand , they have been shown to suffer various limitations due to their sequential nature .",0,0.9105117,20.06028728217707,19
29,"We investigate an alternative LSTM structure for encoding text , which consists of a parallel state for each word .",2,0.503651,52.99687574261254,20
29,"Recurrent steps are used to perform local and global information exchange between words simultaneously , rather than incremental reading of a sequence of words .",0,0.44999745,60.992689520274254,25
29,"Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power , giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers .",3,0.956946,50.70407677555331,32
30,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task-specific modifications and training from scratch .",0,0.91989064,56.79090892899503,24
30,"We propose Universal Language Model Fine-tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine-tuning a language model .",1,0.6353605,24.697823120188318,38
30,"Our method significantly outperforms the state-of-the-art on six text classification tasks , reducing the error by 18-24 % on the majority of datasets .",3,0.91180307,13.50803692136026,31
30,"Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 times more data .",3,0.903875,78.92773268172796,22
30,We open-source our pretrained models and code .,2,0.63152385,21.118213156074386,8
31,The behavior of deep neural networks ( DNNs ) is hard to understand .,0,0.9594951,24.7219656571025,14
31,This makes it necessary to explore post hoc explanation methods .,0,0.62126094,72.9284119669636,11
31,We conduct the first comprehensive evaluation of explanation methods for NLP .,1,0.6093903,33.780295941653414,12
31,"To this end , we design two novel evaluation paradigms that cover two important classes of NLP problems : small context and large context problems .",2,0.6719596,30.780263339992402,26
31,Both paradigms require no manual annotation and are therefore broadly applicable .,0,0.67547977,33.63023264968466,12
31,"We also introduce LIMSSE , an explanation method inspired by LIME that is designed for NLP .",2,0.46494204,83.2988949901873,17
31,"We show empirically that LIMSSE , LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP .",3,0.9528899,70.93175894032065,24
32,"To be informative , an evaluation must measure how well systems generalize to realistic unseen data .",0,0.80414313,71.45976195921236,17
32,We identify limitations of and propose improvements to current evaluations of text-to-SQL systems .,3,0.74906296,35.43971013885941,18
32,"First , we compare human-generated and automatically generated questions , characterizing properties of queries necessary for real-world applications .",2,0.7965694,61.28749285558917,19
32,"To facilitate evaluation on multiple datasets , we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset .",2,0.7229749,37.212336377706755,27
32,"Second , we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked , but only partially tests how well systems generalize to new queries ;",3,0.81563675,57.23485177717367,38
32,"therefore , we propose a complementary dataset split for evaluation of future work .",3,0.81669486,143.2363604011254,14
32,"Finally , we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task .",3,0.91028297,47.24521826411233,21
32,"Our observations highlight key difficulties , and our methodology enables effective measurement of future development .",3,0.97964096,165.02551237414934,16
33,We present a generative model to map natural language questions into SQL queries .,2,0.42822093,16.776569560616494,14
33,"Existing neural network based approaches typically generate a SQL query word-by-word , however , a large portion of the generated results is incorrect or not executable due to the mismatch between question words and table contents .",0,0.8688965,44.52600661304694,39
33,Our approach addresses this problem by considering the structure of table and the syntax of SQL language .,2,0.63641363,55.88203496596516,18
33,"The quality of the generated SQL query is significantly improved through ( 1 ) learning to replicate content from column names , cells or SQL keywords ;",3,0.8242787,194.97930579470992,27
33,and ( 2 ) improving the generation of WHERE clause by leveraging the column-cell relation .,3,0.52876997,164.7967609914693,16
33,"Experiments are conducted on WikiSQL , a recently released dataset with the largest question-SQL pairs .",2,0.7704047,80.1834329015928,18
33,Our approach significantly improves the state-of-the-art execution accuracy from 69.0 % to 74.4 % .,3,0.9243658,10.46880494561807,19
34,"The ability to consolidate information of different types is at the core of intelligence , and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others .",0,0.8850489,60.44997583858488,34
34,"In this paper we tackle the challenging task of improving semantic parsing performance , taking UCCA parsing as a test case , and AMR , SDP and Universal Dependencies ( UD ) parsing as auxiliary tasks .",1,0.7656882,56.051096590778975,37
34,"We experiment on three languages , using a uniform transition-based system and learning architecture for all parsing tasks .",2,0.7785506,73.65385529713717,21
34,"Despite notable conceptual , formal and domain differences , we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings .",3,0.97511804,35.79274428289749,28
35,Character-level models have become a popular approach specially for their accessibility and ability to handle unseen data .,0,0.9375776,39.6837230503488,20
35,"However , little is known on their ability to reveal the underlying morphological structure of a word , which is a crucial skill for high-level semantic analysis tasks , such as semantic role labeling ( SRL ) .",0,0.95101637,26.54499604638009,40
35,"In this work , we train various types of SRL models that use word , character and morphology level information and analyze how performance of characters compare to words and morphology for several languages .",1,0.5676643,83.3422010525513,35
35,"We conduct an in-depth error analysis for each morphological typology and analyze the strengths and limitations of character-level models that relate to out-of-domain data , training data size , long range dependencies and model complexity .",2,0.8385605,35.583888911649076,42
35,Our exhaustive analyses shed light on important characteristics of character-level models and their semantic capability .,3,0.92855555,52.47660447235211,18
36,Abstract meaning representations ( AMRs ) are broad-coverage sentence-level semantic representations .,0,0.8905502,32.48991108904015,14
36,AMRs represent sentences as rooted labeled directed acyclic graphs .,0,0.4862057,131.62961192530886,10
36,AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences .,0,0.8667603,24.398331563943348,24
36,"We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts , relations and alignments .",2,0.645292,62.03302783516266,23
36,"As exact inference requires marginalizing over alignments and is infeasible , we use the variational autoencoding framework and a continuous relaxation of the discrete alignments .",2,0.69878846,51.98053501260949,26
36,We show that joint modeling is preferable to using a pipeline of align and parse .,3,0.9482951,92.35818594022054,16
36,The parser achieves the best reported results on the standard benchmark ( 74.4 % on LDC2016E25 ) .,3,0.89916056,69.48711358076136,18
37,"We demonstrate that an SHRG-based parser can produce semantic graphs much more accurately than previously shown , by relating synchronous production rules to the syntacto-semantic composition process .",3,0.9180238,82.79656742475224,30
37,"Our parser achieves an accuracy of 90.35 for EDS ( 89.51 for DMRS ) in terms of elementary dependency match , which is a 4.87 ( 5.45 ) point improvement over the best existing data-driven model , indicating , in our view , the importance of linguistically-informed derivation for data-driven semantic parsing .",3,0.9436221,50.56440216465656,55
37,"This accuracy is equivalent to that of English Resource Grammar guided models , suggesting that ( recurrent ) neural network models are able to effectively learn deep linguistic knowledge from annotations .",3,0.9551369,94.39640544726704,32
38,"To solve math word problems , previous statistical approaches attempt at learning a direct mapping from a problem description to its corresponding equation system .",0,0.90191686,160.66851330872498,25
38,"However , such mappings do not include the information of a few higher-order operations that cannot be explicitly represented in equations but are required to solve the problem .",0,0.5351431,31.45528185639299,31
38,The gap between natural language and equations makes it difficult for a learned model to generalize from limited data .,0,0.90180093,29.292213215031495,20
38,In this work we present an intermediate meaning representation scheme that tries to reduce this gap .,1,0.80254537,63.40681950076333,17
38,"We use a sequence-to-sequence model with a novel attention regularization term to generate the intermediate forms , then execute them to obtain the final answers .",2,0.8972254,35.26337018123701,27
38,"Since the intermediate forms are latent , we propose an iterative labeling framework for learning by leveraging supervision signals from both equations and answers .",2,0.6966047,83.65379247002194,25
38,Our experiments show using intermediate forms outperforms directly predicting equations .,3,0.9782324,381.65630309177055,11
39,We introduce an open-domain neural semantic parser which generates formal meaning representations in the style of Discourse Representation Theory ( DRT ; Kamp and Reyle 1993 ) .,2,0.68631536,55.36876768549356,28
39,"We propose a method which transforms Discourse Representation Structures ( DRSs ) to trees and develop a structure-aware model which decomposes the decoding process into three stages : basic DRS structure prediction , condition prediction ( i.e. , predicates and relations ) , and referent prediction ( i.e. , variables ) .",2,0.629096,36.572987492530956,54
39,Experimental results on the Groningen Meaning Bank ( GMB ) show that our model outperforms competitive baselines by a wide margin .,3,0.8835791,15.748826309100037,22
40,"Many deep learning architectures have been proposed to model the compositionality in text sequences , requiring substantial number of parameters and expensive computations .",0,0.9092167,28.662465259428295,24
40,"However , there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions .",0,0.9384328,50.21313738438362,18
40,"In this paper , we conduct a point-by-point comparative study between Simple Word-Embedding-based Models ( SWEMs ) , consisting of parameter-free pooling operations , relative to word-embedding-based RNN / CNN models .",1,0.8930855,44.725424297817774,38
40,"Surprisingly , SWEMs exhibit comparable or even superior performance in the majority of cases considered .",3,0.880843,84.88144935129945,16
40,"Based upon this understanding , we propose two additional pooling strategies over learned word embeddings : ( i ) a max-pooling operation for improved interpretability ;",3,0.43595484,71.52913756663199,26
40,"and ( ii ) a hierarchical pooling operation , which preserves spatial ( n-gram ) information within text sequences .",2,0.54863614,146.17766275241104,20
40,We present experiments on 17 datasets encompassing three tasks : ( i ) ( long ) document classification ;,2,0.77698964,331.01670624092975,19
40,( ii ) text sequence matching ;,2,0.48268834,3286.1758768624386,7
40,"and ( iii ) short text tasks , including classification and tagging .",2,0.38078195,333.67447170960145,13
41,"We describe ParaNMT-50M , a dataset of more than 50 million English-English sentential paraphrase pairs .",2,0.38345248,50.81697864623133,19
41,"We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus , following Wieting et al .",2,0.8815518,64.6065557250259,26
41,( 2017 ) .,4,0.79745436,169.31164109005934,4
41,Our hope is that ParaNMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks .,3,0.9592793,24.57513742548451,32
41,"To show its utility , we use ParaNMT-50 M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition , in addition to showing how it can be used for paraphrase generation .",2,0.5684228,53.270746525401556,42
42,"We investigate a new commonsense inference task : given an event described in a short free-form text ( “ X drinks coffee in the morning ” ) , a system reasons about the likely intents ( “ X wants to stay awake ” ) and reactions ( “ X feels alert ” ) of the event ’s participants .",2,0.6641719,41.618794854516615,59
42,"To support this study , we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations .",2,0.567961,27.46733117326348,25
42,"We report baseline performance on this task , demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants .",3,0.8609507,35.27253548455587,35
42,"In addition , we demonstrate how commonsense inference on people ’s intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts .",3,0.7032732,91.51373121360493,27
43,"Japanese predicate-argument structure ( PAS ) analysis involves zero anaphora resolution , which is notoriously difficult .",0,0.9407242,185.48461047346652,19
43,"To improve the performance of Japanese PAS analysis , it is straightforward to increase the size of corpora annotated with PAS .",0,0.6069947,33.687498314295894,22
43,"However , since it is prohibitively expensive , it is promising to take advantage of a large amount of raw corpora .",0,0.7322874,20.001521846033935,22
43,"In this paper , we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus .",1,0.8911947,27.566848585788676,22
43,"In our experiments , our model outperforms existing state-of-the-art models for Japanese PAS analysis .",3,0.9237021,19.45470812392584,20
44,This paper proposes a novel approach for event coreference resolution that models correlations between event coreference chains and document topical structures through an Integer Linear Programming formulation .,1,0.84228057,66.50467864296051,28
44,"We explicitly model correlations between the main event chains of a document with topic transition sentences , inter-coreference chain correlations , event mention distributional characteristics and sub-event structure , and use them with scores obtained from a local coreference relation classifier for jointly resolving multiple event chains in a document .",2,0.8131317,102.03891804856882,51
44,Our experiments across KBP 2016 and 2017 datasets suggest that each of the structures contribute to improving event coreference resolution performance .,3,0.9770328,64.71178495956423,22
45,"Distant supervision can effectively label data for relation extraction , but suffers from the noise labeling problem .",0,0.6121556,129.4906088905758,18
45,"Recent works mainly perform soft bag-level noise reduction strategies to find the relatively better samples in a sentence bag , which is suboptimal compared with making a hard decision of false positive samples in sentence level .",0,0.7978188,110.01370133414451,37
45,"In this paper , we introduce an adversarial learning framework , which we named DSGAN , to learn a sentence-level true-positive generator .",1,0.70811,35.292438276949134,25
45,"Inspired by Generative Adversarial Networks , we regard the positive samples generated by the generator as the negative samples to train the discriminator .",2,0.7285347,26.092419727288483,24
45,The optimal generator is obtained until the discrimination ability of the discriminator has the greatest decline .,3,0.57373244,119.16601180700248,17
45,"We adopt the generator to filter distant supervision training dataset and redistribute the false positive instances into the negative set , in which way to provide a cleaned dataset for relation classification .",2,0.7747717,171.45608397473646,33
45,The experimental results show that the proposed strategy significantly improves the performance of distant supervision relation extraction comparing to state-of-the-art systems .,3,0.9818987,11.58650181672948,28
46,The relational facts in sentences are often complicated .,0,0.9001311,90.20464454371523,9
46,Different relational triplets may have overlaps in a sentence .,0,0.60084873,106.92567025760667,10
46,"We divided the sentences into three types according to triplet overlap degree , including Normal , EntityPairOverlap and SingleEntiyOverlap .",2,0.92196983,138.4309000450539,20
46,Existing methods mainly focus on Normal class and fail to extract relational triplets precisely .,0,0.80662936,139.20144740121657,15
46,"In this paper , we propose an end-to-end model based on sequence-to-sequence learning with copy mechanism , which can jointly extract relational facts from sentences of any of these classes .",1,0.8175953,24.329595823479938,35
46,We adopt two different strategies in decoding process : employing only one united decoder or applying multiple separated decoders .,2,0.78815657,115.39258876280834,20
46,We test our models in two public datasets and our model outperform the baseline method significantly .,3,0.5961335,35.64083568113116,17
47,"Due to the ability of encoding and mapping semantic information into a high-dimensional latent feature space , neural networks have been successfully used for detecting events to a certain extent .",0,0.93301433,35.83953073173325,31
47,"However , such a feature space can be easily contaminated by spurious features inherent in event detection .",0,0.7891899,79.66273377004397,18
47,"In this paper , we propose a self-regulated learning approach by utilizing a generative adversarial network to generate spurious features .",1,0.82491964,22.142886602401138,21
47,"On the basis , we employ a recurrent network to eliminate the fakes .",2,0.7929593,50.9274484497842,14
47,Detailed experiments on the ACE 2005 and TAC-KBP 2015 corpora show that our proposed method is highly effective and adaptable .,3,0.9193524,24.93906457256689,23
48,We propose a context-aware neural network model for temporal information extraction .,1,0.51971465,18.10195485734175,13
48,"This model has a uniform architecture for event-event , event-timex and timex-timex pairs .",2,0.57279044,51.500928976530325,16
48,"A Global Context Layer ( GCL ) , inspired by Neural Turing Machine ( NTM ) , stores processed temporal relations in narrative order , and retrieves them for use when relevant entities come in .",0,0.5953837,129.17269948694437,36
48,Relations are then classified in context .,2,0.46131748,154.2494166437061,7
48,The GCL model has long-term memory and attention mechanisms to resolve irregular long-distance dependencies that regular RNNs such as LSTM cannot recognize .,3,0.4066064,40.62401088811444,23
48,"It does not require any new input features , while outperforming the existing models in literature .",3,0.58905005,64.94077770436705,17
48,To our knowledge it is also the first model to use NTM-like architecture to process the information from global context in discourse-scale natural text processing .,3,0.9533469,56.497768690675215,28
48,We are going to release the source code in the future .,3,0.8147486,7.916673912489669,12
49,"Inspired by the double temporality characteristic of narrative texts , we propose a novel approach for acquiring rich temporal “ before / after ” event knowledge across sentences in narrative stories .",2,0.37741053,60.15102610842251,32
49,"The double temporality states that a narrative story often describes a sequence of events following the chronological order and therefore , the temporal order of events matches with their textual order .",0,0.92400867,74.67409461919834,32
49,We explored narratology principles and built a weakly supervised approach that identifies 287 k narrative paragraphs from three large corpora .,2,0.8221936,100.73535725800438,21
49,We then extracted rich temporal event knowledge from these narrative paragraphs .,2,0.8976526,230.7784651989571,12
49,Such event knowledge is shown useful to improve temporal relation classification and outperforms several recent neural network models on the narrative cloze task .,3,0.7785017,65.04348073753665,24
50,"In this paper , we propose a new strategy , called Text Deconvolution Saliency ( TDS ) , to visualize linguistic information detected by a CNN for text classification .",1,0.9027905,53.19643559680664,30
50,We extend Deconvolution Networks to text in order to present a new perspective on text analysis to the linguistic community .,2,0.48710877,54.05679729061209,21
50,"We empirically demonstrated the efficiency of our Text Deconvolution Saliency on corpora from three different languages : English , French , and Latin .",3,0.58181137,51.633058211070896,24
50,"For every tested dataset , our Text Deconvolution Saliency automatically encodes complex linguistic patterns based on co-occurrences and possibly on grammatical and syntax analysis .",3,0.88258606,78.89574883948464,25
51,"We propose a novel coherence model for written asynchronous conversations ( e.g. , forums , emails ) , and show its applications in coherence assessment and thread reconstruction tasks .",1,0.3795073,83.70259123412427,30
51,We conduct our research in two steps .,2,0.5467106,54.80076046986071,8
51,"First , we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions .",2,0.52462465,73.199149610607,19
51,"Then , we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation .",2,0.68284726,82.31673563729686,24
51,Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models .,3,0.9091152,38.212419538766824,21
51,We also demonstrate its effectiveness in reconstructing thread structures .,3,0.8435926,84.86408751127543,10
52,"Recent neural network models for Chinese zero pronoun resolution gain great performance by capturing semantic information for zero pronouns and candidate antecedents , but tend to be short-sighted , operating solely by making local decisions .",0,0.8973232,116.42629539293915,36
52,They typically predict coreference links between the zero pronoun and one single candidate antecedent at a time while ignoring their influence on future decisions .,0,0.787771,79.15670088774544,25
52,"Ideally , modeling useful information of preceding potential antecedents is crucial for classifying later zero pronoun-candidate antecedent pairs , a need which leads traditional models of zero pronoun resolution to draw on reinforcement learning .",0,0.65735775,131.22113895456397,35
52,"In this paper , we show how to integrate these goals , applying deep reinforcement learning to deal with the task .",1,0.8782384,51.82944734946544,22
52,"With the help of the reinforcement learning agent , our system learns the policy of selecting antecedents in a sequential manner , where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions .",2,0.47098514,46.62175332077731,39
52,Experimental results on OntoNotes 5.0 show that our approach substantially outperforms the state-of-the-art methods under three experimental settings .,3,0.9298677,7.337773330798641,25
53,Predicate argument structure analysis is a task of identifying structured events .,0,0.86923224,129.31555410855333,12
53,"To improve this field , we need to identify a salient entity , which cannot be identified without performing coreference resolution and predicate argument structure analysis simultaneously .",0,0.6987285,109.16870726875987,28
53,This paper presents an entity-centric joint model for Japanese coreference resolution and predicate argument structure analysis .,1,0.8490167,82.04211897115951,18
53,"Each entity is assigned an embedding , and when the result of both analyses refers to an entity , the entity embedding is updated .",2,0.50626945,40.30666146710274,25
53,The analyses take the entity embedding into consideration to access the global information of entities .,2,0.72086674,54.48356276198058,16
53,"Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically , which is a notoriously difficult task in predicate argument structure analysis .",3,0.9820446,64.17302472519208,31
54,This paper reports on two strategies that have been implemented for improving the efficiency and precision of wide-coverage Minimalist Grammar ( MG ) parsing .,1,0.850809,32.03534390568392,26
54,The first extends the formalism presented in Torr and Stabler ( 2016 ) with a mechanism for enforcing fine-grained selectional restrictions and agreements .,2,0.52708447,85.80779161120171,25
54,The second is a method for factoring computationally costly null heads out from bottom-up MG parsing ;,2,0.6489825,317.39380185331913,19
54,this has the additional benefit of rendering the formalism fully compatible for the first time with highly efficient Markovian supertaggers .,3,0.7366617,81.26900347374858,21
54,"These techniques aided in the task of generating MGbank , the first wide-coverage corpus of Minimalist Grammar derivation trees .",3,0.43970945,70.77512917153136,21
55,Linguistic alignment between dialogue partners has been claimed to be affected by their relative social power .,0,0.8869576,59.21129897817663,17
55,A common finding has been that interlocutors of higher power tend to receive more alignment than those of lower power .,0,0.7351413,21.371305343738918,21
55,"However , these studies overlook some low-level linguistic features that can also affect alignment , which casts doubts on these findings .",0,0.7383913,66.67293249054275,22
55,"This work characterizes the effect of power on alignment with logistic regression models in two datasets , finding that the effect vanishes or is reversed after controlling for low-level features such as utterance length .",1,0.50907016,37.05624047355385,35
55,"Thus , linguistic alignment is explained better by low-level features than by social power .",3,0.89882576,111.53657987527615,15
55,"We argue that a wider range of factors , especially cognitive factors , need to be taken into account for future studies on observational data when social factors of language use are in question .",3,0.9714575,48.84064803622,35
56,"The field of Natural Language Processing ( NLP ) is growing rapidly , with new research published daily along with an abundance of tutorials , codebases and other online resources .",0,0.9740857,25.082869795780848,31
56,"In order to learn this dynamic field or stay up-to-date on the latest research , students as well as educators and researchers must constantly sift through multiple sources to find valuable , relevant information .",0,0.8292024,43.53308331110594,39
56,"To address this situation , we introduce TutorialBank , a new , publicly available dataset which aims to facilitate NLP education and research .",1,0.7342712,63.238002271696665,24
56,"We have manually collected and categorized over 5,600 resources on NLP as well as the related fields of Artificial Intelligence ( AI ) , Machine Learning ( ML ) and Information Retrieval ( IR ) .",2,0.829152,13.147789591224237,36
56,Our dataset is notably the largest manually-picked corpus of resources intended for NLP education which does not include only academic papers .,3,0.59449756,95.18436515229631,24
56,"Additionally , we have created both a search engine and a command-line tool for the resources and have annotated the corpus to include lists of research topics , relevant resources for each topic , prerequisite relations among topics , relevant sub-parts of individual resources , among other annotations .",2,0.6800394,57.68647341733391,50
56,We are releasing the dataset and present several avenues for further research .,3,0.8091799,23.324424778724197,13
57,"While argument persuasiveness is one of the most important dimensions of argumentative essay quality , it is relatively little studied in automated essay scoring research .",0,0.92582494,27.585402277421984,26
57,Progress on scoring argument persuasiveness is hindered in part by the scarcity of annotated corpora .,0,0.8833838,34.19551443540275,16
57,"We present the first corpus of essays that are simultaneously annotated with argument components , argument persuasiveness scores , and attributes of argument components that impact an argument ’s persuasiveness .",1,0.34916243,43.070184064581746,31
57,This corpus could trigger the development of novel computational models concerning argument persuasiveness that provide useful feedback to students on why their arguments are ( un ) persuasive in addition to how persuasive they are .,3,0.9187256,58.291033612789896,36
58,"The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality ( henceforth , low coverage bias or LCB ) .",0,0.8756821,76.39960716357936,33
58,"This paper shows that overcoming LCB in Grammatical Error Correction ( GEC ) evaluation cannot be attained by re-scaling or by increasing the number of references in any feasible range , contrary to previous suggestions .",3,0.46320426,65.16029551266313,36
58,This is due to the long-tailed distribution of valid corrections for a sentence .,0,0.6563732,34.91577384453463,14
58,"Concretely , we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction .",3,0.9519567,108.33309620729207,21
58,"Consequently , existing systems obtain comparable or superior performance compared to humans , by making few but targeted changes to the input .",0,0.8596805,86.46046288488235,23
58,Similar effects on Text Simplification further support our claims .,3,0.9701985,196.0700408588312,10
59,"For evaluating generation systems , automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment , leading to systematic bias against certain model improvements .",0,0.8688524,54.381091046458295,34
59,"On the other hand , averaging human judgments , the unbiased gold standard , is often too expensive .",0,0.8662035,126.23512124892802,19
59,"In this paper , we use control variates to combine automatic metrics with human evaluation to obtain an unbiased estimator with lower cost than human evaluation alone .",1,0.6308269,40.95008740702324,28
59,"In practice , however , we obtain only a 7-13 % cost reduction on evaluating summarization and open-response question answering systems .",3,0.80543053,130.16079872336215,24
59,We then prove that our estimator is optimal : there is no unbiased estimator with lower cost .,2,0.5409513,87.20687559927805,18
59,Our theory further highlights the two fundamental bottlenecks — the automatic metric and the prompt shown to human evaluators — both of which need to be improved to obtain greater cost savings .,3,0.929529,43.11606864617627,33
60,Sentence scoring and sentence selection are two main steps in extractive document summarization systems .,0,0.7846617,48.36941179319205,15
60,"However , previous works treat them as two separated subtasks .",0,0.9321902,62.23609843598568,11
60,"In this paper , we present a novel end-to-end neural network framework for extractive document summarization by jointly learning to score and select sentences .",1,0.88247174,15.149546684081672,27
60,It first reads the document sentences with a hierarchical encoder to obtain the representation of sentences .,2,0.6892585,39.61963645135232,17
60,Then it builds the output summary by extracting sentences one by one .,2,0.7274063,61.73599248278158,13
60,"Different from previous methods , our approach integrates the selection strategy into the scoring model , which directly predicts the relative importance given previously selected sentences .",2,0.7365427,61.958265980976634,27
60,Experiments on the CNN / Daily Mail dataset show that the proposed framework significantly outperforms the state-of-the-art extractive summarization models .,3,0.9274105,6.380525012494994,27
61,We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations .,2,0.39118633,19.17942081295069,24
61,Our work combines the strengths of multiple recent approaches while addressing their weaknesses .,3,0.50076777,48.898427350545084,14
61,"Moreover , we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account , and to design custom diversity and informativeness measures .",2,0.5266481,57.127368928699696,32
61,Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art .,3,0.94518954,12.781370143546622,21
61,"Code and data are publicly available , and our system can be interactively tested .",3,0.79286176,48.02761240623454,15
62,"Inspired by how humans summarize long documents , we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively ( i.e. , compresses and paraphrases ) to generate a concise overall summary .",0,0.55024636,32.80903127310647,40
62,"We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way , while maintaining language fluency .",2,0.81604856,41.36792441174241,30
62,"Empirically , we achieve the new state-of-the-art on all metrics ( including human evaluation ) on the CNN / Daily Mail dataset , as well as significantly higher abstractiveness scores .",3,0.9187513,26.396332209604328,37
62,"Moreover , by first operating at the sentence-level and then the word-level , we enable parallel decoding of our neural generative model that results in substantially faster ( 10-20x ) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models .",3,0.75776106,32.45163644635337,50
62,"We also demonstrate the generalization of our model on the test-only DUC-2002 dataset , where we achieve higher scores than a state-of-the-art model .",3,0.8235028,20.498831013650197,32
63,An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document .,0,0.8419929,27.218656149498656,23
63,"We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation , where the former teaches the summarization model how to look for salient questioning-worthy details , and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document .",2,0.6477653,35.055096351958106,61
63,"We also propose novel multi-task architectures with high-level ( semantic ) layer-specific sharing across multiple encoder and decoder layers of the three tasks , as well as soft-sharing mechanisms ( and show performance ablations and analysis examples of each contribution ) .",3,0.38524985,73.37597746648066,44
63,"Overall , we achieve statistically significant improvements over the state-of-the-art on both the CNN / DailyMail and Gigaword datasets , as well as on the DUC-2002 transfer setup .",3,0.9436755,18.65571822149652,35
63,We also present several quantitative and qualitative analysis studies of our model ’s learned saliency and entailment skills .,3,0.6402702,44.51455360869277,19
64,"As the amount of free-form user-generated reviews in e-commerce websites continues to increase , there is an increasing need for automatic mechanisms that sift through the vast amounts of user reviews and identify quality content .",0,0.94448036,15.270135321260119,37
64,Review helpfulness modeling is a task which studies the mechanisms that affect review helpfulness and attempts to accurately predict it .,0,0.9303716,100.02590200676839,21
64,"This paper provides an overview of the most relevant work in helpfulness prediction and understanding in the past decade , discusses the insights gained from said work , and provides guidelines for future research .",1,0.83806413,42.84211565225813,35
65,"Cross-cultural differences and similarities are common in cross-lingual natural language understanding , especially for research in social media .",0,0.8785644,26.456426732742475,19
65,"For instance , people of distinct cultures often hold different opinions on a single named entity .",0,0.89752144,99.13932122471209,17
65,"Also , understanding slang terms across languages requires knowledge of cross-cultural similarities .",0,0.81916577,83.37543086575465,13
65,"In this paper , we study the problem of computing such cross-cultural differences and similarities .",1,0.92960966,39.84284977834418,16
65,"We present a lightweight yet effective approach , and evaluate it on two novel tasks : 1 ) mining cross-cultural differences of named entities and 2 ) finding similar terms for slang across languages .",2,0.38108796,66.04713490398862,35
65,Experimental results show that our framework substantially outperforms a number of baseline methods on both tasks .,3,0.96463126,10.484403810710988,17
65,The framework could be useful for machine translation applications and research in computational social science .,3,0.8737594,35.27152634365683,16
66,"Previous works in computer science , as well as political and social science , have shown correlation in text between political ideologies and the moral foundations expressed within that text .",0,0.92213434,70.70948561288265,31
66,"Additional work has shown that policy frames , which are used by politicians to bias the public towards their stance on an issue , are also correlated with political ideology .",0,0.6838082,38.736475418975296,31
66,"Based on these associations , this work takes a first step towards modeling both the language and how politicians frame issues on Twitter , in order to predict the moral foundations that are used by politicians to express their stances on issues .",1,0.45534334,33.239823340904,43
66,"The contributions of this work includes a dataset annotated for the moral foundations , annotation guidelines , and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans , as opposed to the unigrams of previous works , with policy frames for the prediction of the morality underlying political tweets .",3,0.54923254,76.96016457422978,54
67,Semantic parsing aims at mapping natural language utterances into structured meaning representations .,0,0.8973181,21.896944425074867,13
67,"In this work , we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages .",1,0.7899671,15.043081554031472,22
67,"Given an input utterance , we first generate a rough sketch of its meaning , where low-level information ( such as variable names and arguments ) is glossed over .",2,0.75515914,35.914547115076964,30
67,"Then , we fill in missing details by taking into account the natural language input and the sketch itself .",2,0.779137,37.58114695535111,20
67,"Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance , achieving competitive results despite the use of relatively simple decoders .",3,0.8923647,34.792198375403885,31
68,In this work we focus on confidence modeling for neural semantic parsers which are built upon sequence-to-sequence models .,1,0.7528157,18.602167260964915,21
68,"We outline three major causes of uncertainty , and design various metrics to quantify these factors .",1,0.43214476,73.55032149273843,17
68,These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct .,2,0.653946,15.626633082774736,20
68,"Beyond confidence estimation , we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model , and verify or refine its input .",3,0.4678076,129.76913518656582,29
68,"Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability , and improves the quality of interpretation compared to simply relying on attention scores .",3,0.9705199,29.230290480181413,33
69,"Semantic parsing is the task of transducing natural language ( NL ) utterances into formal meaning representations ( MRs ) , commonly represented as tree structures .",0,0.920655,30.792154146558666,27
69,"Annotating NL utterances with their corresponding MRs is expensive and time-consuming , and thus the limited availability of labeled data often becomes the bottleneck of data-driven , supervised models .",0,0.9275748,48.05152736801433,32
69,"We introduce StructVAE , a variational auto-encoding model for semi-supervised semantic parsing , which learns both from limited amounts of parallel data , and readily-available unlabeled NL utterances .",2,0.5629945,47.69291289405683,31
69,StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables .,2,0.54510516,60.50119035673256,15
69,"Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data , StructVAE outperforms strong supervised models .",3,0.85621196,60.1929741697173,25
70,"This paper proposes a neural semantic parsing approach – Sequence-to-Action , which models semantic parsing as an end-to-end semantic graph generation process .",1,0.8126795,22.9219038344753,27
70,Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing .,3,0.5963629,65.38548132624534,15
70,"Firstly , our model uses a semantic graph to represent the meaning of a sentence , which has a tight-coupling with knowledge bases .",2,0.6656056,28.85842454485435,26
70,"Secondly , by leveraging the powerful representation learning and prediction ability of neural network models , we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation .",2,0.5941922,38.78135820236745,34
70,Experiments show that our method achieves state-of-the-art performance on Overnight dataset and gets competitive performance on Geo and Atis datasets .,3,0.9361645,14.635111211651036,26
71,"Unsupervised machine translation-i.e. , not assuming any cross-lingual supervision signal , whether a dictionary , translations , or comparable corpora-seems impossible , but nevertheless , Lample et al .",0,0.6031177,178.7865278869452,31
71,( 2017 ) recently proposed a fully unsupervised machine translation ( MT ) model .,0,0.83069825,73.47691836919476,15
71,"The model relies heavily on an adversarial , unsupervised cross-lingual word embedding technique for bilingual dictionary induction ( Conneau et al. , 2017 ) , which we examine here .",2,0.659455,51.31771247319088,30
71,"Our results identify the limitations of current unsupervised MT : unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking , when monolingual corpora from different domains or different embedding algorithms are used .",3,0.98909086,52.49998096799492,40
71,"We show that a simple trick , exploiting a weak supervision signal from identical words , enables more robust induction and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric .",3,0.8827172,73.28646182861256,39
72,Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training .,0,0.9352797,14.380536686011546,24
72,"However , their evaluation has focused on favorable conditions , using comparable corpora or closely-related languages , and we show that they often fail in more realistic scenarios .",3,0.55141264,83.38163311098255,31
72,"This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings , and a robust self-learning algorithm that iteratively improves this solution .",1,0.43964842,26.955819966943142,33
72,"Our method succeeds in all tested scenarios and obtains the best published results in standard datasets , even surpassing previous supervised systems .",3,0.9164427,65.52837115460787,23
72,Our implementation is released as an open source project at https://github.com/artetxem/vecmap .,3,0.6826135,17.89151285053254,12
73,We propose a multi-lingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling .,2,0.35791972,18.09034465042368,21
73,"In this new architecture , we combine various transfer models using two layers of parameter sharing .",2,0.7149293,113.89795604374099,17
73,"On the first layer , we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models .",2,0.68646646,51.698787668836495,25
73,"On the second level , we adopt different parameter sharing strategies for different transfer schemes .",2,0.72176504,68.76292576980963,16
73,"This architecture proves to be particularly effective for low-resource settings , when there are less than 200 training sentences for the target task .",3,0.91519594,23.329964168712774,24
73,"Using Name Tagging as a target task , our approach achieved 4.3 %-50.5 % absolute F-score gains compared to the mono-lingual single-task baseline model .",3,0.8848416,44.70816362989085,28
74,"Bilingual tasks , such as bilingual lexicon induction and cross-lingual classification , are crucial for overcoming data sparsity in the target language .",0,0.86910737,30.77288894393654,23
74,"Resources required for such tasks are often out-of-domain , thus domain adaptation is an important problem here .",0,0.9058342,28.774153031305374,21
74,We make two contributions .,3,0.3744423,32.25611623421947,5
74,"First , we test a delightfully simple method for domain adaptation of bilingual word embeddings .",2,0.7750589,23.687963740565035,16
74,We evaluate these embeddings on two bilingual tasks involving different domains : cross-lingual twitter sentiment classification and medical bilingual lexicon induction .,2,0.7240729,67.10491856427117,22
74,"Second , we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks .",2,0.7181656,49.324365496475934,17
74,We show that this method also helps in low-resource setups .,3,0.95108265,22.846485377509463,11
74,"Using both methods together we achieve large improvements over our baselines , by using only additional unlabeled data .",3,0.8343635,42.89141805392101,19
75,"We introduce a neural reading comprehension model that integrates external commonsense knowledge , encoded as a key-value memory , in a cloze-style setting .",2,0.4707768,39.96192980744041,24
75,"Instead of relying only on document-to-question interaction or discrete features as in prior work , our model attends to relevant external knowledge and combines this knowledge with the context representation before inferring the answer .",2,0.5726,42.875427397934644,36
75,"This allows the model to attract and imply knowledge from an external knowledge source that is not explicitly stated in the text , but that is relevant for inferring the answer .",3,0.5123135,28.114972972553495,32
75,"Our model improves results over a very strong baseline on a hard Common Nouns dataset , making it a strong competitor of much more complex models .",3,0.933856,53.69293206643217,27
75,"By including knowledge explicitly , our model can also provide evidence about the background knowledge used in the RC process .",3,0.8480378,110.45465806994676,21
76,"Question Answering ( QA ) , as a research field , has primarily focused on either knowledge bases ( KBs ) or free text as a source of knowledge .",0,0.96604085,34.47588439312189,30
76,"These two sources have historically shaped the kinds of questions that are asked over these sources , and the methods developed to answer them .",0,0.8893668,34.73691398475066,25
76,"In this work , we look towards a practical use-case of QA over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases , and unstructured QA over narrative , introducing the task of multi-relational QA over personal narrative .",1,0.8884855,40.43135926736856,43
76,"As a first step towards this goal , we make three key contributions : ( i ) we generate and release TextWorldsQA , a set of five diverse datasets , where each dataset contains dynamic narrative that describes entities and relations in a simulated world , paired with variably compositional questions over that knowledge , ( ii ) we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task , and ( iii ) we release a lightweight Python-based framework we call TextWorlds for easily generating arbitrary additional worlds and narrative , with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task .",2,0.3332164,34.04350108899892,132
77,We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input .,2,0.50279635,41.37862702268879,24
77,"Most current question answering models cannot scale to document or multi-document input , and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text .",0,0.88903564,52.98813273951795,32
77,We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs .,3,0.93962556,32.06010000680938,26
77,"Our method involves sampling multiple paragraphs from each document , and using an objective function that requires the model to produce globally correct output .",2,0.8226405,68.01841910917614,25
77,We additionally identify and improve upon a number of other design decisions that arise when working with document-level data .,3,0.69853,29.948111760166448,21
77,"Experiments on TriviaQA and SQuAD shows our method advances the state of the art , including a 10 point gain on TriviaQA .",3,0.91596127,15.744264872265965,23
78,"Complex machine learning models for NLP are often brittle , making different predictions for input instances that are extremely similar semantically .",0,0.9323037,66.67439494463805,22
78,"To automatically detect this behavior for individual instances , we present semantically equivalent adversaries ( SEAs ) – semantic-preserving perturbations that induce changes in the model ’s predictions .",2,0.7295048,72.90960113097069,31
78,"We generalize these adversaries into semantically equivalent adversarial rules ( SEARs ) – simple , universal replacement rules that induce adversaries on many instances .",2,0.7567453,203.1781335365119,25
78,"We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains : machine comprehension , visual question-answering , and sentiment analysis .",3,0.6832018,27.12907449410919,41
78,"Via user studies , we demonstrate that we generate high-quality local adversaries for more instances than humans , and that SEARs induce four times as many mistakes as the bugs discovered by human experts .",3,0.86522776,68.1137761276925,35
78,"SEARs are also actionable : retraining models using data augmentation significantly reduces bugs , while maintaining accuracy .",3,0.64695805,90.64907000349861,18
79,Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context .,0,0.9332657,32.56805561042052,24
79,This paper introduces a new method for automatic style transfer .,1,0.8493942,37.86623833600719,11
79,We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties .,2,0.79360986,26.86734036313273,33
79,Then adversarial generation techniques are used to make the output match the desired style .,2,0.627647,45.27483200348055,15
79,"We evaluate this technique on three different style transformations : sentiment , gender and political slant .",2,0.67355365,166.23340082591764,17
79,Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency .,3,0.93747556,19.99813633115018,34
80,"While large-scale knowledge graphs provide vast amounts of structured facts about entities , a short textual description can often be useful to succinctly characterize an entity and its type .",0,0.9139999,34.93431758265588,30
80,"Unfortunately , many knowledge graphs entities lack such textual descriptions .",0,0.9263951,509.19764740327474,11
80,"In this paper , we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words .",1,0.7549738,36.37853423370705,41
80,We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines .,3,0.82698315,48.49468621457121,27
81,We explore story generation : creative systems that can build coherent and fluent passages of text about a topic .,1,0.5055714,147.8103739051948,20
81,We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum .,2,0.906397,30.401621474806333,18
81,"Our dataset enables hierarchical story generation , where the model first generates a premise , and then transforms it into a passage of text .",3,0.4676833,57.90510012141595,25
81,"We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt , and adding a new gated multi-scale self-attention mechanism to model long-range context .",3,0.84020406,37.55558368895639,36
81,Experiments show large improvements over strong baselines on both automated and human evaluations .,3,0.91789025,11.900207349337826,14
81,Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one .,3,0.87047035,29.9048734746932,23
82,"Though impressive results have been achieved in visual captioning , the task of generating abstract stories from photo streams is still a little-tapped problem .",0,0.9081502,34.973510793544555,27
82,"Different from captions , stories have more expressive language styles and contain many imaginary concepts that do not appear in the images .",0,0.80501026,71.81029314909854,23
82,Thus it poses challenges to behavioral cloning algorithms .,0,0.83202696,156.01728651964103,9
82,"Furthermore , due to the limitations of automatic metrics on evaluating story quality , reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost .",0,0.7562205,66.2248722792007,31
82,"Therefore , we propose an Adversarial REward Learning ( AREL ) framework to learn an implicit reward function from human demonstrations , and then optimize policy search with the learned reward function .",2,0.4863848,60.20170028517536,33
82,"Though automatic evaluation indicates slight performance boost over state-of-the-art ( SOTA ) methods in cloning expert behaviors , human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems .",3,0.9428044,36.60165158694524,40
83,We present a deep neural network that leverages images to improve bilingual text embeddings .,1,0.40338004,17.277618998035035,15
83,"Relying on bilingual image tags and descriptions , our approach conditions text embedding induction on the shared visual information for both languages , producing highly correlated bilingual embeddings .",2,0.6652642,88.38386701206915,29
83,"In particular , we propose a novel model based on Partial Canonical Correlation Analysis ( PCCA ) .",2,0.49200296,27.923782377499172,18
83,"While the original PCCA finds linear projections of two views in order to maximize their canonical correlation conditioned on a shared third variable , we introduce a non-linear Deep PCCA ( DPCCA ) model , and develop a new stochastic iterative algorithm for its optimization .",2,0.7336919,69.79474627775292,46
83,We evaluate PCCA and DPCCA on multilingual word similarity and cross-lingual image description retrieval .,2,0.47130153,41.57558412436739,15
83,"Our models outperform a large variety of previous methods , despite not having access to any visual signal during test time inference .",3,0.90845674,67.82998129288404,23
84,"We introduce Picturebook , a large-scale lookup operation to ground language via ‘ snapshots ’ of our physical world accessed through image search .",2,0.52886647,172.93354219314233,24
84,"For each word in a vocabulary , we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding .",2,0.8448533,21.099692576363314,30
84,We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations .,2,0.72286975,35.62425254246243,16
84,"We also introduce Inverse Picturebook , a mechanism to map a Picturebook embedding back into words .",2,0.60479033,79.69411651116876,17
84,"We experiment and report results across a wide range of tasks : word similarity , natural language inference , semantic relatedness , sentiment / topic classification , image-sentence ranking and machine translation .",2,0.47488177,64.64390436577057,33
84,We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings .,3,0.95561683,38.11822340817277,20
85,"Despite recent advances in knowledge representation , automated reasoning , and machine learning , artificial agents still lack the ability to understand basic action-effect relations regarding the physical world , for example , the action of cutting a cucumber most likely leads to the state where the cucumber is broken apart into smaller pieces .",0,0.91723645,41.68784456231306,55
85,"If artificial agents ( e.g. , robots ) ever become our partners in joint tasks , it is critical to empower them with such action-effect understanding so that they can reason about the state of the world and plan for actions .",0,0.9180587,42.267967173135006,43
85,"Towards this goal , this paper introduces a new task on naive physical action-effect prediction , which addresses the relations between concrete actions ( expressed in the form of verb-noun pairs ) and their effects on the state of the physical world as depicted by images .",1,0.7729813,45.352337014088704,47
85,We collected a dataset for this task and developed an approach that harnesses web image data through distant supervision to facilitate learning for action-effect prediction .,2,0.8047251,79.45722831422508,27
85,"Our empirical results have shown that web data can be used to complement a small number of seed examples ( e.g. , three examples for each action ) for model learning .",3,0.98064303,58.83986284918592,32
85,This opens up possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans with a few examples .,3,0.6389376,101.36246839701147,24
86,Target-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence .,0,0.8765479,60.64523259271473,17
86,"RNN with attention seems a good fit for the characteristics of this task , and indeed it achieves the state-of-the-art performance .",3,0.86976385,21.33469122944259,28
86,"After re-examining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task , we propose a new model that achieves new state-of-the-art results on a few benchmarks .",3,0.47639376,20.512960184052737,42
86,"Instead of attention , our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer .",2,0.67840487,56.6341158413394,26
86,"Between the two layers , we propose a component which first generates target-specific representations of words in the sentence , and then incorporates a mechanism for preserving the original contextual information from the RNN layer .",2,0.6451287,37.75474431510098,36
87,Aspect sentiment classification ( ASC ) is a fundamental task in sentiment analysis .,0,0.9335031,40.69281690526637,14
87,"Given an aspect / target and a sentence , the task classifies the sentiment polarity expressed on the target in the sentence .",2,0.62410986,75.00365389143745,23
87,Memory networks ( MNs ) have been used for this task recently and have achieved state-of-the-art results .,0,0.93390435,13.856760169502198,24
87,"In MNs , attention mechanism plays a crucial role in detecting the sentiment context for the given target .",0,0.7796721,70.55802311914341,19
87,"However , we found an important problem with the current MNs in performing the ASC task .",3,0.94464594,85.24317064638637,17
87,Simply improving the attention mechanism will not solve it .,3,0.6620146,81.28260659240667,10
87,"The problem is referred to as target-sensitive sentiment , which means that the sentiment polarity of the ( detected ) context is dependent on the given target and it cannot be inferred from the context alone .",0,0.8891785,36.31956881481147,38
87,"To tackle this problem , we propose the target-sensitive memory networks ( TMNs ) .",0,0.47203645,61.945651975700734,15
87,Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated .,2,0.37965986,55.909967613733514,17
88,Getting manually labeled data in each domain is always an expensive and a time consuming task .,0,0.89204556,27.126144614177722,17
88,Cross-domain sentiment analysis has emerged as a demanding concept where a labeled source domain facilitates a sentiment classifier for an unlabeled target domain .,0,0.9602119,35.299927863496656,24
88,"However , polarity orientation ( positive or negative ) and the significance of a word to express an opinion often differ from one domain to another domain .",0,0.8479463,66.86909181041543,28
88,"Owing to these differences , cross-domain sentiment classification is still a challenging task .",0,0.9087495,31.501505351726255,14
88,"In this paper , we propose that words that do not change their polarity and significance represent the transferable ( usable ) information across domains for cross-domain sentiment classification .",1,0.86105007,59.90097918724503,30
88,We present a novel approach based on χ2 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains .,2,0.4762115,92.25602607237495,25
88,"Furthermore , we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance .",3,0.96374464,27.126396843047736,17
89,The goal of sentiment-to-sentiment “ translation ” is to change the underlying sentiment of a sentence while keeping its content .,0,0.9224934,29.52355269214189,23
89,The main challenge is the lack of parallel data .,0,0.90188545,11.376643338329878,10
89,"To solve this problem , we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module .",2,0.55139315,51.15351156946046,29
89,"We evaluate our approach on two review datasets , Yelp and Amazon .",2,0.67196083,71.98359430923543,13
89,Experimental results show that our approach significantly outperforms the state-of-the-art systems .,3,0.96672475,3.387285951524654,17
89,"Especially , the proposed method substantially improves the content preservation performance .",3,0.94693625,51.6229647678473,12
89,"The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets , respectively .",3,0.9133137,20.553602742878837,21
90,"Natural Language Inference ( NLI ) , also known as Recognizing Textual Entailment ( RTE ) , is one of the most important problems in natural language processing .",0,0.95820713,12.638972877587383,29
90,It requires to infer the logical relationship between two given sentences .,0,0.7526129,34.25370182997554,12
90,"While current approaches mostly focus on the interaction architectures of the sentences , in this paper , we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model .",1,0.54564565,54.13758894393593,36
90,We observe that people usually use some discourse markers such as “ so ” or “ but ” to represent the logical relationship between two sentences .,3,0.93235743,29.91727490226465,27
90,"These words potentially have deep connections with the meanings of the sentences , thus can be utilized to help improve the representations of them .",3,0.5649566,37.79407404179227,25
90,"Moreover , we use reinforcement learning to optimize a new objective function with a reward defined by the property of the NLI datasets to make full use of the labels information .",2,0.8092925,60.13505221973087,32
90,Experiments show that our method achieves the state-of-the-art performance on several large-scale datasets .,3,0.93674314,4.004737811862209,20
91,"During the last years , there has been a lot of interest in achieving some kind of complex reasoning using deep neural networks .",0,0.95221883,19.925566965956094,24
91,"To do that , models like Memory Networks ( MemNNs ) have combined external memory storages and attention mechanisms .",0,0.92128384,108.33888197330963,20
91,"These architectures , however , lack of more complex reasoning mechanisms that could allow , for instance , relational reasoning .",0,0.83620816,216.9421147482043,21
91,"Relation Networks ( RNs ) , on the other hand , have shown outstanding results in relational reasoning tasks .",0,0.9363723,56.1631956966807,20
91,"Unfortunately , their computational cost grows quadratically with the number of memories , something prohibitive for larger problems .",0,0.82371145,91.30285802309011,19
91,"To solve these issues , we introduce the Working Memory Network , a MemNN architecture with a novel working memory storage and reasoning module .",1,0.38272458,81.08150502025023,25
91,Our model retains the relational reasoning abilities of the RN while reducing its computational complexity from quadratic to linear .,3,0.6473065,50.96571027282924,20
91,We tested our model on the text QA dataset bAbI and the visual QA dataset NLVR .,2,0.74009025,43.703935871590474,17
91,"In the jointly trained bAbI-10k , we set a new state-of-the-art , achieving a mean error of less than 0.5 % .",3,0.7753987,32.29219695333445,31
91,"Moreover , a simple ensemble of two of our models solves all 20 tasks in the joint version of the benchmark .",3,0.868353,74.39506367748866,22
92,Sarcasm is a sophisticated speech act which commonly manifests on social communities such as Twitter and Reddit .,0,0.9318074,47.72982566566102,18
92,The prevalence of sarcasm on the social web is highly disruptive to opinion mining systems due to not only its tendency of polarity flipping but also usage of figurative language .,0,0.64634115,53.153242097720444,31
92,Sarcasm commonly manifests with a contrastive theme either between positive-negative sentiments or between literal-figurative scenarios .,0,0.7818315,83.99341887910069,20
92,"In this paper , we revisit the notion of modeling contrast in order to reason with sarcasm .",1,0.9156028,51.543910347442456,18
92,"More specifically , we propose an attention-based neural model that looks in-between instead of across , enabling it to explicitly model contrast and incongruity .",2,0.46833748,38.04155167868836,29
92,"We conduct extensive experiments on six benchmark datasets from Twitter , Reddit and the Internet Argument Corpus .",2,0.82708675,51.22407716560267,18
92,Our proposed model not only achieves state-of-the-art performance on all datasets but also enjoys improved interpretability .,3,0.9469291,7.965572308632988,23
93,Learning by contrasting positive and negative samples is a general strategy adopted by many methods .,0,0.84439987,44.96739929129638,16
93,Noise contrastive estimation ( NCE ) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach .,0,0.681675,59.55411841014649,23
93,"In this work , we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler .",2,0.4774153,60.47981686415358,30
93,"The resulting adaptive sampler finds harder negative examples , which forces the main model to learn a better representation of the data .",3,0.6594099,112.77355286941369,23
93,"We evaluate our proposal on learning word embeddings , order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics .",3,0.59799755,42.34797728582792,27
94,"This paper focuses on detection tasks in information extraction , where positive instances are sparsely distributed and models are usually evaluated using F-measure on positive classes .",1,0.64834064,92.99698043924741,27
94,These characteristics often result in deficient performance of neural network based detection models .,0,0.87750113,49.777886485069445,14
94,"In this paper , we propose adaptive scaling , an algorithm which can handle the positive sparsity problem and directly optimize over F-measure via dynamic cost-sensitive learning .",1,0.8217978,92.26803642474583,28
94,"To this end , we borrow the idea of marginal utility from economics and propose a theoretical framework for instance importance measuring without introducing any additional hyper-parameters .",2,0.65377635,51.68314840236063,28
94,Experiments show that our algorithm leads to a more effective and stable training of neural network based detection models .,3,0.96728456,19.786118049363864,20
95,Novel neural models have been proposed in recent years for learning under domain shift .,0,0.91083014,46.47450598981258,15
95,"Most models , however , only evaluate on a single task , on proprietary datasets , or compare to weak baselines , which makes comparison of models difficult .",0,0.85076743,104.45800392219864,29
95,"In this paper , we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs .",1,0.87674403,35.16718593359424,23
95,recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training .,2,0.3774822,30.09153629721141,21
95,"Extensive experiments on two benchmarks for part-of-speech tagging and sentiment analysis are negative : while our novel method establishes a new state-of-the-art for sentiment analysis , it does not fare consistently the best .",3,0.90324414,23.020490844048144,41
95,"More importantly , we arrive at the somewhat surprising conclusion that classic tri-training , with some additions , outperforms the state-of-the-art for NLP .",3,0.95603937,35.21459873497897,29
95,Hence classic approaches constitute an important and strong baseline .,0,0.83666086,269.4672848523218,10
96,Most of the neural sequence-to-sequence ( seq2seq ) models for grammatical error correction ( GEC ) have two limitations : ( 1 ) a seq2seq model may not be well generalized with only limited error-corrected data ;,0,0.8873805,21.468295011131794,39
96,( 2 ) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference .,3,0.70068145,54.390310271644644,21
96,We attempt to address these limitations by proposing a fluency boost learning and inference mechanism .,1,0.31937003,45.334499337319194,16
96,"Fluency boosting learning generates fluency-boost sentence pairs during training , enabling the error correction model to learn how to improve a sentence ’s fluency from more instances , while fluency boosting inference allows the model to correct a sentence incrementally with multiple inference steps until the sentence ’s fluency stops increasing .",2,0.44961262,45.29197669429148,54
96,"Experiments show our approaches improve the performance of seq2seq models for GEC , achieving state-of-the-art results on both CoNLL-2014 and JFLEG benchmark datasets .",3,0.9507835,15.70921412249511,32
97,The International Classification of Diseases ( ICD ) provides a hierarchy of diagnostic codes for classifying diseases .,0,0.9429792,20.92878608488689,18
97,Medical coding – which assigns a subset of ICD codes to a patient visit – is a mandatory process that is crucial for patient care and billing .,0,0.9308584,74.37307284325942,28
97,"Manual coding is time-consuming , expensive , and error prone .",0,0.8501847,55.169449840840514,13
97,"In this paper , we build a neural architecture for automated coding .",1,0.8546968,46.95805923275573,13
97,It takes the diagnosis descriptions ( DDs ) of a patient as inputs and selects the most relevant ICD codes .,2,0.51683676,97.47078094276321,21
97,"This architecture contains four major ingredients : ( 1 ) tree-of-sequences LSTM encoding of code descriptions ( CDs ) , ( 2 ) adversarial learning for reconciling the different writing styles of DDs and CDs , ( 3 ) isotonic constraints for incorporating the importance order among the assigned codes , and ( 4 ) attentional matching for performing many-to-one and one-to-many mappings from DDs to CDs .",3,0.33084542,56.90378213466945,72
97,We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59 K patient visits .,3,0.62801564,56.97934573195318,18
98,The success of deep neural networks ( DNNs ) is heavily dependent on the availability of labeled data .,0,0.9588002,12.362176433257462,19
98,"However , obtaining labeled data is a big challenge in many real-world problems .",0,0.932521,27.079264351803225,14
98,"In such scenarios , a DNN model can leverage labeled and unlabeled data from a related domain , but it has to deal with the shift in data distributions between the source and the target domains .",0,0.5275076,25.473360715942302,37
98,"In this paper , we study the problem of classifying social media posts during a crisis event ( e.g. , Earthquake ) .",1,0.8853757,33.477906556477116,23
98,"For that , we use labeled and unlabeled data from past similar events ( e.g. , Flood ) and unlabeled data for the current event .",2,0.88037753,38.19596942590684,26
98,We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework .,2,0.4692391,39.948088505588224,34
98,Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines .,3,0.89025176,25.009253856663005,17
99,Existing automated essay scoring ( AES ) models rely on rated essays for the target prompt as training data .,0,0.8789639,138.57513878743714,20
99,"Despite their successes in prompt-dependent AES , how to effectively predict essay ratings under a prompt-independent setting remains a challenge , where the rated essays for the target prompt are not available .",0,0.85990673,96.39471770959348,34
99,"To close this gap , a two-stage deep neural network ( TDNN ) is proposed .",2,0.4338512,43.34182354694094,17
99,"In particular , in the first stage , using the rated essays for non-target prompts as the training data , a shallow model is learned to select essays with an extreme quality for the target prompt , serving as pseudo training data ;",2,0.7633525,97.67791940665998,43
99,"in the second stage , an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step .",2,0.78249675,50.18345623288424,30
99,Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task .,3,0.93710774,130.89454939918656,21
100,The encoder-decoder dialog model is one of the most prominent methods used to build dialog systems in complex domains .,0,0.91485584,13.476377260720511,21
100,"Yet it is limited because it cannot output interpretable actions as in traditional systems , which hinders humans from understanding its generation process .",0,0.9013885,73.5748756588289,24
100,We present an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialog models for interpretable response generation .,1,0.37356922,24.546134087918517,23
100,"Building upon variational autoencoders ( VAEs ) , we present two novel models , DI-VAE and DI-VST that improve VAEs and can discover interpretable semantics via either auto encoding or context predicting .",2,0.5539858,55.04576238610076,37
100,Our methods have been validated on real-world dialog datasets to discover semantic representations and enhance encoder-decoder models with interpretable generation .,3,0.6056965,26.2514862546329,21
101,"In conversation , a general response ( e.g. , “ I do n’t know ” ) could correspond to a large variety of input utterances .",0,0.45209464,44.13532721399639,26
101,"Previous generative conversational models usually employ a single model to learn the relationship between different utterance-response pairs , thus tend to favor general and trivial responses which appear frequently .",0,0.8912089,51.795686386921815,32
101,"To address this problem , we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity .",1,0.55448294,39.02106391244129,25
101,"Specifically , we introduce an explicit specificity control variable into a sequence-to-sequence model , which interacts with the usage representation of words through a Gaussian Kernel layer , to guide the model to generate responses at different specificity levels .",2,0.8368882,43.73997207362227,42
101,We describe two ways to acquire distant labels for the specificity control variable in learning .,3,0.34869054,205.46014142509244,16
101,Empirical studies show that our model can significantly outperform the state-of-the-art response generation models under both automatic and human evaluations .,3,0.9343915,6.236640114151976,27
102,"Human generates responses relying on semantic and functional dependencies , including coreference relation , among dialogue elements and their context .",0,0.5421525,216.75857567246243,21
102,"In this paper , we investigate matching a response with its multi-turn context using dependency information based entirely on attention .",1,0.90518254,84.39854928265979,21
102,"Our solution is inspired by the recently proposed Transformer in machine translation ( Vaswani et al. , 2017 ) and we extend the attention mechanism in two ways .",2,0.4238569,32.702157781475904,29
102,"First , we construct representations of text segments at different granularities solely with stacked self-attention .",2,0.85704666,40.7396944747053,16
102,"Second , we try to extract the truly matched segment pairs with attention across the context and response .",2,0.8223647,146.05037093384533,19
102,We jointly introduce those two kinds of attention in one uniform neural network .,2,0.67495304,111.61510825044336,14
102,Experiments on two large-scale multi-turn response selection tasks show that our proposed model significantly outperforms the state-of-the-art models .,3,0.9283373,6.210804079931359,25
103,Generating emotional language is a key step towards building empathetic natural language processing agents .,0,0.7062862,23.799529544542875,15
103,"However , a major challenge for this line of research is the lack of large-scale labeled training data , and previous studies are limited to only small sets of human annotated sentiment labels .",0,0.9202067,26.49994148083175,34
103,"Additionally , explicitly controlling the emotion and sentiment of generated text is also difficult .",0,0.7783319,86.19026824140413,15
103,"In this paper , we take a more radical approach : we exploit the idea of leveraging Twitter data that are naturally labeled with emojis .",2,0.43961072,44.593064159251554,26
103,We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence .,2,0.86801213,31.412279011582566,26
103,"We investigate several conditional variational autoencoders training on these conversations , which allow us to use emojis to control the emotion of the generated text .",2,0.71226263,36.61019584104634,26
103,"Experimentally , we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions .",3,0.94930804,37.952590299954124,27
104,Taylor ’s law describes the fluctuation characteristics underlying a system in which the variance of an event within a time span grows by a power law with respect to the mean .,0,0.93262434,42.035727313114144,32
104,"Although Taylor ’s law has been applied in many natural and social systems , its application for language has been scarce .",0,0.9366189,42.69482305515763,22
104,This article describes a new way to quantify Taylor ’s law in natural language and conducts Taylor analysis of over 1100 texts across 14 languages .,1,0.87544584,79.23891410425816,26
104,We found that the Taylor exponents of natural language written texts exhibit almost the same value .,3,0.9790243,107.27503590479655,17
104,"The exponent was also compared for other language-related data , such as the child-directed speech , music , and programming languages .",2,0.62761337,133.82104850142593,26
104,The results show how the Taylor exponent serves to quantify the fundamental structural complexity underlying linguistic time series .,3,0.9876331,139.3874919757619,19
104,The article also shows the applicability of these findings in evaluating language models .,3,0.7969166,20.177345609718685,14
105,Language variation and change are driven both by individuals ’ internal cognitive processes and by the social structures through which language propagates .,0,0.8148872,80.358814630138,23
105,A wide range of computational frameworks have been proposed to connect these drivers .,0,0.90870947,41.41261752864811,14
105,We compare the strengths and weaknesses of existing approaches and propose a new analytic framework which combines previous network models ’ ability to capture realistic social structure with practically and more elegant computational properties .,3,0.36721203,73.28740536880761,35
105,The framework privileges the process of language acquisition and embeds learners in a social network but is modular so that population structure can be combined with different acquisition models .,2,0.36983824,93.13637060895155,30
105,We demonstrate two applications for the framework : a test of practical concerns that arise when modeling acquisition in a population setting and an application of the framework to recent work on phonological mergers in progress .,3,0.5943398,73.00286839951725,37
106,"We show that an epsilon-free , chain-free synchronous context-free grammar ( SCFG ) can be converted into a weakly equivalent synchronous tree-adjoining grammar ( STAG ) which is prefix lexicalized .",3,0.5189128,67.86883738838452,33
106,"This transformation at most doubles the grammar ’s rank and cubes its size , but we show that in practice the size increase is only quadratic .",3,0.8451379,129.7758182631779,27
106,"Our results extend Greibach normal form from CFGs to SCFGs and prove new formal properties about SCFG , a formalism with many applications in natural language processing .",3,0.9725451,125.55077596538916,28
107,"In this work , we propose a novel constituency parsing scheme .",1,0.8457999,39.45679139291908,12
107,"The model first predicts a real-valued scalar , named syntactic distance , for each split position in the sentence .",2,0.7141043,47.30683945637376,20
107,The topology of grammar tree is then determined by the values of syntactic distances .,2,0.46312293,69.14614216274094,15
107,"Compared to traditional shift-reduce parsing schemes , our approach is free from the potentially disastrous compounding error .",3,0.8570114,61.319794054763456,18
107,It is also easier to parallelize and much faster .,0,0.5549759,48.52752194112294,10
107,"Our model achieves the state-of-the-art single model F1 score of 92.1 on PTB and 86.4 on CTB dataset , which surpasses the previous single model results by a large margin .",3,0.8902213,9.20479382915842,36
108,"We introduce Latent Vector Grammars ( LVeGs ) , a new framework that extends latent variable grammars such that each nonterminal symbol is associated with a continuous vector space representing the set of ( infinitely many ) subtypes of the nonterminal .",2,0.6354409,47.73888474831948,42
108,We show that previous models such as latent variable grammars and compositional vector grammars can be interpreted as special cases of LVeGs .,3,0.8124632,50.9258214410881,23
108,"We then present Gaussian Mixture LVeGs ( GM-LVeGs ) , a new special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals .",2,0.5867655,96.8631280615623,34
108,"A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the inside-outside algorithm , which enables efficient inference and learning .",3,0.42959625,54.76230907679256,36
108,We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies .,3,0.6235199,27.293491151199916,23
109,We revisit domain adaptation for parsers in the neural era .,1,0.33353448,103.0248465872339,11
109,First we show that recent advances in word representations greatly diminish the need for domain adaptation when the target domain is syntactically similar to the source domain .,3,0.5719167,15.727434662694208,28
109,"As evidence , we train a parser on the Wall Street Journal alone that achieves over 90 % F1 on the Brown corpus .",3,0.52004635,56.66077629715678,24
109,"For more syntactically distant domains , we provide a simple way to adapt a parser using only dozens of partial annotations .",3,0.53376365,81.76689784857743,22
109,"For instance , we increase the percentage of error-free geometry-domain parses in a held-out set from 45 % to 73 % using approximately five dozen training examples .",3,0.82609296,71.10538108459183,32
109,"In the process , we demonstrate a new state-of-the-art single model result on the Wall Street Journal test set of 94.3 % .",3,0.7903841,23.593694859213695,29
109,This is an absolute increase of 1.7 % over the previous state-of-the-art of 92.6 % .,3,0.88764864,9.477315762608043,21
110,Revealing the implicit semantic relation between the constituents of a noun-compound is important for many NLP applications .,0,0.9164268,24.188702076320762,18
110,It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations .,0,0.9216302,43.39056303116663,28
110,"Most existing paraphrasing methods lack the ability to generalize , and have a hard time interpreting infrequent or new noun-compounds .",0,0.8490231,52.66806449090195,21
110,"We propose a neural model that generalizes better by representing paraphrases in a continuous space , generalizing for both unseen noun-compounds and rare paraphrases .",2,0.41776642,49.157564291742325,25
110,Our model helps improving performance on both the noun-compound paraphrasing and classification tasks .,3,0.90422475,60.842744691794586,14
111,"We explore the notion of subjectivity , and hypothesize that word embeddings learnt from input corpora of varying levels of subjectivity behave differently on natural language processing tasks such as classifying a sentence by sentiment , subjectivity , or topic .",1,0.40105113,27.55521123588583,41
111,"Through systematic comparative analyses , we establish this to be the case indeed .",3,0.7175774,82.00093518181235,14
111,"Moreover , based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification , we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource , and is shown to outperform baselines on such tasks .",2,0.47395617,37.42184444790689,52
112,"Metaphoric expressions are widespread in natural language , posing a significant challenge for various natural language processing tasks such as Machine Translation .",0,0.95198035,22.813424082763856,23
112,Current word embedding based metaphor identification models cannot identify the exact metaphorical words within a sentence .,0,0.842382,56.48823265218578,17
112,"In this paper , we propose an unsupervised learning method that identifies and interprets metaphors at word-level without any preprocessing , outperforming strong baselines in the metaphor identification task .",1,0.87572944,23.692211154608668,31
112,"Our model extends to interpret the identified metaphors , paraphrasing them into their literal counterparts , so that they can be better translated by machines .",3,0.75712657,67.4415483549852,26
112,"We evaluated this with two popular translation systems for English to Chinese , showing that our model improved the systems significantly .",3,0.65148515,63.732981184394816,22
113,Traditional word embedding approaches learn semantic information at word level while ignoring the meaningful internal structures of words like morphemes .,0,0.8750489,41.34028798973613,21
113,"Furthermore , existing morphology-based models directly incorporate morphemes to train word embeddings , but still neglect the latent meanings of morphemes .",0,0.83142275,28.85331287188209,24
113,"In this paper , we explore to employ the latent meanings of morphological compositions of words to train and enhance word embeddings .",1,0.90331775,37.44404019834129,23
113,"Based on this purpose , we propose three Latent Meaning Models ( LMMs ) , named LMM-A , LMM-S and LMM-M respectively , which adopt different strategies to incorporate the latent meanings of morphemes during the training process .",2,0.6324958,23.905755218857102,42
113,"Experiments on word similarity , syntactic analogy and text classification are conducted to validate the feasibility of our models .",2,0.63735026,32.18936301736227,20
113,The results demonstrate that our models outperform the baselines on five word similarity datasets .,3,0.9825378,20.434449264659314,15
113,"On Wordsim-353 and RG-65 datasets , our models nearly achieve 5 % and 7 % gains over the classic CBOW model , respectively .",3,0.94151026,218.91683353882834,27
113,"For the syntactic analogy and text classification tasks , our models also surpass all the baselines including a morphology-based model .",3,0.87478656,59.545060259396735,23
114,"The process of translation is ambiguous , in that there are typically many valid translations for a given sentence .",0,0.9271516,32.050202912843204,20
114,"This gives rise to significant variation in parallel corpora , however , most current models of machine translation do not account for this variation , instead treating the problem as a deterministic process .",0,0.9221851,47.7921700799485,34
114,"To this end , we present a deep generative model of machine translation which incorporates a chain of latent variables , in order to account for local lexical and syntactic variation in parallel corpora .",2,0.41369584,21.803501781316335,35
114,We provide an in-depth analysis of the pitfalls encountered in variational inference for training deep generative models .,3,0.49954957,14.19855991306266,19
114,Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines .,3,0.9057436,10.271272782381681,16
115,"Tree-based neural machine translation ( NMT ) approaches , although achieved impressive performance , suffer from a major drawback : they only use the 1-best parse tree to direct the translation , which potentially introduces translation mistakes due to parsing errors .",0,0.91395164,55.323797144471044,44
115,"For statistical machine translation ( SMT ) , forest-based methods have been proven to be effective for solving this problem , while for NMT this kind of approach has not been attempted .",0,0.90953195,31.475011788565695,35
115,"This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework ( i.e. , a forest-to-sequence NMT model ) .",1,0.7163971,35.579842340421685,32
115,"The BLEU score of the proposed method is higher than that of the sequence-to-sequence NMT , tree-based NMT , and forest-based SMT systems .",3,0.86955786,17.819835741669124,30
116,"Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information , even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence .",0,0.87627083,83.91855654836728,30
116,We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed .,2,0.40244547,19.725084591060504,32
116,"We experiment with an English-Russian subtitles dataset , and observe that much of what is captured by our model deals with improving pronoun translation .",3,0.5137895,51.660098664900424,26
116,We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora .,2,0.47566667,64.8187309688982,19
116,It is consistent with gains for sentences where pronouns need to be gendered in translation .,3,0.93183035,69.94150196050232,16
116,"Beside improvements in anaphoric cases , the model also improves in overall BLEU , both over its context-agnostic version ( + 0.7 ) and over simple concatenation of the context and source sentences ( + 0.6 ) .",3,0.93326765,36.71363536795043,39
117,We present a document-level neural machine translation model which takes both source and target document context into account using memory networks .,2,0.41095304,21.844117962377116,23
117,"We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables , i.e. , the source sentences and their unobserved target translations in the document .",2,0.7837204,44.49396896214486,32
117,"The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components , one each for the source and target side , to capture the documental interdependencies .",2,0.7057338,76.38623846040456,33
117,"We train the model end-to-end , and propose an iterative decoding algorithm based on block coordinate descent .",2,0.74052155,47.591138585782,18
117,"Experimental results of English translations from French , German , and Estonian documents show that our model is effective in exploiting both source and target document context , and statistically significantly outperforms the previous work in terms of BLEU and METEOR .",3,0.91439414,21.235626679539582,42
118,"The purpose of text geolocation is to associate geographic information contained in a document with a set ( or sets ) of coordinates , either implicitly by using linguistic features and / or explicitly by using geographic metadata combined with heuristics .",0,0.9273946,49.99136117519744,42
118,We introduce a geocoder ( location mention disambiguator ) that achieves state-of-the-art ( SOTA ) results on three diverse datasets by exploiting the implicit lexical clues .,2,0.56009334,33.030441610617274,31
118,"Moreover , we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text .",3,0.4851555,45.01917965445436,23
118,"To that end , we introduce the Map Vector ( MapVec ) , a sparse representation obtained by plotting prior geographic probabilities , derived from population figures , on a World Map .",2,0.7664255,131.23521821069454,33
118,We then integrate the implicit ( language ) and explicit ( map ) features to significantly improve a range of metrics .,2,0.69032156,78.28293441703032,22
118,We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing .,3,0.43557993,46.099218765550894,24
119,People go to different places to engage in activities that reflect their goals .,0,0.86465627,21.746519173806345,14
119,"For example , people go to restaurants to eat , libraries to study , and churches to pray .",0,0.8098279,45.83305388250904,19
119,We refer to an activity that represents a common reason why people typically go to a location as a prototypical goal activity ( goal-act ) .,2,0.5030823,60.434585357749434,26
119,Our research aims to learn goal-acts for specific locations using a text corpus and semi-supervised learning .,1,0.7789334,79.51662124584625,17
119,"First , we extract activities and locations that co-occur in goal-oriented syntactic patterns .",2,0.86820906,39.361451579522736,14
119,"Next , we create an activity profile matrix and apply a semi-supervised label propagation algorithm to iteratively revise the activity strengths for different locations using a small set of labeled data .",2,0.8721683,43.13940981688443,32
119,We show that this approach outperforms several baseline methods when judged against goal-acts identified by human annotators .,3,0.9564691,62.6855016964087,20
120,Acronyms are abbreviations formed from the initial components of words or phrases .,0,0.7942492,25.142335741537636,13
120,"In enterprises , people often use acronyms to make communications more efficient .",0,0.90937155,63.12583766642854,13
120,"However , acronyms could be difficult to understand for people who are not familiar with the subject matter ( new employees , etc. ) , thereby affecting productivity .",0,0.72686076,44.37599285532899,29
120,"To alleviate such troubles , we study how to automatically resolve the true meanings of acronyms in a given context .",1,0.7326603,64.88422713749239,21
120,Acronym disambiguation for enterprises is challenging for several reasons .,0,0.92989,40.9057276635613,10
120,"First , acronyms may be highly ambiguous since an acronym used in the enterprise could have multiple internal and external meanings .",0,0.8408858,54.13110981334463,22
120,"Second , there are usually no comprehensive knowledge bases such as Wikipedia available in enterprises .",0,0.9087005,113.88589968923604,16
120,"Finally , the system should be generic to work for any enterprise .",3,0.8341898,151.9575271485751,13
120,In this work we propose an end-to-end framework to tackle all these challenges .,1,0.76167065,9.211889490028776,16
120,The framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output .,2,0.6585143,39.32524406764935,18
120,"Our disambiguation models are trained via distant supervised learning , without requiring any manually labeled training examples .",2,0.70754224,51.48830793916402,18
120,"Therefore , our proposed framework can be deployed to any enterprise to support high-quality acronym disambiguation .",3,0.95140105,40.87499874673578,17
120,Experimental results on real world data justified the effectiveness of our system .,3,0.9570199,23.700391831121596,13
121,"Existing temporal relation ( TempRel ) annotation schemes often have low inter-annotator agreements ( IAA ) even between experts , suggesting that the current annotation task needs a better definition .",0,0.85831827,168.73795003675556,31
121,This paper proposes a new multi-axis modeling to better capture the temporal structure of events .,1,0.7913567,31.94674571656653,16
121,"In addition , we identify that event end-points are a major source of confusion in annotation , so we also propose to annotate TempRels based on start-points only .",3,0.8621379,71.98163784202843,29
121,A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60 ’s to 80 ’s ( Cohen ’s Kappa ) .,3,0.90478283,138.16612745674982,28
121,This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator .,3,0.87495303,47.97082775203339,21
121,We hope that this work can foster more interesting studies towards event understanding .,3,0.90633404,72.97982749836133,14
122,"In this paper we present the Exemplar Encoder-Decoder network ( EED ) , a novel conversation model that learns to utilize similar examples from training data to generate responses .",1,0.8425508,30.24402542538382,32
122,Similar conversation examples ( context-response pairs ) from training data are retrieved using a traditional TF-IDF based retrieval model and the corresponding responses are used by our decoder to generate the ground truth response .,2,0.7786334,46.98969766468652,38
122,The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context .,2,0.59240407,43.053521136501516,19
122,"As a result , our model learns to assign higher similarity scores to those retrieved contexts whose responses are crucial for generating the final response .",3,0.83085716,49.2273717068456,26
122,We present detailed experiments on two large data sets and we find that our method out-performs state of the art sequence to sequence generative models on several recently proposed evaluation metrics .,3,0.87534016,20.56037125531601,33
123,The recent advance in deep learning and semantic parsing has significantly improved the translation accuracy of natural language questions to structured queries .,0,0.93575233,24.680146523963167,23
123,"However , further improvement of the existing approaches turns out to be quite challenging .",0,0.686647,40.49007803243572,15
123,"Rather than solely relying on algorithmic innovations , in this work , we introduce DialSQL , a dialogue-based structured query generation framework that leverages human intelligence to boost the performance of existing algorithms via user interaction .",1,0.41824013,45.54456301901506,39
123,DialSQL is capable of identifying potential errors in a generated SQL query and asking users for validation via simple multi-choice questions .,0,0.6495207,41.013500022833384,22
123,User feedback is then leveraged to revise the query .,2,0.5707655,55.39583623292271,10
123,We design a generic simulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset .,2,0.86509633,49.431838038543376,21
123,"Using SQLNet as a black box query generation tool , DialSQL improves its performance from 61.3 % to 69.0 % using only 2.4 validation questions per dialogue .",3,0.8919008,73.88788563427478,28
124,"One of the main challenges online social systems face is the prevalence of antisocial behavior , such as harassment and personal attacks .",0,0.95370024,17.657712672602905,23
124,"In this work , we introduce the task of predicting from the very start of a conversation whether it will get out of hand .",1,0.7712194,20.878081264747514,25
124,"As opposed to detecting undesirable behavior after the fact , this task aims to enable early , actionable prediction at a time when the conversation might still be salvaged .",0,0.68907285,84.91954452336023,30
124,"To this end , we develop a framework for capturing pragmatic devices — such as politeness strategies and rhetorical prompts — used to start a conversation , and analyze their relation to its future trajectory .",1,0.49604172,68.13040749768135,36
124,"Applying this framework in a controlled setting , we demonstrate the feasibility of detecting early warning signs of antisocial behavior in online discussions .",3,0.72336453,21.456270045220453,24
125,One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation ( NMT ) systems .,0,0.7541512,39.87690022472227,21
125,The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted .,0,0.72343034,42.5193383314714,23
125,We propose several variations of the attentive NMT architecture bringing this meeting point back .,3,0.42276278,310.65567083184675,15
125,"Empirical evaluation suggests that the better the translation quality , the worse the learned sentence representations serve in a wide range of classification and similarity tasks .",3,0.53522235,40.196328227903926,27
126,Metric validation in Grammatical Error Correction ( GEC ) is currently done by observing the correlation between human and metric-induced rankings .,0,0.87991905,65.89541518157772,22
126,"However , such correlation studies are costly , methodologically troublesome , and suffer from low inter-rater agreement .",0,0.88141704,69.78369795045309,18
126,"We propose MAEGE , an automatic methodology for GEC metric validation , that overcomes many of the difficulties in the existing methodology .",1,0.5705938,103.60226540937532,23
126,"Experiments with MAEGE shed a new light on metric quality , showing for example that the standard M2 metric fares poorly on corpus-level ranking .",3,0.81117046,109.09033953233522,25
126,"Moreover , we use MAEGE to perform a detailed analysis of metric behavior , showing that some types of valid edits are consistently penalized by existing metrics .",3,0.52221066,78.79807149115132,28
127,Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental .,0,0.8790298,28.10898101123271,18
127,In this opinion / theoretical paper we discuss the role of statistical significance testing in Natural Language Processing ( NLP ) research .,1,0.9033235,47.041215870730525,23
127,"We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks , experimental setups and evaluation measures that affect the choice of significance tests in NLP research .",1,0.45530686,42.98575525906401,33
127,Based on this discussion we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests .,3,0.8089375,61.61312288783395,32
127,"We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results , statistical significance testing is often ignored or misused .",3,0.5565454,89.26147607617249,35
127,We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied .,3,0.7998883,30.39351894258833,23
127,in NLP research in a statistically sound manner .,0,0.4291392,48.72374910776495,9
128,Many natural language processing tasks can be modeled into structured prediction and solved as a search problem .,0,0.8825257,46.72643410106694,18
128,"In this paper , we distill an ensemble of multiple models trained with different initialization into a single model .",1,0.59563243,34.7239883042061,20
128,"In addition to learning to match the ensemble ’s probability output on the reference states , we also use the ensemble to explore the search space and learn from the encountered states in the exploration .",2,0.757729,58.849403013729486,36
128,Experimental results on two typical search-based structured prediction tasks – transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model ’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures .,3,0.9088833,33.37725657767977,67
129,We introduce a novel architecture for dependency parsing : stack-pointer networks ( StackPtr ) .,2,0.4598593,137.25711606499198,17
129,"Combining pointer networks ( Vinyals et al. , 2015 ) with an internal stack , the proposed model first reads and encodes the whole sentence , then builds the dependency tree top-down ( from root-to-leaf ) in a depth-first fashion .",2,0.6447083,67.00720352678572,45
129,The stack tracks the status of the depth-first search and the pointer networks select one child for the word at the top of the stack at each step .,2,0.59505665,67.37475580558382,29
129,"The StackPtr parser benefits from the information of whole sentence and all previously derived subtree structures , and removes the left-to-right restriction in classical transition-based parsers .",3,0.45027733,115.11461537952921,30
129,"Yet the number of steps for building any ( non-projective ) parse tree is linear in the length of the sentence just as other transition-based parsers , yielding an efficient decoding algorithm with O ( n2 ) time complexity .",3,0.5382401,71.62289294812047,42
129,"We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas , and achieve state-of-the-art performances on 21 of them .",2,0.47661656,25.456876981034306,31
130,"Due to the presence of both Twitter-specific conventions and non-standard and dialectal language , Twitter presents a significant parsing challenge to current dependency parsing tools .",0,0.8859471,63.03596068333496,26
130,"We broaden English dependency parsing to handle social media English , particularly social media African-American English ( AAE ) , by developing and annotating a new dataset of 500 tweets , 250 of which are in AAE , within the Universal Dependencies 2.0 framework .",2,0.8187616,61.411821446772365,46
130,"We describe our standards for handling Twitter-and AAE-specific features and evaluate a variety of cross-domain strategies for improving parsing with no , or very little , in-domain labeled data , including a new data synthesis approach .",1,0.41918442,94.47457813307442,42
130,"We analyze these methods ’ impact on performance disparities between AAE and Mainstream American English tweets , and assess parsing accuracy for specific AAE lexical and syntactic features .",2,0.44795004,93.85677793705541,29
130,Our annotated data and a parsing model are available at : http://slanglab.cs.umass.edu/TwitterAAE/ .,3,0.6117761,25.69401945576527,13
131,"Language exhibits hierarchical structure , but recent work using a subject-verb agreement diagnostic argued that state-of-the-art language models , LSTMs , fail to learn long-range syntax sensitive dependencies .",0,0.9160736,63.17290266744043,38
131,"Using the same diagnostic , we show that , in fact , LSTMs do succeed in learning such dependencies — provided they have enough capacity .",3,0.8424403,107.24388841906467,26
131,"We then explore whether models that have access to explicit syntactic information learn agreement more effectively , and how the way in which this structural information is incorporated into the model impacts performance .",3,0.43219218,38.802907824807136,34
131,"We find that the mere presence of syntactic information does not improve accuracy , but when model architecture is determined by syntax , number agreement is improved .",3,0.98315674,79.960797535608,28
131,"Further , we find that the choice of how syntactic structure is built affects how well number agreement is learned : top-down construction outperforms left-corner and bottom-up variants in capturing non-local structural dependencies .",3,0.973189,48.707720771809704,36
132,Existing solutions to task-oriented dialogue systems follow pipeline designs which introduces architectural complexity and fragility .,0,0.8959125,123.48193565082151,18
132,"We propose a novel , holistic , extendable framework based on a single sequence-to-sequence ( seq2seq ) model which can be optimized with supervised or reinforcement learning .",2,0.32958055,34.084482114907324,30
132,"A key contribution is that we design text spans named belief spans to track dialogue believes , allowing task-oriented dialogue systems to be modeled in a seq2seq way .",3,0.5646108,95.54378438118948,31
132,"Based on this , we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability : significantly reducing model complexity in terms of number of parameters and training time by a magnitude .",3,0.5112619,123.22906646759101,34
132,It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary ( OOV ) cases where pipeline-designed competitors totally fail .,3,0.8641127,35.067810850082886,39
133,"We highlight a practical yet rarely discussed problem in dialogue state tracking ( DST ) , namely handling unknown slot values .",1,0.7404745,126.08767292421841,22
133,"Previous approaches generally assume predefined candidate lists and thus are not designed to output unknown values , especially when the spoken language understanding ( SLU ) module is absent as in many end-to-end ( E2E ) systems .",0,0.8716306,45.17045230083828,38
133,We describe in this paper an E2E architecture based on the pointer network ( PtrNet ) that can effectively extract unknown slot values while still obtains state-of-the-art accuracy on the standard DSTC2 benchmark .,1,0.6702659,37.4613275467762,40
133,We also provide extensive empirical evidence to show that tracking unknown values can be challenging and our approach can bring significant improvement with the help of an effective feature dropout technique .,3,0.87513554,35.7643725961685,32
134,"Dialogue state tracking , which estimates user goals and requests given the dialogue context , is an essential part of task-oriented dialogue systems .",0,0.90667963,40.54184466817281,26
134,"In this paper , we propose the Global-Locally Self-Attentive Dialogue State Tracker ( GLAD ) , which learns representations of the user utterance and previous system actions with global-local modules .",1,0.78415674,46.46940929188764,32
134,"Our model uses global modules to shares parameters between estimators for different types ( called slots ) of dialogue states , and uses local modules to learn slot-specific features .",2,0.82023025,130.40817834939043,30
134,We show that this significantly improves tracking of rare states .,3,0.9494578,172.42040718225869,11
134,"GLAD obtains 88.3 % joint goal accuracy and 96.4 % request accuracy on the WoZ state tracking task , outperforming prior work by 3.9 % and 4.8 % .",3,0.89781386,35.58805473085156,29
134,"On the DSTC2 task , our model obtains 74.7 % joint goal accuracy and 97.3 % request accuracy , outperforming prior work by 1.3 % and 0.8 % .",3,0.92326695,27.687380566239973,29
135,End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases .,0,0.911457,22.180216114837222,18
135,"In this paper , we propose a novel yet simple end-to-end differentiable model called memory-to-sequence ( Mem2Seq ) to address this issue .",1,0.86805147,17.735742961775433,26
135,Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network .,0,0.608151,33.71236567841543,21
135,"We empirically show how Mem2Seq controls each generation step , and how its multi-hop attention mechanism helps in learning correlations between memories .",3,0.80457854,82.59160200865603,23
135,"In addition , our model is quite general without complicated task-specific designs .",3,0.6479645,123.74222710784069,13
135,"As a result , we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets .",3,0.9624312,14.415643629879094,32
136,Sequence to sequence ( Seq2Seq ) models have been widely used for response generation in the area of conversation .,0,0.94348156,31.150064285622083,20
136,"However , the requirements for different conversation scenarios are distinct .",0,0.78082716,115.11049863368791,11
136,"For example , customer service requires the generated responses to be specific and accurate , while chatbot prefers diverse responses so as to attract different users .",0,0.64116067,72.1512926761404,27
136,"The current Seq2Seq model fails to meet these diverse requirements , by using a general average likelihood as the optimization criteria .",3,0.5895863,69.32107879429446,22
136,"As a result , it usually generates safe and commonplace responses , such as ‘ I do n’t know ’ .",0,0.8863769,82.19083412995437,21
136,"In this paper , we propose two tailored optimization criteria for Seq2Seq to different conversation scenarios , i.e. , the maximum generated likelihood for specific-requirement scenario , and the conditional value-at-risk for diverse-requirement scenario .",1,0.7825395,57.04899747368654,38
136,"Experimental results on the Ubuntu dialogue corpus ( Ubuntu service scenario ) and Chinese Weibo dataset ( social chatbot scenario ) show that our proposed models not only satisfies diverse requirements for different scenarios , but also yields better performances against traditional Seq2Seq models in terms of both metric-based and human evaluations .",3,0.9085415,32.450468169352476,53
137,"End-to-end neural dialogue generation has shown promising results recently , but it does not employ knowledge to guide the generation and hence tends to generate short , general , and meaningless responses .",0,0.89825684,32.1118285518703,36
137,"In this paper , we propose a neural knowledge diffusion ( NKD ) model to introduce knowledge into dialogue generation .",1,0.8771116,49.05714390511145,21
137,This method can not only match the relevant facts for the input utterance but diffuse them to similar entities .,3,0.398399,65.04782300326583,20
137,"With the help of facts matching and entity diffusion , the neural dialogue generation is augmented with the ability of convergent and divergent thinking over the knowledge base .",3,0.41601703,66.173892666178,29
137,"Our empirical study on a real-world dataset prove that our model is capable of generating meaningful , diverse and natural responses for both factoid-questions and knowledge grounded chi-chats .",3,0.96117187,51.47576363138083,30
137,The experiment results also show that our model outperforms competitive baseline models significantly .,3,0.98020786,15.457138953537209,14
138,"Sentence function is a significant factor to achieve the purpose of the speaker , which , however , has not been touched in large-scale conversation generation so far .",0,0.81720954,73.35761085941813,29
138,"In this paper , we present a model to generate informative responses with controlled sentence function .",1,0.90375495,54.50475339830754,17
138,"Our model utilizes a continuous latent variable to capture various word patterns that realize the expected sentence function , and introduces a type controller to deal with the compatibility of controlling sentence function and generating informative content .",2,0.8210184,92.95495143314456,38
138,"Conditioned on the latent variable , the type controller determines the type ( i.e. , function-related , topic , and ordinary word ) of a word to be generated at each decoding position .",2,0.5865646,81.56327573744429,36
138,"Experiments show that our model outperforms state-of-the-art baselines , and it has the ability to generate responses with both controlled sentence function and informative content .",3,0.9626626,16.33034472671001,32
139,End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating .,3,0.51764894,41.11352751552924,23
139,"However , current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information .",0,0.8960128,38.26537939372557,20
139,"Therefore , we propose to include user sentiment obtained through multimodal information ( acoustic , dialogic and textual ) , in the end-to-end learning framework to make systems more user-adaptive and effective .",3,0.37240815,46.576379744450186,35
139,We incorporated user sentiment information in both supervised and reinforcement learning settings .,2,0.8480808,74.90204422402346,13
139,"In both settings , adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task .",3,0.95888853,121.85736240338164,24
139,This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance .,3,0.8997927,16.22616636429421,32
140,We present a new method for estimating vector space representations of words : embedding learning by concept induction .,2,0.36222067,62.10607339086549,19
140,We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space .,2,0.7909667,55.496276978964964,25
140,An extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches .,3,0.8820092,27.064812778214936,24
141,The transfer or share of knowledge between languages is a potential solution to resource scarcity in NLP .,0,0.7070602,23.63767967662445,18
141,"However , the effectiveness of cross-lingual transfer can be challenged by variation in syntactic structures .",0,0.8098722,18.697089184557477,16
141,"Frameworks such as Universal Dependencies ( UD ) are designed to be cross-lingually consistent , but even in carefully designed resources trees representing equivalent sentences may not always overlap .",0,0.85048914,96.28846023626525,30
141,"In this paper , we measure cross-lingual syntactic variation , or anisomorphism , in the UD treebank collection , considering both morphological and structural properties .",1,0.80210876,57.59997363553598,26
141,We show that reducing the level of anisomorphism yields consistent gains in cross-lingual transfer tasks .,3,0.96376824,22.671773470468157,16
141,"We introduce a source language selection procedure that facilitates effective cross-lingual parser transfer , and propose a typologically driven method for syntactic tree processing which reduces anisomorphism .",1,0.4241238,73.07956133372497,28
141,"Our results show the effectiveness of this method for both machine translation and cross-lingual sentence similarity , demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP .",3,0.98871124,18.513058411344623,31
142,Training language models for Code-mixed ( CM ) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language .,0,0.95280474,40.990302488967245,36
142,We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory .,1,0.5178969,72.85962474904832,20
142,"We show that when training examples are sampled appropriately from this synthetic data and presented in certain order ( aka training curriculum ) along with monolingual and real CM data , it can significantly reduce the perplexity of an RNN-based language model .",3,0.9178563,91.39032137248005,45
142,We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs .,3,0.97517866,44.77831388075095,19
143,"We investigate a lattice-structured LSTM model for Chinese NER , which encodes a sequence of input characters as well as all potential words that match a lexicon .",2,0.6018053,32.31804533742387,29
143,"Compared with character-based methods , our model explicitly leverages word and word sequence information .",3,0.70944875,45.39071720030724,17
143,"Compared with word-based methods , lattice LSTM does not suffer from segmentation errors .",3,0.74341846,26.12814600988607,15
143,Gated recurrent cells allow our model to choose the most relevant characters and words from a sentence for better NER results .,3,0.61336875,84.60025321341523,22
143,"Experiments on various datasets show that lattice LSTM outperforms both word-based and character-based LSTM baselines , achieving the best results .",3,0.94464105,14.127594478128943,24
144,"Neural network based models commonly regard event detection as a word-wise classification task , which suffer from the mismatch problem between words and event triggers , especially in languages without natural word delimiters such as Chinese .",0,0.8918266,45.157939904136605,37
144,"In this paper , we propose Nugget Proposal Networks ( NPNs ) , which can solve the word-trigger mismatch problem by directly proposing entire trigger nuggets centered at each character regardless of word boundaries .",1,0.8196444,87.68940969268814,35
144,"Specifically , NPNs perform event detection in a character-wise paradigm , where a hybrid representation for each character is first learned to capture both structural and semantic information from both characters and words .",0,0.5235466,54.091180903259726,36
144,"Then based on learned representations , trigger nuggets are proposed and categorized by exploiting character compositional structures of Chinese event triggers .",2,0.7246216,276.605408445983,22
144,Experiments on both ACE2005 and TAC KBP 2017 datasets show that NPNs significantly outperform the state-of-the-art methods .,3,0.94550157,14.583460003105483,23
145,Relation Schema Induction ( RSI ) is the problem of identifying type signatures of arguments of relations from unlabeled text .,0,0.90996855,46.835604158229884,21
145,"Most of the previous work in this area have focused only on binary RSI , i.e. , inducing only the subject and object type signatures per relation .",0,0.88335925,84.82581497665906,28
145,"However , in practice , many relations are high-order , i.e. , they have more than two arguments and inducing type signatures of all arguments is necessary .",0,0.69889176,76.45897496757532,28
145,"For example , in the sports domain , inducing a schema win( WinningPlayer , OpponentPlayer , Tournament , Location ) is more informative than inducing just win( WinningPlayer , OpponentPlayer ) .",3,0.5642507,63.24750156732102,32
145,We refer to this problem as Higher-order Relation Schema Induction ( HRSI ) .,0,0.4916799,24.44719099844335,14
145,"In this paper , we propose Tensor Factorization with Back-off and Aggregation ( TFBA ) , a novel framework for the HRSI problem .",1,0.84879404,58.74625495514471,24
145,"To the best of our knowledge , this is the first attempt at inducing higher-order relation schemata from unlabeled text .",3,0.8728715,15.089009151722262,22
145,Using the experimental analysis on three real world datasets we show how TFBA helps in dealing with sparsity and induce higher-order schemata .,3,0.5966797,49.73194304101176,25
146,State-of-the-art relation extraction approaches are only able to recognize relationships between mentions of entity arguments stated explicitly in the text and typically localized to the same sentence .,0,0.83182585,37.37029262094726,32
146,"However , the vast majority of relations are either implicit or not sententially localized .",0,0.8135259,121.77342798227647,15
146,"This is a major problem for Knowledge Base Population , severely limiting recall .",0,0.85060745,223.0644380410661,14
146,"In this paper we propose a new methodology to identify relations between two entities , consisting of detecting a very large number of unary relations , and using them to infer missing entities .",1,0.8906142,30.483078204791894,34
146,We describe a deep learning architecture able to learn thousands of such relations very efficiently by using a common deep learning based representation .,2,0.31110176,67.18829341718354,24
146,"Our approach largely outperforms state of the art relation extraction technology on a newly introduced web scale knowledge base population benchmark , that we release to the research community .",3,0.76447046,87.14581085863594,30
147,Entity linking involves aligning textual mentions of named entities to their corresponding entries in a knowledge base .,0,0.70630604,41.941505079864854,18
147,"Entity linking systems often exploit relations between textual mentions in a document ( e.g. , coreference ) to decide if the linking decisions are compatible .",0,0.89892554,83.91931684604586,26
147,"Unlike previous approaches , which relied on supervised systems or heuristics to predict these relations , we treat relations as latent variables in our neural entity-linking model .",2,0.74594706,43.057113952170525,30
147,We induce the relations without any supervision while optimizing the entity-linking system in an end-to-end fashion .,2,0.72559756,36.72619837725977,20
147,Our multi-relational model achieves the best reported scores on the standard benchmark ( AIDA-CoNLL ) and substantially outperforms its relation-agnostic version .,3,0.90007347,46.47207944869796,26
147,"Its training also converges much faster , suggesting that the injected structural bias helps to explain regularities in the training data .",3,0.9469779,74.49971554269936,22
148,"Document date is essential for many important tasks , such as document retrieval , summarization , event detection , etc .",0,0.91702056,55.66942727006189,21
148,"While existing approaches for these tasks assume accurate knowledge of the document date , this is not always available , especially for arbitrary documents from the Web .",0,0.88964987,83.43035267846784,28
148,Document Dating is a challenging problem which requires inference over the temporal structure of the document .,0,0.9435108,34.31103929846806,17
148,Prior document dating systems have largely relied on handcrafted features while ignoring such document-internal structures .,0,0.9462717,102.97435735769426,16
148,"In this paper , we propose NeuralDater , a Graph Convolutional Network ( GCN ) based document dating approach which jointly exploits syntactic and temporal graph structures of document in a principled way .",1,0.8340047,57.27703299579118,34
148,"To the best of our knowledge , this is the first application of deep learning for the problem of document dating .",3,0.8968935,12.880620636298978,22
148,"Through extensive experiments on real-world datasets , we find that NeuralDater significantly outperforms state-of-the-art baseline by 19 % absolute ( 45 % relative ) accuracy points .",3,0.91054857,32.393320427441104,33
149,The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph .,0,0.8721653,23.356656202349708,24
149,"The current state-of-the-art method uses a sequence-to-sequence model , leveraging LSTM for encoding a linearized AMR structure .",0,0.7015117,13.697903790169745,25
149,"Although being able to model non-local semantic information , a sequence LSTM can lose information from the AMR graph structure , and thus facing challenges with large-graphs , which result in long sequences .",0,0.6229491,89.35273560870357,34
149,"We introduce a neural graph-to-sequence model , using a novel LSTM structure for directly encoding graph-level semantics .",2,0.7630693,34.29482959721358,21
149,"On a standard benchmark , our model shows superior results to existing methods in the literature .",3,0.9432677,35.042127467236575,17
150,"A knowledge base is a large repository of facts that are mainly represented as RDF triples , each of which consists of a subject , a predicate ( relationship ) , and an object .",0,0.8189996,46.64630278871453,35
150,The RDF triple representation offers a simple interface for applications to access the facts .,0,0.6617933,188.73646499853552,15
150,"However , this representation is not in a natural language form , which is difficult for humans to understand .",0,0.89418703,22.900594750287137,20
150,We address this problem by proposing a system to translate a set of RDF triples into natural sentences based on an encoder-decoder framework .,1,0.39252123,17.935353568829555,24
150,"To preserve as much information from RDF triples as possible , we propose a novel graph-based triple encoder .",2,0.38954577,32.582781055239806,21
150,The proposed encoder encodes not only the elements of the triples but also the relationships both within a triple and between the triples .,3,0.4385262,20.742358734795758,24
150,"Experimental results show that the proposed encoder achieves a consistent improvement over the baseline models by up to 17.6 % , 6.0 % , and 16.4 % in three common metrics BLEU , METEOR , and TER , respectively .",3,0.9395557,19.47259259372227,40
151,"Despite their local fluency , long-form text generated from RNNs is often generic , repetitive , and even self-contradictory .",0,0.8753345,49.20738812986847,21
151,We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations .,3,0.5664818,89.33722811410331,32
151,"More concretely , discriminators each specialize in a different principle of communication , such as Grice ’s maxims , and are collectively combined with the base RNN generator through a composite decoding objective .",2,0.35601732,156.70576307608061,34
151,"Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin , significantly enhancing the overall coherence , style , and information of the generations .",3,0.9384576,56.19435028696078,34
152,Automatic pun generation is an interesting and challenging text generation task .,0,0.9321079,31.695844352221084,12
152,"Previous efforts rely on templates or laboriously manually annotated pun datasets , which heavily constrains the quality and diversity of generated puns .",0,0.891898,49.940362526550665,23
152,"Since sequence-to-sequence models provide an effective technique for text generation , it is promising to investigate these models on the pun generation task .",0,0.6342974,30.011998791877488,26
152,"In this paper , we propose neural network models for homographic pun generation , and they can generate puns without requiring any pun data for training .",1,0.8469686,36.58612168800679,27
152,"We first train a conditional neural language model from a general text corpus , and then generate puns from the language model with an elaborately designed decoding algorithm .",2,0.8861883,38.76494043385089,29
152,Automatic and human evaluations show that our models are able to generate homographic puns of good readability and quality .,3,0.94529754,31.666507022934145,20
153,This paper examines the problem of generating natural language descriptions of chess games .,1,0.8464487,29.522827687805197,14
153,We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game .,1,0.48230177,31.45399196478097,22
153,The introduced dataset consists of more than 298 K chess move-commentary pairs across 11 K chess games .,2,0.7552033,84.77223817036544,18
153,We highlight how this task poses unique research challenges in natural language generation : the data contain a large variety of styles of commentary and frequently depend on pragmatic context .,3,0.63307434,85.91669813812324,31
153,We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that may be commented upon to describe a given chess move .,2,0.57340264,38.27294325524698,36
153,"Through a human study on predictions for a subset of the data which deals with direct move descriptions , we observe that outputs from our models are rated similar to ground truth commentary texts in terms of correctness and fluency .",3,0.7708945,68.81130614083258,41
154,"In this work , we study the credit assignment problem in reward augmented maximum likelihood ( RAML ) learning , and establish a theoretical equivalence between the token-level counterpart of RAML and the entropy regularized reinforcement learning .",1,0.76851374,61.311812175421004,40
154,"Inspired by the connection , we propose two sequence prediction algorithms , one extending RAML with fine-grained credit assignment and the other improving Actor-Critic with a systematic entropy regularization .",2,0.58137506,85.98493739978295,31
154,"On two benchmark datasets , we show the proposed algorithms outperform RAML and Actor-Critic respectively , providing new alternatives to sequence prediction .",3,0.88803333,114.97324833741953,23
155,"We propose DuoRC , a novel dataset for Reading Comprehension ( RC ) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets .",1,0.56676656,62.30709537472567,32
155,"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie-one from Wikipedia and the other from IMDb-written by two different authors .",0,0.451393,59.417342408378,45
155,We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version .,2,0.9444949,31.290672061774274,29
155,"This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story , ensures by design , that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version .",3,0.62966603,47.10568183408209,52
155,"Further , since the two versions have different levels of plot detail , narration style , vocabulary , etc. , answering questions from the second version requires deeper language understanding and incorporating external background knowledge .",3,0.6393372,111.04109734960706,36
155,"Additionally , the narrative style of passages arising from movie plots ( as opposed to typical descriptive passages in existing datasets ) exhibits the need to perform complex reasoning over events across multiple sentences .",0,0.68348676,94.45728090481727,35
155,"Indeed , we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset , even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance ( F1 score of 37.42 % on DuoRC v/s 86 % on SQuAD dataset ) .",3,0.9595115,30.77347589581068,60
155,This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding .,3,0.9645989,97.62544183640665,24
156,We propose a simple yet robust stochastic answer network ( SAN ) that simulates multi-step reasoning in machine reading comprehension .,1,0.6513829,37.85731068202314,21
156,"Compared to previous work such as ReasoNet which used reinforcement learning to determine the number of steps , the unique feature is the use of a kind of stochastic prediction dropout on the answer module ( final layer ) of the neural network during the training .",3,0.5927386,51.896342410577155,47
156,"We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset ( SQuAD ) , the Adversarial SQuAD , and the Microsoft MAchine Reading COmprehension Dataset ( MS MARCO ) .",3,0.816455,24.30805023678215,46
157,"This paper describes a novel hierarchical attention network for reading comprehension style question answering , which aims to answer questions for a given narrative paragraph .",1,0.8998134,32.1816126629384,26
157,"In the proposed method , attention and fusion are conducted horizontally and vertically across layers at different levels of granularity between question and paragraph .",2,0.5810214,66.35507185804545,25
157,"Specifically , it first encode the question and paragraph with fine-grained language embeddings , to better capture the respective representations at semantic level .",2,0.619393,51.43507091807134,25
157,Then it proposes a multi-granularity fusion approach to fully fuse information from both global and attended representations .,2,0.5047104,49.23839372067081,18
157,"Finally , it introduces a hierarchical attention network to focuses on the answer span progressively with multi-level soft-alignment .",2,0.40339416,104.2579119782844,19
157,"Extensive experiments on the large-scale SQuAD , TriviaQA dataset validate the effectiveness of the proposed method .",3,0.7507227,11.66071656048588,17
157,"At the time of writing the paper , our model achieves state-of-the-art on the both SQuAD and TriviaQA Wiki leaderboard as well as two adversarial SQuAD datasets .",3,0.6086516,12.114572712110139,33
158,"While sophisticated neural-based techniques have been developed in reading comprehension , most approaches model the answer in an independent manner , ignoring its relations with other answer candidates .",0,0.9164512,62.05394417512762,31
158,"This problem can be even worse in open-domain scenarios , where candidates from multiple passages should be combined to answer a single question .",0,0.7135886,30.377783880611148,24
158,"In this paper , we formulate reading comprehension as an extract-then-select two-stage procedure .",1,0.7941466,36.516248995226036,18
158,"We first extract answer candidates from passages , then select the final answer by combining information from all the candidates .",2,0.8619494,55.55987676861766,21
158,"Furthermore , we regard candidate extraction as a latent variable and train the two-stage process jointly with reinforcement learning .",2,0.7445985,43.120437728052266,21
158,"As a result , our approach has improved the state-of-the-art performance significantly on two challenging open-domain reading comprehension datasets .",3,0.94593316,8.925866409584438,26
158,"Further analysis demonstrates the effectiveness of our model components , especially the information fusion of all the candidates and the joint training of the extract-then-select procedure .",3,0.97151756,46.82997659133905,31
159,Neural models for question answering ( QA ) over documents have achieved significant performance improvements .,0,0.921654,32.380023859369835,16
159,"Although effective , these models do not scale to large corpora due to their complex modeling of interactions between the document and the question .",0,0.83877265,26.860359086612167,25
159,"Moreover , recent work has shown that such models are sensitive to adversarial inputs .",0,0.90465677,20.190906587942226,15
159,"In this paper , we study the minimal context required to answer the question , and find that most questions in existing datasets can be answered with a small set of sentences .",1,0.79782,23.06151556864768,33
159,"Inspired by this observation , we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model .",2,0.4939221,25.815230354453703,25
159,"Our overall system achieves significant reductions in training ( up to 15 times ) and inference times ( up to 13 times ) , with accuracy comparable to or better than the state-of-the-art on SQuAD , NewsQA , TriviaQA and SQuAD-Open .",3,0.92320144,16.23712212313034,49
159,"Furthermore , our experimental results and analyses show that our approach is more robust to adversarial inputs .",3,0.97560173,14.450216130386716,18
160,Distantly supervised open-domain question answering ( DS-QA ) aims to find answers in collections of unlabeled text .,0,0.8950237,29.779316844457124,20
160,Existing DS-QA models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph .,0,0.86164564,37.26276411329064,27
160,They ignore the rich information contained in other paragraphs .,0,0.69209754,60.53376989002779,10
160,"Moreover , distant supervision data inevitably accompanies with the wrong labeling problem , and these noisy data will substantially degrade the performance of DS-QA .",0,0.7227381,105.30594637104355,27
160,"To address these issues , we propose a novel DS-QA model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs .",2,0.40577775,27.871542916933663,38
160,Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines .,3,0.9512495,12.108557795235997,30
161,Answer selection is an important subtask of community question answering ( CQA ) .,0,0.95726097,32.708715549378155,14
161,"In a real-world CQA forum , a question is often represented as two parts : a subject that summarizes the main points of the question , and a body that elaborates on the subject in detail .",0,0.89892226,26.895585000416943,37
161,Previous researches on answer selection usually ignored the difference between these two parts and concatenated them as the question representation .,0,0.8608748,62.37626970275884,21
161,"In this paper , we propose the Question Condensing Networks ( QCN ) to make use of the subject-body relationship of community questions .",1,0.8788762,56.665342516979955,25
161,"In our model , the question subject is the primary part of the question representation , and the question body information is aggregated based on similarity and disparity with the question subject .",2,0.68721485,60.34436767455977,33
161,Experimental results show that QCN outperforms all existing models on two CQA datasets .,3,0.9641739,18.465793473705606,14
162,Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation ( NMT ) models .,0,0.93146384,28.546480437343575,24
162,"In this paper , we propose to improve the robustness of NMT models with adversarial stability training .",1,0.8966591,22.275692792668323,18
162,The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart .,0,0.48426774,19.47832243934196,33
162,"Experimental results on Chinese-English , English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models .",3,0.944919,8.530824845102137,37
163,"In neural machine translation , a source sequence of words is encoded into a vector from which a target sequence is generated in the decoding phase .",0,0.8477472,25.776723707772533,27
163,"Differently from statistical machine translation , the associations between source words and their possible target counterparts are not explicitly stored .",0,0.8250159,122.59795828207778,21
163,"Source and target words are at the two ends of a long information processing procedure , mediated by hidden states at both the source encoding and the target decoding phases .",0,0.7810582,62.43146797601208,31
163,This makes it possible that a source word is incorrectly translated into a target word that is not any of its admissible equivalent counterparts in the target language .,0,0.5817788,23.322428473698693,29
163,"In this paper , we seek to somewhat shorten the distance between source and target words in that procedure , and thus strengthen their association , by means of a method we term bridging source and target word embeddings .",1,0.84723264,40.23637866164823,40
163,"We experiment with three strategies : ( 1 ) a source-side bridging model , where source word embeddings are moved one step closer to the output target sequence ;",2,0.86102813,62.425990601848554,29
163,"( 2 ) a target-side bridging model , which explores the more relevant source word embeddings for the prediction of the target sequence ;",2,0.6855008,115.7812122234006,25
163,"and ( 3 ) a direct bridging model , which directly connects source and target word embeddings seeking to minimize errors in the translation of ones by the others .",2,0.50512177,79.1565121636889,30
163,"Experiments and analysis presented in this paper demonstrate that the proposed bridging models are able to significantly improve quality of both sentence translation , in general , and alignment and translation of individual source words with target words , in particular .",3,0.9548042,43.34621550524506,42
164,"We present a study on reinforcement learning ( RL ) from human bandit feedback for sequence-to-sequence learning , exemplified by the task of bandit neural machine translation ( NMT ) .",1,0.6881992,33.69013282326352,33
164,"We investigate the reliability of human bandit feedback , and analyze the influence of reliability on the learnability of a reward estimator , and the effect of the quality of reward estimates on the overall RL task .",2,0.4549376,49.433629462228744,38
164,Our analysis of cardinal ( 5-point ratings ) and ordinal ( pairwise preferences ) feedback shows that their intra-and inter-annotator α-agreement is comparable .,3,0.96551925,82.17547244134326,26
164,"Best reliability is obtained for standardized cardinal feedback , and cardinal feedback is also easiest to learn and generalize from .",3,0.8594563,139.41660674541043,21
164,"Finally , improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into RL for NMT .",3,0.8881816,75.18322763046721,30
164,"This shows that RL is possible even from small amounts of fairly reliable human feedback , pointing to a great potential for applications at larger scale .",3,0.9847497,76.46524608335662,27
165,"With parallelizable attention networks , the neural Transformer is very fast to train .",3,0.5988928,69.94143525912006,14
165,"However , due to the auto-regressive architecture and self-attention in the decoder , the decoding procedure becomes slow .",0,0.5698701,24.726964423592197,19
165,"To alleviate this issue , we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer .",2,0.49836665,18.33964313155845,26
165,"The average attention network consists of two layers , with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network .",2,0.61225927,38.49498426985102,40
165,We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model .,2,0.7128514,31.785959519449907,22
165,"With masking tricks and dynamic programming , our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance .",3,0.8098525,60.12295274436533,35
165,"We conduct a series of experiments on WMT17 translation tasks , where on 6 different language pairs , we obtain robust and consistent speed-ups in decoding .",2,0.7417306,44.61336491357936,27
166,"With recent advances in network architectures for Neural Machine Translation ( NMT ) recurrent models have effectively been replaced by either convolutional or self-attentional approaches , such as in the Transformer .",0,0.9099128,24.431377119758803,32
166,"While the main innovation of the Transformer architecture is its use of self-attentional layers , there are several other aspects , such as attention with multiple heads and the use of many attention layers , that distinguish the model from previous baselines .",3,0.5652255,27.328366380700004,43
166,In this work we take a fine-grained look at the different architectures for NMT .,1,0.8011215,14.763627681432968,15
166,We introduce an Architecture Definition Language ( ADL ) allowing for a flexible combination of common building blocks .,2,0.5300281,71.67584882343013,19
166,"Making use of this language we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture , but not using self-attention .",3,0.6486194,33.581141785272635,36
166,"Additionally , we find that self-attention is much more important on the encoder side than on the decoder side , where it can be replaced by a RNN or CNN without a loss in performance in most settings .",3,0.9748428,16.76598930608347,39
166,"Surprisingly , even a model without any target side self-attention performs well .",3,0.9544673,111.78427011617491,13
167,Training semantic parsers from weak supervision ( denotations ) rather than strong supervision ( programs ) complicates training in two ways .,0,0.45744953,83.22207222085689,22
167,"First , a large search space of potential programs needs to be explored at training time to find a correct program .",0,0.60686064,41.11825246130156,22
167,"Second , spurious programs that accidentally lead to a correct denotation add noise to training .",0,0.70953035,203.98521658516708,16
167,"In this work we propose that in closed worlds with clear semantic types , one can substantially alleviate these problems by utilizing an abstract representation , where tokens in both the language utterance and program are lifted to an abstract form .",1,0.6716132,79.14100058476392,42
167,We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training .,3,0.8626953,39.635490106761026,30
167,"To test our approach , we develop the first semantic parser for CNLVR , a challenging visual reasoning dataset , where the search space is large and overcoming spuriousness is critical , because denotations are either TRUE or FALSE , and thus random programs are likely to lead to a correct denotation .",2,0.5728824,85.55268991703687,53
167,"Our method substantially improves performance , and reaches 82.5 % accuracy , a 14.7 % absolute accuracy improvement compared to the best reported accuracy so far .",3,0.9206445,54.152847648980405,27
168,Counterfactual learning from human bandit feedback describes a scenario where user feedback on the quality of outputs of a historic system is logged and used to improve a target system .,0,0.87927234,59.313085765422564,31
168,We show how to apply this learning framework to neural semantic parsing .,3,0.4538021,31.51808548442125,13
168,"From a machine learning perspective , the key challenge lies in a proper reweighting of the estimator so as to avoid known degeneracies in counterfactual learning , while still being applicable to stochastic gradient optimization .",0,0.76286244,38.08190652503716,36
168,"To conduct experiments with human users , we devise an easy-to-use interface to collect human feedback on semantic parses .",2,0.68973064,31.066403047842186,24
168,Our work is the first to show that semantic parsers can be improved significantly by counterfactual learning from logged human feedback data .,3,0.969766,35.984358029876326,23
169,We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph .,2,0.36043286,30.628772650131243,26
169,"This allows us to use standard neural techniques for supertagging and dependency tree parsing , constrained by a linguistically principled type system .",2,0.4029373,120.79830078901637,23
169,"We present two approximative decoding algorithms , which achieve state-of-the-art accuracy and outperform strong baselines .",3,0.3393793,17.050853093277645,22
170,"In this paper , we present a sequence-to-sequence based approach for mapping natural language sentences to AMR semantic graphs .",1,0.89824486,18.076711848982143,22
170,We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system .,2,0.82041764,183.3394721998692,28
170,"To address the sparsity issue of neural AMR parsing , we feed feature embeddings from the transition state to provide relevant local information for each decoder state .",2,0.7310155,57.51500223132789,28
170,We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus .,2,0.6130701,78.24777913746806,30
170,"We evaluate our neural transition model on the AMR parsing task , and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models .",3,0.76291853,23.886344647366435,32
171,Stochastic Gradient Descent ( SGD ) with negative sampling is the most prevalent approach to learn word representations .,0,0.7907529,37.76197315735289,19
171,"However , it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution .",0,0.9432888,29.943356773646666,22
171,"Besides , SGD suffers from dramatic fluctuation due to the one-sample learning scheme .",3,0.5039917,84.90930046433881,14
171,"In this work , we propose AllVec that uses batch gradient learning to generate word representations from all training samples .",1,0.4671513,76.7572753098761,21
171,"Remarkably , the time complexity of AllVec remains at the same level as SGD , being determined by the number of positive samples rather than all samples .",3,0.9488894,67.49105866059908,28
171,We evaluate AllVec on several benchmark tasks .,2,0.5114055,187.23163398486068,8
171,"Experiments show that AllVec outperforms sampling-based SGD methods with comparable efficiency , especially for small training corpora .",3,0.9635606,49.039626223978736,20
172,"We introduce structured projection of intermediate gradients ( SPIGOT ) , a new method for backpropagating through neural networks that include hard-decision structured predictions ( e.g. , parsing ) in intermediate layers .",2,0.65602463,211.6151362691912,33
172,"SPIGOT requires no marginal inference , unlike structured attention networks and reinforcement learning-inspired solutions .",3,0.43375355,449.582887482581,17
172,"Like so-called straight-through estimators , SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations , allowing backpropagation before and after them ;",0,0.41360813,301.59699222659924,26
172,"SPIGOT ’s proxy aims to ensure that , after a parameter update , the intermediate structure will remain well-formed .",0,0.44502154,121.16907102443705,22
172,"We experiment on two structured NLP pipelines : syntactic-then-semantic dependency parsing , and semantic parsing followed by sentiment classification .",2,0.8658894,48.005677923055536,22
172,"We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline , the straight-through estimator , and structured attention , reaching a new state of the art on semantic dependency parsing .",3,0.9581892,84.91537386796882,44
173,Heuristic-based active learning ( AL ) methods are limited when the data distribution of the underlying learning problems vary .,0,0.92103356,103.87918393119588,22
173,We introduce a method that learns an AL “ policy ” using “ imitation learning ” ( IL ) .,2,0.7023969,133.17448869786202,20
173,"Our IL-based approach makes use of an efficient and effective “ algorithmic expert ” , which provides the policy learner with good actions in the encountered AL situations .",2,0.52834976,94.63012505548745,31
173,"The AL strategy is then learned with a feedforward network , mapping situations to most informative query datapoints .",2,0.6842483,242.1475503759628,19
173,We evaluate our method on two different tasks : text classification and named entity recognition .,2,0.60203505,17.770929594926073,16
173,Experimental results show that our IL-based AL strategy is more effective than strong previous methods using heuristics and reinforcement learning .,3,0.975832,29.864631027982107,23
174,"Training accurate classifiers requires many labels , but each label provides only limited information ( one bit for binary classification ) .",0,0.67013246,183.03488124124635,22
174,"In this work , we propose BabbleLabble , a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision .",1,0.7098776,46.85119515624,27
174,"A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data , which is used to train a classifier .",2,0.42365178,43.36805789623797,30
174,"On three relation extraction tasks , we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels .",3,0.9492179,48.922530982134674,32
174,"Furthermore , given the inherent imperfection of labeling functions , we find that a simple rule-based semantic parser suffices .",3,0.9203832,52.572341329803876,21
175,"We analyze state-of-the-art deep learning models for three tasks : question answering on ( 1 ) images , ( 2 ) tables , and ( 3 ) passages of text .",2,0.7308018,24.723020737106623,37
175,"Using the notion of “ attribution ” ( word importance ) , we find that these deep networks often ignore important question terms .",3,0.8227032,122.64818507398354,24
175,"Leveraging such behavior , we perturb questions to craft a variety of adversarial examples .",2,0.5872927,95.76216924506315,15
175,"Our strongest attacks drop the accuracy of a visual question answering model from 61.1 % to 19 % , and that of a tabular question answering model from 33.5 % to 3.3 % .",3,0.9352096,21.59548992793096,34
175,"Additionally , we show how attributions can strengthen attacks proposed by Jia and Liang ( 2017 ) on paragraph comprehension models .",3,0.7417237,87.59755161428376,22
175,Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance .,3,0.9900975,91.6226706869591,18
175,"When a model is accurate but for the wrong reasons , attributions can surface erroneous logic in the model that indicates inadequacies in the test data .",0,0.76512605,54.990721901485976,27
176,We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence .,2,0.3712648,28.214435282386205,21
176,We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism .,2,0.47428995,25.536966948446363,16
176,"As compared to models that only take into account sentence-level information ( Heilman and Smith , 2010 ; Du et al. , 2017 ; Zhou et al. , 2017 ) , we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly , producing models that outperform the current state-of-the-art .",3,0.94639355,23.84584238218325,62
176,"We apply our system ( composed of an answer span extraction system and the passage-level QG system ) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs .",2,0.84465325,43.831545917079346,40
176,We provide qualitative analysis for the this large-scale generated corpus from Wikipedia .,3,0.67229325,83.46007564290674,13
177,Machine reading comprehension ( MRC ) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine .,0,0.9489602,64.62599774463729,27
177,"Compared with MRC on a single passage , multi-passage MRC is more challenging , since we are likely to get multiple confusing answer candidates from different passages .",0,0.52021396,56.825961282231425,28
177,"To address this problem , we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations .",1,0.40372634,25.369730415381024,31
177,"Specifically , we jointly train three modules that can predict the final answer based on three factors : the answer boundary , the answer content and the cross-passage answer verification .",2,0.81556886,43.097887741002474,33
177,"The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset , both of which are designed for MRC in real-world settings .",3,0.9230964,11.177363312065363,49
178,A DAG automaton is a formal device for manipulating graphs .,0,0.61738306,108.75938179623037,11
178,"By augmenting a DAG automaton with transduction rules , a DAG transducer has potential applications in fundamental NLP tasks .",3,0.40732586,40.80638086082556,20
178,"In this paper , we propose a novel DAG transducer to perform graph-to-program transformation .",1,0.89386284,26.165842573390954,18
178,The target structure of our transducer is a program licensed by a declarative programming language rather than linguistic structures .,2,0.43213,64.58099113493083,20
178,"By executing such a program , we can easily get a surface string .",0,0.44275093,170.49918658853463,14
178,Our transducer is designed especially for natural language generation ( NLG ) from type-logical semantic graphs .,2,0.5854354,48.01393079858838,17
178,"Taking Elementary Dependency Structures , a format of English Resource Semantics , as input , our NLG system achieves a BLEU-4 score of 68.07 .",3,0.77138716,69.74550817578893,27
178,"This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG , as well as the effectiveness of our design .",3,0.98610747,28.769021989921416,24
179,"Modeling derivational morphology to generate words with particular semantics is useful in many text generation tasks , such as machine translation or abstractive question answering .",0,0.8741446,46.809861336775846,26
179,"In this work , we tackle the task of derived word generation .",1,0.71152383,56.70007403756474,13
179,"That is , we attempt to generate the word “ runner ” for “ someone who runs .",2,0.4040365,64.75975448277644,18
179,We identify two key problems in generating derived words from root words and transformations .,3,0.712971,86.71359890378528,15
179,We contribute a novel aggregation model of derived word generation that learns derivational transformations both as orthographic functions using sequence-to-sequence models and as functions in distributional word embedding space .,2,0.39147633,45.5274638244177,32
179,The model then learns to choose between the hypothesis of each system .,2,0.6494054,90.29979627369961,13
179,We also present two ways of incorporating corpus information into derived word generation .,3,0.5000008,71.19535584656686,14
180,"In this paper , we propose a joint architecture that captures language , rhyme and meter for sonnet modelling .",1,0.8781787,108.63514315745165,20
180,We assess the quality of generated poems using crowd and expert judgements .,2,0.87289244,64.6119779504806,13
180,"The stress and rhyme models perform very well , as generated poems are largely indistinguishable from human-written poems .",3,0.854541,69.4204467636913,19
180,"Expert evaluation , however , reveals that a vanilla language model captures meter implicitly , and that machine-generated poems still underperform in terms of readability and emotion .",3,0.9039555,128.92170034024403,30
180,"Our research shows the importance expert evaluation for poetry generation , and that future research should look beyond rhyme / meter and focus on poetic language .",3,0.987735,135.8270246084343,27
181,"Traditionally , Referring Expression Generation ( REG ) models first decide on the form and then on the content of references to discourse entities in text , typically relying on features such as salience and grammatical function .",0,0.93782634,65.86670229537118,38
181,"In this paper , we present a new approach ( NeuralREG ) , relying on deep neural networks , which makes decisions about form and content in one go without explicit feature extraction .",1,0.83081484,99.18126152142722,34
181,"Using a delexicalized version of the WebNLG corpus , we show that the neural model substantially improves over two strong baselines .",3,0.76768744,23.68131738833081,22
182,"Stock movement prediction is a challenging problem : the market is highly stochastic , and we make temporally-dependent predictions from chaotic data .",0,0.89315593,71.3513505291354,25
182,We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task .,2,0.5789428,77.40637321615039,22
182,"Unlike the case with discriminative or topic modeling , our model introduces recurrent , continuous latent variables for a better treatment of stochasticity , and uses neural variational inference to address the intractable posterior inference .",2,0.57015973,68.36888945566879,36
182,We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies .,3,0.5219991,265.96278167114184,15
182,We demonstrate the state-of-the-art performance of our proposed model on a new stock movement prediction dataset which we collected .,3,0.5536263,17.73299462982472,25
183,Automatic rumor detection is technically very challenging .,0,0.9291621,76.04860975628709,8
183,"In this work , we try to learn discriminative features from tweets content by following their non-sequential propagation structure and generate more powerful representations for identifying different type of rumors .",1,0.6758451,66.36725459337896,31
183,"We propose two recursive neural models based on a bottom-up and a top-down tree-structured neural networks for rumor representation learning and classification , which naturally conform to the propagation layout of tweets .",2,0.60253215,40.9467680326009,35
183,Results on two public Twitter datasets demonstrate that our recursive neural models 1 ) achieve much better performance than state-of-the-art approaches ;,3,0.957087,27.02156229772089,28
183,2 ) demonstrate superior capacity on detecting rumors at very early stage .,3,0.83446896,375.832627868868,13
184,"Everyday billions of multimodal posts containing both images and text are shared in social media sites such as Snapchat , Twitter or Instagram .",0,0.93337244,30.59867207066845,24
184,"This combination of image and text in a single message allows for more creative and expressive forms of communication , and has become increasingly common in such sites .",0,0.82390594,39.35959349211166,29
184,"This new paradigm brings new challenges for natural language understanding , as the textual component tends to be shorter , more informal , and often is only understood if combined with the visual context .",0,0.84799284,58.94539570593742,35
184,"In this paper , we explore the task of name tagging in multimodal social media posts .",1,0.906179,20.712910236755462,17
184,We start by creating two new multimodal datasets : the first based on Twitter posts and the second based on Snapchat captions ( exclusively submitted to public and crowd-sourced stories ) .,2,0.91221136,45.764437191129765,32
184,"We then propose a novel model architecture based on Visual Attention that not only provides deeper visual understanding on the decisions of the model , but also significantly outperforms other state-of-the-art baseline methods for this task .",3,0.4380261,18.784678657988874,43
185,"We introduce the new Multimodal Named Entity Disambiguation ( MNED ) task for multimodal social media posts such as Snapchat or Instagram captions , which are composed of short captions with accompanying images .",2,0.33467135,24.09865060775176,34
185,"Social media posts bring significant challenges for disambiguation tasks because 1 ) ambiguity not only comes from polysemous entities , but also from inconsistent or incomplete notations , 2 ) very limited context is provided with surrounding words , and 3 ) there are many emerging entities often unseen during training .",0,0.8993278,77.10139576146707,52
185,"To this end , we build a new dataset called SnapCaptionsKB , a collection of Snapchat image captions submitted to public and crowd-sourced stories , with named entity mentions fully annotated and linked to entities in an external knowledge base .",2,0.8420881,51.21993720330474,41
185,"We then build a deep zeroshot multimodal network for MNED that 1 ) extracts contexts from both text and image , and 2 ) predicts correct entity in the knowledge graph embeddings space , allowing for zeroshot disambiguation of entities unseen in training set as well .",2,0.78156835,57.614477407754094,47
185,"The proposed model significantly outperforms the state-of-the-art text-only NED models , showing efficacy and potentials of the MNED task .",3,0.94277,23.93688375492302,25
186,Social media user geolocation is vital to many applications such as event detection .,0,0.94598424,37.88355802801025,14
186,"In this paper , we propose GCN , a multiview geolocation model based on Graph Convolutional Networks , that uses both text and network context .",1,0.840434,32.68059893660819,26
186,"We compare GCN to the state-of-the-art , and to two baselines we propose , and show that our model achieves or is competitive with the state-of-the-art over three benchmark geolocation datasets when sufficient supervision is available .",3,0.66928357,16.027967193545223,46
186,"We also evaluate GCN under a minimal supervision scenario , and show it outperforms baselines .",3,0.6841075,60.26471510032688,16
186,We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN .,3,0.98353136,159.63657388137938,19
187,Document modeling is essential to a variety of natural language understanding tasks .,0,0.92946374,13.289514436787663,13
187,We propose to use external information to improve document modeling for problems that can be framed as sentence extraction .,1,0.34590304,70.17865980087434,20
187,We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information .,2,0.7394332,30.65782103077172,22
187,We evaluate our model on extractive document summarization ( where the external information is image captions and the title of the document ) and answer selection ( where the external information is a question ) .,2,0.59095657,29.96210979561077,36
187,"We show that our model consistently outperforms strong baselines , in terms of both informativeness and fluency ( for CNN document summarization ) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA .",3,0.9243293,17.26815542700598,41
188,"Most real-world document collections involve various types of metadata , such as author , source , and date , and yet the most commonly-used approaches to modeling text corpora ignore this information .",0,0.95166886,35.270046322563054,35
188,"While specialized models have been developed for particular applications , few are widely used in practice , as customization typically requires derivation of a custom inference algorithm .",0,0.8973911,64.76512780085363,28
188,"In this paper , we build on recent advances in variational inference methods and propose a general neural framework , based on topic models , to enable flexible incorporation of metadata and allow for rapid exploration of alternative models .",1,0.8120298,47.03938777977752,40
188,"Our approach achieves strong performance , with a manageable tradeoff between perplexity , coherence , and sparsity .",3,0.8161822,66.10949040852174,18
188,"Finally , we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration .",3,0.7212396,27.397401406241883,21
189,Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems .,0,0.94579864,75.85148723874367,17
189,"While fairly successful , previous techniques generally require two-stage training , and the binary constraints are handled ad-hoc .",0,0.645197,116.32174949281512,21
189,"In this paper , we present an end-to-end Neural Architecture for Semantic Hashing ( NASH ) , where the binary hashing codes are treated as Bernoulli latent variables .",1,0.77410007,32.0714300065532,29
189,"A neural variational inference framework is proposed for training , where gradients are directly backpropagated through the discrete latent variable to optimize the hash function .",2,0.6830692,46.206394624811615,26
189,"We also draw the connections between proposed method and rate-distortion theory , which provides a theoretical foundation for the effectiveness of our framework .",3,0.6163945,43.2039773788587,24
189,Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsupervised and supervised scenarios .,3,0.9323256,4.714682011033832,28
190,"We present a new large-scale corpus of Question-Answer driven Semantic Role Labeling ( QA-SRL ) annotations , and the first high-quality QA-SRL parser .",1,0.4209073,25.63038257679993,30
190,"Our corpus , QA-SRL Bank 2.0 , consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that we show has high precision and good recall at modest cost .",2,0.5405725,43.93148050289416,47
190,We also present neural models for two QA-SRL subtasks : detecting argument spans for a predicate and generating questions to label the semantic relationship .,2,0.5036637,133.92989023664654,27
190,The best models achieve question accuracy of 82.6 % and span-level accuracy of 77.6 % ( under human evaluation ) on the full pipelined QA-SRL prediction task .,3,0.92969227,50.591630796261384,30
190,"They can also , as we show , be used to gather additional annotations at low cost .",3,0.80597353,61.740849946377516,18
191,Semantic role labeling ( SRL ) is dedicated to recognizing the predicate-argument structure of a sentence .,0,0.9035124,45.87869870647077,19
191,Previous studies have shown syntactic information has a remarkable contribution to SRL performance .,0,0.88775885,54.07810559255591,14
191,"However , such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone .",0,0.9058479,100.64509312327246,22
191,This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework .,1,0.90793604,80.95506078248792,18
191,We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information .,2,0.38629216,236.94414993365663,21
191,"Our model achieves state-of-the-art results on the CoNLL-2008 , 2009 benchmarks for both English and Chinese , showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models .",3,0.8851914,50.56173797091133,42
192,We propose a learning approach for mapping context-dependent sequential instructions to actions .,1,0.46674338,65.17169951248101,13
192,We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world .,2,0.51154125,18.170875941069852,30
192,"To train from start and goal states without access to demonstrations , we propose SESTRA , a learning algorithm that takes advantage of single-step reward observations and immediate expected reward maximization .",2,0.5685967,144.3283224262067,32
192,"We evaluate on the SCONE domains , and show absolute accuracy improvements of 9.8%-25.3 % across the domains over approaches that use high-level logical representations .",3,0.8644827,69.35480295844647,29
193,"The success of many natural language processing ( NLP ) tasks is bound by the number and quality of annotated data , but there is often a shortage of such training data .",0,0.96362543,16.206212697700224,33
193,"In this paper , we ask the question : “ Can we combine a neural network ( NN ) with regular expressions ( RE ) to improve supervised learning for NLP ? ” .",1,0.88848066,47.928334746795876,34
193,"In answer , we develop novel methods to exploit the rich expressiveness of REs at different levels within a NN , showing that the combination significantly enhances the learning effectiveness when a small number of training examples are available .",3,0.7183784,43.62271591938648,40
193,We evaluate our approach by applying it to spoken language understanding for intent detection and slot filling .,2,0.41076759,48.92747678521166,18
193,"Experimental results show that our approach is highly effective in exploiting the available training data , giving a clear boost to the RE-unaware NN .",3,0.9652714,35.35507990981164,27
194,"Despite the effectiveness of recurrent neural network language models , their maximum likelihood estimation suffers from two limitations .",0,0.84054166,27.666621021768293,19
194,"It treats all sentences that do not match the ground truth as equally poor , ignoring the structure of the output space .",2,0.3420435,58.05962551298589,23
194,"Second , it suffers from ’ exposure bias ’ : during training tokens are predicted given ground-truth sequences , while at test time prediction is conditioned on generated output sequences .",0,0.59082675,109.38813403169554,33
194,To overcome these limitations we build upon the recent reward augmented maximum likelihood approach that encourages the model to predict sentences that are close to the ground truth according to a given performance metric .,2,0.6636608,32.25523953399017,35
194,"We extend this approach to token-level loss smoothing , and propose improvements to the sequence-level smoothing approach .",2,0.3832846,33.05356303263551,22
194,"Our experiments on two different tasks , image captioning and machine translation , show that token-level and sequence-level loss smoothing are complementary , and significantly improve results .",3,0.95027125,36.99345111326644,32
195,Numeracy is the ability to understand and work with numbers .,0,0.92317563,35.655046228694545,11
195,"It is a necessary skill for composing and understanding documents in clinical , scientific , and other technical domains .",0,0.86174184,68.04518218348329,20
195,"In this paper , we explore different strategies for modelling numerals with language models , such as memorisation and digit-by-digit composition , and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary .",1,0.87964696,28.951942563262357,45
195,"Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude , respectively , over non-hierarchical models .",3,0.9421478,29.997977448730783,40
195,A combination of strategies can further improve perplexity .,3,0.5567262,58.72922588623332,9
195,"Our continuous probability density function model reduces mean absolute percentage errors by 18 % and 54 % in comparison to the second best strategy for each dataset , respectively .",3,0.94914466,107.0527011266504,30
196,"With the recent success of Recurrent Neural Networks ( RNNs ) in Machine Translation ( MT ) , attention mechanisms have become increasingly popular .",0,0.96151346,15.255208535967258,25
196,The purpose of this paper is two-fold ;,1,0.8473222,23.69567403826845,9
196,"firstly , we propose a novel attention model on Tree Long Short-Term Memory Networks ( Tree-LSTMs ) , a tree-structured generalization of standard LSTM .",2,0.56021106,23.135249848152636,29
196,"Secondly , we study the interaction between attention and syntactic structures , by experimenting with three LSTM variants : bidirectional-LSTMs , Constituency Tree-LSTMs , and Dependency Tree-LSTMs .",2,0.8098862,24.095450539957458,30
196,"Our models are evaluated on two semantic relatedness tasks : semantic relatedness scoring for sentence pairs ( SemEval 2012 , Task 6 and SemEval 2014 , Task 1 ) and paraphrase detection for question pairs ( Quora , 2017 ) .",2,0.7401432,33.38316971566479,41
197,"Although much effort has recently been devoted to training high-quality sentence embeddings , we still have a poor understanding of what they are capturing .",0,0.9197332,16.092774467281984,25
197,"“ Downstream ” tasks , often based on sentence classification , are commonly used to evaluate the quality of sentence representations .",0,0.8085579,82.09569273601957,22
197,The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations .,0,0.83094966,19.552274107983852,21
197,"We introduce here 10 probing tasks designed to capture simple linguistic features of sentences , and we use them to study embeddings generated by three different encoders trained in eight distinct ways , uncovering intriguing properties of both encoders and training methods .",2,0.58053046,50.714437930443445,43
198,Distant supervision has become the standard method for relation extraction .,0,0.9114859,42.568502712456045,11
198,The resulted distantly-supervised training samples are often very noisy .,3,0.51303965,41.225365948698276,12
198,"To combat the noise , most of the recent state-of-the-art approaches focus on selecting one-best sentence or calculating soft attention weights over the set of the sentences of one specific entity pair .",0,0.8580834,53.794517483979384,41
198,"However , these methods are suboptimal , and the false positive problem is still a key stumbling bottleneck for the performance .",0,0.8046956,58.40833659776791,22
198,"We argue that those incorrectly-labeled candidate sentences must be treated with a hard decision , rather than being dealt with soft attention weights .",3,0.84632695,98.55312296098694,26
198,"To do this , our paper describes a radical solution — We explore a deep reinforcement learning strategy to generate the false-positive indicator , where we automatically recognize false positives for each relation type without any supervised information .",1,0.40500575,79.49736200686468,40
198,"Unlike the removal operation in the previous studies , we redistribute them into the negative examples .",2,0.50163555,103.95202363537895,17
198,The experimental results show that the proposed strategy significantly improves the performance of distant supervision comparing to state-of-the-art systems .,3,0.98054093,8.743452710414434,26
199,Embedding models for entities and relations are extremely useful for recovering missing facts in a knowledge base .,0,0.8800188,41.94069511732807,18
199,"Intuitively , a relation can be modeled by a matrix mapping entity vectors .",0,0.6372722,65.59780608646594,14
199,"However , relations reside on low dimension sub-manifolds in the parameter space of arbitrary matrices – for one reason , composition of two relations M1 , M2 may match a third M3 ( e.g .",0,0.7445911,111.96479271642886,35
199,"composition of relations currency_of_country and country_of_film usually matches currency_of_film_budget ) , which imposes compositional constraints to be satisfied by the parameters ( i.e .",3,0.47645533,71.03848274663498,26
199,M1 * M2 =M3 ) .,3,0.4811626,104.64196261414264,7
199,"In this paper we investigate a dimension reduction technique by training relations jointly with an autoencoder , which is expected to better capture compositional constraints .",1,0.7886545,41.98902023638963,26
199,"We achieve state-of-the-art on Knowledge Base Completion tasks with strongly improved Mean Rank , and show that joint training with an autoencoder leads to interpretable sparse codings of relations , helps discovering compositional constraints and benefits from compositional training .",3,0.90449876,59.25838401329765,42
199,Our source code is released at github.com/tianran / glimvec .,3,0.4357811,51.94831048638533,12
200,"Most previous supervised event extraction methods have relied on features derived from manual annotations , and thus cannot be applied to new event types without extra annotation effort .",0,0.8351503,33.91839494348497,29
200,We take a fresh look at event extraction and model it as a generic grounding problem : mapping each event mention to a specific type in a target event ontology .,2,0.5455659,48.69382220292325,31
200,We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space .,2,0.7309043,41.130351581519925,26
200,"Based on this new framework , we can select , for each event mention , the event type which is semantically closest in this space as its type .",3,0.5003313,99.62786157879782,29
200,"By leveraging manual annotations available for a small set of existing event types , our framework can be applied to new unseen event types without additional manual annotations .",3,0.7595568,37.87599887768166,29
200,"When tested on 23 unseen event types , our zero-shot framework , without manual annotations , achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions .",3,0.8875889,74.7090336331905,33
201,Fine-grained opinion analysis aims to extract aspect and opinion terms from each sentence for opinion summarization .,0,0.8413076,45.62588009504161,17
201,Supervised learning methods have proven to be effective for this task .,0,0.9057635,16.287234964581504,12
201,"However , in many domains , the lack of labeled data hinders the learning of a precise extraction model .",0,0.9064245,40.93397143409887,20
201,"In this case , unsupervised domain adaptation methods are desired to transfer knowledge from the source domain to any unlabeled target domain .",0,0.82548714,22.312820608645257,23
201,"In this paper , we develop a novel recursive neural network that could reduce domain shift effectively in word level through syntactic relations .",1,0.90744334,75.6836329581551,24
201,We treat these relations as invariant “ pivot information ” across domains to build structural correspondences and generate an auxiliary task to predict the relation between any two adjacent words in the dependency tree .,2,0.8080269,59.98824557006835,35
201,"In the end , we demonstrate state-of-the-art results on three benchmark datasets .",3,0.5221502,7.297773722512161,19
202,Training a task-completion dialogue agent via reinforcement learning ( RL ) is costly because it requires many interactions with real users .,0,0.8367674,52.45288822351039,23
202,One common alternative is to use a user simulator .,0,0.78687924,42.18642850330374,10
202,"However , a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent .",0,0.8671812,71.08853195433099,26
202,"To address these issues , we present Deep Dyna-Q , which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning .",1,0.51977956,62.085583548969765,33
202,"We incorporate into the dialogue agent a model of the environment , referred to as the world model , to mimic real user response and generate simulated experience .",2,0.8342389,113.93386112594352,29
202,"During dialogue policy learning , the world model is constantly updated with real user experience to approach real user behavior , and in turn , the dialogue agent is optimized using both real experience and simulated experience .",2,0.5300881,97.58290309232882,38
202,The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings .,3,0.6017642,14.091441898580701,23
203,Asking good questions in open-domain conversational systems is quite significant but rather untouched .,0,0.765958,73.93729809934067,14
203,"This task , substantially different from traditional question generation , requires to question not only with various patterns but also on diverse and relevant topics .",0,0.8967845,157.19281654591944,26
203,"We observe that a good question is a natural composition of interrogatives , topic words , and ordinary words .",3,0.9479146,200.10141445447982,20
203,"Interrogatives lexicalize the pattern of questioning , topic words address the key information for topic transition in dialogue , and ordinary words play syntactical and grammatical roles in making a natural sentence .",0,0.8587437,153.66871730669075,33
203,We devise two typed decoders ( soft typed decoder and hard typed decoder ) in which a type distribution over the three types is estimated and the type distribution is used to modulate the final generation distribution .,2,0.8654342,30.872920083308788,38
203,Extensive experiments show that the typed decoders outperform state-of-the-art baselines and can generate more meaningful questions .,3,0.93557495,13.787631275816558,23
204,"Chit-chat models are known to have several problems : they lack specificity , do not display a consistent personality and are often not very captivating .",0,0.8349228,56.496906609717435,28
204,In this work we present the task of making chit-chat more engaging by conditioning on profile information .,1,0.8733497,22.364231688465228,19
204,We collect data and train models to ( i ) condition on their given profile information ;,2,0.88113725,307.12237521148234,17
204,"and ( ii ) information about the person they are talking to , resulting in improved dialogues , as measured by next utterance prediction .",3,0.486633,57.48766571547604,25
204,"Since ( ii ) is initially unknown our model is trained to engage its partner with personal topics , and we show the resulting dialogue can be used to predict profile information about the interlocutors .",3,0.49726087,86.53656120509456,36
205,"In this paper , we explore the task of mapping spoken language utterances to one of thousands of natural language understanding domains in intelligent personal digital assistants ( IPDAs ) .",1,0.89138067,43.49627384989442,31
205,This scenario is observed in mainstream IPDAs in industry that allow third parties to develop thousands of new domains to augment built-in first party domains to rapidly increase domain coverage and overall IPDA capabilities .,0,0.7650114,148.01441752861828,37
205,"We propose a scalable neural model architecture with a shared encoder , a novel attention mechanism that incorporates personalization information and domain-specific classifiers that solves the problem efficiently .",2,0.40852144,45.197502548966526,29
205,Our architecture is designed to efficiently accommodate incremental domain additions achieving two orders of magnitude speed up compared to full model retraining .,3,0.5252805,86.09558772753809,23
205,"We consider the practical constraints of real-time production systems , and design to minimize memory footprint and runtime latency .",2,0.5460588,59.24713895380761,20
205,We demonstrate that incorporating personalization significantly improves domain classification accuracy in a setting with thousands of overlapping domains .,3,0.946891,55.02293147964051,19
206,"Multimodal affective computing , learning to recognize and interpret human affect and subjective information from multiple data sources , is still a challenge because : ( i ) it is hard to extract informative features to represent human affects from heterogeneous inputs ;",0,0.9364153,78.7749293994358,43
206,"( ii ) current fusion strategies only fuse different modalities at abstract levels , ignoring time-dependent interactions between modalities .",0,0.7912516,146.0319865386582,22
206,"Addressing such issues , we introduce a hierarchical multimodal architecture with attention and word-level fusion to classify utterance-level sentiment and emotion from text and audio data .",2,0.39469796,37.82453383466793,29
206,"Our introduced model outperforms state-of-the-art approaches on published datasets , and we demonstrate that our model is able to visualize and interpret synchronized attention over modalities .",3,0.8863307,32.89729909913347,32
207,Analyzing human multimodal language is an emerging area of research in NLP .,0,0.957577,16.823653697325287,13
207,"Intrinsically this language is multimodal ( heterogeneous ) , sequential and asynchronous ;",0,0.8413279,190.96551047029988,13
207,"it consists of the language ( words ) , visual ( expressions ) and acoustic ( paralinguistic ) modalities all in the form of asynchronous coordinated sequences .",0,0.8286556,78.92539930432318,28
207,"From a resource perspective , there is a genuine need for large scale datasets that allow for in-depth studies of this form of language .",0,0.84873295,29.664929827513586,27
207,"In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity ( CMU-MOSEI ) , the largest dataset of sentiment analysis and emotion recognition to date .",1,0.86433107,19.671554539772707,30
207,"Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph ( DFG ) , we conduct experimentation to exploit how modalities interact with each other in human multimodal language .",2,0.6985155,51.00584924131189,36
207,"Unlike previously proposed fusion techniques , DFG is highly interpretable and achieves competative performance when compared to the previous state of the art .",3,0.76111156,39.56924546936376,24
208,"Multimodal research is an emerging field of artificial intelligence , and one of the main research problems in this field is multimodal fusion .",0,0.93942827,20.834800092348253,24
208,The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation .,0,0.9358486,13.905585608118717,19
208,Previous research in this field has exploited the expressiveness of tensors for multimodal representation .,0,0.94643295,25.44702826272732,15
208,"However , these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor .",0,0.8956654,65.33592673295458,23
208,"In this paper , we propose the Low-rank Multimodal Fusion method , which performs multimodal fusion using low-rank tensors to improve efficiency .",1,0.8744949,22.540265282884338,23
208,"We evaluate our model on three different tasks : multimodal sentiment analysis , speaker trait analysis , and emotion recognition .",2,0.6528919,46.950267690298574,21
208,Our model achieves competitive results on all these tasks while drastically reducing computational complexity .,3,0.8878931,25.928400367777076,15
208,"Additional experiments also show that our model can perform robustly for a wide range of low-rank settings , and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations .",3,0.95077467,27.200965216481244,38
209,Theories of discourse coherence posit relations between discourse segments as a key feature of coherent text .,0,0.91798615,54.156140065536135,17
209,Our prior work suggests that multiple discourse relations can be simultaneously operative between two segments for reasons not predicted by the literature .,0,0.794649,111.30562505702645,23
209,"Here we test how this joint presence can lead participants to endorse seemingly divergent conjunctions ( e.g. , BUT and SO ) to express the link they see between two segments .",1,0.6012661,153.12977473977205,32
209,"These apparent divergences are not symptomatic of participant naivety or bias , but arise reliably from the concurrent availability of multiple relations between segments – some available through explicit signals and some via inference .",3,0.8145844,127.01697946265847,35
209,We believe that these new results can both inform future progress in theoretical work on discourse coherence and lead to higher levels of performance in discourse parsing .,3,0.98028445,32.65642256485682,28
210,"We present a generative probabilistic model of documents as sequences of sentences , and show that inference in it can lead to extraction of long-range latent discourse structure from a collection of documents .",3,0.34011117,29.92338836569532,35
210,"The approach is based on embedding sequences of sentences from longer texts into a 2-or 3-D spatial grids , in which one or two coordinates model smooth topic transitions , while the third captures the sequential nature of the modeled text .",2,0.7403567,80.5713773688432,46
210,"A significant advantage of our approach is that the learned models are naturally visualizable and interpretable , as semantic similarity and sequential structure are modeled along orthogonal directions in the grid .",3,0.64573056,46.34036136554483,32
210,"We show that the method is effective in capturing discourse structures in narrative text across multiple genres , including biographies , stories , and newswire reports .",3,0.92102355,46.45725703106258,27
210,"In particular , our method outperforms or is competitive with state-of-the-art generative approaches on tasks such as predicting the outcome of a story , and sentence ordering .",3,0.9252902,19.83135745414956,34
211,Understanding temporal and causal relations between events is a fundamental natural language understanding task .,0,0.9535051,17.477106404075286,15
211,"Because a cause must occur earlier than its effect , temporal and causal relations are closely related and one relation often dictates the value of the other .",0,0.8848883,52.962101790286304,28
211,"However , limited attention has been paid to studying these two relations jointly .",0,0.9503029,53.05753527141113,14
211,This paper presents a joint inference framework for them using constrained conditional models ( CCMs ) .,1,0.7523476,91.94531343900155,17
211,"Specifically , we formulate the joint problem as an integer linear programming ( ILP ) problem , enforcing constraints that are inherent in the nature of time and causality .",2,0.721887,63.673504837065494,30
211,We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text .,3,0.97284603,30.509995226446023,24
212,Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people ’s mental states — a capability that is trivial for humans but remarkably hard for machines .,0,0.91119015,36.01456139408058,37
212,"To facilitate research addressing this challenge , we introduce a new annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions .",1,0.60136914,86.6401954689351,36
212,"Our work presents a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks , suggesting avenues for future research .",3,0.9537226,32.98291827760473,26
213,"In the era of big data , focused analysis for diverse topics with a short response time becomes an urgent demand .",0,0.94682294,80.66736781187062,22
213,"As a fundamental task , information filtering therefore becomes a critical necessity .",0,0.935851,193.41400082192018,13
213,"In this paper , we propose a novel deep relevance model for zero-shot document filtering , named DAZER .",1,0.87926245,48.45948104340475,19
213,DAZER estimates the relevance between a document and a category by taking a small set of seed words relevant to the category .,2,0.44312912,49.33397422575476,23
213,"With pre-trained word embeddings from a large external corpus , DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space .",2,0.6891407,36.888554794460134,30
213,The relevance signals are extracted through a gated convolutional process .,2,0.6760525,74.15896721519722,11
213,The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner .,3,0.35946226,197.5535700878587,17
213,"Experiments on two document collections of two different tasks ( i.e. , topic categorization and sentiment analysis ) demonstrate that DAZER significantly outperforms the existing alternative solutions , including the state-of-the-art deep relevance ranking models .",3,0.8835184,36.99636180336543,42
214,Recurrent neural network ( RNN ) has achieved remarkable performance in text categorization .,0,0.94293326,28.642005740596023,14
214,"RNN can model the entire sequence and capture long-term dependencies , but it does not do well in extracting key patterns .",0,0.6470444,58.6053537668825,22
214,"In contrast , convolutional neural network ( CNN ) is good at extracting local and position-invariant features .",0,0.8403646,44.896588873306975,20
214,"In this paper , we present a novel model named disconnected recurrent neural network ( DRNN ) , which incorporates position-invariance into RNN .",1,0.84118354,44.77151379662247,26
214,"By limiting the distance of information flow in RNN , the hidden state at each time step is restricted to represent words near the current position .",2,0.44364986,55.16560917535469,27
214,The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text categorization .,3,0.8905353,21.66678905950264,24
215,"Word embeddings are effective intermediate representations for capturing semantic regularities between words , when learning the representations of text sequences .",0,0.8599943,52.95106681177459,21
215,We propose to view text classification as a label-word joint embedding problem : each label is embedded in the same space with the word vectors .,2,0.5610599,44.65327020649583,28
215,We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels .,2,0.5957838,36.27057316243856,17
215,"The attention is learned on a training set of labeled samples to ensure that , given a text sequence , the relevant words are weighted higher than the irrelevant ones .",2,0.5893613,51.2452584897861,31
215,"Our method maintains the interpretability of word embeddings , and enjoys a built-in ability to leverage alternative sources of information , in addition to input text sequences .",3,0.7343018,44.03509690138818,30
215,"Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin , in terms of both accuracy and speed .",3,0.92220074,8.00040822873444,36
216,"Topic models with sparsity enhancement have been proven to be effective at learning discriminative and coherent latent topics of short texts , which is critical to many scientific and engineering applications .",0,0.9181016,35.19670340682273,32
216,"However , the extensions of these models require carefully tailored graphical models and re-deduced inference algorithms , limiting their variations and applications .",0,0.84649354,142.08658624545674,23
216,"We propose a novel sparsity-enhanced topic model , Neural Sparse Topical Coding ( NSTC ) base on a sparsity-enhanced topic model called Sparse Topical Coding ( STC ) .",2,0.39184698,20.395005080942546,33
216,"It focuses on replacing the complex inference process with the back propagation , which makes the model easy to explore extensions .",2,0.40239698,98.6720889981981,22
216,"Moreover , the external semantic information of words in word embeddings is incorporated to improve the representation of short texts .",2,0.49622193,24.05919940544747,21
216,"To illustrate the flexibility offered by the neural network based framework , we present three extensions base on NSTC without re-deduced inference algorithms .",2,0.44902384,99.40521133966682,24
216,Experiments on Web Snippet and 20 Newsgroups datasets demonstrate that our models outperform existing methods .,3,0.88536876,23.021231805189387,16
217,Measuring similarity between texts is an important task for several applications .,0,0.94836724,29.539225583443464,12
217,"Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths , such as a long document and its summary .",0,0.88422453,42.866494039853606,25
217,"This is because of the lexical , contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information .",0,0.87765115,89.7836302958657,27
217,"In this paper , we present a document matching approach to bridge this gap , by comparing the texts in a common space of hidden topics .",1,0.867693,64.51850830979232,27
217,We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines .,3,0.65024483,15.984859127516911,20
217,We also highlight the benefits of the incorporation of domain knowledge to text matching .,3,0.7706142,34.300095683290536,15
218,"Predicting a reader ’s rating of text quality is a challenging task that involves estimating different subjective aspects of the text , like structure , clarity , etc .",0,0.93121076,63.18606787194776,29
218,Such subjective aspects are better handled using cognitive information .,0,0.52085674,230.81181086331608,10
218,One such source of cognitive information is gaze behaviour .,0,0.8601416,156.19563841945157,10
218,"In this paper , we show that gaze behaviour does indeed help in effectively predicting the rating of text quality .",1,0.73981434,58.03706663244035,21
218,"To do this , we first we model text quality as a function of three properties-organization , coherence and cohesion .",2,0.7943136,51.911600626581894,22
218,"Then , we demonstrate how capturing gaze behaviour helps in predicting each of these properties , and hence the overall quality , by reporting improvements obtained by adding gaze features to traditional textual features for score prediction .",3,0.5765083,123.2444626011407,38
218,"We also hypothesize that if a reader has fully understood the text , the corresponding gaze behaviour would give a better indication of the assigned rating , as opposed to partial understanding .",3,0.46249753,62.69240684606977,33
218,Our experiments validate this hypothesis by showing greater agreement between the given rating and the predicted rating when the reader has a full understanding of the text .,3,0.88281196,29.514058657252694,28
219,We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding .,1,0.44657135,31.631178189903356,35
219,"A sequence-to-sequence model with attention is applied for single-input correction , and a new decoder with multi-input attention averaging is developed to search for consensus among multiple sequences .",2,0.66695124,43.43939875176485,33
219,"We design two ways of training the correction model without human annotation , either training to match noisily observed textual variants or bootstrapping from a uniform error model .",2,0.8858842,86.0094593821139,29
219,"On two corpora of historical newspapers and books , we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and , with the addition of multi-input decoding , can rival supervised methods .",3,0.7823167,68.23076905373598,42
220,Text in many domains involves a significant amount of named entities .,0,0.9386273,38.859400549477954,12
220,Predicting the entity names is often challenging for a language model as they appear less frequent on the training corpus .,0,0.79724574,28.14919987153444,21
220,"In this paper , we propose a novel and effective approach to building a language model which can learn the entity names by leveraging their entity type information .",1,0.89175135,19.23888513481173,29
220,"We also introduce two benchmark datasets based on recipes and Java programming codes , on which we evaluate the proposed model .",2,0.59259576,57.87774378382693,22
220,Experimental results show that our model achieves 52.2 % better perplexity in recipe generation and 22.06 % on code generation than state-of-the-art language models .,3,0.96696645,18.938392226713585,31
221,"Hypertext documents , such as web pages and academic papers , are of great importance in delivering information in our daily life .",0,0.9654998,42.10908284967425,23
221,"Although being effective on plain documents , conventional text embedding methods suffer from information loss if directly adapted to hyper-documents .",0,0.8469624,119.20374810824852,21
221,"In this paper , we propose a general embedding approach for hyper-documents , namely , hyperdoc2vec , along with four criteria characterizing necessary information that hyper-document embedding models should preserve .",1,0.827779,72.15645352330853,31
221,"Systematic comparisons are conducted between hyperdoc2vec and several competitors on two tasks , i.e. , paper classification and citation recommendation , in the academic paper domain .",2,0.7073099,138.01441696786978,27
221,Analyses and experiments both validate the superiority of hyperdoc2vec to other models w.r.t .,3,0.92630565,51.820859874242416,14
221,the four criteria .,2,0.44665372,235.64535863230958,4
222,"This paper presents the Entity-Duet Neural Ranking Model ( EDRM ) , which introduces knowledge graphs to neural search systems .",1,0.77784956,87.40762183409298,23
222,EDRM represents queries and documents by their words and entity annotations .,2,0.48810405,202.5706190925448,12
222,"The semantics from knowledge graphs are integrated in the distributed representations of their entities , while the ranking is conducted by interaction-based neural ranking networks .",2,0.61367524,82.98158195757489,28
222,"The two components are learned end-to-end , making EDRM a natural combination of entity-oriented search and neural information retrieval .",3,0.40922207,57.72152805079602,21
222,Our experiments on a commercial search log demonstrate the effectiveness of EDRM .,3,0.9122665,74.43111432949138,13
222,Our analyses reveal that knowledge graph semantics significantly improve the generalization ability of neural ranking models .,3,0.98546,38.67231421474833,17
223,Modeling natural language inference is a very challenging task .,0,0.9267126,27.013471801380934,10
223,"With the availability of large annotated data , it has recently become feasible to train complex models such as neural-network-based inference models , which have shown to achieve the state-of-the-art performance .",0,0.9446272,12.280642174575602,42
223,"In this paper , we enrich the state-of-the-art neural natural language inference models with external knowledge .",1,0.83792406,12.054928510215861,23
223,We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets .,3,0.9431892,10.854004829146248,28
224,"We consider the problem of learning textual entailment models with limited supervision ( 5K-10 K training examples ) , and present two complementary approaches for it .",2,0.45380715,54.2993922713211,29
224,"First , we propose knowledge-guided adversarial example generators for incorporating large lexical resources in entailment models via only a handful of rule templates .",2,0.73584336,79.58907471089583,25
224,"Second , to make the entailment model — a discriminator — more robust , we propose the first GAN-style approach for training it using a natural language example generator that iteratively adjusts to the discriminator ’s weaknesses .",2,0.69972867,51.56243323683125,40
224,"We demonstrate effectiveness using two entailment datasets , where the proposed methods increase accuracy by 4.7 % on SciTail and by 2.8 % on a 1 % sub-sample of SNLI .",3,0.8040179,40.59611648921369,31
224,"Notably , even a single hand-written rule , negate , improves the accuracy of negation examples in SNLI by 6.1 % .",3,0.955847,103.52335202573731,23
225,Research on distributed word representations is focused on widely-used languages such as English .,0,0.91906583,30.2606940315611,16
225,"Although the same methods can be used for other languages , language-specific knowledge can enhance the accuracy and richness of word vector representations .",3,0.5322824,31.525991726896596,25
225,"In this paper , we look at improving distributed word representations for Korean using knowledge about the unique linguistic structure of Korean .",1,0.9282941,51.129942234392786,23
225,"Specifically , we decompose Korean words into the jamo-level , beyond the character-level , allowing a systematic use of subword information .",2,0.85038877,109.88378564149883,24
225,"To evaluate the vectors , we develop Korean test sets for word similarity and analogy and make them publicly available .",2,0.74933016,99.30051229815712,21
225,The results show that our simple method outperforms word2vec and character-level Skip-Grams on semantic and syntactic similarity and analogy tasks and contributes positively toward downstream NLP tasks such as sentiment analysis .,3,0.98635477,35.23188156110944,36
226,"Sememes are minimum semantic units of concepts in human languages , such that each word sense is composed of one or multiple sememes .",0,0.9197786,42.30062070069414,24
226,"Words are usually manually annotated with their sememes by linguists , and form linguistic common-sense knowledge bases widely used in various NLP tasks .",0,0.89588964,46.5586711562796,24
226,"Recently , the lexical sememe prediction task has been introduced .",0,0.9464761,70.1247703320068,11
226,"It consists of automatically recommending sememes for words , which is expected to improve annotation efficiency and consistency .",0,0.5094901,137.74840476878504,19
226,"However , existing methods of lexical sememe prediction typically rely on the external context of words to represent the meaning , which usually fails to deal with low-frequency and out-of-vocabulary words .",0,0.8908872,26.924815800322982,34
226,"To address this issue for Chinese , we propose a novel framework to take advantage of both internal character information and external context information of words .",1,0.54038477,28.245961427212965,27
226,"We experiment on HowNet , a Chinese sememe knowledge base , and demonstrate that our framework outperforms state-of-the-art baselines by a large margin , and maintains a robust performance even for low-frequency words .",3,0.69142675,21.69609875384753,40
227,"Because word semantics can substantially change across communities and contexts , capturing domain-specific word semantics is an important challenge .",0,0.92324734,66.65993077657461,20
227,"Here , we propose SemAxis , a simple yet powerful framework to characterize word semantics using many semantic axes in word-vector spaces beyond sentiment .",1,0.79076266,95.32335210015825,25
227,We demonstrate that SemAxis can capture nuanced semantic representations in multiple online communities .,3,0.9553118,104.2635795336229,14
227,"We also show that , when the sentiment axis is examined , SemAxis outperforms the state-of-the-art approaches in building domain-specific sentiment lexicons .",3,0.96846485,33.15069970669119,29
228,We present a novel end-to-end reinforcement learning approach to automatic taxonomy induction from a set of terms .,2,0.37083098,19.128894795032448,20
228,"While prior methods treat the problem as a two-phase task ( i.e. , , detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy ) , we argue that such two-phase methods may suffer from error propagation , and cannot effectively optimize metrics that capture the holistic structure of a taxonomy .",0,0.6184889,37.51118940829397,58
228,"In our approach , the representations of term pairs are learned using multiple sources of information and used to determine which term to select and where to place it on the taxonomy via a policy network .",2,0.74457365,37.28740796595913,37
228,"All components are trained in an end-to-end manner with cumulative rewards , measured by a holistic tree metric over the training taxonomies .",2,0.6741999,54.69829206134051,24
228,Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6 % on ancestor F1 .,3,0.9116932,17.4763355507036,32
229,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,0,0.93994534,21.57641188427935,20
229,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods .,0,0.50183064,42.911823997870954,21
229,"However , previous neural networks for WSD always rely on massive labeled data ( context ) , ignoring lexical resources like glosses ( sense definitions ) .",0,0.90215886,149.85524288894993,27
229,"In this paper , we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge .",1,0.67640185,23.725629598779687,32
229,"Therefore , we propose GAS : a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word .",1,0.5296289,44.888218984102345,23
229,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge-based methods .",2,0.5109883,72.27201544691414,33
229,We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information .,2,0.6724941,93.65564371753354,21
229,The experimental results show that our model outperforms the state-of-the-art systems on several English all-words WSD datasets .,3,0.97460043,10.63786639621473,26
230,Sentiment analysis in low-resource languages suffers from a lack of annotated corpora to estimate high-performing models .,0,0.9437273,21.732908218019485,19
230,Machine translation and bilingual word embeddings provide some relief through cross-lingual sentiment approaches .,0,0.60505736,54.26297449037243,14
230,"However , they either require large amounts of parallel data or do not sufficiently capture sentiment information .",0,0.8807217,27.988949152324984,18
230,"We introduce Bilingual Sentiment Embeddings ( BLSE ) , which jointly represent sentiment information in a source and target language .",2,0.70701015,34.588439125793805,21
230,"This model only requires a small bilingual lexicon , a source-language corpus annotated for sentiment , and monolingual word embeddings for each language .",2,0.5152569,34.44376037546247,26
230,"We perform experiments on three language combinations ( Spanish , Catalan , Basque ) for sentence-level cross-lingual sentiment classification and find that our model significantly outperforms state-of-the-art methods on four out of six experimental setups , as well as capturing complementary information to machine translation .",3,0.7009183,18.521292085219727,54
230,Our analysis of the resulting embedding space provides evidence that it represents sentiment information in the resource-poor target language without any annotated data in that language .,3,0.9535234,35.525491569443176,29
231,Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words .,0,0.9417924,21.839806127919086,19
231,"Given reviews from different domains , some existing methods for word embeddings exploit sentiment information , but they cannot produce domain-sensitive embeddings .",0,0.83018965,45.53015584312581,23
231,"On the other hand , some other existing methods can generate domain-sensitive word embeddings , but they cannot distinguish words with similar contexts but opposite sentiment polarity .",0,0.666617,48.527753338931205,28
231,We propose a new method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words .,1,0.44902515,23.96588715378435,28
231,Our method can automatically determine and produce domain-common embeddings and domain-specific embeddings .,3,0.78012174,25.613937690422617,13
231,The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time .,0,0.54173946,29.82588624209033,35
231,Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level .,3,0.9803868,18.423133318495058,32
232,The task of adopting a model with good performance to a target domain that is different from the source domain used for training has received considerable attention in sentiment analysis .,0,0.9323147,20.13642444209549,31
232,Most existing approaches mainly focus on learning representations that are domain-invariant in both the source and target domains .,0,0.86743504,12.798129170564554,19
232,"Few of them pay attention to domain-specific information , which should also be informative .",0,0.7297647,34.019508751800295,15
232,"In this work , we propose a method to simultaneously extract domain specific and invariant representations and train a classifier on each of the representation , respectively .",1,0.71129733,27.811005502203695,28
232,And we introduce a few target domain labeled data for learning domain-specific information .,2,0.4111628,54.99512731002518,14
232,"To effectively utilize the target domain labeled data , we train the domain invariant representation based classifier with both the source and target domain labeled data and train the domain-specific representation based classifier with only the target domain labeled data .",2,0.829925,16.88576318650143,41
232,These two classifiers then boost each other in a co-training style .,2,0.4307306,48.676028073216436,12
232,Extensive sentiment analysis experiments demonstrated that the proposed method could achieve better performance than state-of-the-art methods .,3,0.9571159,8.750801933281657,22
233,"Aspect based sentiment analysis ( ABSA ) can provide more detailed information than general sentiment analysis , because it aims to predict the sentiment polarities of the given aspects or entities in text .",0,0.93297315,46.39653231776735,34
233,We summarize previous approaches into two subtasks : aspect-category sentiment analysis ( ACSA ) and aspect-term sentiment analysis ( ATSA ) .,0,0.4929857,40.015749694200856,22
233,"Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets , which are often complicated and need more training time .",0,0.8724082,51.623309389467046,30
233,"We propose a model based on convolutional neural networks and gating mechanisms , which is more accurate and efficient .",1,0.32032764,22.54064146728495,20
233,"First , the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity .",2,0.56004685,189.5310024204676,21
233,The architecture is much simpler than attention layer used in the existing models .,3,0.44267237,73.9156540525637,14
233,"Second , the computations of our model could be easily parallelized during training , because convolutional layers do not have time dependency as in LSTM layers , and gating units also work independently .",3,0.7650664,56.60960038115251,34
233,The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models .,3,0.90055484,17.09095498864945,14
234,"Deep convolutional neural networks excel at sentiment polarity classification , but tend to require substantial amounts of training data , which moreover differs quite significantly between domains .",0,0.81422657,67.12965766907939,28
234,"In this work , we present an approach to feed generic cues into the training process of such networks , leading to better generalization abilities given limited training data .",1,0.8269594,51.2044183764689,30
234,"We propose to induce sentiment embeddings via supervision on extrinsic data , which are then fed into the model via a dedicated memory-based component .",2,0.62651926,35.79463880449651,27
234,We observe significant gains in effectiveness on a range of different datasets in seven different languages .,3,0.92884654,32.24455185979955,17
235,"The use of user / product information in sentiment analysis is important , especially for cold-start users / products , whose number of reviews are very limited .",0,0.8048966,70.22612750488777,28
235,"However , current models do not deal with the cold-start problem which is typical in review websites .",0,0.8829967,53.142078603468065,18
235,"In this paper , we present Hybrid Contextualized Sentiment Classifier ( HCSC ) , which contains two modules : ( 1 ) a fast word encoder that returns word vectors embedded with short and long range dependency features ;",1,0.5830083,69.05918522999096,39
235,"and ( 2 ) Cold-Start Aware Attention ( CSAA ) , an attention mechanism that considers the existence of cold-start problem when attentively pooling the encoded word vectors .",2,0.4868507,82.95883307288634,29
235,"HCSC introduces shared vectors that are constructed from similar users / products , and are used when the original distinct vectors do not have sufficient information ( i.e .",0,0.6240176,117.43487476138385,29
235,cold-start ) .,0,0.34383848,133.9212690431207,3
235,This is decided by a frequency-guided selective gate vector .,2,0.55014086,258.64492146991154,12
235,"Our experiments show that in terms of RMSE , HCSC performs significantly better when compared with on famous datasets , despite having less complexity , and thus can be trained much faster .",3,0.9739576,82.97378728944459,33
235,"More importantly , our model performs significantly better than previous models when the training data is sparse and has cold-start problems .",3,0.9564681,28.9795802758379,22
236,This paper studies how the argumentation strategies of participants in deliberative discussions can be supported computationally .,1,0.9157983,39.42133266296316,17
236,Our ultimate goal is to predict the best next deliberative move of each participant .,1,0.54643965,42.44791956993543,15
236,"In this paper , we present a model for deliberative discussions and we illustrate its operationalization .",1,0.8599917,46.34415112276864,17
236,"Previous models have been built manually based on a small set of discussions , resulting in a level of abstraction that is not suitable for move recommendation .",0,0.8574359,44.35269110058479,28
236,"In contrast , we derive our model statistically from several types of metadata that can be used for move description .",3,0.56195134,124.75634684659602,21
236,"Applied to six million discussions from Wikipedia talk pages , our approach results in a model with 13 categories along three dimensions : discourse acts , argumentative relations , and frames .",3,0.58789223,131.6488196889643,32
236,"On this basis , we automatically generate a corpus with about 200,000 turns , labeled for the 13 categories .",2,0.74916303,65.93310019096812,20
236,We then operationalize the model with three supervised classifiers and provide evidence that the proposed categories can be predicted .,2,0.5361329,40.24972487701509,20
237,"We present a new dataset of image caption annotations , Conceptual Captions , which contains an order of magnitude more images than the MS-COCO dataset ( Lin et al. , 2014 ) and represents a wider variety of both images and image caption styles .",2,0.460318,31.873663438508746,47
237,We achieve this by extracting and filtering image caption annotations from billions of webpages .,2,0.6518628,33.16991136617024,15
237,"We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 ( Szegedy et al. , 2016 ) for image-feature extraction and Transformer ( Vaswani et al. , 2017 ) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset .",3,0.74427664,31.77054129075075,57
238,We conduct the most comprehensive study to date into translating words via images .,1,0.42040104,76.03472237090854,14
238,"To facilitate research on the task , we introduce a large-scale multilingual corpus of images , each labeled with the word it represents .",2,0.6581597,28.931048788013907,24
238,Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings .,0,0.92427987,40.625221596024474,18
238,"In contrast , we have collected by far the largest available dataset for this task , with images for approximately 10,000 words in each of 100 languages .",0,0.43041408,37.69840142518775,28
238,"We run experiments on a dozen high resource languages and 20 low resources languages , demonstrating the effect of word concreteness and part-of-speech on translation quality .",2,0.69054264,31.906296214166193,28
238,"% We find that while image features work best for concrete nouns , they are sometimes effective on other parts of speech .",3,0.98086846,61.01293505181483,23
238,"To improve image-based translation , we introduce a novel method of predicting word concreteness from images , which improves on a previous state-of-the-art unsupervised technique .",2,0.49211887,17.826970535301133,33
238,"This allows us to predict when image-based translation may be effective , enabling consistent improvements to a state-of-the-art text-based word translation system .",3,0.6730225,22.963267499536048,33
238,Our code and the Massively Multilingual Image Dataset ( MMID ) are available at http://multilingual-images.org/ .,3,0.53427863,16.84392983985885,16
239,Medical imaging is widely used in clinical practice for diagnosis and treatment .,0,0.9655885,14.491520985913374,13
239,"Report-writing can be error-prone for unexperienced physicians , and time-consuming and tedious for experienced physicians .",0,0.77011,40.836279294604516,18
239,"To address these issues , we study the automatic generation of medical imaging reports .",1,0.6144409,60.15053851224481,15
239,This task presents several challenges .,0,0.9291603,82.2182728596755,6
239,"First , a complete report contains multiple heterogeneous forms of information , including findings and tags .",0,0.8130375,192.50172852855482,17
239,"Second , abnormal regions in medical images are difficult to identify .",0,0.89723223,129.55502591698473,12
239,"Third , the reports are typically long , containing multiple sentences .",0,0.7928505,134.3595440217131,12
239,"To cope with these challenges , we ( 1 ) build a multi-task learning framework which jointly performs the prediction of tags and the generation of paragraphs , ( 2 ) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them , ( 3 ) develop a hierarchical LSTM model to generate long paragraphs .",2,0.7242967,39.30973943791427,59
239,We demonstrate the effectiveness of the proposed methods on two publicly available dataset .,3,0.53959996,15.253364624966318,14
240,"Visual language grounding is widely studied in modern neural image captioning systems , which typically adopts an encoder-decoder framework consisting of two principal components : a convolutional neural network ( CNN ) for image feature extraction and a recurrent neural network ( RNN ) for language caption generation .",0,0.88905114,19.671728072909154,49
240,"To study the robustness of language grounding to adversarial perturbations in machine vision and perception , we propose Show-and-Fool , a novel algorithm for crafting adversarial examples in neural image captioning .",1,0.62653184,32.536855949590475,34
240,"The proposed algorithm provides two evaluation approaches , which check if we can mislead neural image captioning systems to output some randomly chosen captions or keywords .",3,0.4543563,114.98355562061006,27
240,"Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords , and the adversarial examples can be made highly transferable to other image captioning systems .",3,0.9411373,46.49571868766957,36
240,"Consequently , our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding .",3,0.96380764,67.63061623379969,21
241,"In this paper , we study the problem of geometric reasoning ( a form of visual reasoning ) in the context of question-answering .",1,0.89032847,22.34185894762157,26
241,"We introduce Dynamic Spatial Memory Network ( DSMN ) , a new deep network architecture that specializes in answering questions that admit latent visual representations , and learns to generate and reason over such representations .",2,0.4187915,56.17565010224656,36
241,"Further , we propose two synthetic benchmarks , FloorPlanQA and ShapeIntersection , to evaluate the geometric reasoning capability of QA systems .",2,0.60805416,57.43848176930977,22
241,Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks .,3,0.9782793,50.65913847675265,14
242,Building intelligent agents that can communicate with and learn from humans in natural language is of great value .,0,0.9066897,19.160622391917652,19
242,"Supervised language learning is limited by the ability of capturing mainly the statistics of training data , and is hardly adaptive to new scenarios or flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting .",0,0.87097186,120.78090650370078,37
242,We highlight the perspective that conversational interaction serves as a natural interface both for language learning and for novel knowledge acquisition and propose a joint imitation and reinforcement approach for grounded language learning through an interactive conversational game .,3,0.57295734,64.13188080905525,39
242,The agent trained with this approach is able to actively acquire information by asking questions about novel objects and use the just-learned knowledge in subsequent conversations in a one-shot fashion .,3,0.7118003,43.707218241386826,32
242,Results compared with other methods verified the effectiveness of the proposed approach .,3,0.9790558,17.852843751961917,13
243,"Recently , there has been growing interest in multi-speaker speech recognition , where the utterances of multiple speakers are recognized from their mixture .",0,0.9623395,30.684314406415258,24
243,"Promising techniques have been proposed for this task , but earlier works have required additional training data such as isolated source signals or senone alignments for effective learning .",0,0.8803745,79.86850414780145,29
243,"In this paper , we propose a new sequence-to-sequence framework to directly decode multiple label sequences from a single speech sequence by unifying source separation and speech recognition functions in an end-to-end manner .",1,0.88066673,23.410611926605313,38
243,We further propose a new objective function to improve the contrast between the hidden vectors to avoid generating similar hypotheses .,3,0.4784736,73.50299016818808,21
243,"Experimental results show that the model is directly able to learn a mapping from a speech mixture to multiple label sequences , achieving 83.1 % relative improvement compared to a model trained without the proposed objective .",3,0.95571655,51.965987519550325,37
243,"Interestingly , the results are comparable to those produced by previous end-to-end works featuring explicit separation and recognition modules .",3,0.9692322,48.086848897202174,22
244,"Statistical morphological inflectors are typically trained on fully supervised , type-level data .",0,0.82015705,151.70912358834482,13
244,"To this end , we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation .",2,0.50032204,14.44445690398423,21
244,"To enable posterior inference over the latent variables , we derive an efficient variational inference procedure based on the wake-sleep algorithm .",2,0.7345909,35.093504690074475,22
244,"We experiment on 23 languages , using the Universal Dependencies corpora in a simulated low-resource setting , and find improvements of over 10 % absolute accuracy in some cases .",3,0.5466779,44.59049133424609,30
245,"The rise of neural networks , and particularly recurrent neural networks , has produced significant advances in part-of-speech tagging accuracy .",0,0.9427091,25.43543071759401,22
245,One characteristic common among these models is the presence of rich initial word encodings .,0,0.8342322,39.739423148921524,15
245,These encodings typically are composed of a recurrent character-based representation with dynamically and pre-trained word embeddings .,0,0.73240536,29.571682078410102,19
245,"However , these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts .",0,0.73718303,51.65842361714171,29
245,"In this paper , we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations .",1,0.8788022,32.36797518271955,26
245,In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states .,3,0.8701008,69.32266544603056,28
246,Morphological analysis involves predicting the syntactic traits of a word ( e.g .,0,0.82863873,34.35502069441459,13
246,"POS : Noun , Case : Acc , Gender : Fem ) .",3,0.5470195,277.76866199967503,13
246,"Previous work in morphological tagging improves performance for low-resource languages ( LRLs ) through cross-lingual training with a high-resource language ( HRL ) from the same family , but is limited by the strict , often false , assumption that tag sets exactly overlap between the HRL and LRL .",0,0.9289975,42.20622734760157,51
246,In this paper we propose a method for cross-lingual morphological tagging that aims to improve information sharing between languages by relaxing this assumption .,1,0.9120442,19.612380070981235,24
246,"The proposed model uses factorial conditional random fields with neural network potentials , making it possible to ( 1 ) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms , ( 2 ) model pairwise and transitive relationships between tags , and ( 3 ) accurately generate tag sets that are unseen or rare in the training data .",2,0.4849904,56.942352383979326,67
246,Experiments on four languages from the Universal Dependencies Treebank demonstrate superior tagging accuracies over existing cross-lingual approaches .,3,0.7681826,22.579271372847355,18
247,"Shi , Huang , and Lee ( 2017a ) obtained state-of-the-art results for English and Chinese dependency parsing by combining dynamic-programming implementations of transition-based dependency parsers with a minimal set of bidirectional LSTM features .",3,0.5435398,21.848232704289558,43
247,"However , their results were limited to projective parsing .",0,0.5272374,140.8333781707792,10
247,"In this paper , we extend their approach to support non-projectivity by providing the first practical implementation of the MH₄ algorithm , an O ( n4 ) mildly nonprojective dynamic-programming parser with very high coverage on non-projective treebanks .",1,0.6682476,84.11766935752075,41
247,"To make MH₄ compatible with minimal transition-based feature sets , we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions .",2,0.6883309,80.49852866953084,32
247,"We thus obtain the first implementation of global decoding for non-projective transition-based parsing , and demonstrate empirically that it is effective than its projective counterpart in parsing a number of highly non-projective languages .",3,0.89175344,34.36757960109854,36
248,We reformulate the problem of encoding a multi-scale representation of a sequence in a language model by casting it in a continuous learning framework .,2,0.663685,21.319919810856973,25
248,We propose a hierarchical multi-scale language model in which short time-scale dependencies are encoded in the hidden state of a lower-level recurrent neural network while longer time-scale dependencies are encoded in the dynamic of the lower-level network by having a meta-learner update the weights of the lower-level neural network in an online meta-learning fashion .,2,0.6160175,16.902233035326006,63
248,We use elastic weights consolidation as a higher-level to prevent catastrophic forgetting in our continuous learning framework .,2,0.72602123,155.63098678002584,20
249,"Increasing the capacity of recurrent neural networks ( RNN ) usually involves augmenting the size of the hidden layer , with significant increase of computational cost .",0,0.90558815,45.51879183044108,27
249,"Recurrent neural tensor networks ( RNTN ) increase capacity using distinct hidden layer weights for each word , but with greater costs in memory usage .",0,0.8153012,138.35237162068492,26
249,"In this paper , we introduce restricted recurrent neural tensor networks ( r-RNTN ) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words .",1,0.56673557,58.011473614388244,35
249,"Perplexity evaluations show that for fixed hidden layer sizes , r-RNTNs improve language model performance over RNNs using only a small fraction of the parameters of unrestricted RNTNs .",3,0.9515732,57.66653424989973,29
249,These results hold for r-RNTNs using Gated Recurrent Units and Long Short-Term Memory .,3,0.9889915,94.53482783534217,16
250,We present a set of experiments to demonstrate that deep recurrent neural networks ( RNNs ) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision .,1,0.38773787,53.071515254759156,31
250,We consider four syntax tasks at different depths of the parse tree ;,2,0.8012521,172.0226922838225,13
250,"for each word , we predict its part of speech as well as the first ( parent ) , second ( grandparent ) and third level ( great-grandparent ) constituent labels that appear above it .",2,0.8072091,59.61202088282522,37
250,"These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives : dependency parsing , semantic role labeling , machine translation , or language modeling .",2,0.637631,64.7532082839117,35
250,"In every case , we find a correspondence between network depth and syntactic depth , suggesting that a soft syntactic hierarchy emerges .",3,0.9652155,62.15960982400831,23
250,"This effect is robust across all conditions , indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision .",3,0.9739819,38.32371179034716,28
251,"Measuring the performance of automatic speech recognition ( ASR ) systems requires manually transcribed data in order to compute the word error rate ( WER ) , which is often time-consuming and expensive .",0,0.95417756,24.222584052264104,36
251,"In this paper , we propose a novel approach to estimate WER , or e-WER , which does not require a gold-standard transcription of the test set .",1,0.86273235,42.58270370225909,28
251,"Our e-WER framework uses a comprehensive set of features : ASR recognised text , character recognition results to complement recognition output , and internal decoder features .",2,0.6861657,279.21240202567134,27
251,We report results for the two features ;,3,0.89325154,370.34920354579543,8
251,black-box and glass-box using unseen 24 Arabic broadcast programs .,2,0.8151358,501.4249557401666,14
251,"Our system achieves 16.9 % WER root mean squared error ( RMSE ) across 1,400 sentences .",3,0.884609,60.394484769084094,17
251,"The estimated overall WER e-WER was 25.3 % for the three hours test set , while the actual WER was 28.5 % .",3,0.9621227,72.9072023159499,23
252,"Written text often provides sufficient clues to identify the author , their gender , age , and other important attributes .",0,0.868083,69.0942645711346,21
252,"Consequently , the authorship of training and evaluation corpora can have unforeseen impacts , including differing model performance for different user groups , as well as privacy implications .",0,0.59143287,58.457542647341526,29
252,"In this paper , we propose an approach to explicitly obscure important author characteristics at training time , such that representations learned are invariant to these attributes .",1,0.86011684,62.728529284180155,28
252,"Evaluating on two tasks , we show that this leads to increased privacy in the learned representations , as well as more robust models to varying evaluation conditions , including out-of-domain corpora .",3,0.8745127,31.526202186299344,35
253,We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier .,2,0.3375229,29.51845692239621,21
253,We find that only a few manipulations are needed to greatly decrease the accuracy .,3,0.97349155,25.160343370326583,15
253,"Our method relies on an atomic flip operation , which swaps one token for another , based on the gradients of the one-hot input vectors .",2,0.7316043,42.691667607663426,27
253,"Due to efficiency of our method , we can perform adversarial training which makes the model more robust to attacks at test time .",3,0.76555985,27.32996274694247,24
253,"With the use of a few semantics-preserving constraints , we demonstrate that HotFlip can be adapted to attack a word-level classifier as well .",3,0.8592956,33.372537960705976,27
254,Generic word embeddings are trained on large-scale generic corpora ;,2,0.41467598,30.8118721742723,10
254,Domain Specific ( DS ) word embeddings are trained only on data from a domain of interest .,0,0.5574058,42.140880183665615,18
254,This paper proposes a method to combine the breadth of generic embeddings with the specificity of domain specific embeddings .,1,0.85656244,16.00472723000695,20
254,"The resulting embeddings , called Domain Adapted ( DA ) word embeddings , are formed by aligning corresponding word vectors using Canonical Correlation Analysis ( CCA ) or the related nonlinear Kernel CCA .",2,0.5681744,56.3046148499442,34
254,"Evaluation results on sentiment classification tasks show that the DA embeddings substantially outperform both generic , DS embeddings when used as input features to standard or state-of-the-art sentence encoding algorithms for classification .",3,0.95943755,37.50050361006016,39
255,Semantic parsing requires training data that is expensive and slow to collect .,0,0.83807856,45.237498914699046,13
255,We apply active learning to both traditional and “ overnight ” data collection approaches .,2,0.8439595,84.02230074666608,15
255,We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset .,3,0.94201005,19.730310123505408,25
255,We show that uncertainty sampling based on least confidence score is competitive in traditional data collection but not applicable for overnight collection .,3,0.96896756,162.3716909716638,23
255,We propose several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best .,3,0.754927,112.07611063810762,22
256,"In this paper we suggest to leverage the partition of articles into sections , in order to learn thematic similarity metric between sentences .",1,0.7054878,79.13884958391633,24
256,We assume that a sentence is thematically closer to sentences within its section than to sentences from other sections .,2,0.7239367,39.90072335975717,20
256,"Based on this assumption , we use Wikipedia articles to automatically create a large dataset of weakly labeled sentence triplets , composed of a pivot sentence , one sentence from the same section and one from another section .",2,0.86907804,45.755065522724664,39
256,We train a triplet network to embed sentences from the same section closer .,2,0.7701269,125.555206221918,14
256,"To test the performance of the learned embeddings , we create and release a sentence clustering benchmark .",2,0.69578826,29.869003201690965,18
256,"We show that the triplet network learns useful thematic metrics , that significantly outperform state-of-the-art semantic similarity methods and multipurpose embeddings on the task of thematic clustering of sentences .",3,0.9315942,36.575490126229944,36
256,We also show that the learned embeddings perform well on the task of sentence semantic similarity prediction .,3,0.9270229,16.304683663170472,18
257,We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction .,2,0.8935657,57.381650749039686,17
257,We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data .,2,0.6366665,62.39004238167302,20
257,"Our replicable benchmarks demonstrate that the proposed graph-based approach , Triframes , shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task .",3,0.95199263,49.398307971963305,43
258,Identification of distinct and independent participants ( entities of interest ) in a narrative is an important task for many NLP applications .,0,0.91439086,34.22565247444109,23
258,This task becomes challenging because these participants are often referred to using multiple aliases .,0,0.85159177,62.978636313312045,15
258,"In this paper , we propose an approach based on linguistic knowledge for identification of aliases mentioned using proper nouns , pronouns or noun phrases with common noun headword .",1,0.8885644,77.67617631631484,30
258,We use Markov Logic Network ( MLN ) to encode the linguistic knowledge for identification of aliases .,2,0.87316054,66.21590459813235,18
258,We evaluate on four diverse history narratives of varying complexity .,2,0.722293,218.23239182521507,11
258,Our approach performs better than the state-of-the-art approach as well as a combination of standard named entity recognition and coreference resolution techniques .,3,0.9270811,9.924850857868021,29
259,We present a new architecture for named entity recognition .,1,0.54759884,19.646726951050503,10
259,Our model employs multiple independent bidirectional LSTM units across the same input and promotes diversity among them by employing an inter-model regularization term .,2,0.7048408,47.34385977349161,24
259,By distributing computation across multiple smaller LSTMs we find a significant reduction in the total number of parameters .,3,0.9204331,37.55826101510597,19
259,We find our architecture achieves state-of-the-art performance on the CoNLL 2003 NER dataset .,3,0.9572334,8.530759760321995,19
260,State-of-the-art knowledge base completion ( KBC ) models predict a score for every known or unknown fact via a latent factorization over entity and relation embeddings .,0,0.83929193,47.912430994386995,31
260,"We observe that when they fail , they often make entity predictions that are incompatible with the type required by the relation .",3,0.94376916,55.32522170669105,23
260,"In response , we enhance each base factorization with two type-compatibility terms between entity-relation pairs , and combine the signals in a novel manner .",2,0.7831601,108.3903475581971,26
260,"Without explicit supervision from a type catalog , our proposed modification obtains up to 7 % MRR gains over base models , and new state-of-the-art results on several datasets .",3,0.85158885,45.38566361217968,36
260,Further analysis reveals that our models better represent the latent types of entities and their embeddings also predict supervised types better than the embeddings fitted by baseline models .,3,0.97661793,31.92589039181137,29
261,We present a novel graph-based neural network model for relation extraction .,1,0.4385909,10.894937949773862,14
261,Our model treats multiple pairs in a sentence simultaneously and considers interactions among them .,2,0.7759056,35.37847897897263,15
261,All the entities in a sentence are placed as nodes in a fully-connected graph structure .,2,0.5083143,20.627274174493035,18
261,The edges are represented with position-aware contexts around the entity pairs .,2,0.58534133,91.98399104657723,14
261,"In order to consider different relation paths between two entities , we construct up to l-length walks between each pair .",2,0.8330178,131.01406697753774,21
261,The resulting walks are merged and iteratively used to update the edge representations into longer walks representations .,2,0.58918184,183.3266214585434,18
261,We show that the model achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools .,3,0.94001013,8.97194535219082,29
262,"This paper addresses the tasks of automatic seed selection for bootstrapping relation extraction , and noise reduction for distantly supervised relation extraction .",1,0.90522873,45.19888188832856,23
262,We first point out that these tasks are related .,1,0.39124006,47.43991344297794,10
262,"Then , inspired by ranking relation instances and patterns computed by the HITS algorithm , and selecting cluster centroids using the K-means , LSA , or NMF method , we propose methods for selecting the initial seeds from an existing resource , or reducing the level of noise in the distantly labeled data .",2,0.7528873,117.41342974920126,54
262,Experiments show that our proposed methods achieve a better performance than the baseline systems in both tasks .,3,0.96295077,14.062729286073141,18
263,Located Near relation is a kind of commonsense knowledge describing two physical objects that are typically found near each other in real life .,0,0.9056303,51.847269355692966,24
263,"In this paper , we study how to automatically extract such relationship through a sentence-level relation classifier and aggregating the scores of entity pairs from a large corpus .",1,0.9132874,31.988695539251513,31
263,"Also , we release two benchmark datasets for evaluation and future research .",3,0.5101482,31.066965970325306,13
264,Coreference resolution aims to identify in a text all mentions that refer to the same real world entity .,0,0.9191841,45.650710576424714,19
264,The state-of-the-art end-to-end neural coreference model considers all text spans in a document as potential mentions and learns to link an antecedent for each possible mention .,0,0.7018625,15.555178210966623,32
264,"In this paper , we propose to improve the end-to-end coreference resolution system by ( 1 ) using a biaffine attention model to get antecedent scores for each possible mention , and ( 2 ) jointly optimizing the mention detection accuracy and mention clustering accuracy given the mention cluster labels .",1,0.7177737,26.785000159656157,53
264,Our model achieves the state-of-the-art performance on the CoNLL-2012 shared task English test set .,3,0.8224632,10.278560731960688,21
265,This paper proposes an improvement to the existing data-driven Neural Belief Tracking ( NBT ) framework for Dialogue State Tracking ( DST ) .,1,0.8392007,40.66253876988432,24
265,The existing NBT model uses a hand-crafted belief state update mechanism which involves an expensive manual retuning step whenever the model is deployed to a new dialogue domain .,0,0.6468296,59.557980611231876,29
265,"We show that this update mechanism can be learned jointly with the semantic decoding and context modelling parts of the NBT model , eliminating the last rule-based module from this DST framework .",3,0.8952949,81.9474623291431,34
265,We propose two different statistical update mechanisms and show that dialogue dynamics can be modelled with a very small number of additional model parameters .,3,0.654773,41.799979585074894,25
265,"In our DST evaluation over three languages , we show that this model achieves competitive performance and provides a robust framework for building resource-light DST models .",3,0.88603777,27.332882057804206,29
266,"We study the role of linguistic context in predicting quantifiers ( ‘ few ’ , ‘ all ’ ) .",1,0.6543708,47.82742639404633,20
266,We collect crowdsourced data from human participants and test various models in a local ( single-sentence ) and a global context ( multi-sentence ) condition .,2,0.92537373,25.040195391985247,26
266,Models significantly out-perform humans in the former setting and are only slightly better in the latter .,3,0.9052561,19.99540926272068,18
266,"While human performance improves with more linguistic context ( especially on proportional quantifiers ) , model performance suffers .",3,0.56175774,165.75121144818334,19
266,Models are very effective in exploiting lexical and morpho-syntactic patterns ;,0,0.81160265,42.272139451076,11
266,humans are better at genuinely understanding the meaning of the ( global ) context .,0,0.8899031,94.79103552664326,15
267,"We ask how to practically build a model for German named entity recognition ( NER ) that performs at the state of the art for both contemporary and historical texts , i.e. , a big-data and a small-data scenario .",1,0.77911407,35.21638708889624,42
267,The two best-performing model families are pitted against each other ( linear-chain CRFs and BiLSTM ) to observe the trade-off between expressiveness and data requirements .,2,0.6143224,42.772102438483074,28
267,BiLSTM outperforms the CRF when large datasets are available and performs inferior for the smallest dataset .,3,0.8793307,48.158741010195946,17
267,"BiLSTMs profit substantially from transfer learning , which enables them to be trained on multiple corpora , resulting in a new state-of-the-art model for German NER on two contemporary German corpora ( CoNLL 2003 and GermEval 2014 ) and two historic corpora .",0,0.47419703,34.81285102276421,49
268,Software developers and testers have long struggled with how to elicit proactive responses from their coworkers when reviewing code for security vulnerabilities and errors .,0,0.94491875,41.47892198720981,25
268,"For a code review to be successful , it must not only identify potential problems but also elicit an active response from the colleague responsible for modifying the code .",0,0.794082,30.02496721055964,30
268,"To understand the factors that contribute to this outcome , we analyze a novel dataset of more than one million code reviews for the Google Chromium project , from which we extract linguistic features of feedback that elicited responsive actions from coworkers .",2,0.66349244,40.600230216702265,43
268,"Using a manually-labeled subset of reviewer comments , we trained a highly accurate classifier to identify acted-upon comments ( AUC = 0.85 ) .",2,0.56311244,49.141990777329234,26
268,"Our results demonstrate the utility of our dataset , the feasibility of using NLP for this new task , and the potential of NLP to improve our understanding of how communications between colleagues can be authored to elicit positive , proactive responses .",3,0.9880255,39.484147695746024,43
269,Humans rely on multiple sensory modalities when examining and reasoning over images .,0,0.9470038,35.29140332424705,13
269,"In this paper , we describe a new multimodal dataset that consists of gaze measurements and spoken descriptions collected in parallel during an image inspection task .",1,0.8760207,36.117273809436746,27
269,The task was performed by multiple participants on 100 general-domain images showing everyday objects and activities .,2,0.92332,88.37379499984134,18
269,We demonstrate the usefulness of the dataset by applying an existing visual-linguistic data fusion framework in order to label important image regions with appropriate linguistic labels .,3,0.6786059,44.60149598560274,29
270,Analogical reasoning is effective in capturing linguistic regularities .,0,0.5805254,44.42896707545994,9
270,This paper proposes an analogical reasoning task on Chinese .,1,0.8812257,121.23981183487832,10
270,"After delving into Chinese lexical knowledge , we sketch 68 implicit morphological relations and 28 explicit semantic relations .",2,0.56680566,79.79918279206545,19
270,"A big and balanced dataset CA8 is then built for this task , including 17813 questions .",2,0.6898987,367.78230738911503,17
270,"Furthermore , we systematically explore the influences of vector representations , context features , and corpora on analogical reasoning .",2,0.4921594,91.82661958721049,20
270,"With the experiments , CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings .",3,0.90122145,40.27947474472551,18
271,Metaphors are frequently used to convey emotions .,0,0.9402672,18.835997438118675,8
271,"However , there is little research on the construction of metaphor corpora annotated with emotion for the analysis of emotionality of metaphorical expressions .",0,0.9237682,34.316250602129244,24
271,"Furthermore , most studies focus on English , and few in other languages , particularly Sino-Tibetan languages such as Chinese , for emotion analysis from metaphorical texts , although there are likely to be many differences in emotional expressions of metaphorical usages across different languages .",0,0.7003838,38.22970616248119,46
271,"We therefore construct a significant new corpus on metaphor , with 5,605 manually annotated sentences in Chinese .",2,0.5746802,96.2157141735298,18
271,"We present an annotation scheme that contains annotations of linguistic metaphors , emotional categories ( joy , anger , sadness , fear , love , disgust and surprise ) , and intensity .",2,0.7162573,79.81645991396829,33
271,The annotation agreement analyses for multiple annotators are described .,3,0.6549669,119.64473678366959,10
271,We also use the corpus to explore and analyze the emotionality of metaphors .,2,0.6190524,32.29911914983647,14
271,"To the best of our knowledge , this is the first relatively large metaphor corpus with an annotation of emotions in Chinese .",3,0.955716,24.97900564691583,23
272,Comments of online articles provide extended views and improve user engagement .,0,0.56224006,114.49372299162957,12
272,"Automatically making comments thus become a valuable functionality for online forums , intelligent chatbots , etc .",0,0.7105198,283.2327066131805,17
272,"This paper proposes the new task of automatic article commenting , and introduces a large-scale Chinese dataset with millions of real comments and a human-annotated subset characterizing the comments ’ varying quality .",1,0.77371055,50.11205407815404,33
272,"Incorporating the human bias of comment quality , we further develop automatic metrics that generalize a broad set of popular reference-based metrics and exhibit greatly improved correlations with human evaluations .",3,0.47469077,48.72951130539161,33
273,Plagiarism is a major issue in science and education .,0,0.9602241,11.7861868946928,10
273,"Complex plagiarism , such as plagiarism of ideas , is hard to detect , and therefore it is especially important to track improvement of methods correctly .",0,0.92372185,69.1846305526128,27
273,"In this paper , we study the performance of plagdet , the main measure for plagiarim detection , on manually paraphrased datasets ( such as PAN Summary ) .",1,0.88790363,268.03865637840033,29
273,"We reveal its fallibility under certain conditions and propose an evaluation framework with normalization of inner terms , which is resilient to the dataset imbalance .",3,0.70197517,96.53924667141699,26
273,We conclude with the experimental justification of the proposed measure .,3,0.8849653,84.18259307576088,11
273,The implementation of the new framework is made publicly available as a Github repository .,3,0.7218733,16.126270154160352,15
274,"In neural abstractive summarization , the conventional sequence-to-sequence ( seq2seq ) model often suffers from repetition and semantic irrelevance .",0,0.93757254,26.000481398496913,22
274,"To tackle the problem , we propose a global encoding framework , which controls the information flow from the encoder to the decoder based on the global information of the source context .",2,0.47379753,17.389978443002875,33
274,It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information .,0,0.42065576,38.46229722839084,20
274,"Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models , and the analysis shows that our model is capable of generating summary of higher quality and reducing repetition .",3,0.95964366,31.84653798151685,37
275,We herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator .,1,0.67257524,28.465719062690464,32
275,"More specifically , the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words .",2,0.46981227,40.53806546608832,25
275,"Subsequently , a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression .",2,0.70900005,34.682197032070555,30
275,"An empirical study shows that the proposed model can effectively generate more readable compression , comparable or superior to several strong baselines .",3,0.9420544,32.369912239468185,23
275,"Furthermore , we introduce a 200-sentence test set for a large-scale dataset , setting a new baseline for the future research .",3,0.544031,27.02837926161604,24
276,"In the age of social news , it is important to understand the types of reactions that are evoked from news sources with various levels of credibility .",0,0.9128303,23.72993468543068,28
276,"In the present work we seek to better understand how users react to trusted and deceptive news sources across two popular , and very different , social media platforms .",1,0.907162,28.352781322872747,30
276,Twitter posts and 6.2 M Reddit comments .,2,0.55643874,131.87645026022673,8
276,"We show that there are significant differences in the speed and the type of reactions between trusted and deceptive news sources on Twitter , but far smaller differences on Reddit .",3,0.97502905,44.19259720606172,31
277,Online petitions are a cost-effective way for citizens to collectively engage with policy-makers in a democracy .,3,0.50144637,18.16715922977903,19
277,Predicting the popularity of a petition — commonly measured by its signature count — based on its textual content has utility for policymakers as well as those posting the petition .,0,0.728924,39.18952869375239,31
277,"In this work , we model this task using CNN regression with an auxiliary ordinal regression objective .",2,0.5283886,95.56688549859946,18
277,We demonstrate the effectiveness of our proposed approach using UK and US government petition datasets .,3,0.584599,45.56629640150743,16
278,We introduce a new approach to tackle the problem of offensive language in online social media .,1,0.6606513,15.845281237074888,17
278,Our approach uses unsupervised text style transfer to translate offensive sentences into non-offensive ones .,2,0.8102022,24.358349110218438,15
278,"We propose a new method for training encoder-decoders using non-parallel data that combines a collaborative classifier , attention and the cycle consistency loss .",1,0.35733697,55.60244084441231,24
278,Experimental results on data from Twitter and Reddit show that our method outperforms a state-of-the-art text style transfer system in two out of three quantitative metrics and produces reliable non-offensive transferred sentences .,3,0.94170964,21.19008356035224,39
279,Natural languages change over time because they evolve to the needs of their users and the socio-technological environment .,0,0.9389728,26.508459619248498,19
279,This study investigates the diachronic accuracy of pre-trained language models for downstream tasks in machine learning and user profiling .,1,0.93935287,21.965399804610577,20
279,"To our knowledge , this is the first study to show that it is possible to measure diachronic semantic drifts within social media and within the span of a few years .",3,0.9532017,19.02678550321322,32
280,"In this paper , we make a move to build a dialogue system for automatic diagnosis .",1,0.92491174,41.045204001194136,17
280,We first build a dataset collected from an online medical forum by extracting symptoms from both patients ’ self-reports and conversational data between patients and doctors .,2,0.92626256,32.867225849779146,27
280,"Then we propose a task-oriented dialogue system framework to make diagnosis for patients automatically , which can converse with patients to collect additional symptoms beyond their self-reports .",2,0.34335995,60.635516956518266,29
280,Experimental results on our dataset show that additional symptoms extracted from conversation can greatly improve the accuracy for disease identification and our dialogue system is able to collect these symptoms automatically and make a better diagnosis .,3,0.9784501,37.79747126962354,37
281,Building multi-turn information-seeking conversation systems is an important and challenging research topic .,0,0.90673727,29.1127089442609,15
281,"Although several advanced neural text matching models have been proposed for this task , they are generally not efficient for industrial applications .",0,0.90659606,31.033867177449,23
281,"Furthermore , they rely on a large amount of labeled data , which may not be available in real-world applications .",0,0.7452274,13.1433798399231,21
281,"To alleviate these problems , we study transfer learning for multi-turn information seeking conversations in this paper .",1,0.6574379,81.30074762592699,18
281,We first propose an efficient and effective multi-turn conversation model based on convolutional neural networks .,2,0.46708342,11.887333291572167,16
281,"After that , we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance .",2,0.5739227,24.65458701125726,23
281,"Finally , we deployed our model in an industrial chatbot called AliMe Assist and observed a significant improvement over the existing online model .",3,0.87770265,56.09984154282425,24
282,We present a novel multi-task modeling approach to learning multilingual distributed representations of text .,1,0.5204934,18.300345806653407,15
282,Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model .,2,0.6580525,18.35485703406327,22
282,"Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings , thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone .",3,0.84716403,33.43467338444033,32
282,Our model shows competitive performance in a standard cross-lingual document classification task .,3,0.9051075,16.299506549041684,13
282,We also show the effectiveness of our method in a limited resource scenario .,3,0.8038286,16.299246182073418,14
283,We investigate the behavior of maps learned by machine translation methods .,1,0.55740774,67.37138257738499,12
283,The maps translate words by projecting between word embedding spaces of different languages .,2,0.4581904,90.13292750691413,14
283,"We locally approximate these maps using linear maps , and find that they vary across the word embedding space .",2,0.58896804,62.961160955889156,20
283,This demonstrates that the underlying maps are non-linear .,3,0.95482564,47.6551765210698,9
283,"Importantly , we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained .",3,0.96772355,39.38268504671198,29
283,"Our results can be used to test non-linear methods , and to drive the design of more accurate maps for word translation .",3,0.9862574,48.09869349225133,23
284,We learn a joint multilingual sentence embedding and use the distance between sentences in different languages to filter noisy parallel data and to mine for parallel data in large news collections .,2,0.7792263,66.49814631199007,32
284,We are able to improve a competitive baseline on the WMT ’14 English to German task by 0.3 BLEU by filtering out 25 % of the training data .,3,0.9158717,25.764828475183933,30
284,The same approach is used to mine additional bitexts for the WMT ’14 system and to obtain competitive results on the BUCC shared task to identify parallel sentences in comparable corpora .,3,0.6062569,75.05778514835619,33
284,"The approach is generic , it can be applied to many language pairs and it is independent of the architecture of the machine translation system .",3,0.73376983,20.800563104447786,26
285,This paper proposes hybrid semi-Markov conditional random fields ( SCRFs ) for neural sequence labeling in natural language processing .,1,0.7561246,70.84255644833642,20
285,"Based on conventional conditional random fields ( CRFs ) , SCRFs have been designed for the tasks of assigning labels to segments by extracting features from and describing transitions between segments instead of words .",0,0.86617804,80.15338626253472,35
285,"In this paper , we improve the existing SCRF methods by employing word-level and segment-level information simultaneously .",1,0.82828003,37.8274287464312,19
285,"First , word-level labels are utilized to derive the segment scores in SCRFs .",2,0.8087895,119.14209181663254,16
285,"Second , a CRF output layer and an SCRF output layer are integrated into a unified neural network and trained jointly .",2,0.75277036,33.06671834376234,22
285,Experimental results on CoNLL 2003 named entity recognition ( NER ) shared task show that our model achieves state-of-the-art performance when no external knowledge is used .,3,0.9130173,12.14660333848419,33
286,"In this work , we discuss the importance of external knowledge for performing Named Entity Recognition ( NER ) .",1,0.90794396,33.60959245000034,20
286,We present a novel modular framework that divides the knowledge into four categories according to the depth of knowledge they convey .,1,0.41006568,28.055913467587025,22
286,"Each category consists of a set of features automatically generated from different information sources , such as a knowledge-base , a list of names , or document-specific semantic annotations .",2,0.4189089,43.658705969368825,32
286,"Further , we show the effects on performance when incrementally adding deeper knowledge and discuss effectiveness / efficiency trade-offs .",3,0.656196,70.77415048016569,20
287,News related content has been extensively studied in both topic modeling research and named entity recognition .,0,0.9436619,38.2702697292704,17
287,"However , expressive power of named entities and their potential for improving the quality of discovered topics has not received much attention .",0,0.94946223,40.44616838481515,23
287,In this paper we use named entities as domain-specific terms for news-centric content and present a new weighting model for Latent Dirichlet Allocation .,1,0.6220043,30.913946293514464,25
287,"Our experimental results indicate that involving more named entities in topic descriptors positively influences the overall quality of topics , improving their interpretability , specificity and diversity .",3,0.98910856,68.27330550785979,28
288,We consider the task of detecting contractual obligations and prohibitions .,1,0.34403107,89.55380951718062,11
288,"We show that a self-attention mechanism improves the performance of a BILSTM classifier , the previous state of the art for this task , by allowing it to focus on indicative tokens .",3,0.91724265,25.485236828927505,33
288,"We also introduce a hierarchical BILSTM , which converts each sentence to an embedding , and processes the sentence embeddings to classify each sentence .",2,0.7761394,39.379661723218085,25
288,"Apart from being faster to train , the hierarchical BILSTM outperforms the flat one , even when the latter considers surrounding sentences , because the hierarchical model has a broader discourse view .",3,0.8856478,130.25343340552828,33
289,We present a paper abstract writing system based on an attentive neural sequence-to-sequence model that can take a title as input and automatically generate an abstract .,1,0.48321337,21.82435717247048,30
289,We design a novel Writing-editing Network that can attend to both the title and the previously generated abstract drafts and then iteratively revise and polish the abstract .,2,0.7775685,66.56657735171432,29
289,"With two series of Turing tests , where the human judges are asked to distinguish the system-generated abstracts from human-written ones , our system passes Turing tests by junior domain experts at a rate up to 30 % and by non-expert at a rate up to 80 % .",3,0.6470481,34.99183491370898,50
290,We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words .,2,0.4735138,153.19316745284848,27
290,"In this work , we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms .",1,0.89574414,41.6872680952122,29
290,Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words ’ ambiguity and polysemy leads to performance improvement .,3,0.9585262,43.89306790243033,25
291,The task of Question Answering is at the very core of machine comprehension .,0,0.9545655,18.639161705358948,14
291,"In this paper , we propose a Convolutional Neural Network ( CNN ) model for text-based multiple choice question answering where questions are based on a particular article .",1,0.870165,25.680490835687827,31
291,"Given an article and a multiple choice question , our model assigns a score to each question-option tuple and chooses the final option accordingly .",2,0.7706277,50.75475413062146,25
291,We test our model on Textbook Question Answering ( TQA ) and SciQ dataset .,2,0.7848932,29.778763054098587,15
291,Our model outperforms several LSTM-based baseline models on the two datasets .,3,0.88139683,13.407991965651473,14
292,"Story comprehension requires a deep semantic understanding of the narrative , making it a challenging task .",0,0.9315109,36.898397619307985,17
292,"Inspired by previous studies on ROC Story Cloze Test , we propose a novel method , tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect .",2,0.53033376,111.71323993551361,36
292,"Evaluated on the task of story ending prediction , our model demonstrates superior performance to a collection of competitive baselines , setting a new state of the art .",3,0.86932015,25.957632078616115,29
293,"Effectively using full syntactic parsing information in Neural Networks ( NNs ) for solving relational tasks , e.g. , question similarity , is still an open problem .",0,0.8942282,115.85648625501884,28
293,"In this paper , we propose to inject structural representations in NNs by ( i ) learning a model with Tree Kernels ( TKs ) on relatively few pairs of questions ( few thousands ) as gold standard ( GS ) training data is typically scarce , ( ii ) predicting labels on a very large corpus of question pairs , and ( iii ) pre-training NNs on such large corpus .",1,0.6375824,57.736998482010144,72
293,"The results on Quora and SemEval question similarity datasets show that NNs using our approach can learn more accurate models , especially after fine tuning on GS .",3,0.9781116,90.3322249938289,28
294,We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation ( NMT ) .,1,0.42638716,36.14586512585194,26
294,"Unlike the popular length normalization and coverage models , our model does not require training nor reranking the limited n-best outputs .",3,0.5173919,86.33970930520697,22
294,"Moreover , it is robust to large beam sizes , which is not well studied in previous work .",3,0.6334517,49.73974557165485,19
294,"On the Chinese-English and English-German translation tasks , our approach yields + 0.4 1.5 BLEU improvements over the state-of-the-art baselines .",3,0.87337434,8.745927482647279,27
295,Traditional Neural machine translation ( NMT ) involves a fixed training procedure where each sentence is sampled once during each epoch .,0,0.8596055,61.49788715759599,22
295,"In reality , some sentences are well-learned during the initial few epochs ;",0,0.6752645,144.01648724526515,15
295,"however , using this approach , the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs , which results in a wastage of time .",3,0.7855853,53.861894107975225,39
295,"Here , we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training .",1,0.64483714,24.88566785013137,20
295,"In this approach , a weight is assigned to each sentence based on the measured difference between the training costs of two iterations .",2,0.660928,56.77582736319939,24
295,"Further , in each epoch , a certain percentage of sentences are dynamically sampled according to their weights .",2,0.68649495,82.01845434261412,19
295,Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance .,3,0.9544958,8.592109959311184,33
296,"Neural machine translation ( NMT ) models are typically trained with fixed-size input and output vocabularies , which creates an important bottleneck on their accuracy and generalization capability .",0,0.94656897,27.761154514774308,31
296,"As a solution , various studies proposed segmenting words into sub-word units and performing translation at the sub-lexical level .",0,0.8423596,71.98253025907083,20
296,"However , statistical word segmentation methods have recently shown to be prone to morphological errors , which can lead to inaccurate translations .",0,0.94669825,36.167105736341796,23
296,"In this paper , we propose to overcome this problem by replacing the source-language embedding layer of NMT with a bi-directional recurrent neural network that generates compositional representations of the input at any desired level of granularity .",1,0.76671606,14.903630532040346,40
296,"We test our approach in a low-resource setting with five languages from different morphological typologies , and under different composition assumptions .",2,0.7805976,36.342635746088455,22
296,"By training NMT to compose word representations from character n-grams , our approach consistently outperforms ( from 1.71 to 2.48 BLEU points ) NMT learning embeddings of statistically generated sub-word units .",3,0.79940283,43.998324520234995,32
297,"Every person speaks or writes their own flavor of their native language , influenced by a number of factors : the content they tend to talk about , their gender , their social status , or their geographical origin .",0,0.90956926,40.32100192412627,40
297,"When attempting to perform Machine Translation ( MT ) , these variations have a significant effect on how the system should perform translation , but this is not captured well by standard one-size-fits-all models .",0,0.9182159,37.33975349766577,37
297,"In this paper , we propose a simple and parameter-efficient adaptation technique that only requires adapting the bias of the output softmax to each particular user of the MT system , either directly or through a factored approximation .",1,0.7848822,56.39226160537238,41
297,"Experiments on TED talks in three languages demonstrate improvements in translation accuracy , and better reflection of speaker traits in the target text .",3,0.7937381,89.72764943556632,24
298,We explore strategies for incorporating target syntax into Neural Machine Translation .,1,0.5249673,77.0813982983604,12
298,We specifically focus on syntax in ensembles containing multiple sentence representations .,2,0.4403767,64.16874085659916,12
298,"We formulate beam search over such ensembles using WFSTs , and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax .",2,0.51189345,132.3223553660998,29
298,Our approach gives state-of-the-art performance on a difficult Japanese-English task .,3,0.8225604,16.340727984208126,18
299,"We empirically investigate learning from partial feedback in neural machine translation ( NMT ) , when partial feedback is collected by asking users to highlight a correct chunk of a translation .",2,0.43644512,58.41128890649441,32
299,We propose a simple and effective way of utilizing such feedback in NMT training .,1,0.44074005,29.564533797955637,15
299,We demonstrate how the common machine translation problem of domain mismatch between training and deployment can be reduced solely based on chunk-level user feedback .,3,0.816581,65.04475237105613,27
299,We conduct a series of simulation experiments to test the effectiveness of the proposed method .,2,0.79040205,10.150674624653902,16
299,Our results show that chunk-level feedback outperforms sentence based feedback by up to 2.61 % BLEU absolute .,3,0.9870907,39.293772500074276,19
300,A sentence can be translated into more than one correct sentences .,0,0.74857366,61.86872378410793,12
300,"However , most of the existing neural machine translation models only use one of the correct translations as the targets , and the other correct sentences are punished as the incorrect sentences in the training stage .",0,0.83359736,38.901810148743046,37
300,"Since most of the correct translations for one sentence share the similar bag-of-words , it is possible to distinguish the correct translations from the incorrect ones by the bag-of-words .",0,0.5429612,18.292681370812485,30
300,"In this paper , we propose an approach that uses both the sentences and the bag-of-words as targets in the training stage , in order to encourage the model to generate the potentially correct sentences that are not appeared in the training set .",1,0.73368126,23.086794072732452,44
300,"We evaluate our model on a Chinese-English translation dataset , and experiments show our model outperforms the strong baselines by the BLEU score of 4.55 .",3,0.65615803,14.278488590897227,27
301,"To achieve high translation performance , neural machine translation models usually rely on the beam search algorithm for decoding sentences .",0,0.89643365,42.98116413025577,21
301,The beam search finds good candidate translations by considering multiple hypotheses of translations simultaneously .,2,0.46028653,121.29468750339076,15
301,"However , as the algorithm produces hypotheses in a monotonic left-to-right order , a hypothesis can not be revisited once it is discarded .",3,0.5961918,38.98523457735435,25
301,We found such monotonicity forces the algorithm to sacrifice some good decoding paths .,3,0.9704232,204.49904720857188,14
301,"To mitigate this problem , we relax the monotonic constraint of the beam search by maintaining all found hypotheses in a single priority queue and using a universal score function for hypothesis selection .",2,0.76991147,66.22701965064789,34
301,The proposed algorithm allows discarded hypotheses to be recovered in a later step .,3,0.5883293,83.1691117718458,14
301,"Despite its simplicity , we show that the proposed decoding algorithm enhances the quality of selected hypotheses and improve the translations even for high-performance models in English-Japanese translation task .",3,0.9477715,56.737640456134244,34
302,"Verb-noun combinations ( VNCs )-e.g. , blow the whistle , hit the roof , and see stars-are a common type of English idiom that are ambiguous with literal usages .",0,0.9325442,130.81985950094634,32
302,"In this paper we propose and evaluate models for classifying VNC usages as idiomatic or literal , based on a variety of approaches to forming distributed representations .",1,0.88906014,64.75808699195096,28
302,"Our results show that a model based on averaging word embeddings performs on par with , or better than , a previously-proposed approach based on skip-thoughts .",3,0.9864423,29.56639472233358,29
302,Idiomatic usages of VNCs are known to exhibit lexico-syntactic fixedness .,0,0.8851418,31.48481383577079,11
302,"We further incorporate this information into our models , demonstrating that this rich linguistic knowledge is complementary to the information carried by distributed representations .",3,0.6675444,45.68937594591952,25
303,The methods proposed recently for specializing word embeddings according to a particular perspective generally rely on external knowledge .,0,0.7872883,74.50472462789112,19
303,"In this article , we propose Pseudofit , a new method for specializing word embeddings according to semantic similarity without any external knowledge .",1,0.88865864,35.553122774727825,24
303,Pseudofit exploits the notion of pseudo-sense for building several representations for each word and uses these representations for making the initial embeddings more generic .,0,0.40077332,50.92458300662398,25
303,We illustrate the interest of Pseudofit for acquiring synonyms and study several variants of Pseudofit according to this perspective .,1,0.4989858,72.10548054247828,20
304,Methods for unsupervised hypernym detection may broadly be categorized according to two paradigms : pattern-based and distributional methods .,0,0.7908703,25.42173519921612,20
304,"In this paper , we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform distributional methods on common benchmark datasets .",1,0.7149067,24.665122837785933,32
304,Our results show that pattern-based models provide important contextual constraints which are not yet captured in distributional methods .,3,0.9891167,50.76029663459879,20
305,"Recent BIO-tagging-based neural semantic role labeling models are very high performing , but assume gold predicates as part of the input and cannot incorporate span-level features .",0,0.799489,71.13712389161002,31
305,"We propose an end-to-end approach for jointly predicting all predicates , arguments spans , and the relations between them .",2,0.44814977,46.78138881034393,21
305,"The model makes independent decisions about what relationship , if any , holds between every possible word-span pair , and learns contextualized span representations that provide rich , shared input features for each decision .",2,0.64729893,129.59957453937562,35
305,Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates .,3,0.94909894,44.60226162759579,19
306,"In neural machine translation , words are sometimes dropped from the source or generated repeatedly in the translation .",0,0.9001532,58.249827708273436,19
306,We explore novel strategies to address the coverage problem that change only the attention transformation .,3,0.37333784,134.76358413534433,16
306,"Our approach allocates fertilities to source words , used to bound the attention each word can receive .",2,0.7515302,269.283218689846,18
306,"We experiment with various sparse and constrained attention transformations and propose a new one , constrained sparsemax , shown to be differentiable and sparse .",2,0.57605684,115.59404027294794,25
306,Empirical evaluation is provided in three languages pairs .,2,0.5395705,48.38737070945404,9
307,"Attention-based neural machine translation ( NMT ) models selectively focus on specific source positions to produce a translation , which brings significant improvements over pure encoder-decoder sequence-to-sequence models .",0,0.86165714,24.356525628818314,33
307,This work investigates NMT while replacing the attention component .,1,0.8067061,214.50782422890177,10
307,"We study a neural hidden Markov model ( HMM ) consisting of neural network-based alignment and lexicon models , which are trained jointly using the forward-backward algorithm .",2,0.66104007,57.80630841407083,32
307,We show that the attention component can be effectively replaced by the neural network alignment model and the neural HMM approach is able to provide comparable performance with the state-of-the-art attention-based models on the WMT 2017 German ↔English and Chinese →English translation tasks .,3,0.9311097,17.59404733224847,53
308,"Gender prediction has typically focused on lexical and social network features , yielding good performance , but making systems highly language-, topic-, and platform dependent .",0,0.85060465,140.80604890555182,30
308,"Cross-lingual embeddings circumvent some of these limitations , but capture gender-specific style less .",0,0.52338606,76.53761987014255,14
308,"We propose an alternative : bleaching text , i.e. , transforming lexical strings into more abstract features .",2,0.41168532,165.42558586834704,18
308,This study provides evidence that such features allow for better transfer across languages .,3,0.95831823,39.99687350182575,14
308,"Moreover , we present a first study on the ability of humans to perform cross-lingual gender prediction .",3,0.62108076,17.918257236237697,18
308,"We find that human predictive power proves similar to that of our bleached models , and both perform better than lexical models .",3,0.9755197,98.29087120513027,23
309,"Recent embedding-based methods in bilingual lexicon induction show good results , but do not take advantage of orthographic features , such as edit distance , which can be helpful for pairs of related languages .",0,0.71349776,37.66538499416935,37
309,"This work extends embedding-based methods to incorporate these features , resulting in significant accuracy gains for related languages .",3,0.44765037,57.74927867901534,21
310,We propose an entity-centric neural crosslingual coreference model that builds on multi-lingual embeddings and language independent features .,1,0.5021399,27.714518129890063,19
310,We perform both intrinsic and extrinsic evaluations of our model .,2,0.74030817,14.513809346810051,11
310,"In the intrinsic evaluation , we show that our model , when trained on English and tested on Chinese and Spanish , achieves competitive results to the models trained directly on Chinese and Spanish respectively .",3,0.92989844,29.826000019034034,36
310,"In the extrinsic evaluation , we show that our English model helps achieve superior entity linking accuracy on Chinese and Spanish test sets than the top 2015 TAC system without using any annotated data from Chinese or Spanish .",3,0.93815166,46.610872523833706,39
311,Multilingual learning for Neural Named Entity Recognition ( NNER ) involves jointly training a neural network for multiple languages .,0,0.8665527,35.754917524208835,20
311,"Typically , the goal is improving the NER performance of one of the languages ( the primary language ) using the other assisting languages .",0,0.86030257,89.03222769204683,25
311,We show that the divergence in the tag distributions of the common named entities between the primary and assisting languages can reduce the effectiveness of multilingual learning .,3,0.9516963,55.446443664156504,28
311,"To alleviate this problem , we propose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language .",2,0.5217456,58.99679835035331,27
311,"We empirically show that our data selection strategy improves NER performance in many languages , including those with very limited training data .",3,0.9310153,34.27007184509991,23
312,"Conventional Open Information Extraction ( Open IE ) systems are usually built on hand-crafted patterns from other NLP tools such as syntactic parsing , yet they face problems of error propagation .",0,0.9506078,40.39701837069429,33
312,"In this paper , we propose a neural Open IE approach with an encoder-decoder framework .",1,0.8591393,22.380911220444236,16
312,"Distinct from existing methods , the neural Open IE approach learns highly confident arguments and relation tuples bootstrapped from a state-of-the-art Open IE system .",2,0.41567308,55.82743606293523,30
312,"An empirical study on a large benchmark dataset shows that the neural Open IE system significantly outperforms several baselines , while maintaining comparable computational efficiency .",3,0.8775076,36.94717546909822,26
313,Document-level information is very important for event detection even at sentence level .,0,0.7529113,45.84102068703643,15
313,"In this paper , we propose a novel Document Embedding Enhanced Bi-RNN model , called DEEB-RNN , to detect events in sentences .",1,0.863466,39.3638071598452,25
313,"This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN , which pays word-level attention to event triggers and sentence-level attention to those sentences containing events .",2,0.69422495,52.3908339446861,37
313,It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences .,2,0.67515385,52.19333801940478,23
313,"Through experiments on the ACE-2005 dataset , we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods .",3,0.60008454,20.62967426170824,33
314,We propose a method that can leverage unlabeled data to learn a matching model for response selection in retrieval-based chatbots .,1,0.48252416,21.825736099071072,23
314,"The method employs a sequence-to-sequence architecture ( Seq2Seq ) model as a weak annotator to judge the matching degree of unlabeled pairs , and then performs learning with both the weak signals and the unlabeled data .",2,0.8374282,26.66357741697693,37
314,Experimental results on two public data sets indicate that matching models get significant improvements when they are learned with the proposed method .,3,0.91319555,36.14140135737084,23
315,"We present a generative neural network model for slot filling based on a sequence-to-sequence ( Seq2Seq ) model together with a pointer network , in the situation where only sentence-level slot annotations are available in the spoken dialogue data .",2,0.64736176,22.814011517895477,43
315,"This model predicts slot values by jointly learning to copy a word which may be out-of-vocabulary ( OOV ) from an input utterance through a pointer network , or generate a word within the vocabulary through an attentional Seq2Seq model .",2,0.5829314,34.648402057120876,42
315,"Experimental results show the effectiveness of our slot filling model , especially at addressing the OOV problem .",3,0.9720497,59.471453578161245,18
315,"Additionally , we integrate the proposed model into a spoken language understanding system and achieve the state-of-the-art performance on the benchmark data .",3,0.5618025,10.649628482215245,29
316,Robust dialogue belief tracking is a key component in maintaining good quality dialogue systems .,0,0.86504406,69.0350188498812,15
316,"The tasks that dialogue systems are trying to solve are becoming increasingly complex , requiring scalability to multi-domain , semantically rich dialogues .",0,0.93351525,56.40635368694436,23
316,"However , most current approaches have difficulty scaling up with domains because of the dependency of the model parameters on the dialogue ontology .",0,0.86062294,58.22772246058006,24
316,"In this paper , a novel approach is introduced that fully utilizes semantic similarity between dialogue utterances and the ontology terms , allowing the information to be shared across domains .",1,0.6557695,36.740473770041625,31
316,"The evaluation is performed on a recently collected multi-domain dialogues dataset , one order of magnitude larger than currently available corpora .",2,0.66054136,37.000622408144515,22
316,"Our model demonstrates great capability in handling multi-domain dialogues , simultaneously outperforming existing state-of-the-art models in single-domain dialogue tracking tasks .",3,0.9485092,18.25693175894666,27
317,Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance .,0,0.8950544,46.08826213031172,14
317,"Most existing approaches design sophisticated features or exploit various off-the-shelf tools , but achieve little success .",0,0.84832174,63.6865314215392,19
317,"In this paper , we propose a new transition-based discourse parser that makes use of memory networks to take discourse cohesion into account .",1,0.919238,34.110398842691744,26
317,"The automatically captured discourse cohesion benefits discourse parsing , especially for long span scenarios .",3,0.7902518,698.8196518847884,15
317,"Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods , and the memory based discourse cohesion can improve the overall parsing performance significantly .",3,0.9459642,67.92471785463576,30
318,Annotation corpus for discourse relations benefits NLP tasks such as machine translation and question answering .,0,0.81702864,53.57265708056103,16
318,"In this paper , we present SciDTB , a domain-specific discourse treebank annotated on scientific articles .",1,0.8128089,60.36468584677171,17
318,"Different from widely-used RST-DT and PDTB , SciDTB uses dependency trees to represent discourse structure , which is flexible and simplified to some extent but do not sacrifice structural integrity .",3,0.39988515,115.17034315423248,35
318,"We discuss the labeling framework , annotation workflow and some statistics about SciDTB .",3,0.43424395,407.4148302704698,14
318,"Furthermore , our treebank is made as a benchmark for evaluating discourse dependency parsers , on which we provide several baselines as fundamental work .",3,0.6812637,72.39922305754536,25
319,"Because obtaining training data is often the most difficult part of an NLP or ML project , we develop methods for predicting how much data is required to achieve a desired test accuracy by extrapolating results from models trained on a small pilot training dataset .",0,0.50681823,27.204000466657828,46
319,"We model how accuracy varies as a function of training size on subsets of the pilot data , and use that model to predict how much training data would be required to achieve the desired accuracy .",2,0.7218239,24.805763396571937,37
319,We introduce a new performance extrapolation task to evaluate how well different extrapolations predict accuracy on larger training sets .,1,0.40790528,48.11532441892972,20
319,We show that details of hyperparameter optimisation and the extrapolation models can have dramatic effects in a document classification task .,3,0.88805956,42.99097211437294,21
319,We believe this is an important first step in developing methods for estimating the resources required to meet specific engineering performance targets .,3,0.93603414,33.475432303891075,23
320,"We investigate the influence that document context exerts on human acceptability judgements for English sentences , via two sets of experiments .",1,0.5265571,57.531871258153075,22
320,The first compares ratings for sentences presented on their own with ratings for the same set of sentences given in their document contexts .,2,0.8183023,59.7064958448796,24
320,The second assesses the accuracy with which two types of neural models — one that incorporates context during training and one that does not — predict these judgements .,2,0.78553456,26.835215915874603,29
320,"Our results indicate that : ( 1 ) context improves acceptability ratings for ill-formed sentences , but also reduces them for well-formed sentences ;",3,0.9904433,81.06994570464397,26
320,and ( 2 ) context helps unsupervised systems to model acceptability .,3,0.58790267,138.4661533375447,12
321,"Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other , or to a shared space .",0,0.92044044,28.65468960441677,30
321,"The predicted vectors are then used to perform e.g. , retrieval or labeling .",2,0.4358851,83.5838958553359,14
321,"Thus , the success of the whole system relies on the ability of the mapping to make the neighborhood structure ( i.e. , the pairwise similarities ) of the predicted vectors akin to that of the target vectors .",3,0.5674394,42.039505814012934,39
321,"However , whether this is achieved has not been investigated yet .",0,0.9359839,29.015475419671215,12
321,"Here , we propose a new similarity measure and two ad hoc experiments to shed light on this issue .",1,0.6665784,28.411266770525668,20
321,In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings ( up to five layers ) using a rich diversity of image and text features and loss functions .,2,0.5208211,35.57024099847666,41
321,"Our results reveal that , surprisingly , the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors .",3,0.9887828,45.62762061743963,29
321,"In a second experiment , we further show that untrained nets do not significantly disrupt the neighborhood ( i.e. , semantic ) structure of the input vectors .",3,0.89337367,63.882402605300925,28
322,"Dynamic oracles provide strong supervision for training constituency parsers with exploration , but must be custom defined for a given parser ’s transition system .",3,0.51726913,160.86967089894432,25
322,We explore using a policy gradient method as a parser-agnostic alternative .,2,0.4943783,80.93545314778275,12
322,"In addition to directly optimizing for a tree-level metric such as F1 , policy gradient has the potential to reduce exposure bias by allowing exploration during training ;",3,0.6571329,90.71760727924433,28
322,"moreover , it does not require a dynamic oracle for supervision .",3,0.4778856,100.35339349227482,12
322,"On four constituency parsers in three languages , the method substantially outperforms static oracle likelihood training in almost all settings .",3,0.84990835,72.68421453763396,21
322,"For parsers where a dynamic oracle is available ( including a novel oracle which we define for the transition system of Dyer et al. , 2016 ) , policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle .",3,0.63616544,71.37057609982786,45
323,"Recently , span-based constituency parsing has achieved competitive accuracies with extremely simple models by using bidirectional RNNs to model “ spans ” .",0,0.92298,53.35655823325801,23
323,"However , the minimal span parser of Stern et al .",0,0.62878746,167.55196547120693,11
323,"( 2017a ) which holds the current state of the art accuracy is a chart parser running in cubic time , O ( n3 ) , which is too slow for longer sentences and for applications beyond sentence boundaries such as end-to-end discourse parsing and joint sentence boundary detection and parsing .",3,0.48506048,83.24874372281691,54
323,"We propose a linear-time constituency parser with RNNs and dynamic programming using graph-structured stack and beam search , which runs in time O( n b2 ) where b is the beam size .",2,0.55906653,100.13275052810981,34
323,We further speed this up to O( n b log b ) by integrating cube pruning .,2,0.49267253,223.17614968793944,18
323,"Compared with chart parsing baselines , this linear-time parser is substantially faster for long sentences on the Penn Treebank and orders of magnitude faster for discourse parsing , and achieves the highest F1 accuracy on the Penn Treebank among single model end-to-end systems .",3,0.87161314,36.31448618486676,46
324,"While syntactic dependency annotations concentrate on the surface or functional structure of a sentence , semantic dependency annotations aim to capture between-word relationships that are more closely related to the meaning of a sentence , using graph-structured representations .",0,0.8430723,26.68962242181981,41
324,We extend the LSTM-based syntactic parser of Dozat and Manning ( 2017 ) to train on and generate these graph structures .,2,0.7472014,70.65090989845254,24
324,"The resulting system on its own achieves state-of-the-art performance , beating the previous , substantially more complex state-of-the-art system by 0.6 % labeled F1 .",3,0.91389716,22.52325219167048,37
324,"Adding linguistically richer input representations pushes the margin even higher , allowing us to beat it by 1.9 % labeled F1 .",3,0.9143771,109.72424890702361,22
325,An abugida is a writing system where the consonant letters represent syllables with a default vowel and other vowels are denoted by diacritics .,0,0.8808845,35.608627959930686,24
325,We investigate the feasibility of recovering the original text written in an abugida after omitting subordinate diacritics and merging consonant letters with similar phonetic values .,1,0.60710335,79.88701524811246,26
325,This is crucial for developing more efficient input methods by reducing the complexity in abugidas .,3,0.5377038,100.38263546253349,16
325,"Four abugidas in the southern Brahmic family , i.e. , Thai , Burmese , Khmer , and Lao , were studied using a newswire 20,000-sentence dataset .",2,0.75614655,76.16975137649143,29
325,"We compared the recovery performance of a support vector machine and an LSTM-based recurrent neural network , finding that the abugida graphemes could be recovered with 94 %-97 % accuracy at the top-1 level and 98 %-99 % at the top-4 level , even after omitting most diacritics ( 10-30 types ) and merging the remaining 30-50 characters into 21 graphemes .",3,0.8217227,41.4251390667344,74
326,"As more and more academic papers are being submitted to conferences and journals , evaluating all these papers by professionals is time-consuming and can cause inequality due to the personal factors of the reviewers .",0,0.8449971,36.4572081215336,37
326,"In this paper , in order to assist professionals in evaluating academic papers , we propose a novel task : automatic academic paper rating ( AAPR ) , which automatically determine whether to accept academic papers .",1,0.87581265,53.20549204290011,37
326,We build a new dataset for this task and propose a novel modularized hierarchical convolutional neural network to achieve automatic academic paper rating .,2,0.41618314,31.697786529413236,24
326,Evaluation results show that the proposed model outperforms the baselines by a large margin .,3,0.9708647,5.184095761172816,15
326,The dataset and code are available at https://github.com/lancopku/AAPR .,3,0.600703,13.35343306186974,9
327,"In this work , we present an approach based on combining string kernels and word embeddings for automatic essay scoring .",1,0.8355533,37.48746122691459,21
327,"String kernels capture the similarity among strings based on counting common character n-grams , which are a low-level yet powerful type of feature , demonstrating state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification .",0,0.57639986,41.06591627274319,48
327,"To our best knowledge , we are the first to apply string kernels to automatically score essays .",3,0.87044555,91.56829390735001,18
327,"We are also the first to combine them with a high-level semantic feature representation , namely the bag-of-super-word-embeddings .",3,0.7165567,36.57645808971703,21
327,"We report the best performance on the Automated Student Assessment Prize data set , in both in-domain and cross-domain settings , surpassing recent state-of-the-art deep learning approaches .",3,0.89444494,23.626253316703583,36
328,Predicting how Congressional legislators will vote is important for understanding their past and future behavior .,0,0.8529726,34.53635209030711,16
328,"However , previous work on roll-call prediction has been limited to single session settings , thus not allowing for generalization across sessions .",0,0.89541054,34.97376094448165,23
328,"In this paper , we show that text alone is insufficient for modeling voting outcomes in new contexts , as session changes lead to changes in the underlying data generation process .",1,0.6495943,53.162113742326476,32
328,"We propose a novel neural method for encoding documents alongside additional metadata , achieving an average of a 4 % boost in accuracy over the previous state-of-the-art .",3,0.5210963,28.49878976128624,33
329,"For extracting meaningful topics from texts , their structures should be considered properly .",0,0.5329196,219.97484582741026,14
329,"In this paper , we aim to analyze structured time-series documents such as a collection of news articles and a series of scientific papers , wherein topics evolve along time depending on multiple topics in the past and are also related to each other at each time .",1,0.9128841,30.342294022453824,49
329,"To this end , we propose a dynamic and static topic model , which simultaneously considers the dynamic structures of the temporal topic evolution and the static structures of the topic hierarchy at each time .",2,0.56238115,41.08128103303116,36
329,"We show the results of experiments on collections of scientific papers , in which the proposed method outperformed conventional models .",3,0.8012691,36.65027308131646,21
329,"Moreover , we show an example of extracted topic structures , which we found helpful for analyzing research activities .",3,0.7457826,87.9782726155614,20
330,"Recent emerged phrase-level topic models are able to provide topics of phrases , which are easy to read for humans .",0,0.9198379,103.2766301334406,22
330,But these models are lack of the ability to capture the correlation structure among the discovered numerous topics .,0,0.7370025,113.4964398099425,19
330,We propose a novel topic model PhraseCTM and a two-stage method to find out the correlated topics at phrase level .,2,0.58325917,59.8823304434828,22
330,"In the first stage , we train PhraseCTM , which models the generation of words and phrases simultaneously by linking the phrases and component words within Markov Random Fields when they are semantically coherent .",2,0.8613225,73.8102740176845,35
330,"In the second stage , we generate the correlation of topics from PhraseCTM .",2,0.8176456,142.4152249569532,14
330,"We evaluate our method by a quantitative experiment and a human study , showing the correlated topic modeling on phrases is a good and practical way to interpret the underlying themes of a corpus .",3,0.6234095,80.83675381024311,35
331,"In this paper , we address the problem of finding a novel document descriptor based on the covariance matrix of the word vectors of a document .",1,0.87210464,15.129127749819013,27
331,"Our descriptor has a fixed length , which makes it easy to use in many supervised and unsupervised applications .",3,0.6037413,23.561557639689546,20
331,We tested our novel descriptor in different tasks including supervised and unsupervised settings .,2,0.6149055,50.98972664594207,14
331,Our evaluation shows that our document covariance descriptor fits different tasks with competitive performance against state-of-the-art methods .,3,0.9744441,39.141289825359216,23
332,We report an empirical study on the task of negation scope extraction given the negation cue .,1,0.6539016,56.26945477894825,17
332,"Our key observation is that certain useful information such as features related to negation cue , long-distance dependencies as well as some latent structural information can be exploited for such a task .",3,0.94106233,54.81092638679591,33
332,"We design approaches based on conditional random fields ( CRF ) , semi-Markov CRF , as well as latent-variable CRF models to capture such information .",2,0.86860526,46.6738251130066,27
332,Extensive experiments on several standard datasets demonstrate that our approaches are able to achieve better results than existing approaches reported in the literature .,3,0.90173227,12.26276844999585,24
333,"This work deals with SciTail , a natural entailment challenge derived from a multi-choice question answering problem .",1,0.65263265,46.374966830372976,18
333,"The premises and hypotheses in SciTail were generated with no awareness of each other , and did not specifically aim at the entailment task .",3,0.55910337,84.56234159888722,25
333,This makes it more challenging than other entailment data sets and more directly useful to the end-task – question answering .,0,0.5379632,56.02867691668798,21
333,We propose DEISTE ( deep explorations of inter-sentence interactions for textual entailment ) for this entailment task .,2,0.35044262,95.444108139608,18
333,"Given word-to-word interactions between the premise-hypothesis pair ( P , H ) , DEISTE consists of : ( i ) a parameter-dynamic convolution to make important words in P and H play a dominant role in learnt representations ;",2,0.41779807,98.08028153246764,43
333,and ( ii ) a position-aware attentive convolution to encode the representation and position information of the aligned word pairs .,2,0.61949444,61.34652483386042,23
333,Experiments show that DEISTE gets ≈ 5 % improvement over prior state of the art and that the pretrained DEISTE on SciTail generalizes well on RTE-5 .,3,0.9327714,41.37926828083253,29
334,A homographic pun is a form of wordplay in which one signifier ( usually a word ) suggests two or more meanings by exploiting polysemy for an intended humorous or rhetorical effect .,0,0.92832077,65.61088220863681,33
334,"In this paper , we focus on the task of pun location , which aims to identify the pun word in a given short text .",1,0.8819034,25.346284726571923,26
334,We propose a sense-aware neural model to address this challenging task .,1,0.52487314,25.852555585091043,12
334,"Our model first obtains several WSD results for the text , and then leverages a bidirectional LSTM network to model each sequence of word senses .",2,0.6405998,38.59634994653631,26
334,The outputs at each time step for different LSTM networks are then concatenated for prediction .,2,0.743094,26.620555576602086,16
334,Evaluation results on the benchmark SemEval 2017 dataset demonstrate the efficacy of our proposed model .,3,0.9485999,13.90465402598563,16
335,Word Embeddings have recently imposed themselves as a standard for representing word meaning in NLP .,0,0.9529267,42.37608529343866,16
335,"Semantic similarity between word pairs has become the most common evaluation benchmark for these representations , with vector cosine being typically used as the only similarity metric .",0,0.9277253,58.39541504021296,28
335,"In this paper , we report experiments with a rank-based metric for WE , which performs comparably to vector cosine in similarity estimation and outperforms it in the recently-introduced and challenging task of outlier detection , thus suggesting that rank-based measures can improve clustering quality .",1,0.61592644,35.7637671920389,52
336,Word embeddings are crucial to many natural language processing tasks .,0,0.91906786,9.992695459420416,11
336,The quality of embeddings relies on large non-noisy corpora .,0,0.6757941,27.275276860490614,10
336,"Arabic dialects lack large corpora and are noisy , being linguistically disparate with no standardized spelling .",0,0.9276945,90.27065051044276,17
336,We make three contributions to address this noise .,3,0.34793288,49.56980233808218,9
336,"First , we describe simple but effective adaptations to word embedding tools to maximize the informative content leveraged in each training sentence .",2,0.5082074,66.08181860227779,23
336,"Second , we analyze methods for representing disparate dialects in one embedding space , either by mapping individual dialects into a shared space or learning a joint model of all dialects .",2,0.775011,34.629357835446896,32
336,"Finally , we evaluate via dictionary induction , showing that two metrics not typically reported in the task enable us to analyze our contributions ’ effects on low and high frequency words .",3,0.6955794,135.05544864355764,33
336,"In addition to boosting performance between 2-53 % , we specifically improve on noisy , low frequency forms without compromising accuracy on high frequency forms .",3,0.6590664,136.77338390958184,28
337,Negative sampling is an important component in word2vec for distributed word representation learning .,0,0.62788606,33.69925881275194,14
337,"We hypothesize that taking into account global , corpus-level information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution .",3,0.445553,60.25862327967802,39
337,In this purpose we pre-compute word co-occurrence statistics from the corpus and apply to it network algorithms such as random walk .,2,0.66289395,52.344038781906015,22
337,We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5 % and improves the performance on word similarity tasks by about 1 % compared to the skip-gram negative sampling baseline .,3,0.6275374,26.499701394940352,45
338,This paper presents the first study aimed at capturing stylistic similarity between words in an unsupervised manner .,1,0.70844245,16.133861580398,18
338,"We propose extending the continuous bag of words ( CBOW ) embedding model ( Mikolov et al. , 2013 b ) to learn style-sensitive word vectors using a wider context window under the assumption that the style of all the words in an utterance is consistent .",2,0.6461657,57.517909385561055,47
338,"In addition , we introduce a novel task to predict lexical stylistic similarity and to create a benchmark dataset for this task .",2,0.605913,20.776475152868382,23
338,Our experiment with this dataset supports our assumption and demonstrates that the proposed extensions contribute to the acquisition of style-sensitive word embeddings .,3,0.97955185,31.24829656356708,23
339,Attention-based long short-term memory ( LSTM ) networks have proven to be useful in aspect-level sentiment classification .,0,0.91755337,22.736156208782276,20
339,"However , due to the difficulties in annotating aspect-level data , existing public datasets for this task are all relatively small , which largely limits the effectiveness of those neural models .",0,0.88541543,45.37926897852027,32
339,"In this paper , we explore two approaches that transfer knowledge from document-level data , which is much less expensive to obtain , to improve the performance of aspect-level sentiment classification .",1,0.8532384,31.725526545496155,34
339,"We demonstrate the effectiveness of our approaches on 4 public datasets from SemEval 2014 , 2015 , and 2016 , and we show that attention-based LSTM benefits from document-level knowledge in multiple ways .",3,0.716348,25.62911768081824,38
340,Humor is one of the most attractive parts in human communication .,0,0.9632253,27.762028206824127,12
340,"However , automatically recognizing humor in text is challenging due to the complex characteristics of humor .",0,0.94357157,33.97372931490549,17
340,This paper proposes to model sentiment association between discourse units to indicate how the punchline breaks the expectation of the setup .,1,0.82513624,133.9280382297521,22
340,"We found that discourse relation , sentiment conflict and sentiment transition are effective indicators for humor recognition .",3,0.9868684,262.8966873627719,18
340,"On the perspective of using sentiment related features , sentiment association in discourse is more useful than counting the number of emotional words .",3,0.49102095,97.57457437117633,24
341,One key task of fine-grained sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on .,0,0.9178766,26.536390246141796,25
341,This paper focuses on supervised aspect extraction using deep learning .,1,0.8496984,57.26512628105023,11
341,"Unlike other highly sophisticated supervised deep learning models , this paper proposes a novel and yet simple CNN model employing two types of pre-trained embeddings for aspect extraction : general-purpose embeddings and domain-specific embeddings .",1,0.3363704,22.27102496640121,37
341,"Without using any additional supervision , this model achieves surprisingly good results , outperforming state-of-the-art sophisticated existing methods .",3,0.8650678,32.303470344100035,25
341,"To our knowledge , this paper is the first to report such double embeddings based CNN model for aspect extraction and achieve very good results .",3,0.954381,64.59962456540146,26
342,"The process of obtaining high quality labeled data for natural language understanding tasks is often slow , error-prone , complicated and expensive .",0,0.9401791,31.486142525736316,23
342,"With the vast usage of neural networks , this issue becomes more notorious since these networks require a large amount of labeled data to produce satisfactory results .",0,0.90871775,30.356917796113514,28
342,We propose a methodology to blend high quality but scarce strong labeled data with noisy but abundant weak labeled data during the training of neural networks .,1,0.37243465,46.92210130730913,27
342,Experiments in the context of topic-dependent evidence detection with two forms of weak labeled data show the advantages of the blending scheme .,3,0.8615774,62.81422379899214,23
342,"In addition , we provide a manually annotated data set for the task of topic-dependent evidence detection .",2,0.62127125,33.284232933031056,18
342,"We believe that blending weak and strong labeled data is a general notion that may be applicable to many language understanding tasks , and can especially assist researchers who wish to train a network but have a small amount of high quality labeled data for their task of interest .",3,0.97512895,43.38855612500135,50
343,"We propose a tri-modal architecture to predict Big Five personality trait scores from video clips with different channels for audio , text , and video data .",2,0.46924788,55.919086060148395,27
343,"For each channel , stacked Convolutional Neural Networks are employed .",2,0.7091067,62.67907550887543,11
343,The channels are fused both on decision-level and by concatenating their respective fully connected layers .,2,0.617025,110.12328948008002,18
343,"It is shown that a multimodal fusion approach outperforms each single modality channel , with an improvement of 9.4 % over the best individual modality ( video ) .",3,0.9591687,41.4043048445408,29
343,"Full backpropagation is also shown to be better than a linear combination of modalities , meaning complex interactions between modalities can be leveraged to build better models .",3,0.9081038,21.835994925120044,28
343,"Furthermore , we can see the prediction relevance of each modality for each trait .",3,0.9171493,79.32140182525941,15
343,The described model can be used to increase the emotional intelligence of virtual agents .,3,0.92133605,34.12247782180107,15
344,This paper investigates the construction of a strong baseline based on general purpose sequence-to-sequence models for constituency parsing .,1,0.8840927,24.7355023897523,21
344,"We incorporate several techniques that were mainly developed in natural language generation tasks , e.g. , machine translation and summarization , and demonstrate that the sequence-to-sequence model achieves the current top-notch parsers ’ performance ( almost ) without requiring any explicit task-specific knowledge or architecture of constituent parsing .",3,0.6737242,51.1154134278102,53
345,How to make the most of multiple heterogeneous treebanks when training a monolingual dependency parser is an open question .,0,0.7488459,20.67448117926505,20
345,"We start by investigating previously suggested , but little evaluated , strategies for exploiting multiple treebanks based on concatenating training sets , with or without fine-tuning .",1,0.3837587,75.61888133391096,27
345,We go on to propose a new method based on treebank embeddings .,3,0.46529257,14.598237644522344,13
345,"We perform experiments for several languages and show that in many cases fine-tuning and treebank embeddings lead to substantial improvements over single treebanks or concatenation , with average gains of 2.0 – 3.5 LAS points .",3,0.84785193,22.805060212612798,36
345,"We argue that treebank embeddings should be preferred due to their conceptual simplicity , flexibility and extensibility .",3,0.8377444,29.6301738833145,18
346,"Chart constraints , which specify at which string positions a constituent may begin or end , have been shown to speed up chart parsers for PCFGs .",0,0.6746707,121.1984257607091,27
346,We generalize chart constraints to more expressive grammar formalisms and describe a neural tagger which predicts chart constraints at very high precision .,2,0.41051504,127.81964792710193,23
346,"Our constraints accelerate both PCFG and TAG parsing , and combine effectively with other pruning techniques ( coarse-to-fine and supertagging ) for an overall speedup of two orders of magnitude , while improving accuracy .",3,0.85968417,86.72050433916294,36
347,Neural vector representations are ubiquitous throughout all subfields of NLP .,0,0.9128372,49.38347058296585,11
347,"While word vectors have been studied in much detail , thus far only little light has been shed on the properties of sentence embeddings .",0,0.9458132,27.929914652932514,25
347,"In this paper , we assess to what extent prominent sentence embedding methods exhibit select semantic properties .",1,0.9503968,103.69607172790407,18
347,We propose a framework that generate triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings .,1,0.40248775,27.033496335759462,32
348,"We present the Supervised Directional Similarity Network , a novel neural architecture for learning task-specific transformation functions on top of general-purpose word embeddings .",1,0.44896466,28.531932893214876,27
348,"Relying on only a limited amount of supervision from task-specific scores on a subset of the vocabulary , our architecture is able to generalise and transform a general-purpose distributional vector space to model the relation of lexical entailment .",3,0.7291372,33.222115703325876,42
348,"Experiments show excellent performance on scoring graded lexical entailment , raising the state-of-the-art on the HyperLex dataset by approximately 25 % .",3,0.924897,45.23574091884431,27
349,"Intelligent systems require common sense , but automatically extracting this knowledge from text can be difficult .",0,0.9306313,79.00266322187622,17
349,"We propose and assess methods for extracting one type of commonsense knowledge , object-property comparisons , from pre-trained embeddings .",1,0.5055468,65.29391269503265,22
349,"In experiments , we show that our approach exceeds the accuracy of previous work but requires substantially less hand-annotated knowledge .",3,0.9034281,38.51790835775827,23
349,"Further , we show that an active learning approach that synthesizes common-sense queries can boost accuracy .",3,0.94437337,43.11390996913409,17
350,We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge .,2,0.5581815,24.85596616241911,29
350,"The new examples are simpler than the SNLI test set , containing sentences that differ by at most one word from sentences in the training set .",3,0.75016606,46.27935912515921,27
350,"Yet , the performance on the new test set is substantially worse across systems trained on SNLI , demonstrating that these systems are limited in their generalization ability , failing to capture many simple inferences .",3,0.95856977,47.23357258115286,36
351,Neural Machine Translation ( NMT ) is notorious for its need for large amounts of bilingual data .,0,0.9661894,21.201863300875356,18
351,An effective approach to compensate for this requirement is Multi-Task Learning ( MTL ) to leverage different linguistic resources as a source of inductive bias .,0,0.9004604,37.92735309322557,26
351,"Current MTL architectures are based on the Seq2Seq transduction , and ( partially ) share different components of the models among the tasks .",0,0.8085124,72.57467222221638,24
351,"However , this MTL approach often suffers from task interference and is not able to fully capture commonalities among subsets of tasks .",0,0.8150855,45.09131222087042,23
351,We address this issue by extending the recurrent units with multiple “ blocks ” along with a trainable “ routing network ” .,2,0.45324013,60.45052351216497,23
351,"The routing network enables adaptive collaboration by dynamic sharing of blocks conditioned on the task at hand , input , and model state .",3,0.37038332,191.83802602270708,24
351,"Empirical evaluation of two low-resource translation tasks , English to Vietnamese and Farsi , show + 1 BLEU score improvements compared to strong baselines .",3,0.5613588,28.058127630841994,25
352,"Simultaneous interpretation , translation of the spoken word in real-time , is both highly challenging and physically demanding .",0,0.9296105,74.60191775040337,19
352,"Methods to predict interpreter confidence and the adequacy of the interpreted message have a number of potential applications , such as in computer-assisted interpretation interfaces or pedagogical tools .",0,0.6268169,56.627958978834066,30
352,We propose the task of predicting simultaneous interpreter performance by building on existing methodology for quality estimation ( QE ) of machine translation output .,1,0.62046885,97.70027866690099,25
352,"In experiments over five settings in three language pairs , we extend a QE pipeline to estimate interpreter performance ( as approximated by the METEOR evaluation metric ) and propose novel features reflecting interpretation strategy and evaluation measures that further improve prediction accuracy .",2,0.5112203,83.17914588218578,44
353,"Previous approaches to multilingual semantic dependency parsing treat languages independently , without exploiting the similarities between semantic structures across languages .",0,0.867026,74.45546549546533,21
353,We experiment with a new approach where we combine resources from different languages in the CoNLL 2009 shared task to build a single polyglot semantic dependency parser .,2,0.8394464,40.52825664959029,28
353,"Notwithstanding the absence of parallel data , and the dissimilarity in annotations between languages , our approach results in improvement in parsing performance on several languages over a monolingual baseline .",3,0.9011073,33.11221502791156,31
353,Analysis of the polyglot models ’ performance provides a new understanding of the similarities and differences between languages in the shared task .,3,0.81654716,25.017579130439692,23
354,"With the development of several multilingual datasets used for semantic parsing , recent research efforts have looked into the problem of learning semantic parsers in a multilingual setup .",0,0.9513263,23.943219360800448,29
354,"However , how to improve the performance of a monolingual semantic parser for a specific language by leveraging data annotated in different languages remains a research question that is under-explored .",0,0.88583297,16.005299613890674,31
354,"In this work , we present a study to show how learning distributed representations of the logical forms from data annotated in different languages can be used for improving the performance of a monolingual semantic parser .",1,0.91038185,21.684111624981956,37
354,We extend two existing monolingual semantic parsers to incorporate such cross-lingual distributed logical representations as features .,2,0.69115454,40.51740690197725,17
354,Experiments show that our proposed approach is able to yield improved semantic parsing results on the standard multilingual GeoQuery dataset .,3,0.96294767,25.102839722797498,21
355,We propose a novel neural method to extract drug-drug interactions ( DDIs ) from texts using external drug molecular structure information .,1,0.64082265,59.59198446606108,22
355,"We encode textual drug pairs with convolutional neural networks and their molecular pairs with graph convolutional networks ( GCNs ) , and then we concatenate the outputs of these two networks .",2,0.90512615,30.124384181482398,32
355,"In the experiments , we show that GCNs can predict DDIs from the molecular structures of drugs in high accuracy and the molecular information can enhance text-based DDI extraction by 2.39 percent points in the F-score on the DDIExtraction 2013 shared task data set .",3,0.94056886,81.06681453494863,47
356,Named Entity Disambiguation ( NED ) systems perform well on news articles and other texts covering a specific time interval .,0,0.8326922,26.05589742070876,21
356,"However , NED quality drops when inputs span long time periods like in archives or historic corpora .",0,0.53860325,223.34573904878593,18
356,This paper presents the first time-aware method for NED that resolves ambiguities even when mention contexts give only few cues .,1,0.7413978,73.27220536650896,23
356,The method is based on computing temporal signatures for entities and comparing these to the temporal contexts of input mentions .,2,0.6348137,59.11397124747002,21
356,Our experiments show superior quality on a newly created diachronic corpus .,3,0.98083055,86.10351143218953,12
357,Many corpora span broad periods of time .,0,0.91359985,59.978005988494466,8
357,"Language processing models trained during one time period may not work well in future time periods , and the best model may depend on specific times of year ( e.g. , people might describe hotels differently in reviews during the winter versus the summer ) .",3,0.52119845,67.1620274951749,46
357,"This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals , considering both seasonal intervals ( intervals that repeat across years , e.g. , winter ) and non-seasonal intervals ( e.g. , specific years ) .",1,0.94211423,40.70024925739288,46
357,"We show experimentally that classification performance varies over time , and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time .",3,0.8441808,30.616083589130803,30
358,Query auto-completion is a search engine feature whereby the system suggests completed queries as the user types .,0,0.744113,60.7091748381034,18
358,"Recently , the use of a recurrent neural network language model was suggested as a method of generating query completions .",0,0.9050667,33.03730938637481,21
358,We show how an adaptable language model can be used to generate personalized completions and how the model can use online updating to make predictions for users not seen during training .,3,0.7870427,36.14521879319535,32
358,The personalized predictions are significantly better than a baseline that uses no user information .,3,0.9480218,45.83984033232329,15
359,"In this paper , we focus on the problem of building assistive systems that can help users to write reviews .",1,0.9285716,24.332328072517075,21
359,We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases ( e.g .,2,0.6725038,48.169558211033674,19
359,"review summaries , product titles ) provided as input to the system .",2,0.70634025,229.95395323201615,13
359,We incorporate aspect-level information via an aspect encoder that learns aspect-aware user and item representations .,2,0.8086229,45.757454624934205,16
359,An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders .,2,0.6048268,57.03262356864869,18
359,Experimental results show that our model successfully learns representations capable of generating coherent and diverse reviews .,3,0.97290933,23.537179018551587,17
359,"In addition , the learned aspect-aware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences .",3,0.83675647,90.03730761247606,28
360,Text simplification ( TS ) is a monolingual text-to-text transformation task where an original ( complex ) text is transformed into a target ( simpler ) text .,0,0.92441094,25.10476097597996,32
360,Most recent work is based on sequence-to-sequence neural models similar to those used for machine translation ( MT ) .,0,0.88039345,17.000871191631123,22
360,"Different from MT , TS data comprises more elaborate transformations , such as sentence splitting .",0,0.646888,446.54879234847147,16
360,"It can also contain multiple simplifications of the same original text targeting different audiences , such as school grade levels .",0,0.6530384,83.76663538218735,21
360,We explore these two features of TS to build models tailored for specific grade levels .,2,0.38610995,138.63587757030933,16
360,Our approach uses a standard sequence-to-sequence architecture where the original sequence is annotated with information about the target audience and / or the ( predicted ) type of simplification operation .,2,0.7893617,39.73322723259736,33
360,"We show that it outperforms state-of-the-art TS approaches ( up to 3 and 12 BLEU and SARI points , respectively ) , including when training data for the specific complex-simple combination of grade levels is not available , i.e .",3,0.9225065,58.38363774911475,46
360,zero-shot learning .,2,0.36639303,16.997300588590875,3
361,Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in NLP .,0,0.93099684,17.444855479750146,22
361,"We show that while vanilla seq2seq models can reach high scores on the proposed benchmark ( Narayan et al. , 2017 ) , they suffer from memorization of the training set which contains more than 89 % of the unique simple sentences from the validation and test sets .",3,0.9290967,59.91609092682191,49
361,"To aid this , we present a new train-development-test data split and neural models augmented with a copy-mechanism , outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task .",3,0.47217014,63.345321672506785,38
362,Most of the current abstractive text summarization models are based on the sequence-to-sequence model ( Seq2Seq ) .,0,0.91705525,8.824910395198835,18
362,"The source content of social media is long and noisy , so it is difficult for Seq2Seq to learn an accurate semantic representation .",0,0.92949533,25.318381320730065,24
362,"Compared with the source content , the annotated summary is short and well written .",3,0.8330823,49.64542657166101,15
362,"Moreover , it shares the same meaning as the source content .",0,0.65984184,60.67644303552362,12
362,"In this work , we supervise the learning of the representation of the source content with that of the summary .",2,0.40855202,41.63570656006528,21
362,"In implementation , we regard a summary autoencoder as an assistant supervisor of Seq2Seq .",3,0.57487404,57.335646815095124,15
362,"Following previous work , we evaluate our model on a popular Chinese social media dataset .",2,0.72070366,25.88175732459437,16
362,Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset .,3,0.9687715,4.658703996220549,21
363,LSTMs were introduced to combat vanishing gradients in simple RNNs by augmenting them with gated additive recurrent connections .,0,0.64175045,62.622643039904354,19
363,We present an alternative view to explain the success of LSTMs : the gates themselves are versatile recurrent models that provide more representational power than previously appreciated .,3,0.46334505,93.90073704835176,28
363,"We do this by decoupling the LSTM ’s gates from the embedded simple RNN , producing a new class of RNNs where the recurrence computes an element-wise weighted sum of context-independent functions of the input .",2,0.76468277,45.32722574188508,39
363,"Ablations on a range of problems demonstrate that the gating mechanism alone performs as well as an LSTM in most settings , strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients .",3,0.9439026,40.87313742552418,39
364,"While Recurrent Neural Networks ( RNNs ) are famously known to be Turing complete , this relies on infinite precision in the states and unbounded computation time .",0,0.9343208,49.94062447486367,28
364,We consider the case of RNNs with finite precision whose computation time is linear in the input length .,2,0.60230756,39.48941033513434,19
364,"Under these limitations , we show that different RNN variants have different computational power .",3,0.893396,48.21758742343767,15
364,"In particular , we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU .",3,0.9661794,49.35569190772793,28
364,This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior .,3,0.64338,78.95091966320219,15
364,We show empirically that the LSTM does indeed learn to effectively use the counting mechanism .,3,0.94235706,32.677536956910565,16
365,"Multi-choice reading comprehension is a challenging task , which involves the matching between a passage and a question-answer pair .",0,0.9511169,19.01946979492889,22
365,"This paper proposes a new co-matching approach to this problem , which jointly models whether a passage can match both a question and a candidate answer .",1,0.75378895,39.918688085998404,27
365,Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance .,3,0.9439687,4.652997274647652,19
366,The Story Cloze Test ( SCT ) is a recent framework for evaluating story comprehension and script learning .,0,0.9517072,96.8691788705165,19
366,There have been a variety of models tackling the SCT so far .,0,0.932348,46.51032050868865,13
366,"Although the original goal behind the SCT was to require systems to perform deep language understanding and commonsense reasoning for successful narrative understanding , some recent models could perform significantly better than the initial baselines by leveraging human-authorship biases discovered in the SCT dataset .",0,0.54638594,58.01885984557033,45
366,"In order to shed some light on this issue , we have performed various data analysis and analyzed a variety of top performing models presented for this task .",2,0.5904651,32.141095673004564,29
366,"Given the statistics we have aggregated , we have designed a new crowdsourcing scheme that creates a new SCT dataset , which overcomes some of the biases .",2,0.48564515,54.32878760870112,28
366,We benchmark a few models on the new dataset and show that the top-performing model on the original SCT dataset fails to keep up its performance .,3,0.82573515,21.762852158240577,27
366,Our findings further signify the importance of benchmarking NLP systems on various evolving test sets .,3,0.9904,69.1406691186648,16
367,Deep learning approaches for sentiment classification do not fully exploit sentiment linguistic knowledge .,0,0.85843307,44.566226905306806,14
367,"In this paper , we propose a Multi-sentiment-resource Enhanced Attention Network ( MEAN ) to alleviate the problem by integrating three kinds of sentiment linguistic knowledge ( e.g. , sentiment lexicon , negation words , intensity words ) into the deep neural network via attention mechanisms .",1,0.68403465,43.940143422028115,47
367,"By using various types of sentiment resources , MEAN utilizes sentiment-relevant information from different representation sub-spaces , which makes it more effective to capture the overall semantics of the sentiment , negation and intensity words for sentiment prediction .",3,0.52727765,88.27911523790955,40
367,The experimental results demonstrate that MEAN has robust superiority over strong competitors .,3,0.9785436,76.36260307361228,13
368,The huge cost of creating labeled training data is a common problem for supervised learning tasks such as sentiment classification .,0,0.92050093,27.692001782007093,21
368,Recent studies showed that pretraining with unlabeled data via a language model can improve the performance of classification models .,0,0.9095946,16.564177696403476,20
368,"In this paper , we take the concept a step further by using a conditional language model , instead of a language model .",1,0.61929524,23.74910490990305,24
368,"Specifically , we address a sentiment classification task for a tweet analysis service as a case study and propose a pretraining strategy with unlabeled dialog data ( tweet-reply pairs ) via an encoder-decoder model .",2,0.7837869,44.562465666594534,37
368,Experimental results show that our strategy can improve the performance of sentiment classifiers and outperform several state-of-the-art strategies including language model pretraining .,3,0.9673305,10.816941140073046,29
369,The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models .,0,0.89282054,26.316062918810818,27
369,This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection .,1,0.8992244,47.8147137979057,19
369,"We analyze the ambiguity of hashtag usages and propose a novel neural network-based model , which incorporates linguistic information from different aspects , to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection .",2,0.41377842,41.28304288754274,45
369,"Furthermore , we apply our model to prune the self-labeled training data .",2,0.56765425,25.461775451959003,14
369,Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data .,3,0.97674555,47.62611033922984,24
370,"In stance classification , the target on which the stance is made defines the boundary of the task , and a classifier is usually trained for prediction on the same target .",0,0.8490512,62.13240624769817,32
370,"In this work , we explore the potential for generalizing classifiers between different targets , and propose a neural model that can apply what has been learned from a source target to a destination target .",1,0.86498946,27.45565728327562,36
370,We show that our model can find useful information shared between relevant targets which improves generalization in certain scenarios .,3,0.94845176,43.636821231179034,20
371,"Extractive reading comprehension systems can often locate the correct answer to a question in a context document , but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context .",0,0.90183365,22.195705298814172,40
371,"Existing datasets either focus exclusively on answerable questions , or use automatically generated unanswerable questions that are easy to identify .",0,0.89996445,35.8762837018636,21
371,"To address these weaknesses , we present SQuADRUn , a new dataset that combines the existing Stanford Question Answering Dataset ( SQuAD ) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones .",2,0.43324485,27.878826891219035,39
371,"To do well on SQuADRUn , systems must not only answer questions when possible , but also determine when no answer is supported by the paragraph and abstain from answering .",0,0.75781107,66.95648371125273,31
371,SQuADRUn is a challenging natural language understanding task for existing models : a strong neural system that gets 86 % F1 on SQuAD achieves only 66 % F1 on SQuADRUn .,0,0.6404233,36.86307592867778,31
371,We release SQuADRUn to the community as the successor to SQuAD .,3,0.46592394,41.13695171052783,12
372,We propose a novel paradigm of grounding comparative adjectives within the realm of color descriptions .,1,0.5548238,152.1788329974932,16
372,"Given a reference RGB color and a comparative term ( e.g. , lighter , darker ) , our model learns to ground the comparative as a direction in the RGB space such that the colors along the vector , rooted at the reference color , satisfy the comparison .",2,0.685519,75.80213288003387,49
372,Our model generates grounded representations of comparative adjectives with an average accuracy of 0.65 cosine similarity to the desired direction of change .,3,0.7045824,59.545968851842936,23
372,"These vectors approach colors with Delta-E scores of under 7 compared to the target colors , indicating the differences are very small with respect to human perception .",3,0.9383142,91.56235590944468,28
372,Our approach makes use of a newly created dataset for this task derived from existing labeled color data .,2,0.74401355,49.084637522684474,19
373,"Currently , researchers have paid great attention to retrieval-based dialogues in open-domain .",0,0.9380618,43.49977915922598,15
373,"In particular , people study the problem by investigating context-response matching for multi-turn response selection based on publicly recognized benchmark data sets .",0,0.696544,130.32034270697739,23
373,"State-of-the-art methods require a response to interact with each utterance in a context from the beginning , but the interaction is performed in a shallow way .",0,0.7717731,22.012604036172405,31
373,"In this work , we let utterance-response interaction go deep by proposing an interaction-over-interaction network ( IoI ) .",1,0.7801079,74.10173863308991,23
373,The model performs matching by stacking multiple interaction blocks in which residual information from one time of interaction initiates the interaction process again .,2,0.6045987,152.95156864024335,24
373,"Thus , matching information within an utterance-response pair is extracted from the interaction of the pair in an iterative fashion , and the information flows along the chain of the blocks via representations .",0,0.41683882,56.41165257496422,36
373,Evaluation results on three benchmark data sets indicate that IoI can significantly outperform state-of-the-art methods in terms of various matching metrics .,3,0.9408768,11.988798887788843,28
373,"Through further analysis , we also unveil how the depth of interaction affects the performance of IoI .",3,0.84393936,75.18308422997754,18
374,Document Grounded Conversations is a task to generate dialogue responses when chatting about the content of a given document .,0,0.89498055,31.773919780913985,20
374,"Obviously , document knowledge plays a critical role in Document Grounded Conversations , while existing dialogue models do not exploit this kind of knowledge effectively enough .",0,0.71568054,55.38904804703434,27
374,"In this paper , we propose a novel Transformer-based architecture for multi-turn document grounded conversations .",1,0.90845686,21.87207177900709,18
374,"In particular , we devise an Incremental Transformer to encode multi-turn utterances along with knowledge in related documents .",2,0.6476882,58.47736493931124,19
374,"Motivated by the human cognitive process , we design a two-pass decoder ( Deliberation Decoder ) to improve context coherence and knowledge correctness .",2,0.59421134,57.71387695003097,25
374,Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance .,3,0.94507486,34.38156118571488,28
375,Recent research has achieved impressive results in single-turn dialogue modelling .,0,0.90304685,38.71034778238233,11
375,"In the multi-turn setting , however , current models are still far from satisfactory .",0,0.8090249,66.13928679246058,15
375,"One major challenge is the frequently occurred coreference and information omission in our daily conversation , making it hard for machines to understand the real intention .",0,0.89994514,82.85865400595755,27
375,"In this paper , we propose rewriting the human utterance as a pre-process to help multi-turn dialgoue modelling .",1,0.84910965,77.207756122435,19
375,Each utterance is first rewritten to recover all coreferred and omitted information .,2,0.717804,177.92678859909105,13
375,The next processing steps are then performed based on the rewritten utterance .,2,0.63654685,67.87142642845029,13
375,"To properly train the utterance rewriter , we collect a new dataset with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network .",2,0.78097785,45.132452046978294,29
375,We show the proposed architecture achieves remarkably good performance on the utterance rewriting task .,3,0.95284355,40.00881435309102,15
375,The trained utterance rewriter can be easily integrated into online chatbots and brings general improvement over different domains .,3,0.91157424,68.69295747819602,19
376,Neural generative models have been become increasingly popular when building conversational agents .,0,0.93290824,25.709755664965467,13
376,"They offer flexibility , can be easily adapted to new domains , and require minimal domain engineering .",0,0.7008374,64.93229352642797,18
376,A common criticism of these systems is that they seldom understand or use the available dialog history effectively .,0,0.9272312,50.321286709404944,19
376,"In this paper , we take an empirical approach to understanding how these models use the available dialog history by studying the sensitivity of the models to artificially introduced unnatural changes or perturbations to their context at test time .",1,0.84199893,46.368554414601924,40
376,"We experiment with 10 different types of perturbations on 4 multi-turn dialog datasets and find that commonly used neural dialog architectures like recurrent and transformer-based seq2seq models are rarely sensitive to most perturbations such as missing or reordering utterances , shuffling words , etc .",3,0.7175883,36.85147648233586,47
376,"Also , by open-sourcing our code , we believe that it will serve as a useful diagnostic tool for evaluating dialog systems in the future .",3,0.96992433,21.438086775276606,27
377,Neural models have become one of the most important approaches to dialog response generation .,0,0.9383059,17.970152151635006,15
377,"However , they still tend to generate the most common and generic responses in the corpus all the time .",3,0.64771926,44.272281516360714,20
377,"To address this problem , we designed an iterative training process and ensemble method based on boosting .",2,0.6311466,51.41835906029627,18
377,"We combined our method with different training and decoding paradigms as the base model , including mutual-information-based decoding and reward-augmented maximum likelihood learning .",2,0.7675245,49.770695012982195,30
377,"Empirical results show that our approach can significantly improve the diversity and relevance of the responses generated by all base models , backed by objective measurements and human evaluation .",3,0.9796631,30.823819285528245,30
378,Response selection plays an important role in fully automated dialogue systems .,0,0.93286484,30.118581512418576,12
378,"Given the dialogue context , the goal of response selection is to identify the best-matched next utterance ( i.e. , response ) from multiple candidates .",0,0.69531924,38.705022862110525,26
378,"Despite the efforts of many previous useful models , this task remains challenging due to the huge semantic gap and also the large size of candidate set .",0,0.92104316,42.524184284908095,28
378,"To address these issues , we propose a Spatio-Temporal Matching network ( STM ) for response selection .",1,0.65338576,25.107089420674278,18
378,"In detail , soft alignment is first used to obtain the local relevance between the context and the response .",2,0.6575561,81.71560376854217,20
378,"And then , we construct spatio-temporal features by aggregating attention images in time dimension and make use of 3D convolution and pooling operations to extract matching information .",2,0.87764037,56.92766492235969,28
378,Evaluation on two large-scale multi-turn response selection tasks has demonstrated that our proposed model significantly outperforms the state-of-the-art model .,3,0.9206312,8.973275957654119,26
378,"Particularly , visualization analysis shows that the spatio-temporal features enables matching information in segment pairs and time sequences , and have good interpretability for multi-turn text matching .",3,0.961116,94.99053146701256,28
379,Semantic parsing converts natural language queries into structured logical forms .,0,0.71926093,47.99443981117996,11
379,The lack of training data is still one of the most serious problems in this area .,0,0.88911,14.624350733150871,17
379,"In this work , we develop a semantic parsing framework with the dual learning algorithm , which enables a semantic parser to make full use of data ( labeled and even unlabeled ) through a dual-learning game .",1,0.5876857,40.2898957545646,39
379,"This game between a primal model ( semantic parsing ) and a dual model ( logical form to query ) forces them to regularize each other , and can achieve feedback signals from some prior-knowledge .",0,0.3828481,189.1045494045455,36
379,"By utilizing the prior-knowledge of logical form structures , we propose a novel reward signal at the surface and semantic levels which tends to generate complete and reasonable logical forms .",2,0.51418215,94.3456008989991,31
379,Experimental results show that our approach achieves new state-of-the-art performance on ATIS dataset and gets competitive performance on OVERNIGHT dataset .,3,0.96270144,9.270958286552618,27
380,We investigate the capacity of mechanisms for compositional semantic parsing to describe relations between sentences and semantic representations .,1,0.8211343,35.46128824136177,19
380,"We prove that in order to represent certain relations , mechanisms which are syntactically projective must be able to remember an unbounded number of locations in the semantic representations , where nonprojective mechanisms need not .",3,0.6891554,65.36811735977082,36
380,"This is the first result of this kind , and has consequences both for grammar-based and for neural systems .",3,0.93241864,36.57666738248781,20
381,We propose an attention-based model that treats AMR parsing as sequence-to-graph transduction .,1,0.40073955,30.640707208143994,18
381,"Unlike most AMR parsers that rely on pre-trained aligners , external semantic resources , or data augmentation , our proposed parser is aligner-free , and it can be effectively trained with limited amounts of labeled AMR data .",3,0.7222095,36.6858456869102,39
381,"Our experimental results outperform all previously reported SMATCH scores , on both AMR 2.0 ( 76.3 % on LDC2017T10 ) and AMR 1.0 ( 70.2 % on LDC2014T12 ) .",3,0.9728621,25.492906088937865,30
382,Structured information about entities is critical for many semantic parsing tasks .,0,0.938419,34.291976108866336,12
382,We present an approach that uses a Graph Neural Network ( GNN ) architecture to incorporate information about relevant entities and their relations during parsing .,2,0.44989133,22.82300442665697,26
382,"Combined with a decoder copy mechanism , this approach provides a conceptually simple mechanism to generate logical forms with entities .",3,0.5315377,54.007097688790665,21
382,"We demonstrate that this approach is competitive with the state-of-the-art across several tasks without pre-training , and outperforms existing approaches when combined with BERT pre-training .",3,0.92782515,11.171475454088151,32
383,"Vector representations of sentences , trained on massive text corpora , are widely used as generic sentence embeddings across a variety of NLP problems .",0,0.90100586,31.457374293071307,25
383,"The learned representations are generally assumed to be continuous and real-valued , giving rise to a large memory footprint and slow retrieval speed , which hinders their applicability to low-resource ( memory and computation ) platforms , such as mobile devices .",0,0.8691769,33.290486757141274,42
383,"In this paper , we propose four different strategies to transform continuous and generic sentence embeddings into a binarized form , while preserving their rich semantic information .",1,0.82526755,31.6052537365004,28
383,"The introduced methods are evaluated across a wide range of downstream tasks , where the binarized sentence embeddings are demonstrated to degrade performance by only about 2 % relative to their continuous counterparts , while reducing the storage requirement by over 98 % .",3,0.7154265,39.23342104282478,44
383,"Moreover , with the learned binary representations , the semantic relatedness of two sentences can be evaluated by simply calculating their Hamming distance , which is more computational efficient compared with the inner product operation between continuous embeddings .",3,0.6201238,83.86891202035558,39
383,Detailed analysis and case study further validate the effectiveness of proposed methods .,3,0.92197186,23.83736711851413,13
384,Classical non-neural dependency parsers put considerable effort on the design of feature functions .,0,0.91791636,66.98864226229331,14
384,"Especially , they benefit from information coming from structural features , such as features drawn from neighboring tokens in the dependency tree .",0,0.5366827,69.04772654574022,23
384,"In contrast , their BiLSTM-based successors achieve state-of-the-art performance without explicit information about the structural context .",3,0.71944845,28.00619771787199,25
384,We show that features drawn from partial subtrees become redundant when the BiLSTMs are used .,3,0.9498189,72.14792112201613,16
384,We provide a deep insight into information flow in transition-and graph-based neural architectures to demonstrate where the implicit information comes from when the parsers make their decisions .,3,0.6423645,43.50383447005996,32
384,"Finally , with model ablations we demonstrate that the structural context is not only present in the models , but it significantly influences their performance .",3,0.9471246,43.032103912598494,26
385,"We propose a new domain adaptation method for Combinatory Categorial Grammar ( CCG ) parsing , based on the idea of automatic generation of CCG corpora exploiting cheaper resources of dependency trees .",1,0.5247344,56.20917018233778,33
385,"Our solution is conceptually simple , and not relying on a specific parser architecture , making it applicable to the current best-performing parsers .",3,0.79047996,37.63918112105735,24
385,We conduct extensive parsing experiments with detailed discussion ;,2,0.50484973,1380.4660642093427,9
385,"on top of existing benchmark datasets on ( 1 ) biomedical texts and ( 2 ) question sentences , we create experimental datasets of ( 3 ) speech conversation and ( 4 ) math problems .",2,0.8308959,61.93501921242983,36
385,"When applied to the proposed method , an off-the-shelf CCG parser shows significant performance gains , improving from 90.7 % to 96.6 % on speech conversation , and from 88.5 % to 96.8 % on math problems .",3,0.90646106,22.487613131530733,38
386,"We study a variant of domain adaptation for named-entity recognition where multiple , heterogeneously tagged training sets are available .",2,0.39344972,74.139061221376,20
386,"Furthermore , the test tag-set is not identical to any individual training tag-set .",3,0.5379804,68.05903823729315,14
386,"Yet , the relations between all tags are provided in a tag hierarchy , covering the test tags as a combination of training tags .",0,0.40058985,96.2084196528504,25
386,This setting occurs when various datasets are created using different annotation schemes .,0,0.74666977,68.59080404805725,13
386,This is also the case of extending a tag-set with a new tag by annotating only the new tag in a new dataset .,3,0.5929439,22.737359640926954,24
386,We propose to use the given tag hierarchy to jointly learn a neural network that shares its tagging layer among all tag-sets .,2,0.644349,58.75715278836832,23
386,We compare this model to combining independent models and to a model based on the multitasking approach .,2,0.6593878,59.505947147351456,18
386,"Our experiments show the benefit of the tag-hierarchy model , especially when facing non-trivial consolidation of tag-sets .",3,0.9826362,55.19084145091373,18
387,"In cross-lingual transfer , NLP models over one or more source languages are applied to a low-resource target language .",0,0.67499006,16.546076527443955,20
387,"While most prior work has used a single source model or a few carefully selected models , here we consider a “ massive ” setting with many such models .",0,0.4835714,55.05237724548,30
387,"This setting raises the problem of poor transfer , particularly from distant languages .",0,0.8995184,137.09953923938437,14
387,"We propose two techniques for modulating the transfer , suitable for zero-shot or few-shot learning , respectively .",2,0.5018721,49.811662321953555,18
387,"Evaluating on named entity recognition , we show that our techniques are much more effective than strong baselines , including standard ensembling , and our unsupervised method rivals oracle selection of the single best individual model .",3,0.9155551,60.63927579780707,37
388,Word embeddings are widely used on a variety of tasks and can substantially improve the performance .,0,0.8912069,14.098088876343805,17
388,"However , their quality is not consistent throughout the vocabulary due to the long-tail distribution of word frequency .",0,0.62579036,62.526773737762014,19
388,"Without sufficient contexts , rare word embeddings are usually less reliable than those of common words .",0,0.7683005,53.16241793925168,17
388,"However , current models typically trust all word embeddings equally regardless of their reliability and thus may introduce noise and hurt the performance .",0,0.89007604,46.285218475201575,24
388,"Since names often contain rare and uncommon words , this problem is particularly critical for name tagging .",0,0.9351226,72.67021383838033,18
388,"In this paper , we propose a novel reliability-aware name tagging model to tackle this issue .",1,0.8964323,38.30879383493605,19
388,We design a set of word frequency-based reliability signals to indicate the quality of each word embedding .,2,0.8826485,39.441601637578266,20
388,"Guided by the reliability signals , the model is able to dynamically select and compose features such as word embedding and character-level representation using gating mechanisms .",3,0.5255494,45.866154152526484,29
388,"For example , if an input word is rare , the model relies less on its word embedding and assigns higher weights to its character and contextual features .",3,0.62023354,45.32515087364625,29
388,Experiments on OntoNotes 5.0 show that our model outperforms the baseline model by 2.7 % absolute gain in F-score .,3,0.9460738,11.973542143624046,20
388,"In cross-genre experiments on five genres in OntoNotes , our model improves the performance for most genre pairs and obtains up to 5 % absolute F-score gain .",3,0.8531119,37.60514961412034,29
389,Unsupervised neural machine translation ( NMT ) has attracted a lot of attention recently .,0,0.9667695,11.154080457932324,15
389,"While state-of-the-art methods for unsupervised translation usually perform well between similar languages ( e.g. , English-German translation ) , they perform poorly between distant languages , because unsupervised alignment does not work well for distant languages .",0,0.82821,22.95607464762452,44
389,"In this work , we introduce unsupervised pivot translation for distant languages , which translates a language to a distant language through multiple hops , and the unsupervised translation on each hop is relatively easier than the original direct translation .",1,0.5031594,36.6489186999072,41
389,We propose a learning to route ( LTR ) method to choose the translation path between the source and target languages .,1,0.33767015,44.17176124641505,22
389,LTR is trained on language pairs whose best translation path is available and is applied on the unseen language pairs for path selection .,2,0.62099355,53.64850402249221,24
389,"Experiments on 20 languages and 294 distant language pairs demonstrate the advantages of the unsupervised pivot translation for distant languages , as well as the effectiveness of the proposed LTR for path selection .",3,0.8736619,48.6012547858794,34
389,"Specifically , in the best case , LTR achieves an improvement of 5.58 BLEU points over the conventional direct unsupervised method .",3,0.9370507,29.940422766792203,22
390,"Recent work on bilingual lexicon induction ( BLI ) has frequently depended either on aligned bilingual lexicons or on distribution matching , often with an assumption about the isometry of the two spaces .",0,0.95076317,70.0221236994595,34
390,We propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant .,3,0.35065177,33.07080236290861,33
390,"We then propose Bilingual Lexicon Induction with Semi-Supervision ( BLISS ) — a semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings , as well as a novel hubness filtering technique .",2,0.6250369,43.98084113476552,45
390,"Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset , and does particularly well when the embedding spaces do n’t appear to be isometric .",3,0.9040286,45.168341533046025,35
390,"In addition , we also show that adding supervision stabilizes the learning procedure , and is effective even with minimal supervision .",3,0.9534971,52.24454472465367,22
391,"While machine translation has traditionally relied on large amounts of parallel corpora , a recent research line has managed to train both Neural Machine Translation ( NMT ) and Statistical Machine Translation ( SMT ) systems using monolingual corpora only .",0,0.95579726,18.2747738593306,41
391,"In this paper , we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information , developing a theoretically well founded unsupervised tuning method , and incorporating a joint refinement procedure .",1,0.8741609,69.54091093844755,36
391,"Moreover , we use our improved SMT system to initialize a dual NMT model , which is further fine-tuned through on-the-fly back-translation .",2,0.5071508,27.038098662955566,28
391,"Together , we obtain large improvements over the previous state-of-the-art in unsupervised machine translation .",3,0.92418265,7.942175143508955,21
391,"For instance , we get 22.5 BLEU points in English-to-German WMT 2014 , 5.5 points more than the previous best unsupervised system , and 0.5 points more than the ( supervised ) shared task winner back in 2014 .",3,0.9122569,31.039387360030293,42
392,"A regularization technique based on adversarial perturbation , which was initially developed in the field of image processing , has been successfully applied to text classification tasks and has yielded attractive improvements .",0,0.8527569,24.968323837999517,33
392,"We aim to further leverage this promising methodology into more sophisticated and critical neural models in the natural language processing field , i.e. , neural machine translation ( NMT ) models .",1,0.8477244,51.686339956606155,32
392,"However , it is not trivial to apply this methodology to such models .",0,0.8443663,41.46002775228902,14
392,"Thus , this paper investigates the effectiveness of several possible configurations of applying the adversarial perturbation and reveals that the adversarial regularization technique can significantly and consistently improve the performance of widely used NMT models , such as LSTM-based and Transformer-based models .",1,0.47116885,17.657763191796363,47
393,"It has been shown that the performance of neural machine translation ( NMT ) drops starkly in low-resource conditions , underperforming phrase-based statistical machine translation ( PBSMT ) and requiring large amounts of auxiliary data to achieve competitive results .",0,0.9065307,24.391544051107434,42
393,"In this paper , we re-assess the validity of these results , arguing that they are the result of lack of system adaptation to low-resource settings .",1,0.8212151,19.332326457682544,27
393,"We discuss some pitfalls to be aware of when training low-resource NMT systems , and recent techniques that have shown to be especially helpful in low-resource settings , resulting in a set of best practices for low-resource NMT .",1,0.5034626,20.881152745419612,39
393,"In our experiments on German –English with different amounts of IWSLT14 training data , we show that , without the use of any auxiliary monolingual or multilingual data , an optimized NMT system can outperform PBSMT with far less data than previously claimed .",3,0.8648131,44.470085733084616,45
393,"We also apply these techniques to a low-resource Korean –English dataset , surpassing previously reported results by 4 BLEU .",3,0.51721364,50.502173866560426,21
394,"We investigate adaptive ensemble weighting for Neural Machine Translation , addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain .",1,0.5425378,69.13133956639618,31
394,"We adapt sequentially across two Spanish-English and three English-German tasks , comparing unregularized fine-tuning , L2 and Elastic Weight Consolidation .",2,0.87479997,79.96579249622299,24
394,"We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information , and report strong improvements across test domains without access to the domain label .",3,0.68386596,114.31293849952083,33
395,We study relation extraction for knowledge base ( KB ) enrichment .,1,0.49163985,217.24697740629068,12
395,"Specifically , we aim to extract entities and their relationships from sentences in the form of triples and map the elements of the extracted triples to an existing KB in an end-to-end manner .",2,0.6794527,18.054749409175077,36
395,Previous studies focus on the extraction itself and rely on Named Entity Disambiguation ( NED ) to map triples into the KB space .,0,0.9174146,52.90697511299737,24
395,"This way , NED errors may cause extraction errors that affect the overall precision and recall .",3,0.5474882,126.0901380031082,17
395,"To address this problem , we propose an end-to-end relation extraction model for KB enrichment based on a neural encoder-decoder model .",1,0.4097535,13.18867279111431,25
395,We collect high-quality training data by distant supervision with co-reference resolution and paraphrase detection .,2,0.8540247,51.22670297881581,15
395,We propose an n-gram based attention model that captures multi-word entity names in a sentence .,2,0.4793505,27.231969232006758,16
395,Our model employs jointly learned word and entity embeddings to support named entity disambiguation .,2,0.7327432,27.008403592709076,15
395,"Finally , our model uses a modified beam search and a triple classifier to help generate high-quality triples .",2,0.5453931,31.97995653573366,19
395,Our model outperforms state-of-the-art baselines by 15.51 % and 8.38 % in terms of F1 score on two real-world datasets .,3,0.8803605,7.429728740374048,26
396,Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text .,0,0.9100908,45.12119805614709,18
396,"However , how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question .",0,0.92016745,26.010513311824813,24
396,Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results .,0,0.60324806,93.33078202094983,22
396,"In this work , we propose Attention Guided Graph Convolutional Networks ( AGGCNs ) , a novel model which directly takes full dependency trees as inputs .",1,0.68557394,40.18263566552735,27
396,Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task .,3,0.6996027,28.190780161501046,27
396,"Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees , giving significantly better results than previous approaches .",3,0.9492495,18.717083887124662,43
397,Spatial aggregation refers to merging of documents created at the same spatial location .,0,0.86072654,103.5700606966189,14
397,We show that by spatial aggregation of a large collection of documents and applying a traditional topic discovery algorithm on the aggregated data we can efficiently discover spatially distinct topics .,3,0.8987403,47.86089434505802,31
397,"By looking at topic discovery through matrix factorization lenses we show that spatial aggregation allows low rank approximation of the original document-word matrix , in which spatially distinct topics are preserved and non-spatial topics are aggregated into a single topic .",3,0.5481957,70.28927779660735,43
397,Our experiments on synthetic data confirm this observation .,3,0.9447006,60.976492116897546,9
397,Our experiments on 4.7 million tweets collected during the Sandy Hurricane in 2012 show that spatial and temporal aggregation allows rapid discovery of relevant spatial and temporal topics during that period .,3,0.8234754,62.63291599871055,32
397,Our work indicates that different forms of document aggregation might be effective in rapid discovery of various types of distinct topics from large collections of documents .,3,0.9882818,38.42064157793224,27
398,Link prediction is critical for the application of incomplete knowledge graph ( KG ) in the downstream tasks .,0,0.8185929,44.15687240111501,19
398,"As a family of effective approaches for link predictions , embedding methods try to learn low-rank representations for both entities and relations such that the bilinear form defined therein is a well-behaved scoring function .",0,0.8529444,74.99650132666845,37
398,"Despite of their successful performances , existing bilinear forms overlook the modeling of relation compositions , resulting in lacks of interpretability for reasoning on KG .",0,0.82718533,128.67106620677572,26
398,"To fulfill this gap , we propose a new model called DihEdral , named after dihedral symmetry group .",1,0.4401812,186.42059964097402,19
398,This new model learns knowledge graph embeddings that can capture relation compositions by nature .,2,0.4503804,54.25041382481463,15
398,"Furthermore , our approach models the relation embeddings parametrized by discrete values , thereby decrease the solution space drastically .",3,0.7581834,65.49944341014402,20
398,"Our experiments show that DihEdral is able to capture all desired properties such as ( skew-) symmetry , inversion and ( non-) Abelian composition , and outperforms existing bilinear form based approach and is comparable to or better than deep learning models such as ConvE .",3,0.9610386,81.0934526668287,50
399,"Pretrained contextual and non-contextual subword embeddings have become available in over 250 languages , allowing massively multilingual NLP .",0,0.8889491,30.994026688277287,19
399,"However , while there is no dearth of pretrained embeddings , the distinct lack of systematic evaluations makes it difficult for practitioners to choose between them .",0,0.8673005,30.85148587308897,27
399,"In this work , we conduct an extensive evaluation comparing non-contextual subword embeddings , namely FastText and BPEmb , and a contextual representation method , namely BERT , on multilingual named entity recognition and part-of-speech tagging .",1,0.7285002,39.96612220060639,38
399,"We find that overall , a combination of BERT , BPEmb , and character representations works best across languages and tasks .",3,0.9796265,90.30199227361175,22
399,"A more detailed analysis reveals different strengths and weaknesses : Multilingual BERT performs well in medium-to high-resource languages , but is outperformed by non-contextual subword embeddings in a low-resource setting .",3,0.9335974,24.01930311047278,33
400,"Today , the dominant paradigm for training neural networks involves minimizing task loss on a large dataset .",0,0.95943546,51.999066037294845,18
400,"Using world knowledge to inform a model , and yet retain the ability to perform end-to-end training remains an open question .",0,0.8169467,28.546970474363487,24
400,"In this paper , we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction .",1,0.90830755,16.65040721207939,25
400,Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign .,2,0.49547333,180.71985168187175,22
400,"We evaluate our modeling strategy on three tasks : machine comprehension , natural language inference , and text chunking .",2,0.71363574,76.29883496717753,20
400,"Our experiments show that knowledge-augmented networks can strongly improve over baselines , especially in low-data regimes .",3,0.979658,42.90207498523631,18
401,Not all types of supervision signals are created equal : Different types of feedback have different costs and effects on learning .,0,0.6098226,44.33927408277217,22
401,We show how self-regulation strategies that decide when to ask for which kind of feedback from a teacher ( or from oneself ) can be cast as a learning-to-learn problem leading to improved cost-aware sequence-to-sequence learning .,3,0.6820705,39.56708513242784,44
401,"In experiments on interactive neural machine translation , we find that the self-regulator discovers an 𝜖-greedy strategy for the optimal cost-quality trade-off by mixing different feedback types including corrections , error markups , and self-supervision .",3,0.9019488,72.5206373065292,38
401,"Furthermore , we demonstrate its robustness under domain shift and identify it as a promising alternative to active learning .",3,0.9358721,40.439698370186164,20
402,"In recent NLP research , a topic of interest is universal sentence encoding , sentence representations that can be used in any supervised task .",0,0.92244226,55.9762835478709,25
402,"At the word sequence level , fully attention-based models suffer from two problems : a quadratic increase in memory consumption with respect to the sentence length and an inability to capture and use syntactic information .",0,0.750001,36.261113945218504,38
402,Recursive neural nets can extract very good syntactic information by traversing a tree structure .,0,0.7778345,59.94749782556463,15
402,"To this end , we propose Tree Transformer , a model that captures phrase level syntax for constituency trees as well as word-level dependencies for dependency trees by doing recursive traversal only with attention .",2,0.46231788,61.3215776936404,35
402,Evaluation of this model on four tasks gets noteworthy results compared to the standard transformer and LSTM-based models as well as tree-structured LSTMs .,3,0.92276335,19.060438575499557,26
402,Ablation studies to find whether positional information is inherently encoded in the trees and which type of attention is suitable for doing the recursive traversal are provided .,3,0.33371785,71.82703934948687,28
403,We present three results about the generalization of neural parsers in a zero-shot setting : training on trees from one corpus and evaluating on out-of-domain corpora .,3,0.6868913,21.852171081809743,28
403,"First , neural and non-neural parsers generalize comparably to new domains .",0,0.46911982,33.6578020583816,12
403,"Second , incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains , but does not give a larger relative improvement for out-of-domain treebanks .",3,0.9023499,28.5764017212144,32
403,"Finally , despite the rich input representations they learn , neural parsers still benefit from structured output prediction of output trees , yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora .",3,0.4977665,66.20880078336779,41
403,"We analyze generalization on English and Chinese corpora , and in the process obtain state-of-the-art parsing results for the Brown , Genia , and English Web treebanks .",2,0.60445637,59.19797396010666,34
404,We propose a novel self-attention mechanism that can learn its optimal attention span .,1,0.49908966,16.446124160728175,14
404,"This allows us to extend significantly the maximum context size used in Transformer , while maintaining control over their memory footprint and computational time .",3,0.7136861,51.485472314560674,25
404,"We show the effectiveness of our approach on the task of character level language modeling , where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8 k characters .",3,0.7087197,23.065392191550355,41
405,Personalized news recommendation is important to help users find their interested news and improve reading experience .,0,0.9288732,41.115566429677706,17
405,A key problem in news recommendation is learning accurate user representations to capture their interests .,0,0.89930373,81.40373976868436,16
405,Users usually have both long-term preferences and short-term interests .,0,0.83091944,17.880199608813466,10
405,"However , existing news recommendation methods usually learn single representations of users , which may be insufficient .",0,0.89908993,101.95944519474699,18
405,"In this paper , we propose a neural news recommendation approach which can learn both long-and short-term user representations .",1,0.8578564,31.433338408320974,24
405,The core of our approach is a news encoder and a user encoder .,2,0.56444436,19.431247521446494,14
405,"In the news encoder , we learn representations of news from their titles and topic categories , and use attention network to select important words .",2,0.8120699,75.18175778841429,26
405,"In the user encoder , we propose to learn long-term user representations from the embeddings of their IDs .",2,0.61433893,45.680030539567106,19
405,"In addition , we propose to learn short-term user representations from their recently browsed news via GRU network .",2,0.5978372,105.0650455130311,21
405,"Besides , we propose two methods to combine long-term and short-term user representations .",2,0.6350976,22.54732784879402,14
405,The first one is using the long-term user representation to initialize the hidden state of the GRU network in short-term user representation .,2,0.7694474,50.777884174181004,25
405,The second one is concatenating both long-and short-term user representations as a unified user vector .,2,0.6331372,52.655169984296606,20
405,Extensive experiments on a real-world dataset show our approach can effectively improve the performance of neural news recommendation .,3,0.85989654,13.761286338761863,19
406,"In this paper , we automatically create sentiment dictionaries for predicting financial outcomes .",1,0.79231614,100.91025792425744,14
406,"We compare three approaches : ( i ) manual adaptation of the domain-general dictionary H4N , ( ii ) automatic adaptation of H4N and ( iii ) a combination consisting of first manual , then automatic adaptation .",2,0.84528196,44.882365270409004,38
406,"In our experiments , we demonstrate that the automatically adapted sentiment dictionary outperforms the previous state of the art in predicting the financial outcomes excess return and volatility .",3,0.9000542,76.29799818301443,29
406,"In particular , automatic adaptation performs better than manual adaptation .",3,0.83929974,78.70483083411067,11
406,"In our analysis , we find that annotation based on an expert ’s a priori belief about a word ’s meaning can be incorrect – annotation should be performed based on the word ’s contexts in the target domain instead .",3,0.97751427,28.678159647758196,41
407,We propose two novel manipulation strategies for increasing and decreasing the difficulty of C-tests automatically .,3,0.3492831,74.63034596573227,16
407,This is a crucial step towards generating learner-adaptive exercises for self-directed language learning and preparing language assessment tests .,3,0.7710235,47.69780262160589,21
407,"To reach the desired difficulty level , we manipulate the size and the distribution of gaps based on absolute and relative gap difficulty predictions .",2,0.7692242,67.40827237841646,25
407,We evaluate our approach in corpus-based experiments and in a user study with 60 participants .,2,0.60133505,22.8172318014038,16
407,We find that both strategies are able to generate C-tests with the desired difficulty level .,3,0.98045474,60.65827596824262,16
408,Text classification aims at mapping documents into a set of predefined categories .,0,0.8998671,28.414254161843523,13
408,Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy .,0,0.92354465,17.89714441982353,25
408,This is particularly true when the number of target categories is in the tens or the hundreds .,0,0.68600583,16.687207857687707,18
408,"In this work , we explore an unsupervised approach to classify documents into categories simply described by a label .",1,0.61048853,39.56817000304523,20
408,It draws on textual similarity between the most relevant words in each document and a dictionary of keywords for each category reflecting its semantics and lexical field .,2,0.5939193,67.50280620215794,28
408,"The novelty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models , both generic and domain specific .",3,0.51690596,56.29742001015638,29
408,Our experiments on 5 standard corpora show that the proposed method increases F1-score over relying solely on human expertise and can also be on par with simple supervised approaches .,3,0.94754773,28.048322421808525,32
408,"It thus provides a practical alternative to situations where low cost text categorization is needed , as we illustrate with our application to operational risk incidents classification .",3,0.86325854,146.90829659362447,28
409,Clinical letters are infamously impenetrable for the lay patient .,0,0.9155739,64.86297538098331,10
409,This work uses neural text simplification methods to automatically improve the understandability of clinical letters for patients .,1,0.6196089,46.245056542472945,18
409,We take existing neural text simplification software and augment it with a new phrase table that links complex medical terminology to simpler vocabulary by mining SNOMED-CT .,2,0.832208,151.7636781676169,29
409,"In an evaluation task using crowdsourcing , we show that the results of our new system are ranked easier to understand ( average rank 1.93 ) than using the original system ( 2.34 ) without our phrase table .",3,0.9113363,79.60934314033567,39
409,We also show improvement against baselines including the original text ( 2.79 ) and using the phrase table without the neural text simplification software ( 2.94 ) .,3,0.95266414,67.1098464554788,28
409,Our methods can easily be transferred outside of the clinical domain by using domain-appropriate resources to provide effective neural text simplification for any domain without the need for costly annotation .,3,0.9357161,40.87809788841906,32
410,Predicting financial risk is an essential task in financial market .,0,0.9551352,28.616613917403775,11
410,Prior research has shown that textual information in a firm ’s financial statement can be used to predict its stock ’s risk level .,0,0.9395685,32.40647565548527,24
410,"Nowadays , firm CEOs communicate information not only verbally through press releases and financial reports , but also nonverbally through investor meetings and earnings conference calls .",0,0.94730055,91.4196978678193,27
410,"There are anecdotal evidences that CEO ’s vocal features , such as emotions and voice tones , can reveal the firm ’s performance .",0,0.8969443,105.48501016744174,24
410,"However , how vocal features can be used to predict risk levels , and to what extent , is still unknown .",0,0.918868,53.560320056890674,22
410,"To fill the gap , we obtain earnings call audio recordings and textual transcripts for S&P 500 companies in recent years .",2,0.5651743,79.67527018638316,22
410,We propose a multimodal deep regression model ( MDRM ) that jointly model CEO ’s verbal ( from text ) and vocal ( from audio ) information in a conference call .,2,0.4058966,73.23654142142632,32
410,Empirical results show that our model that jointly considers verbal and vocal features achieves significant and substantial prediction error reduction .,3,0.98162997,55.838804214703394,21
410,We also discuss several interesting findings and the implications to financial markets .,3,0.76617557,52.91444313146918,13
410,The processed earnings conference calls data ( text and audio ) are released for readers who are interested in reproducing the results or designing trading strategy .,2,0.46461406,103.1962424972632,27
411,"Motivated by infamous cheating scandals in the media industry , the wine industry , and political campaigns , we address the problem of detecting concealed information in technical settings .",1,0.50284815,87.34125207578424,30
411,"In this work , we explore acoustic-prosodic and linguistic indicators of information concealment by collecting a unique corpus of professionals practicing for oral exams while concealing information .",1,0.8875857,113.41837249263935,28
411,"We reveal subtle signs of concealing information in speech and text , compare and contrast them with those in deception detection literature , uncovering the link between concealing information and deception .",3,0.7246952,62.41991842266914,32
411,We then present a series of experiments that automatically detect concealed information from text and speech .,3,0.37340727,51.912306102708975,17
411,"We compare the use of acoustic-prosodic , linguistic , and individual feature sets , using different machine learning models .",2,0.77150565,166.96258045372872,20
411,"Finally , we present a multi-task learning framework with acoustic , linguistic , and individual features , that outperforms human performance by over 15 % .",3,0.577919,55.56557305856039,26
412,The information revolution brought with it information pollution .,0,0.92920333,161.77406707344295,9
412,Information retrieval and extraction help us cope with abundant information from diverse sources .,0,0.9534166,82.37626269461045,14
412,"Not all information sources are equally trustworthy , and simply accepting the majority view is often wrong .",0,0.8602071,75.31211173778361,18
412,"This paper develops a general framework for estimating the trustworthiness of information sources in an environment where multiple sources provide claims and supporting evidence , and each claim can potentially be produced by multiple sources .",1,0.75841296,30.13268796382138,36
412,"We consider two settings : one in which information sources directly assert claims , and a more realistic and challenging one , in which claims are inferred from evidence provided by sources , via ( possibly noisy ) NLP techniques .",2,0.7810348,88.76009057659775,41
412,"Our key contribution is to develop a family of probabilistic models that jointly estimate the trustworthiness of sources , and the credibility of claims they assert .",1,0.59242934,27.659095739571846,27
412,This is done while accounting for the ( possibly noisy ) NLP needed to infer claims from evidence supplied by sources .,2,0.7225994,158.18195359882836,22
412,"We evaluate our framework on several datasets , showing strong results and significant improvement over baselines .",3,0.69819593,30.86172652217708,17
413,This paper tackles the problem of disentangling the latent representations of style and content in language models .,1,0.8369389,17.68016584566281,18
413,"We propose a simple yet effective approach , which incorporates auxiliary multi-task and adversarial objectives , for style prediction and bag-of-words prediction , respectively .",2,0.3930355,48.8794280240472,25
413,"We show , both qualitatively and quantitatively , that the style and content are indeed disentangled in the latent space .",3,0.9427936,33.32054212833299,21
413,This disentangled latent representation learning can be applied to style transfer on non-parallel corpora .,3,0.5361025,32.20004007592572,15
413,"We achieve high performance in terms of transfer accuracy , content preservation , and language fluency , in comparison to various previous approaches .",3,0.9038039,59.33492401618289,24
414,Automatic grammatical error correction ( GEC ) research has made remarkable progress in the past decade .,0,0.9754871,25.573780097928697,17
414,"However , all existing approaches to GEC correct errors by considering a single sentence alone and ignoring crucial cross-sentence context .",0,0.80655324,64.25238917527832,21
414,Some errors can only be corrected reliably using cross-sentence context and models can also benefit from the additional contextual information in correcting other errors .,0,0.5063228,46.80459395238384,25
414,"In this paper , we address this serious limitation of existing approaches and improve strong neural encoder-decoder models by appropriately modeling wider contexts .",1,0.89638555,37.22374766350377,24
414,We employ an auxiliary encoder that encodes previous sentences and incorporate the encoding in the decoder via attention and gating mechanisms .,2,0.7583927,51.027585117447835,22
414,Our approach results in statistically significant improvements in overall GEC performance over strong baselines across multiple test sets .,3,0.9547825,25.198614117055733,19
414,Analysis of our cross-sentence GEC model on a synthetic dataset shows high performance in verb tense corrections that require cross-sentence context .,3,0.92589134,59.066465819593745,22
415,"Given the overwhelming number of emails , an effective subject line becomes essential to better inform the recipient of the email ’s content .",0,0.8597008,67.24290814106918,24
415,"In this paper , we propose and study the task of email subject line generation : automatically generating an email subject line from the email body .",1,0.8976369,27.51094751198028,27
415,We create the first dataset for this task and find that email subject line generation favor extremely abstractive summary which differentiates it from news headline generation or news single document summarization .,3,0.8351086,66.49808289444607,32
415,We then develop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems .,2,0.62873006,9.851835054011133,29
415,We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric .,3,0.3477825,24.75992378986217,23
415,Our system outperforms competitive baselines given both automatic and human evaluations .,3,0.89628124,14.084055939155016,12
415,"To our knowledge , this is the first work to tackle the problem of effective email subject line generation .",3,0.84898347,25.420165445061116,20
416,State-of-the-art models of lexical semantic change detection suffer from noise stemming from vector space alignment .,0,0.8381245,25.564404224529124,20
416,"We have empirically tested the Temporal Referencing method for lexical semantic change and show that , by avoiding alignment , it is less affected by this noise .",3,0.6978288,59.05655254140374,28
416,"We show that , trained on a diachronic corpus , the skip-gram with negative sampling architecture with temporal referencing outperforms alignment models on a synthetic task as well as a manual testset .",3,0.93966514,106.04385040664731,33
416,We introduce a principled way to simulate lexical semantic change and systematically control for possible biases .,2,0.4669674,91.32489019819036,17
417,"In this paper , we propose a neural network-based approach , namely Adversarial Attention Network , to the task of multi-dimensional emotion regression , which automatically rates multiple emotion dimension scores for an input text .",1,0.8322721,44.74739623959906,38
417,"Especially , to determine which words are valuable for a particular emotion dimension , an attention layer is trained to weight the words in an input sequence .",2,0.46430254,57.71679415658889,28
417,"Furthermore , adversarial training is employed between two attention layers to learn better word weights via a discriminator .",2,0.6470319,74.22894575516392,19
417,"In particular , a shared attention layer is incorporated to learn public word weights between two emotion dimensions .",2,0.649656,175.32798178925185,19
417,Empirical evaluation on the EMOBANK corpus shows that our approach achieves notable improvements in r-values on both EMOBANK Reader ’s and Writer ’s multi-dimensional emotion regression tasks in all domains over the state-of-the-art baselines .,3,0.92199194,30.033709023642825,41
418,"We propose a general strategy named ‘ divide , conquer and combine ’ for multimodal fusion .",1,0.41220725,141.41162139969998,17
418,"Instead of directly fusing features at holistic level , we conduct fusion hierarchically so that both local and global interactions are considered for a comprehensive interpretation of multimodal embeddings .",2,0.74223953,49.71830933807565,30
418,"In the ‘ divide ’ and ‘ conquer ’ stages , we conduct local fusion by exploring the interaction of a portion of the aligned feature vectors across various modalities lying within a sliding window , which ensures that each part of multimodal embeddings are explored sufficiently .",2,0.73759145,73.0518631790141,48
418,"On its basis , global fusion is conducted in the ‘ combine ’ stage to explore the interconnection across local interactions , via an Attentive Bi-directional Skip-connected LSTM that directly connects distant local interactions and integrates two levels of attention mechanism .",2,0.54473215,92.07184346564605,42
418,"In this way , local interactions can exchange information sufficiently and thus obtain an overall view of multimodal information .",3,0.52238595,91.30063768372497,20
418,Our method achieves state-of-the-art performance on multimodal affective computing with higher efficiency .,3,0.9078274,14.949763243865224,18
419,"Every fiscal quarter , companies hold earnings calls in which company executives respond to questions from analysts .",0,0.7251857,78.8320079820602,18
419,"After these calls , analysts often change their price target recommendations , which are used in equity re-search reports to help investors make deci-sions .",0,0.7906338,128.25250874538096,25
419,"In this paper , we examine analysts ’ decision making behavior as it pertains to the language content of earnings calls .",1,0.919366,69.61795038121885,22
419,We identify a set of 20 pragmatic features of analysts ’ questions which we correlate with analysts ’ pre-call investor recommendations .,2,0.49193677,147.49945516318118,22
419,We also analyze the degree to which semantic and pragmatic features from an earnings call complement market data in predicting analysts ’ post-call changes in price targets .,3,0.40228763,96.78787097182017,28
419,Our results show that earnings calls are moderately predictive of analysts ’ decisions even though these decisions are influenced by a number of other factors including private communication with company executives and market conditions .,3,0.9874566,48.775493877459354,35
419,A breakdown of model errors indicates disparate performance on calls from different market sectors .,3,0.9329747,218.6680104348296,15
420,Aspect-based sentiment analysis produces a list of aspect terms and their corresponding sentiments for a natural language sentence .,0,0.5301733,29.627708515884603,20
420,"This task is usually done in a pipeline manner , with aspect term extraction performed first , followed by sentiment predictions toward the extracted aspect terms .",0,0.7190775,94.53329520533175,27
420,"While easier to develop , such an approach does not fully exploit joint information from the two subtasks and does not use all available sources of training information that might be helpful , such as document-level labeled sentiment corpus .",3,0.54919,49.47122897556443,41
420,"In this paper , we propose an interactive multi-task learning network ( IMN ) which is able to jointly learn multiple related tasks simultaneously at both the token level as well as the document level .",1,0.8477646,16.963010782753674,36
420,"Unlike conventional multi-task learning methods that rely on learning common features for the different tasks , IMN introduces a message passing architecture where information is iteratively passed to different tasks through a shared set of latent variables .",0,0.5352534,37.59454460817952,38
420,Experimental results demonstrate superior performance of the proposed method against multiple baselines on three benchmark datasets .,3,0.9416587,11.5992187644873,17
421,This work presents an approach decomposing propositions into four functional components and identify the patterns linking those components to determine argument structure .,1,0.7434601,88.74058130654755,23
421,The entities addressed by a proposition are target concepts and the features selected to make a point about the target concepts are aspects .,0,0.5557903,60.861025048764404,24
421,A line of reasoning is followed by providing evidence for the points made about the target concepts via aspects .,2,0.5206207,76.42540407933281,20
421,Opinions on target concepts and opinions on aspects are used to support or attack the ideas expressed by target concepts and aspects .,0,0.38516387,67.25794779989539,23
421,"The relations between aspects , target concepts , opinions on target concepts and aspects are used to infer the argument relations .",2,0.6666879,104.94627805926349,22
421,Propositions are connected iteratively to form a graph structure .,2,0.4906077,39.13995537057698,10
421,"The approach is generic in that it is not tuned for a specific corpus and evaluated on three different corpora from the literature : AAEC , AMT , US2016G1tv and achieved an F score of 0.79 , 0.77 and 0.64 , respectively .",3,0.65026444,57.68881156431183,43
422,Emotion recognition in conversations is a challenging task that has recently gained popularity due to its potential applications .,0,0.9607899,22.267998571154546,19
422,"Until now , however , a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing .",0,0.9588585,65.44078369681027,22
422,"Thus , we propose the Multimodal EmotionLines Dataset ( MELD ) , an extension and enhancement of EmotionLines .",1,0.5397708,27.478649696384334,19
422,"MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends .",0,0.46502358,37.834509137011516,14
422,"Each utterance is annotated with emotion and sentiment labels , and encompasses audio , visual and textual modalities .",2,0.49815983,51.3379777926422,19
422,We propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations .,3,0.5930888,29.218738088163047,21
422,The full dataset is available for use at http://affective-meld.github.io .,3,0.42081636,14.750739831466873,10
423,Open-domain targeted sentiment analysis aims to detect opinion targets along with their sentiment polarities from a sentence .,0,0.8875813,56.20994746372027,18
423,Prior work typically formulates this task as a sequence tagging problem .,0,0.9145725,37.3428427898221,12
423,"However , such formulation suffers from problems such as huge search space and sentiment inconsistency .",0,0.8602644,117.73753760629943,16
423,"To address these problems , we propose a span-based extract-then-classify framework , where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries , and corresponding polarities are then classified using their span representations .",2,0.6717429,59.118143179911726,45
423,"We further investigate three approaches under this framework , namely the pipeline , joint , and collapsed models .",3,0.4253426,207.64556168409507,19
423,Experiments on three benchmark datasets show that our approach consistently outperforms the sequence tagging baseline .,3,0.8941186,12.298515561407129,16
423,"Moreover , we find that the pipeline model achieves the best performance compared with the other two models .",3,0.9804483,18.08058679597661,19
424,Aspect-level sentiment classification aims to determine the sentiment polarity of a sentence towards an aspect .,0,0.8691546,22.435062575391783,17
424,"Due to the high cost in annotation , the lack of aspect-level labeled data becomes a major obstacle in this area .",0,0.90291536,26.951013172886043,22
424,"On the other hand , document-level labeled data like reviews are easily accessible from online websites .",0,0.77548987,47.801639881829615,18
424,These reviews encode sentiment knowledge in abundant contexts .,0,0.82184094,809.7019691928513,9
424,"In this paper , we propose a Transfer Capsule Network ( TransCap ) model for transferring document-level knowledge to aspect-level sentiment classification .",1,0.89254874,41.22898314006605,24
424,"To this end , we first develop an aspect routing approach to encapsulate the sentence-level semantic representations into semantic capsules from both the aspect-level and document-level data .",2,0.7329084,36.420832559462816,31
424,We then extend the dynamic routing approach to adaptively couple the semantic capsules with the class capsules under the transfer learning framework .,2,0.66007775,121.62579739721811,23
424,Experiments on SemEval datasets demonstrate the effectiveness of TransCap .,3,0.8894697,37.66595074645525,10
425,"In aspect-level sentiment classification ( ASC ) , it is prevalent to equip dominant neural models with attention mechanisms , for the sake of acquiring the importance of each context word on the given aspect .",0,0.9189526,80.57168472484237,36
425,"However , such a mechanism tends to excessively focus on a few frequent words with sentiment polarities , while ignoring infrequent ones .",0,0.71090806,89.68384780136897,23
425,"In this paper , we propose a progressive self-supervised attention learning approach for neural ASC models , which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms .",1,0.8477115,65.35898520316304,33
425,"Specifically , we iteratively conduct sentiment predictions on all training instances .",2,0.84968096,102.80627123976329,12
425,"Particularly , at each iteration , the context word with the maximum attention weight is extracted as the one with active / misleading influence on the correct / incorrect prediction of every instance , and then the word itself is masked for subsequent iterations .",2,0.57138455,101.36623847164934,45
425,"Finally , we augment the conventional training objective with a regularization term , which enables ASC models to continue equally focusing on the extracted active context words while decreasing weights of those misleading ones .",2,0.5446318,165.31834239560996,35
425,"Experimental results on multiple datasets show that our proposed approach yields better attention mechanisms , leading to substantial improvements over the two state-of-the-art neural ASC models .",3,0.9550078,16.151196027960083,33
425,Source code and trained models are available at https://github.com/DeepLearnXMU/PSSAttention .,3,0.62514573,34.3306860358084,10
426,We experiment with two recent contextualized word embedding methods ( ELMo and BERT ) in the context of open-domain argument search .,2,0.8558127,32.52191864441824,22
426,"For the first time , we show how to leverage the power of contextualized word embeddings to classify and cluster topic-dependent arguments , achieving impressive results on both tasks and across multiple datasets .",3,0.52618575,26.552192890415643,34
426,"For argument classification , we improve the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater-Evidence Sentences dataset by 7.4 percentage points .",3,0.8210808,33.838640036066884,37
426,"For the understudied task of argument clustering , we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset , and by 12.3 percentage points for the Argument Facet Similarity ( AFS ) Corpus .",2,0.5343297,33.05491852079583,42
427,"Many NLP learning tasks can be decomposed into several distinct sub-tasks , each associated with a partial label .",0,0.8426074,21.61165280114658,19
427,"In this paper we focus on a popular class of learning problems , sequence prediction applied to several sentiment analysis tasks , and suggest a modular learning approach in which different sub-tasks are learned using separate functional modules , combined to perform the final task while sharing information .",1,0.7748048,56.313260612346646,49
427,Our experiments show this approach helps constrain the learning process and can alleviate some of the supervision efforts .,3,0.96221656,40.503740162274006,19
428,"This paper focuses on two related subtasks of aspect-based sentiment analysis , namely aspect term extraction and aspect sentiment classification , which we call aspect term-polarity co-extraction .",1,0.8556567,27.328268647020543,28
428,"The former task is to extract aspects of a product or service from an opinion document , and the latter is to identify the polarity expressed in the document about these extracted aspects .",0,0.54655397,29.9518605925644,34
428,"Most existing algorithms address them as two separate tasks and solve them one by one , or only perform one task , which can be complicated for real applications .",0,0.8736388,47.49627346782046,30
428,"In this paper , we treat these two tasks as two sequence labeling problems and propose a novel Dual crOss-sharEd RNN framework ( DOER ) to generate all aspect term-polarity pairs of the input sentence simultaneously .",1,0.5557516,75.35421191142889,39
428,"Specifically , DOER involves a dual recurrent neural network to extract the respective representation of each task , and a cross-shared unit to consider the relationship between them .",2,0.51030576,74.25687775851189,29
428,Experimental results demonstrate that the proposed framework outperforms state-of-the-art baselines on three benchmark datasets .,3,0.94475234,3.8094741249725974,21
429,"Existing argumentation datasets have succeeded in allowing researchers to develop computational methods for analyzing the content , structure and linguistic features of argumentative text .",0,0.93320173,76.75892235999333,25
429,They have been much less successful in fostering studies of the effect of “ user ” traits — characteristics and beliefs of the participants — on the debate / argument outcome as this type of user information is generally not available .,0,0.8123809,60.95204422335241,42
429,"This paper presents a dataset of 78,376 debates generated over a 10-year period along with surprisingly comprehensive participant profiles .",1,0.609611,49.328411049100716,22
429,We also complete an example study using the dataset to analyze the effect of selected user traits on the debate outcome in comparison to the linguistic features typically employed in studies of this kind .,3,0.46266752,45.088893399266205,35
430,"In the literature , most of the previous studies on English implicit discourse relation recognition only use sentence-level representations , which cannot provide enough semantic information in Chinese due to its unique paratactic characteristics .",0,0.8253291,56.42642215358136,37
430,"In this paper , we propose a topic tensor network to recognize Chinese implicit discourse relations with both sentence-level and topic-level representations .",1,0.8652549,36.00716055664688,25
430,"In particular , besides encoding arguments ( discourse units ) using a gated convolutional network to obtain sentence-level representations , we train a simplified topic model to infer the latent topic-level representations .",2,0.8051578,52.556700946584996,35
430,"Moreover , we feed the two pairs of representations to two factored tensor networks , respectively , to capture both the sentence-level interactions and topic-level relevance using multi-slice tensors .",2,0.80316746,63.31430831787143,32
430,"Experimentation on CDTB , a Chinese discourse corpus , shows that our proposed model significantly outperforms several state-of-the-art baselines in both micro and macro F1-scores .",3,0.9246536,18.285517133387884,34
431,Pragmatic reasoning allows humans to go beyond the literal meaning when interpret-ing language in context .,0,0.9079583,57.7562184204442,16
431,Previous work has shown that such reasoning can improve the performance of already-trained language understanding systems .,0,0.90318775,21.99672866865154,19
431,"Here , we explore whether pragmatic reasoning during training can improve the quality of learned meanings .",1,0.84295785,50.48511513796584,17
431,"Our experiments on reference game data show that end-to-end pragmatic training produces more accurate utterance interpretation models , especially when data is sparse and language is complex .",3,0.9745127,44.06601617503267,30
432,"We address the task of assessing discourse coherence , an aspect of text quality that is essential for many NLP tasks , such as summarization and language assessment .",1,0.6171196,29.682737081485534,29
432,"We propose a hierarchical neural network trained in a multi-task fashion that learns to predict a document-level coherence score ( at the network ’s top layers ) along with word-level grammatical roles ( at the bottom layers ) , taking advantage of inductive transfer between the two tasks .",2,0.7191465,31.785519977417334,53
432,"We assess the extent to which our framework generalizes to different domains and prediction tasks , and demonstrate its effectiveness not only on standard binary evaluation coherence tasks , but also on real-world tasks involving the prediction of varying degrees of coherence , achieving a new state of the art .",3,0.50141,26.092457052821,51
433,This paper investigates the advantages and limits of data programming for the task of learning discourse structure .,1,0.9168429,41.99206367929448,18
433,"The data programming paradigm implemented in the Snorkel framework allows a user to label training data using expert-composed heuristics , which are then transformed via the “ generative step ” into probability distributions of the class labels given the training candidates .",2,0.59873545,54.74481635966515,42
433,These results are later generalized using a discriminative model .,3,0.74613863,36.218267121426024,10
433,"Snorkel ’s attractive promise to create a large amount of annotated data from a smaller set of training data by unifying the output of a set of heuristics has yet to be used for computationally difficult tasks , such as that of discourse attachment , in which one must decide where a given discourse unit attaches to other units in a text in order to form a coherent discourse structure .",0,0.758085,38.56117720233612,71
433,"Although approaching this problem using Snorkel requires significant modifications to the structure of the heuristics , we show that weak supervision methods can be more than competitive with classical supervised learning approaches to the attachment problem .",3,0.8967724,48.58401573012532,37
434,Discourse structure is integral to understanding a text and is helpful in many NLP tasks .,0,0.9278084,27.972298085904367,16
434,Learning latent representations of discourse is an attractive alternative to acquiring expensive labeled discourse data .,0,0.9305495,87.06523267442401,16
434,"Liu and Lapata ( 2018 ) propose a structured attention mechanism for text classification that derives a tree over a text , akin to an RST discourse tree .",0,0.560814,88.54960879499896,29
434,"We examine this model in detail , and evaluate on additional discourse-relevant tasks and datasets , in order to assess whether the structured attention improves performance on the end task and whether it captures a text ’s discourse structure .",2,0.43160227,83.21326297756418,40
434,We find the learned latent trees have little to no structure and instead focus on lexical cues ;,3,0.93847525,154.55842287450872,18
434,"even after obtaining more structured trees with proposed model modifications , the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser .",3,0.94806904,119.40633005848272,32
434,"Finally , ablation studies show the structured attention provides little benefit , sometimes even hurting performance .",3,0.78991336,169.48878353455004,17
435,Zero-shot learning in Language & Vision is the task of correctly labelling ( or naming ) objects of novel categories .,0,0.92895937,115.94469031937783,21
435,"Another strand of work in L&V aims at pragmatically informative rather than “ correct ” object descriptions , e.g .",0,0.6620682,141.5672687332509,20
435,in reference games .,3,0.3656508,443.3395845191573,4
435,"We combine these lines of research and model zero-shot reference games , where a speaker needs to successfully refer to a novel object in an image .",2,0.5859489,58.933029767031265,27
435,"Inspired by models of “ rational speech acts ” , we extend a neural generator to become a pragmatic speaker reasoning about uncertain object categories .",2,0.6045546,117.19466830378468,26
435,"As a result of this reasoning , the generator produces fewer nouns and names of distractor categories as compared to a literal speaker .",3,0.8513367,86.557525778897,24
435,"We show that this conversational strategy for dealing with novel objects often improves communicative success , in terms of resolution accuracy of an automatic listener .",3,0.95596826,118.259629983389,26
436,Recent neural network models have significantly advanced the task of coreference resolution .,0,0.9443061,22.502115206393732,13
436,"However , current neural coreference models are usually trained with heuristic loss functions that are computed over a sequence of local decisions .",0,0.89795834,45.178594757774086,23
436,"In this paper , we introduce an end-to-end reinforcement learning based coreference resolution model to directly optimize coreference evaluation metrics .",1,0.85429126,18.98934315837577,23
436,"Specifically , we modify the state-of-the-art higher-order mention ranking approach in Lee et al .",2,0.63546324,22.391516414064675,23
436,( 2018 ) to a reinforced policy gradient model by incorporating the reward associated with a sequence of coreference linking actions .,2,0.46881714,160.28109097185717,22
436,"Furthermore , we introduce maximum entropy regularization for adequate exploration to prevent the model from prematurely converging to a bad local optimum .",2,0.6368804,84.56766433199752,23
436,Our proposed model achieves new state-of-the-art performance on the English OntoNotes v5.0 benchmark .,3,0.8853808,8.939385768121161,19
437,"Discourse relation identification has been an active area of research for many years , and the challenge of identifying implicit relations remains largely an unsolved task , especially in the context of an open-domain dialogue system .",0,0.93353426,25.108113046956586,37
437,"Previous work primarily relies on a corpora of formal text which is inherently non-dialogic , i.e. , news and journals .",0,0.90451646,55.73638808804191,21
437,This data however is not suitable to handle the nuances of informal dialogue nor is it capable of navigating the plethora of valid topics present in open-domain dialogue .,3,0.5744089,42.24385866863063,29
437,"In this paper , we designed a novel discourse relation identification pipeline specifically tuned for open-domain dialogue systems .",1,0.8732481,47.30300480978723,19
437,"We firstly propose a method to automatically extract the implicit discourse relation argument pairs and labels from a dataset of dialogic turns , resulting in a novel corpus of discourse relation pairs ;",2,0.7104057,81.38689522538581,33
437,the first of its kind to attempt to identify the discourse relations connecting the dialogic turns in open-domain discourse .,1,0.4655028,57.18086697049327,20
437,"Moreover , we have taken the first steps to leverage the dialogue features unique to our task to further improve the identification of such relations by performing feature ablation and incorporating dialogue features to enhance the state-of-the-art model .",3,0.7173937,25.14967395777507,45
438,"A key challenge in coreference resolution is to capture properties of entity clusters , and use those in the resolution process .",0,0.90196675,42.62535542831504,22
438,"Here we provide a simple and effective approach for achieving this , via an “ Entity Equalization ” mechanism .",1,0.6071973,53.370833323071736,20
438,The Equalization approach represents each mention in a cluster via an approximation of the sum of all mentions in the cluster .,2,0.5575331,34.014747996661164,22
438,"We show how this can be done in a fully differentiable end-to-end manner , thus enabling high-order inferences in the resolution process .",3,0.7345934,30.780226647081026,25
438,"Our approach , which also employs BERT embeddings , results in new state-of-the-art results on the CoNLL-2012 coreference resolution task , improving average F1 by 3.6 % .",3,0.8821623,17.739667472343644,36
439,Coherence is an important aspect of text quality and is crucial for ensuring its readability .,0,0.9172203,20.29837761111601,16
439,One important limitation of existing coherence models is that training on one domain does not easily generalize to unseen categories of text .,0,0.84602624,29.829399305405055,23
439,"Previous work advocates for generative models for cross-domain generalization , because for discriminative models , the space of incoherent sentence orderings to discriminate against during training is prohibitively large .",0,0.90118146,59.473070017466675,30
439,"In this work , we propose a local discriminative neural model with a much smaller negative sampling space that can efficiently learn against incorrect orderings .",1,0.7277031,47.82619489162086,26
439,"The proposed coherence model is simple in structure , yet it significantly outperforms previous state-of-art methods on a standard benchmark dataset on the Wall Street Journal corpus , as well as in multiple new challenging settings of transfer to unseen categories of discourse on Wikipedia articles .",3,0.84073067,42.93368292831941,51
440,"In this work , we introduce the MOldavian and ROmanian Dialectal COrpus ( MOROCO ) , which is freely available for download at https://github.com/butnaruandrei/MOROCO .",1,0.35871598,50.52006945209753,25
440,The corpus contains 33564 samples of text ( with over 10 million tokens ) collected from the news domain .,2,0.7423175,111.82254814608024,20
440,"The samples belong to one of the following six topics : culture , finance , politics , science , sports and tech .",2,0.54342157,63.19140101231604,23
440,"The data set is divided into 21719 samples for training , 5921 samples for validation and another 5924 samples for testing .",2,0.82826024,49.2740120394253,22
440,"For each sample , we provide corresponding dialectal and category labels .",2,0.8319957,139.07121144563652,12
440,"This allows us to perform empirical studies on several classification tasks such as ( i ) binary discrimination of Moldavian versus Romanian text samples , ( ii ) intra-dialect multi-class categorization by topic and ( iii ) cross-dialect multi-class categorization by topic .",2,0.45834264,24.698806505887884,43
440,"We perform experiments using a shallow approach based on string kernels , as well as a novel deep approach based on character-level convolutional neural networks containing Squeeze-and-Excitation blocks .",2,0.75276154,41.39170078694571,35
440,"We also present and analyze the most discriminative features of our best performing model , before and after named entity removal .",3,0.46244174,66.00164214159425,22
441,"The well-known problem of knowledge acquisition is one of the biggest issues in Word Sense Disambiguation ( WSD ) , where annotated data are still scarce in English and almost absent in other languages .",0,0.9670601,21.910961102743475,36
441,"In this paper we formulate the assumption of One Sense per Wikipedia Category and present OneSeC , a language-independent method for the automatic extraction of hundreds of thousands of sentences in which a target word is tagged with its meaning .",1,0.78457326,50.30720358061538,43
441,Our automatically-generated data consistently lead a supervised WSD model to state-of-the-art performance when compared with other automatic and semi-automatic methods .,3,0.9485484,23.42777023725125,29
441,"Moreover , our approach outperforms its competitors on multilingual and domain-specific settings , where it beats the existing state of the art on all languages and most domains .",3,0.91787463,21.52403540615572,29
441,All the training data are available for research purposes at http://trainomatic.org/onesec .,3,0.6170677,39.916756109302334,12
442,Cross-lingual word embeddings ( CLEs ) facilitate cross-lingual transfer of NLP models .,0,0.8179078,13.132917695413507,13
442,"Despite their ubiquitous downstream usage , increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction ( BLI ) .",0,0.9369344,123.51408873952356,25
442,"Even the BLI evaluations vary greatly , hindering our ability to correctly interpret performance and properties of different CLE models .",0,0.5313921,94.48079509815288,21
442,"In this work , we take the first step towards a comprehensive evaluation of CLE models : we thoroughly evaluate both supervised and unsupervised CLE models , for a large number of language pairs , on BLI and three downstream tasks , providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP .",1,0.49106482,27.74641180003053,59
442,We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance .,3,0.9131548,26.305116650130326,28
442,"We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines , which still display competitive performance across the board .",3,0.9645059,78.93499670139593,28
442,We hope our work catalyzes further research on CLE evaluation and model analysis .,3,0.92014545,82.41335142091602,14
443,Selectional Preference ( SP ) is a commonly observed language phenomenon and proved to be useful in many natural language processing tasks .,0,0.96215755,25.117597042851575,23
443,"To provide a better evaluation method for SP models , we introduce SP-10K , a large-scale evaluation set that provides human ratings for the plausibility of 10,000 SP pairs over five SP relations , covering 2,500 most frequent verbs , nouns , and adjectives in American English .",2,0.80794084,47.571535679220624,51
443,Three representative SP acquisition methods based on pseudo-disambiguation are evaluated with SP-10K .,2,0.7052061,82.20333721522383,15
443,"To demonstrate the importance of our dataset , we investigate the relationship between SP-10 K and the commonsense knowledge in ConceptNet5 and show the potential of using SP to represent the commonsense knowledge .",2,0.34349892,32.97718610893304,36
443,We also use the Winograd Schema Challenge to prove that the proposed new SP relations are essential for the hard pronoun coreference resolution problem .,3,0.4539983,65.08241639790637,25
444,"We perform an interdisciplinary large-scale evaluation for detecting lexical semantic divergences in a diachronic and in a synchronic task : semantic sense changes across time , and semantic sense changes across domains .",2,0.67327946,43.43308158771169,33
444,"Our work addresses the superficialness and lack of comparison in assessing models of diachronic lexical change , by bringing together and extending benchmark models on a common state-of-the-art evaluation task .",3,0.5875534,50.099020892889335,37
444,"In addition , we demonstrate that the same evaluation task and modelling approaches can successfully be utilised for the synchronic detection of domain-specific sense divergences in the field of term extraction .",3,0.9497941,62.226127936000104,32
445,"Though error analysis is crucial to understanding and improving NLP models , the common practice of manual , subjective categorization of a small sample of errors can yield biased and incomplete conclusions .",0,0.82590866,53.780103427060716,33
445,"This paper codifies model and task agnostic principles for informative error analysis , and presents Errudite , an interactive tool for better supporting this process .",1,0.7955999,149.25343274177754,26
445,"First , error groups should be precisely defined for reproducibility ;",3,0.5336906,302.61488627876327,11
445,Errudite supports this with an expressive domain-specific language .,3,0.4782466,70.50938966987235,9
445,"Second , to avoid spurious conclusions , a large set of instances should be analyzed , including both positive and negative examples ;",3,0.5650544,123.52433708729932,23
445,Errudite enables systematic grouping of relevant instances with filtering queries .,3,0.46673724,304.65401914055155,11
445,"Third , hypotheses about the cause of errors should be explicitly tested ;",3,0.57686454,202.36922585854876,13
445,Errudite supports this via automated counterfactual rewriting .,0,0.41319615,239.47998624801974,8
445,"We validate our approach with a user study , finding that Errudite ( 1 ) enables users to perform high quality and reproducible error analyses with less effort , ( 2 ) reveals substantial ambiguities in prior published error analyses practices , and ( 3 ) enhances the error analysis experience by allowing users to test and revise prior beliefs .",3,0.8064784,50.88957934062355,61
446,"Multiple entities in a document generally exhibit complex inter-sentence relations , and cannot be well handled by existing relation extraction ( RE ) methods that typically focus on extracting intra-sentence relations for single entity pairs .",0,0.90220284,36.31839983352558,36
446,"In order to accelerate the research on document-level RE , we introduce DocRED , a new dataset constructed from Wikipedia and Wikidata with three features : ( 1 ) DocRED annotates both named entities and relations , and is the largest human-annotated dataset for document-level RE from plain text ;",2,0.5564996,32.179801958010174,53
446,( 2 ) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document ;,3,0.413868,116.88063742878307,26
446,"( 3 ) along with the human-annotated data , we also offer large-scale distantly supervised data , which enables DocRED to be adopted for both supervised and weakly supervised scenarios .",3,0.58352745,50.08614633614781,31
446,"In order to verify the challenges of document-level RE , we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED .",2,0.5234829,22.537363499953354,35
446,"Empirical results show that DocRED is challenging for existing RE methods , which indicates that document-level RE remains an open problem and requires further efforts .",3,0.98006415,53.269146256740825,27
446,"Based on the detailed analysis on the experiments , we discuss multiple promising directions for future research .",3,0.9483278,31.8738838182035,18
446,We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED .,3,0.5061716,9.473592725748171,14
447,Cloze-style reading comprehension in Chinese is still limited due to the lack of various corpora .,0,0.9150796,22.535477541377595,16
447,"In this paper we propose a large-scale Chinese cloze test dataset ChID , which studies the comprehension of idiom , a unique language phenomenon in Chinese .",1,0.89316046,53.29335863271835,27
447,"In this corpus , the idioms in a passage are replaced by blank symbols and the correct answer needs to be chosen from well-designed candidate idioms .",2,0.43697375,49.98160057422376,29
447,We carefully study how the design of candidate idioms and the representation of idioms affect the performance of state-of-the-art models .,1,0.37773034,11.072303835470759,27
447,"Results show that the machine accuracy is substantially worse than that of human , indicating a large space for further research .",3,0.9854484,39.27087352159258,22
448,"Topic models are typically evaluated with respect to the global topic distributions that they generate , using metrics such as coherence , but without regard to local ( token-level ) topic assignments .",0,0.8200174,64.09962653340112,35
448,Token-level assignments are important for downstream tasks such as classification .,0,0.6526668,38.50716529578777,13
448,"Even recent models , which aim to improve the quality of these token-level topic assignments , have been evaluated only with respect to global metrics .",0,0.8691357,72.31631266423787,28
448,We propose a task designed to elicit human judgments of token-level topic assignments .,1,0.49338385,75.99967072574414,16
448,We use a variety of topic model types and parameters and discover that global metrics agree poorly with human assignments .,3,0.6828349,129.77099156221297,21
448,Since human evaluation is expensive we propose a variety of automated metrics to evaluate topic models at a local level .,0,0.69999975,49.6221616777805,21
448,"Finally , we correlate our proposed metrics with human judgments from the task on several datasets .",3,0.62392205,38.109054625561264,17
448,We show that an evaluation based on the percent of topic switches correlates most strongly with human judgment of local topic quality .,3,0.9719438,68.72129670400857,23
448,"We suggest that this new metric , which we call consistency , be adopted alongside global metrics such as topic coherence when evaluating new topic models .",3,0.89450824,81.77130378023608,27
449,"One of the key steps in language resource creation is the identification of the text segments to be annotated , or markables , which depending on the task may vary from nominal chunks for named entity resolution to ( potentially nested ) noun phrases in coreference resolution ( or mentions ) to larger text segments in text segmentation .",0,0.87752914,74.7693694893284,59
449,"Markable identification is typically carried out semi-automatically , by running a markable identifier and correcting its output by hand –which is increasingly done via annotators recruited through crowdsourcing and aggregating their responses .",0,0.8502771,75.79136235374305,34
449,"In this paper , we present a method for identifying markables for coreference annotation that combines high-performance automatic markable detectors with checking with a Game-With-A-Purpose ( GWAP ) and aggregation using a Bayesian annotation model .",1,0.8964678,67.42817168907011,39
449,"The method was evaluated both on news data and data from a variety of other genres and results in an improvement on F1 of mention boundaries of over seven percentage points when compared with a state-of-the-art , domain-independent automatic mention detector , and almost three points over an in-domain mention detector .",3,0.70299244,35.81351268908928,59
449,"One of the key contributions of our proposal is its applicability to the case in which markables are nested , as is the case with coreference markables ;",3,0.74155897,56.536198530535884,28
449,but the GWAP and several of the proposed markable detectors are task and language-independent and are thus applicable to a variety of other annotation scenarios .,3,0.8596682,111.94642841572563,28
450,Over-dependence on domain ontology and lack of sharing knowledge across domains are two practical and yet less studied problems of dialogue state tracking .,0,0.779573,46.608772233433186,24
450,Existing approaches generally fall short when tracking unknown slot values during inference and often have difficulties in adapting to new domains .,0,0.84811777,70.53339950571129,22
450,"In this paper , we propose a Transferable Dialogue State Generator ( TRADE ) that generates dialogue states from utterances using copy mechanism , facilitating transfer when predicting ( domain , slot , value ) triplets not encountered during training .",1,0.82360893,99.69914646881168,41
450,"Our model is composed of an utterance encoder , a slot gate , and a state generator , which are shared across domains .",2,0.7943675,45.964042301033096,24
450,"Empirical results demonstrate that TRADE achieves state-of-the-art 48.62 % joint goal accuracy for the five domains of MultiWOZ , a human-human dialogue dataset .",3,0.95606923,30.350288829657078,27
450,"In addition , we show the transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains .",3,0.6052412,57.98719143874503,20
450,"TRADE achieves 60.58 % joint goal accuracy in one of the zero-shot domains , and is able to adapt to few-shot cases without forgetting already trained domains .",3,0.8508312,75.52075940857567,28
451,We present methods for multi-task learning that take advantage of natural groupings of related tasks .,1,0.49298164,28.60198971711677,16
451,"Task groups may be defined along known properties of the tasks , such as task domain or language .",0,0.44715142,98.23497269957235,19
451,Such task groups represent supervised information at the inter-task level and can be encoded into the model .,3,0.43117705,55.99198039263125,18
451,"We investigate two variants of neural network architectures that accomplish this , learning different feature spaces at the levels of individual tasks , task groups , as well as the universe of all tasks : ( 1 ) parallel architectures encode each input simultaneously into feature spaces at different levels ;",2,0.6862807,137.156164948072,51
451,( 2 ) serial architectures encode each input successively into feature spaces at different levels in the task hierarchy .,0,0.4208176,221.70277281345903,20
451,"We demonstrate the methods on natural language understanding ( NLU ) tasks , where a grouping of tasks into different task domains leads to improved performance on ATIS , Snips , and a large in-house dataset .",3,0.48274758,79.90469240075726,38
452,Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems .,0,0.90353084,15.790047930167145,20
452,"Avenues like the E2E NLG Challenge have encouraged the development of neural approaches , particularly sequence-to-sequence ( Seq2Seq ) models for this problem .",0,0.9354557,35.40978625270463,25
452,"The semantic representations used , however , are often underspecified , which places a higher burden on the generation model for sentence planning , and also limits the extent to which generated responses can be controlled in a live system .",0,0.74669045,58.68608714635134,41
452,"In this paper , we ( 1 ) propose using tree-structured semantic representations , like those used in traditional rule-based NLG systems , for better discourse-level structuring and sentence-level planning ;",1,0.78456646,62.35265797423546,35
452,( 2 ) introduce a challenging dataset using this representation for the weather domain ;,2,0.6120004,722.8087503032536,15
452,( 3 ) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness ;,2,0.47666737,89.94363334407987,20
452,and ( 4 ) demonstrate promising results on our dataset and the E2E dataset .,3,0.9447391,50.95255225213006,15
453,We study a conversational reasoning model that strategically traverses through a large-scale common fact knowledge graph ( KG ) to introduce engaging and contextually diverse entities and attributes .,2,0.45595044,70.97882219828355,29
453,"KG parallel corpus called OpenDialKG , where each utterance from 15 K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1 M + facts .",2,0.6842292,85.58594406002123,40
453,"We then propose the DialKG Walker model that learns the symbolic transitions of dialog contexts as structured traversals over KG , and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic , attention-based graph path decoder .",2,0.6327907,104.43519368142395,43
453,"Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-of-the-art baselines or rule-based models , in both in-domain and cross-domain tasks .",3,0.9526044,12.392615995483553,39
453,"The proposed model also generates a KG walk path for each entity retrieved , providing a natural way to explain conversational reasoning .",3,0.75209785,79.87029412857505,23
454,"In this paper , we present an approach to incorporate retrieved datapoints as supporting evidence for context-dependent semantic parsing , such as generating source code conditioned on the class environment .",1,0.88952327,45.29876942786587,32
454,"Our approach naturally combines a retrieval model and a meta-learner , where the former learns to find similar datapoints from the training data , and the latter considers retrieved datapoints as a pseudo task for fast adaptation .",2,0.7012799,40.40817307134254,38
454,"Specifically , our retriever is a context-aware encoder-decoder model with a latent variable which takes context environment into consideration , and our meta-learner learns to utilize retrieved datapoints in a model-agnostic meta-learning paradigm for fast adaptation .",2,0.7575796,32.87347969819703,42
454,"We conduct experiments on CONCODE and CSQA datasets , where the context refers to class environment in JAVA codes and conversational history , respectively .",2,0.8728275,111.36460662383983,25
454,"We use sequence-to-action model as the base semantic parser , which performs the state-of-the-art accuracy on both datasets .",2,0.60303456,22.86396067178215,27
454,"Results show that both the context-aware retriever and the meta-learning strategy improve accuracy , and our approach performs better than retrieve-and-edit baselines .",3,0.98666763,31.23329551404534,28
455,"Resolving pronoun coreference requires knowledge support , especially for particular domains ( e.g. , medicine ) .",0,0.6083288,221.84997850173588,17
455,"In this paper , we explore how to leverage different types of knowledge to better resolve pronoun coreference with a neural model .",1,0.9286896,39.36970142064463,23
455,"To ensure the generalization ability of our model , we directly incorporate knowledge in the format of triplets , which is the most common format of modern knowledge graphs , instead of encoding it with features or rules as that in conventional approaches .",2,0.63822234,53.692061578793734,44
455,"Moreover , since not all knowledge is helpful in certain contexts , to selectively use them , we propose a knowledge attention module , which learns to select and use informative knowledge based on contexts , to enhance our model .",2,0.51746505,70.92053062686556,41
455,"Experimental results on two datasets from different domains prove the validity and effectiveness of our model , where it outperforms state-of-the-art baselines by a large margin .",3,0.88194513,6.918462834040406,33
455,"Moreover , since our model learns to use external knowledge rather than only fitting the training data , it also demonstrates superior performance to baselines in the cross-domain setting .",3,0.89814097,27.67711762467675,30
456,Natural Language Inference ( NLI ) datasets often contain hypothesis-only biases — artifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis .,0,0.95076436,41.037102043254976,29
456,We propose two probabilistic methods to build models that are more robust to such biases and better transfer across datasets .,2,0.47430968,22.733234627320023,21
456,"In contrast to standard approaches to NLI , our methods predict the probability of a premise given a hypothesis and NLI label , discouraging models from ignoring the premise .",2,0.46682385,73.78588757858627,30
456,We evaluate our methods on synthetic and existing NLI datasets by training on datasets containing biases and testing on datasets containing no ( or different ) hypothesis-only biases .,2,0.7604718,97.02788183070774,29
456,"Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts , transferring better than a baseline architecture in 9 out of 12 NLI datasets .",3,0.98980623,45.89946436276713,31
456,"Additionally , we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets , as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets .",3,0.53535694,28.471719187529636,37
457,Fact verification ( FV ) is a challenging task which requires to retrieve relevant evidence from plain text and use the evidence to verify given claims .,0,0.9623261,34.63077794511856,27
457,Many claims require to simultaneously integrate and reason over several pieces of evidence for verification .,0,0.9141075,69.34167503505346,16
457,"However , previous work employs simple models to extract information from evidence without letting evidence communicate with each other , e.g. , merely concatenate the evidence for processing .",0,0.90630025,61.08428425551069,29
457,"Therefore , these methods are unable to grasp sufficient relational and logical information among the evidence .",0,0.8278165,111.59935558178017,17
457,"To alleviate this issue , we propose a graph-based evidence aggregating and reasoning ( GEAR ) framework which enables information to transfer on a fully-connected evidence graph and then utilizes different aggregators to collect multi-evidence information .",2,0.4265019,62.002597838610754,40
457,"We further employ BERT , an effective pre-trained language representation model , to improve the performance .",2,0.51059055,31.067395575924902,17
457,Experimental results on a large-scale benchmark dataset FEVER have demonstrated that GEAR could leverage multi-evidence information for FV and thus achieves the promising result with a test FEVER score of 67.10 % .,3,0.9383928,75.89381655031197,33
457,Our code is available at https://github.com/thunlp/GEAR .,3,0.60971785,4.758277534618272,7
458,"We present SherLIiC , a testbed for lexical inference in context ( LIiC ) , consisting of 3985 manually annotated inference rule candidates ( InfCands ) , accompanied by ( i ) ~ 960 k unlabeled InfCands , and ( ii ) ~ 190 k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09 .",2,0.5738787,135.5013120423417,61
458,"Each InfCand consists of one of these relations , expressed as a lemmatized dependency path , and two argument placeholders , each linked to one or more Freebase types .",0,0.4086529,170.57286061644524,30
458,"Due to our candidate selection process based on strong distributional evidence , SherLIiC is much harder than existing testbeds because distributional evidence is of little utility in the classification of InfCands .",3,0.8257791,93.09347957575568,32
458,"We also show that , due to its construction , many of SherLIiC ’s correct InfCands are novel and missing from existing rule bases .",3,0.9279631,270.2616345737581,25
458,"We evaluate a large number of strong baselines on SherLIiC , ranging from semantic vector space models to state of the art neural models of natural language inference ( NLI ) .",2,0.6924594,57.20863045785505,32
458,We show that SherLIiC poses a tough challenge to existing NLI systems .,3,0.93333614,91.82027078676741,13
459,"This paper describes novel models tailored for a new application , that of extracting the symptoms mentioned in clinical conversations along with their status .",1,0.8568443,121.36538590801727,25
459,"Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus , consisting of about 3 K conversations annotated by professional medical scribes .",0,0.5034437,55.46323492329388,32
459,"We propose two novel deep learning approaches to infer the symptom names and their status : ( 1 ) a new hierarchical span-attribute tagging ( SA-T ) model , trained using curriculum learning , and ( 2 ) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation .",2,0.7157816,57.94186243096276,66
459,This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations .,0,0.6145676,98.57234529591103,22
459,"To reflect this application , we define multiple metrics .",2,0.5412633,251.3182456573944,10
459,"From inter-rater agreement , we find that the task is inherently difficult .",3,0.94376945,20.994319870781215,13
459,We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition .,3,0.77839136,18.111124027948915,29
459,"Our analysis not only reveals the inherent challenges of the task , but also provides useful directions to improve the models .",3,0.9584442,20.436519949318924,22
460,The quality of a counseling intervention relies highly on the active collaboration between clients and counselors .,0,0.8761575,42.68258936286635,17
460,"In this paper , we explore several linguistic aspects of the collaboration process occurring during counseling conversations .",1,0.9342276,76.14611035638612,18
460,"Specifically , we address the differences between high-quality and low-quality counseling .",1,0.6211248,28.775971066297686,12
460,"Our approach examines participants ’ turn-by-turn interaction , their linguistic alignment , the sentiment expressed by speakers during the conversation , as well as the different topics being discussed .",2,0.6783993,71.78632787767675,30
460,"Our results suggest important language differences in low-and high-quality counseling , which we further use to derive linguistic features able to capture the differences between the two groups .",3,0.9893293,83.69640502434828,30
460,These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88 % .,2,0.48746377,35.57175057843305,22
461,Mental health counseling is an enterprise with profound societal importance where conversations play a primary role .,0,0.9077034,82.35411171780747,17
461,"In order to acquire the conversational skills needed to face a challenging range of situations , mental health counselors must rely on training and on continued experience with actual clients .",0,0.6673624,73.00453932169417,31
461,"However , in the absence of large scale longitudinal studies , the nature and significance of this developmental process remain unclear .",0,0.94914037,41.37883419731009,22
461,"For example , prior literature suggests that experience might not translate into consequential changes in counselor behavior .",0,0.8486471,159.6425875260146,18
461,This has led some to even argue that counseling is a profession without expertise .,0,0.8867932,56.184865461282804,15
461,"In this work , we develop a computational framework to quantify the extent to which individuals change their linguistic behavior with experience and to study the nature of this evolution .",1,0.8722511,28.58460592538438,31
461,"We use our framework to conduct a large longitudinal study of mental health counseling conversations , tracking over 3,400 counselors across their tenure .",2,0.7939505,63.65280139256473,24
461,"We reveal that overall , counselors do indeed change their conversational behavior to become more diverse across interactions , developing an individual voice that distinguishes them from other counselors .",3,0.98620903,131.14676298490375,30
461,"Furthermore , a finer-grained investigation shows that the rate and nature of this diversification vary across functionally different conversational components .",3,0.8976845,55.82309707441837,22
462,We discuss ongoing work into automating a multilingual digital helpdesk service available via text messaging to pregnant and breastfeeding mothers in South Africa .,1,0.50029963,51.772388963214645,24
462,"Our anonymized dataset consists of short informal questions , often in low-resource languages , with unreliable language labels , spelling errors and code-mixing , as well as template answers with some inconsistencies .",2,0.7199727,115.55832833273932,33
462,"We explore cross-lingual word embeddings , and train parametric and non-parametric models on 90 K samples for answer selection from a set of 126 templates .",2,0.8552591,44.54369609415742,26
462,"Preliminary results indicate that LSTMs trained end-to-end perform best , with a test accuracy of 62.13 % and a recall@5 of 89.56 % , and demonstrate that we can accelerate response time by several orders of magnitude .",3,0.96556515,36.92833805054275,40
463,"Negative medical findings are prevalent in clinical reports , yet discriminating them from positive findings remains a challenging task for in-formation extraction .",0,0.91984487,68.10016871089641,23
463,"Most of the existing systems treat this task as a pipeline of two separate tasks , i.e. , named entity recognition ( NER ) and rule-based negation detection .",0,0.88684404,27.978721181526172,30
463,We consider this as a multi-task problem and present a novel end-to-end neural model to jointly extract entities and negations .,2,0.4459595,14.386627146097709,23
463,We extend a standard hierarchical encoder-decoder NER model and first adopt a shared encoder followed by separate decoders for the two tasks .,2,0.8153792,22.572833842631162,23
463,This architecture performs considerably better than the previous rule-based and machine learning-based systems .,3,0.80668736,30.696885371674856,16
463,"To overcome the problem of increased parameter size especially for low-resource settings , we propose the Conditional Softmax Shared Decoder architecture which achieves state-of-art results for NER and negation detection on the 2010 i2 b2 / VA challenge dataset and a proprietary de-identified clinical dataset .",2,0.5239662,62.12962136834895,50
464,"We present HEAD-QA , a multi-choice question answering testbed to encourage research on complex reasoning .",1,0.4606143,50.5053647413423,18
464,"The questions come from exams to access a specialized position in the Spanish healthcare system , and are challenging even for highly specialized humans .",0,0.73110276,106.44214364640999,25
464,We then consider monolingual ( Spanish ) and cross-lingual ( to English ) experiments with information retrieval and neural techniques .,2,0.7381473,52.80196694901372,21
464,"We show that : ( i ) HEAD-QA challenges current methods , and ( ii ) the results lag well behind human performance , demonstrating its usefulness as a benchmark for future work .",3,0.9571647,72.8676853798265,36
465,"With the advancement in argument detection , we suggest to pay more attention to the challenging task of identifying the more convincing arguments .",3,0.71119845,43.128385451933205,24
465,Machines capable of responding and interacting with humans in helpful ways have become ubiquitous .,0,0.9535048,49.01353671609882,15
465,"We now expect them to discuss with us the more delicate questions in our world , and they should do so armed with effective arguments .",3,0.48953706,85.94177438067386,26
465,"In this paper , we present a new data set , IBM-EviConv , of pairs of evidence labeled for convincingness , designed to be more challenging than existing alternatives .",1,0.853841,88.42361841205009,32
465,We also propose a Siamese neural network architecture shown to outperform several baselines on both a prior convincingness data set and our own .,3,0.43633497,50.062400387392046,24
465,"Finally , we provide insights into our experimental results and the various kinds of argumentative value our method is capable of detecting .",3,0.8114804,35.707041148574916,23
466,"When debating a controversial topic , it is often desirable to expand the boundaries of discussion .",0,0.9115334,47.84072407856475,17
466,"For example , we may consider the pros and cons of possible alternatives to the debate topic , make generalizations , or give specific examples .",0,0.43752003,91.55606903725278,26
466,"We introduce the task of Debate Topic Expansion-finding such related topics for a given debate topic , along with a novel annotated dataset for the task .",1,0.4247288,87.8950805754264,29
466,"We focus on relations between Wikipedia concepts , and show that they differ from well-studied lexical-semantic relations such as hypernyms , hyponyms and antonyms .",3,0.42750397,24.6249204107417,27
466,We present algorithms for finding both consistent and contrastive expansions and demonstrate their effectiveness empirically .,3,0.45781672,86.75400557113862,16
466,We suggest that debate topic expansion may have various use cases in argumentation mining .,3,0.9679882,152.58631942583045,15
467,Studies on emotion recognition ( ER ) show that combining lexical and acoustic information results in more robust and accurate models .,0,0.9287107,33.08095152074031,22
467,The majority of the studies focus on settings where both modalities are available in training and evaluation .,0,0.6465884,17.49770702496115,18
467,"However , in practice , this is not always the case ;",0,0.8391335,52.554069608726316,12
467,getting ASR output may represent a bottleneck in a deployment pipeline due to computational complexity or privacy-related constraints .,0,0.6604502,74.67676522068234,21
467,"To address this challenge , we study the problem of efficiently combining acoustic and lexical modalities during training while still providing a deployable acoustic model that does not require lexical inputs .",1,0.71167713,31.41899011513198,32
467,We first experiment with multimodal models and two attention mechanisms to assess the extent of the benefits that lexical information can provide .,2,0.6948769,27.829086557560657,23
467,"Then , we frame the task as a multi-view learning problem to induce semantic information from a multimodal model into our acoustic-only network using a contrastive loss function .",2,0.8431284,30.337216062441843,29
467,Our multimodal model outperforms the previous state of the art on the USC-IEMOCAP dataset reported on lexical and acoustic information .,3,0.89496815,37.750495878014206,21
467,"Additionally , our multi-view-trained acoustic network significantly surpasses models that have been exclusively trained with acoustic features .",3,0.9413031,51.81761059644509,19
468,"Emotion cause extraction ( ECE ) , the task aimed at extracting the potential causes behind certain emotions in text , has gained much attention in recent years due to its wide applications .",0,0.9660731,57.83597509877707,34
468,"However , it suffers from two shortcomings : 1 ) the emotion must be annotated before cause extraction in ECE , which greatly limits its applications in real-world scenarios ;",0,0.7698998,72.40640412273014,30
468,2 ) the way to first annotate emotion and then extract the cause ignores the fact that they are mutually indicative .,3,0.55291826,116.44883722670903,22
468,"In this work , we propose a new task : emotion-cause pair extraction ( ECPE ) , which aims to extract the potential pairs of emotions and corresponding causes in a document .",1,0.79126346,35.39801957098465,35
468,"We propose a 2-step approach to address this new ECPE task , which first performs individual emotion extraction and cause extraction via multi-task learning , and then conduct emotion-cause pairing and filtering .",2,0.5311801,75.01188018423902,37
468,The experimental results on a benchmark emotion cause corpus prove the feasibility of the ECPE task as well as the effectiveness of our approach .,3,0.9520539,32.999074356201014,25
469,"What they often do is rely on ” first principles ” , commonplace arguments which are relevant to many topics , and which they have refined in past debates .",0,0.8657494,110.93334647315037,30
469,"In this work we aim to explicitly define a taxonomy of such principled recurring arguments , and , given a controversial topic , to automatically identify which of these arguments are relevant to the topic .",1,0.9268774,44.859482116565076,36
469,"As far as we know , this is the first time that this approach to argument invention is formalized and made explicit in the context of NLP .",3,0.8020855,19.601754502864935,28
469,The main goal of this work is to show that it is possible to define such a taxonomy .,1,0.8677102,10.727820399025259,19
469,"While the taxonomy suggested here should be thought of as a ” first attempt ” it is nonetheless coherent , covers well the relevant topics and coincides with what professional debaters actually argue in their speeches , and facilitates automatic argument invention for new topics .",3,0.9128563,151.2702906724333,46
470,The most important obstacles facing multi-document summarization include excessive redundancy in source descriptions and the looming shortage of training data .,0,0.8804646,53.140266815935085,21
470,"These obstacles prevent encoder-decoder models from being used directly , but optimization-based methods such as determinantal point processes ( DPPs ) are known to handle them well .",0,0.8973303,50.89671405718438,30
470,In this paper we seek to strengthen a DPP-based method for extractive multi-document summarization by presenting a novel similarity measure inspired by capsule networks .,1,0.9208686,32.445323608843495,27
470,The approach measures redundancy between a pair of sentences based on surface form and semantic information .,2,0.6133022,67.96777651680607,17
470,"We show that our DPP system with improved similarity measure performs competitively , outperforming strong summarization baselines on benchmark datasets .",3,0.9622532,59.14916001382039,21
470,Our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions .,3,0.98986876,115.10056416009233,19
471,We propose a global optimization method under length constraint ( GOLC ) for neural text summarization models .,1,0.4245036,83.82417311783027,18
471,"GOLC increases the probabilities of generating summaries that have high evaluation scores , ROUGE in this paper , within a desired length .",3,0.5092073,113.14202570843545,23
471,"We compared GOLC with two optimization methods , a maximum log-likelihood and a minimum risk training , on CNN / Daily Mail and a Japanese single document summarization data set of The Mainichi Shimbun Newspapers .",2,0.88444126,80.46038332872315,36
471,The experimental results show that a state-of-the-art neural summarization model optimized with GOLC generates fewer overlength summaries while maintaining the fastest processing speed ;,3,0.98427767,45.56649195125661,30
471,"only 6.70 % overlength summaries on CNN / Daily and 7.8 % on long summary of Mainichi , compared to the approximately 20 % to 50 % on CNN / Daily Mail and 10 % to 30 % on Mainichi with the other optimization methods .",3,0.9467723,84.46038726935538,46
471,We also demonstrate the importance of the generation of in-length summaries for post-editing with the dataset Mainich that is created with strict length constraints .,3,0.9262053,80.28313378616812,25
471,The ex-perimental results show approximately 30 % to 40 % improved post-editing time by use of in-length summaries .,3,0.98528165,52.48887958872356,20
472,The recent years have seen remarkable success in the use of deep neural networks on text summarization .,0,0.9395528,12.257030624994892,18
472,"However , there is no clear understanding of why they perform so well , or how they might be improved .",0,0.923435,19.701255664883686,21
472,"In this paper , we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures , transferable knowledge and learning schemas .",1,0.95026535,33.27656804590539,29
472,"Besides , we find an effective way to improve the current framework and achieve the state-of-the-art result on CNN / DailyMail by a large margin based on our observations and analysis .",3,0.8972697,14.469449689974148,37
472,"Hopefully , our work could provide more hints for future research on extractive summarization .",3,0.9706148,29.65857217817547,15
473,"Research on summarization has mainly been driven by empirical approaches , crafting systems to perform well on standard datasets with the notion of information Importance remaining latent .",0,0.9240938,133.47436732458746,28
473,We argue that establishing theoretical models of Importance will advance our understanding of the task and help to further improve summarization systems .,3,0.8774845,48.34324085801907,23
473,"To this end , we propose simple but rigorous definitions of several concepts that were previously used only intuitively in summarization : Redundancy , Relevance , and Informativeness .",1,0.3905963,54.080349060257205,29
473,Importance arises as a single quantity naturally unifying these concepts .,0,0.7642072,236.7896385297055,11
473,"Additionally , we provide intuitions to interpret the proposed quantities and experiments to demonstrate the potential of the framework to inform and guide subsequent works .",3,0.5999771,53.22984046870888,26
474,Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly .,0,0.94331175,29.52602347170644,21
474,Single document summarization ( SDS ) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets .,0,0.95627743,21.584757418572615,23
474,"However , multi-document summarization ( MDS ) of news articles has been limited to datasets of a couple of hundred examples .",0,0.9547086,31.558923152129776,22
474,"In this paper , we introduce Multi-News , the first large-scale MDS news dataset .",1,0.84670055,40.57036918869084,15
474,"Additionally , we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets .",2,0.46576855,17.436784383596116,28
474,We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting .,3,0.7561913,39.15002556207483,21
475,"We address the problem of adversarial attacks on text classification , which is rarely studied comparing to attacks on image classification .",1,0.69375813,39.6297828160207,22
475,"The challenge of this task is to generate adversarial examples that maintain lexical correctness , grammatical correctness and semantic similarity .",0,0.71394277,43.38634242896424,21
475,"Based on the synonyms substitution strategy , we introduce a new word replacement order determined by both the word saliency and the classification probability , and propose a greedy algorithm called probability weighted word saliency ( PWWS ) for text adversarial attack .",2,0.82970303,73.48210395786343,43
475,"Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent , and keeps a very low word substitution rate .",3,0.91404605,42.13790632664468,33
475,A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive .,3,0.8850835,35.89192304033884,23
475,Performing adversarial training using our perturbed datasets improves the robustness of the models .,3,0.71969587,27.45219470001262,14
475,"At last , our method also exhibits a good transferability on the generated adversarial examples .",3,0.91186816,53.993257444839216,16
476,Authorship verification is the task of determining whether two texts were written by the same author .,0,0.91047555,19.847335625734036,17
476,"We deal with the adversary task , called authorship obfuscation : preventing verification by altering a to-be-obfuscated text .",2,0.43520334,83.79296198458813,21
476,"Our new obfuscation approach ( 1 ) models writing style difference as the Jensen-Shannon distance between the character n-gram distributions of texts , and ( 2 ) manipulates an author ’s subconsciously encoded writing style in a sophisticated manner using heuristic search .",2,0.5529937,79.70061496694775,45
476,"To obfuscate , we analyze the huge space of textual variants for a paraphrased version of the to-be-obfuscated text that has a sufficient Jensen-Shannon distance at minimal costs in terms of text quality .",2,0.7721394,53.61381380236119,38
476,"We analyze , quantify , and illustrate the rationale of this approach , define paraphrasing operators , derive obfuscation thresholds , and develop an effective obfuscation framework .",3,0.3715255,111.0188611973687,28
476,"Our authorship obfuscation approach defeats state-of-the-art verification approaches , including unmasking and compression models , while keeping text changes at a minimum .",3,0.83447057,87.97982482770558,28
477,Distributions of the senses of words are often highly skewed and give a strong influence of the domain of a document .,0,0.83489215,49.39955640002466,22
477,"This paper follows the assumption and presents a method for text categorization by leveraging the predominant sense of words depending on the domain , i.e. , domain-specific senses .",1,0.6481346,57.34557200943711,29
477,The key idea is that the features learned from predominant senses are possible to discriminate the domain of the document and thus improve the overall performance of text categorization .,0,0.40493566,35.22709392340312,30
477,"We propose multi-task learning framework based on the neural network model , transformer , which trains a model to simultaneously categorize documents and predicts a predominant sense for each word .",2,0.7351079,57.296646238151865,31
477,"The experimental results using four benchmark datasets show that our method is comparable to the state-of-the-art categorization approach , especially our model works well for categorization of multi-label documents .",3,0.93876326,18.073600423892717,36
478,Automatically validating a research artefact is one of the frontiers in Artificial Intelligence ( AI ) that directly brings it close to competing with human intellect and intuition .,0,0.95595396,55.595362241291554,29
478,"Although criticised sometimes , the existing peer review system still stands as the benchmark of research validation .",0,0.8593853,128.25923603360786,18
478,"The present-day peer review process is not straightforward and demands profound domain knowledge , expertise , and intelligence of human reviewer ( s ) , which is somewhat elusive with the current state of AI .",0,0.92033273,112.13020704816648,37
478,"However , the peer review texts , which contains rich sentiment information of the reviewer , reflecting his / her overall attitude towards the research in the paper , could be a valuable entity to predict the acceptance or rejection of the manuscript under consideration .",3,0.50206095,59.31110601063523,46
478,"Here in this work , we investigate the role of reviewer sentiment embedded within peer review texts to predict the peer review outcome .",1,0.9389408,40.35833793684525,24
478,"Our proposed deep neural architecture takes into account three channels of information : the paper , the corresponding reviews , and review ’s polarity to predict the overall recommendation score as well as the final decision .",2,0.576661,85.71467565689761,37
478,We achieve significant performance improvement over the baselines ( ∼ 29 % error reduction ) proposed in a recently released dataset of peer reviews .,3,0.91696966,60.90433948573003,25
478,"An AI of this kind could assist the editors / program chairs as an additional layer of confidence , especially when non-responding / missing reviewers are frequent in present day peer review .",3,0.8107018,203.57040298578173,33
479,We present a novel conversational-context aware end-to-end speech recognizer based on a gated neural network that incorporates conversational-context / word / speech embeddings .,1,0.4738834,23.777665359990962,30
479,"Unlike conventional speech recognition models , our model learns longer conversational-context information that spans across sentences and is consequently better at recognizing long conversations .",3,0.63159037,46.351333728635176,27
479,"Specifically , we propose to use text-based external word and / or sentence embeddings ( i.e. , fastText , BERT ) within an end-to-end framework , yielding significant improvement in word error rate with better conversational-context representation .",2,0.5286348,38.66233924696088,43
479,We evaluated the models on the Switchboard conversational speech corpus and show that our model outperforms standard end-to-end speech recognition models .,3,0.75623405,17.233438361328933,24
480,Personal health mention detection deals with predicting whether or not a given sentence is a report of a health condition .,0,0.88019925,38.225048848685816,21
480,"Past work mentions errors in this prediction when symptom words , i.e. , names of symptoms of interest , are used in a figurative sense .",0,0.863559,94.07588229672162,26
480,"Therefore , we combine a state-of-the-art figurative usage detection with CNN-based personal health mention detection .",2,0.70023686,49.83103588652233,24
480,"To do so , we present two methods : a pipeline-based approach and a feature augmentation-based approach .",2,0.74912983,20.616531305957903,22
480,"F-score of personal health mention detection , in the case of the feature augmentation-based approach .",3,0.60228753,92.97671724909542,18
480,This paper demonstrates the promise of using figurative usage detection to improve personal health mention detection .,3,0.6239781,89.91666054382328,17
481,Complex Word Identification ( CWI ) is concerned with detection of words in need of simplification and is a crucial first step in a simplification pipeline .,0,0.94214576,41.95542688050411,27
481,It has been shown that reliable CWI systems considerably improve text simplification .,0,0.8140762,55.79127039235807,13
481,"However , most CWI systems to date address the task on a word-by-word basis , not taking the context into account .",0,0.88584745,35.18929444983274,25
481,"In this paper , we present a novel approach to CWI based on sequence modelling .",1,0.9145697,31.41922233299052,16
481,"Our system is capable of performing CWI in context , does not require extensive feature engineering and outperforms state-of-the-art systems on this task .",3,0.8898324,27.514450305577842,30
482,News recommendation can help users find interested news and alleviate information overload .,0,0.79385674,76.99657707195654,13
482,The topic information of news is critical for learning accurate news and user representations for news recommendation .,0,0.8847883,86.41046831279203,18
482,"However , it is not considered in many existing news recommendation methods .",0,0.83467036,84.99254389691417,13
482,"In this paper , we propose a neural news recommendation approach with topic-aware news representations .",1,0.86387116,41.652893269023174,16
482,The core of our approach is a topic-aware news encoder and a user encoder .,2,0.5674801,23.323868687731935,15
482,In the news encoder we learn representations of news from their titles via CNN networks and apply attention networks to select important words .,2,0.7338131,76.70744131198464,24
482,"In addition , we propose to learn topic-aware news representations by jointly training the news encoder with an auxiliary topic classification task .",2,0.64210254,26.795187833084043,23
482,In the user encoder we learn the representations of users from their browsed news and use attention networks to select informative news for user representation learning .,2,0.74161404,50.06543217400823,27
482,Extensive experiments on a real-world dataset validate the effectiveness of our approach .,3,0.72001785,6.894032585203746,13
483,The word ordering in a Sanskrit verse is often not aligned with its corresponding prose order .,0,0.8844617,91.02599407463278,17
483,Conversion of the verse to its corresponding prose helps in better comprehension of the construction .,3,0.4906542,82.5005205961146,16
483,"Owing to the resource constraints , we formulate this task as a word ordering ( linearisation ) task .",2,0.58384967,71.95603705001689,19
483,"In doing so , we completely ignore the word arrangement at the verse side .",2,0.5788409,165.01905989599737,15
483,"kāvya guru , the approach we propose , essentially consists of a pipeline of two pretraining steps followed by a seq2seq model .",2,0.5095407,66.64318169498615,23
483,The first pretraining step learns task-specific token embeddings from pretrained embeddings .,2,0.6918551,18.73679619997995,12
483,"In the next step , we generate multiple possible hypotheses for possible word arrangements of the input %using another pretraining step .",2,0.73581266,212.22386763534064,22
483,We then use them as inputs to a neural seq2seq model for the final prediction .,2,0.7705779,23.629165725009422,16
483,We empirically show that the hypotheses generated by our pretraining step result in predictions that consistently outperform predictions based on the original order in the verse .,3,0.90116394,31.56423570816412,27
483,"Overall , kāvya guru outperforms current state of the art models in linearisation for the poetry to prose conversion task in Sanskrit .",3,0.9626367,135.72744890730533,23
484,"In visual communication , text emphasis is used to increase the comprehension of written text to convey the author ’s intent .",0,0.91866314,52.72807109619024,22
484,"We study the problem of emphasis selection , i.e .",1,0.6876672,58.91285637344264,10
484,"choosing candidates for emphasis in short written text , to enable automated design assistance in authoring .",0,0.38502628,241.80797535267448,17
484,"Without knowing the author ’s intent and only considering the input text , multiple emphasis selections are valid .",3,0.59240425,139.28670082375118,19
484,"We propose a model that employs end-to-end label distribution learning ( LDL ) on crowd-sourced data and predicts a selection distribution , capturing the inter-subjectivity ( common-sense ) in the audience as well as the ambiguity of the input .",2,0.64791286,51.42804465517626,42
484,We compare the model with several baselines in which the problem is transformed to single-label learning by mapping label distributions to absolute labels via majority voting .,2,0.62303716,65.13044326595539,29
485,"In this study , we propose a new multi-task learning approach for rumor detection and stance classification tasks .",1,0.92153686,25.4463002278235,19
485,This neural network model has a shared layer and two task specific layers .,2,0.5070318,61.907175930671464,14
485,"We incorporate the user credibility information into the rumor detection layer , and we also apply attention mechanism in the rumor detection process .",2,0.8001379,68.204680999649,24
485,"The attended information include not only the hidden states in the rumor detection layer , but also the hidden states from the stance detection layer .",2,0.44307408,49.611633342538624,26
485,The experiments on two datasets show that our proposed model outperforms the state-of-the-art rumor detection approaches .,3,0.9353176,7.065321537204239,23
486,Human trafficking is a worldwide crisis .,0,0.9396222,40.49414239648419,7
486,Traffickers exploit their victims by anonymously offering sexual services through online advertisements .,0,0.9262947,121.94554127376185,13
486,These ads often contain clues that law enforcement can use to separate out potential trafficking cases from volunteer sex advertisements .,0,0.7617897,77.21847018412035,21
486,The problem is that the sheer volume of ads is too overwhelming for manual processing .,0,0.88939816,46.90912606480669,16
486,"Ideally , a centralized semi-automated tool can be used to assist law enforcement agencies with this task .",0,0.5233207,30.444561774917975,18
486,"Here , we present an approach using natural language processing to identify trafficking ads on these websites .",1,0.87210673,46.36555857254913,18
486,"We propose a classifier by integrating multiple text feature sets , including the publicly available pre-trained textual language model Bi-directional Encoder Representation from transformers ( BERT ) .",2,0.7141001,63.07447660043052,28
486,"In this paper , we demonstrate that a classifier using this composite feature set has significantly better performance compared to any single feature set alone .",1,0.67537737,36.92215787762007,26
487,"Lattices are an efficient and effective method to encode ambiguity of upstream systems in natural language processing tasks , for example to compactly capture multiple speech recognition hypotheses , or to represent multiple linguistic analyses .",0,0.6330836,89.94131739670469,36
487,"Previous work has extended recurrent neural networks to model lattice inputs and achieved improvements in various tasks , but these models suffer from very slow computation speeds .",0,0.91256565,59.077507565121,28
487,This paper extends the recently proposed paradigm of self-attention to handle lattice inputs .,1,0.62148106,33.90721277659542,14
487,Self-attention is a sequence modeling technique that relates inputs to one another by computing pairwise similarities and has gained popularity for both its strong results and its computational efficiency .,0,0.93114084,38.26925694056221,30
487,"To extend such models to handle lattices , we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available .",2,0.7471636,65.64261358579562,27
487,We also propose a method for adapting positional embeddings to lattice structures .,3,0.52104646,25.4957689794117,13
487,We apply the proposed model to a speech translation task and find that it outperforms all examined baselines while being much faster to compute than previous neural lattice models during both training and inference .,3,0.86044055,20.354680985023165,35
488,"Though machine translation errors caused by the lack of context beyond one sentence have long been acknowledged , the development of context-aware NMT systems is hampered by several problems .",0,0.9272472,25.161441155151756,32
488,"Firstly , standard metrics are not sensitive to improvements in consistency in document-level translations .",0,0.72338575,69.26140740313993,16
488,"Secondly , previous work on context-aware NMT assumed that the sentence-aligned parallel data consisted of complete documents while in most practical scenarios such document-level data constitutes only a fraction of the available parallel data .",0,0.80543673,35.20172191028222,40
488,"To address the first issue , we perform a human study on an English-Russian subtitles dataset and identify deixis , ellipsis and lexical cohesion as three main sources of inconsistency .",2,0.5086773,54.00565556295006,33
488,We then create test sets targeting these phenomena .,2,0.7682527,197.48020122753854,9
488,"To address the second shortcoming , we consider a set-up in which a much larger amount of sentence-level data is available compared to that aligned at the document level .",2,0.6795314,29.92792611927011,32
488,We introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU .,3,0.4886552,36.60996889942704,30
489,Multilingual neural machine translation ( Multi-NMT ) with one encoder-decoder model has made remarkable progress due to its simple deployment .,0,0.94459975,30.33169058735083,22
489,"However , this multilingual translation paradigm does not make full use of language commonality and parameter sharing between encoder and decoder .",0,0.8419271,32.478185461008806,22
489,"Furthermore , this kind of paradigm cannot outperform the individual models trained on bilingual corpus in most cases .",0,0.5375859,74.03936423862267,19
489,"In this paper , we propose a compact and language-sensitive method for multilingual translation .",1,0.9138626,22.4703290533615,16
489,"To maximize parameter sharing , we first present a universal representor to replace both encoder and decoder models .",2,0.697907,93.30906679815894,19
489,"To make the representor sensitive for specific languages , we further introduce language-sensitive embedding , attention , and discriminator with the ability to enhance model performance .",2,0.62174934,102.101314043087,28
489,"We verify our methods on various translation scenarios , including one-to-many , many-to-many and zero-shot .",3,0.46347252,25.74908925277321,22
489,Extensive experiments demonstrate that our proposed methods remarkably outperform strong standard multilingual translation systems on WMT and IWSLT datasets .,3,0.9459383,19.59394676933399,20
489,"Moreover , we find that our model is especially helpful in low-resource and zero-shot translation scenarios .",3,0.97670764,15.535157014640285,17
490,Mining parallel sentences from comparable corpora is important .,0,0.9166198,123.07579774586246,9
490,"Most previous work relies on supervised systems , which are trained on parallel data , thus their applicability is problematic in low-resource scenarios .",0,0.869781,43.40933313094764,24
490,Recent developments in building unsupervised bilingual word embeddings made it possible to mine parallel sentences based on cosine similarities of source and target language words .,0,0.9226711,30.618207800774417,26
490,"We show that relying only on this information is not enough , since sentences often have similar words but different meanings .",3,0.910278,39.08781663569702,22
490,We detect continuous parallel segments in sentence pair candidates and rely on them when mining parallel sentences .,2,0.7348835,187.557517776785,18
490,We show better mining accuracy on three language pairs in a standard shared task on artificial data .,3,0.94464624,132.80155338406237,18
490,We also provide the first experiments showing that parallel sentences mined from real life sources improve unsupervised MT .,3,0.87926036,65.23400615841761,19
490,"Our code is available , we hope it will be used to support low-resource MT research .",3,0.95686364,39.50574875493736,17
491,"Unsupervised bilingual word embedding ( UBWE ) , together with other technologies such as back-translation and denoising , has helped unsupervised neural machine translation ( UNMT ) achieve remarkable results in several language pairs .",0,0.9063661,30.044824322234547,36
491,"In previous methods , UBWE is first trained using non-parallel monolingual corpora and then this pre-trained UBWE is used to initialize the word embedding in the encoder and decoder of UNMT .",0,0.46841007,21.070495311201526,32
491,"That is , the training of UBWE and UNMT are separate .",3,0.64374393,279.7115858477983,12
491,"In this paper , we first empirically investigate the relationship between UBWE and UNMT .",1,0.829209,50.512806885492125,15
491,The empirical findings show that the performance of UNMT is significantly affected by the performance of UBWE .,3,0.9851953,35.247450079677265,18
491,"Thus , we propose two methods that train UNMT with UBWE agreement .",2,0.3261387,152.80679228177956,13
491,Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT .,3,0.9379092,20.61786832834548,16
492,"Transfer learning or multilingual model is essential for low-resource neural machine translation ( NMT ) , but the applicability is limited to cognate languages by sharing their vocabularies .",0,0.92865795,30.621952903943185,29
492,"This paper shows effective techniques to transfer a pretrained NMT model to a new , unrelated language without shared vocabularies .",1,0.57007056,61.16496173233861,21
492,"We relieve the vocabulary mismatch by using cross-lingual word embedding , train a more language-agnostic encoder by injecting artificial noises , and generate synthetic data easily from the pretraining data without back-translation .",2,0.73866856,54.66953098297398,37
492,Our methods do not require restructuring the vocabulary or retraining the model .,3,0.53334814,43.02239937523482,13
492,"We improve plain NMT transfer by up to + 5.1 % BLEU in five low-resource translation tasks , outperforming multilingual joint training by a large margin .",3,0.8808905,41.91347547629745,27
492,"We also provide extensive ablation studies on pretrained embedding , synthetic data , vocabulary size , and parameter freezing for a better understanding of NMT transfer .",3,0.7563993,100.06487728481541,27
493,"Zero-shot translation , translating between language pairs on which a Neural Machine Translation ( NMT ) system has never been trained , is an emergent property when training the system in multilingual settings .",0,0.94169927,37.780461160911884,34
493,"However , naive training for zero-shot NMT easily fails , and is sensitive to hyper-parameter setting .",3,0.6471342,71.2708635770805,17
493,The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot .,3,0.8420062,76.32452506764876,22
493,"In this work , we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language IDs of the source and decoded sentences .",1,0.7167816,45.57692248722622,30
493,"Inspired by this analysis , we propose to use two simple but effective approaches : ( 1 ) decoder pre-training ;",2,0.4513236,46.52502674712937,21
493,( 2 ) back-translation .,2,0.5032467,116.69753080136175,7
493,"These methods show significant improvement ( 4 22 BLEU points ) over the vanilla zero-shot translation on three challenging multilingual datasets , and achieve similar or better results than the pivot-based approach .",3,0.91864485,37.17116595157079,34
494,"Standard decoders for neural machine translation autoregressively generate a single target token per timestep , which slows inference especially for long outputs .",0,0.7384931,78.84614310069928,23
494,"While architectural advances such as the Transformer fully parallelize the decoder computations at training time , inference still proceeds sequentially .",0,0.70045,77.35158091871854,21
494,"Recent developments in non-and semi-autoregressive decoding produce multiple tokens per timestep independently of the others , which improves inference speed but deteriorates translation quality .",0,0.7742796,68.80628612044146,25
494,"In this work , we propose the syntactically supervised Transformer ( SynST ) , which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse .",1,0.45253822,63.28427578117032,38
494,A series of controlled experiments demonstrates that SynST decodes sentences ~ 5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets .,3,0.9008798,31.840092341315344,34
495,Noise and domain are important aspects of data quality for neural machine translation .,0,0.80692697,46.149232536061135,14
495,"Existing research focus separately on domain-data selection , clean-data selection , or their static combination , leaving the dynamic interaction across them not explicitly examined .",0,0.86611307,291.1881860557298,26
495,"This paper introduces a “ co-curricular learning ” method to compose dynamic domain-data selection with dynamic clean-data selection , for transfer learning across both capabilities .",1,0.84147197,140.88778389302414,28
495,We apply an EM-style optimization procedure to further refine the “ co-curriculum ” .,2,0.7226746,61.2977513920783,14
495,Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum .,3,0.95467025,32.835089817235435,23
496,"Prior researches suggest that neural machine translation ( NMT ) captures word alignment through its attention mechanism , however , this paper finds attention may almost fail to capture word alignment for some NMT models .",0,0.5831491,54.40464142419445,36
496,This paper thereby proposes two methods to induce word alignment which are general and agnostic to specific NMT models .,1,0.77376074,59.72890615664338,20
496,Experiments show that both methods induce much better word alignment than attention .,3,0.9426596,68.59361687712207,13
496,This paper further visualizes the translation through the word alignment induced by NMT .,3,0.43721527,93.56043558223354,14
496,"In particular , it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics .",3,0.5552822,43.06720623522352,41
497,Non-autoregressive translation models ( NAT ) have achieved impressive inference speedup .,0,0.91868335,43.39599455719269,12
497,"A potential issue of the existing NAT algorithms , however , is that the decoding is conducted in parallel , without directly considering previous context .",0,0.75273603,110.37189360581158,26
497,"In this paper , we propose an imitation learning framework for non-autoregressive machine translation , which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart .",1,0.85076797,21.88627603994358,33
497,"We conduct experiments on the IWSLT16 , WMT14 and WMT16 datasets .",2,0.7238872,15.172449188205407,12
497,"Our proposed model achieves a significant speedup over the autoregressive models , while keeping the translation quality comparable to the autoregressive models .",3,0.9251142,13.929712424070459,23
497,"By sampling sentence length in parallel at inference time , we achieve the performance of 31.85 BLEU on WMT16 Ro→En and 30.68 BLEU on IWSLT16 En→De .",3,0.79307395,23.607705703915265,27
498,"Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking , with applications to live and streaming scenarios .",0,0.7686124,108.18303262445887,25
498,Simultaneous systems must carefully schedule their reading of the source sentence to balance quality against latency .,0,0.71982557,111.3547299442824,17
498,We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation ( NMT ) model that attends over all source tokens read thus far .,1,0.448444,69.1668513338613,32
498,"We do so by introducing Monotonic Infinite Lookback ( MILk ) attention , which maintains both a hard , monotonic attention head to schedule the reading of the source sentence , and a soft attention head that extends from the monotonic head back to the beginning of the source .",2,0.82740563,52.35205142116382,50
498,We show that MILk ’s adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values .,3,0.9301369,93.70545117447597,35
499,Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks .,0,0.87927777,7.898367658928429,21
499,"In this work , we investigate how to learn a general-purpose embedding of textual relations , defined as the shortest dependency path between entities .",1,0.8773564,46.365503300529106,27
499,"Textual relation embedding provides a level of knowledge between word / phrase level and sentence level , and we show that it can facilitate downstream tasks requiring relational understanding of the text .",3,0.86828226,62.55492551743826,33
499,"To learn such an embedding , we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase .",2,0.79331356,108.93247420602435,23
499,We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding .,2,0.85087746,57.92879546658602,20
499,Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding .,3,0.8056008,41.921300687529836,16
499,The data and code can be found at https://github.com/czyssrs/GloREPlus .,3,0.667756,26.206836137303714,10
500,"In this paper , we propose a novel graph neural network with generated parameters ( GP-GNNs ) .",1,0.8242797,31.516793017411135,20
500,"The parameters in the propagation module , i.e .",3,0.41077167,56.3954884887856,9
500,"the transition matrices used in message passing procedure , are produced by a generator taking natural language sentences as inputs .",2,0.61393744,122.5281193636635,21
500,"We verify GP-GNNs in relation extraction from text , both on bag-and instance-settings .",3,0.77021277,350.7578073612129,18
500,Experimental results on a human-annotated dataset and two distantly supervised datasets show that multi-hop reasoning mechanism yields significant improvements .,3,0.9143419,18.015529923874794,20
500,We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning .,3,0.72942114,41.012512418303984,21
501,"In this paper , we propose a new paradigm for the task of entity-relation extraction .",1,0.9164621,15.963040312352968,17
501,"We cast the task as a multi-turn question answering problem , i.e. , the extraction of entities and elations is transformed to the task of identifying answer spans from the context .",2,0.6692665,34.56426051835485,32
501,"This multi-turn QA formalization comes with several key advantages : firstly , the question query encodes important information for the entity / relation class we want to identify ;",3,0.5417857,81.18123868299601,29
501,"secondly , QA provides a natural way of jointly modeling entity and relation ;",0,0.6014427,146.2830218478209,14
501,"and thirdly , it allows us to exploit the well developed machine reading comprehension ( MRC ) models .",0,0.3585172,53.12837136786577,19
501,Experiments on the ACE and the CoNLL04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models .,3,0.9150295,18.626789489341217,19
501,"We are able to obtain the state-of-the-art results on all of the ACE04 , ACE05 and CoNLL04 datasets , increasing the SOTA results on the three datasets to 49.6 ( + 1.2 ) , 60.3 ( + 0.7 ) and 69.2 ( + 1.4 ) , respectively .",3,0.93084496,13.731992463908478,54
501,"Additionally , we construct and will release a newly developed dataset RESUME , which requires multi-step reasoning to construct entity dependencies , as opposed to the single-step dependency extraction in the triplet exaction in previous datasets .",3,0.5467336,95.11086588782827,37
501,The proposed multi-turn QA model also achieves the best performance on the RESUME dataset .,3,0.9375674,28.739543105194645,15
502,"In practical scenario , relation extraction needs to first identify entity pairs that have relation and then assign a correct relation class .",0,0.72913873,75.55321238444289,23
502,"However , the number of non-relation entity pairs in context ( negative instances ) usually far exceeds the others ( positive instances ) , which negatively affects a model ’s performance .",0,0.6569557,69.29843990844662,32
502,"To mitigate this problem , we propose a multi-task architecture which jointly trains a model to perform relation identification with cross-entropy loss and relation classification with ranking loss .",2,0.62358844,24.502433286122706,29
502,"Meanwhile , we observe that a sentence may have multiple entities and relation mentions , and the patterns in which the entities appear in a sentence may contain useful semantic information that can be utilized to distinguish between positive and negative instances .",3,0.9173093,29.566232591688276,43
502,Thus we further incorporate the embeddings of character-wise / word-wise BIO tag from the named entity recognition task into character / word embeddings to enrich the input representation .,2,0.5359246,31.33714676897947,33
502,"Experiment results show that our proposed approach can significantly improve the performance of a baseline model with more than 10 % absolute increase in F1-score , and outperform the state-of-the-art models on ACE 2005 Chinese and English corpus .",3,0.96290046,12.195324224956135,46
502,"Moreover , BIO tag embeddings are particularly effective and can be used to improve other models as well .",3,0.65835583,26.947113102386623,19
503,We develop a new paradigm for the task of joint entity relation extraction .,1,0.4621796,41.831024895094,14
503,"It first identifies entity spans , then performs a joint inference on entity types and relation types .",2,0.629086,59.39252841174706,18
503,"To tackle the joint type inference task , we propose a novel graph convolutional network ( GCN ) running on an entity-relation bipartite graph .",2,0.5952401,29.74787361139883,26
503,"By introducing a binary relation classification task , we are able to utilize the structure of entity-relation bipartite graph in a more efficient and interpretable way .",3,0.44065866,25.906143038089837,28
503,Experiments on ACE05 show that our model outperforms existing joint models in entity performance and is competitive with the state-of-the-art in relation performance .,3,0.93786484,14.513864712704482,30
504,Many approaches to extract multiple relations from a paragraph require multiple passes over the paragraph .,0,0.81357765,55.762572760019985,16
504,"In practice , multiple passes are computationally expensive and this makes difficult to scale to longer paragraphs and larger text corpora .",0,0.80431837,78.72243408531504,22
504,"In this work , we focus on the task of multiple relation extractions by encoding the paragraph only once .",1,0.6222967,70.82695163941656,20
504,"We build our solution upon the pre-trained self-attentive models ( Transformer ) , where we first add a structured prediction layer to handle extraction between multiple entity pairs , then enhance the paragraph embedding to capture multiple relational information associated with each entity with entity-aware attention .",2,0.762368,55.950532106105996,49
504,We show that our approach is not only scalable but can also perform state-of-the-art on the standard benchmark ACE 2005 .,3,0.89870477,12.839939261730873,26
505,Unsupervised relation extraction aims at extracting relations between entities in text .,0,0.8588607,16.94538680983814,12
505,Previous unsupervised approaches are either generative or discriminative .,0,0.8568444,15.937895622707106,9
505,"In a supervised setting , discriminative approaches , such as deep neural network classifiers , have demonstrated substantial improvement .",0,0.814688,56.77238921863441,20
505,"However , these models are hard to train without supervision , and the currently proposed solutions are unstable .",0,0.77593756,57.17991266978275,19
505,"To overcome this limitation , we introduce a skewness loss which encourages the classifier to predict a relation with confidence given a sentence , and a distribution distance loss enforcing that all relations are predicted in average .",2,0.7546362,50.217591070973924,38
505,"These losses improve the performance of discriminative based models , and enable us to train deep neural networks satisfactorily , surpassing current state of the art on three different datasets .",3,0.8336012,31.598374774865405,31
506,"Distantly supervised relation extraction is widely used to extract relational facts from text , but suffers from noisy labels .",0,0.920859,38.703537183179556,20
506,Current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification .,0,0.8951322,57.77346122852946,28
506,"While achieving state-of-the-art results , we observed these models to be biased towards recognizing a limited set of relations with high precision , while ignoring those in the long tail .",3,0.9600206,24.323343550354732,36
506,"To address this gap , we utilize a pre-trained language model , the OpenAI Generative Pre-trained Transformer ( GPT ) ( Radford et al. , 2018 ) .",2,0.6843234,36.02191222287149,28
506,"The GPT and similar models have been shown to capture semantic and syntactic features , and also a notable amount of “ common-sense ” knowledge , which we hypothesize are important features for recognizing a more diverse set of relations .",3,0.6606678,40.2458577610671,41
506,"By extending the GPT to the distantly supervised setting , and fine-tuning it on the NYT10 dataset , we show that it predicts a larger set of distinct relation types with high confidence .",3,0.8524368,39.508998410631925,34
506,"Manual and automated evaluation of our model shows that it achieves a state-of-the-art AUC score of 0.422 on the NYT10 dataset , and performs especially well at higher recall levels .",3,0.9154975,17.829159564396097,37
507,Distant supervision is widely used in relation classification in order to create large-scale training data by aligning a knowledge base with an unlabeled corpus .,0,0.8706525,27.72834479419869,25
507,"However , it also introduces amounts of noisy labels where a contextual sentence actually does not express the labeled relation .",0,0.7422648,168.28107114087456,21
507,"In this paper , we propose ARNOR , a novel Attention Regularization based NOise Reduction framework for distant supervision relation classification .",1,0.8837932,97.11064160655589,22
507,ARNOR assumes that a trustable relation label should be explained by the neural attention model .,2,0.38134575,117.60063168143441,16
507,"Specifically , our ARNOR framework iteratively learns an interpretable model and utilizes it to select trustable instances .",2,0.5727298,91.3796888855519,18
507,"We first introduce attention regularization to force the model to pay attention to the patterns which explain the relation labels , so as to make the model more interpretable .",2,0.7101563,28.073718679862353,30
507,"Then , if the learned model can clearly locate the relation patterns of a candidate instance in the training set , we will select it as a trustable instance for further training step .",3,0.49667826,52.142691204387404,34
507,"According to the experiments on NYT data , our ARNOR framework achieves significant improvements over state-of-the-art methods in both relation classification performance and noise reduction effect .",3,0.9445402,33.16816367280083,33
508,"In this paper , we present GraphRel , an end-to-end relation extraction model which uses graph convolutional networks ( GCNs ) to jointly learn named entities and relations .",1,0.82641745,17.158063906256352,31
508,"In contrast to previous baselines , we consider the interaction between named entities and relations via a 2nd-phase relation-weighted GCN to better extract relations .",2,0.7414003,50.016170496259896,29
508,"Linear and dependency structures are both used to extract both sequential and regional features of the text , and a complete word graph is further utilized to extract implicit features among all word pairs of the text .",2,0.64993703,63.98654389421668,38
508,"With the graph-based approach , the prediction for overlapping relations is substantially improved over previous sequential approaches .",3,0.8631886,61.15679587405106,19
508,We evaluate GraphRel on two public datasets : NYT and WebNLG .,2,0.7739251,175.32990466641775,12
508,Results show that GraphRel maintains high precision while increasing recall substantially .,3,0.98825306,201.62054172205347,12
508,"Also , GraphRel outperforms previous work by 3.2 % and 5.8 % ( F1 score ) , achieving a new state-of-the-art for relation extraction .",3,0.9535362,20.909319909341175,31
509,Pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction .,0,0.8585104,33.04865379370015,22
509,"However , these methods require significant expert labor to write relation-specific patterns , which makes them too sophisticated to generalize quickly .",0,0.8402658,90.5824849839889,23
509,"To ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types , we propose a neural pattern diagnosis framework , DIAG-NRE , that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop .",2,0.49886787,78.05589814123728,49
509,"To demonstrate the effectiveness of DIAG-NRE , we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods .",3,0.4317441,13.5534845517762,33
510,"This paper presents a novel framework , MGNER , for Multi-Grained Named Entity Recognition where multiple entities or entity mentions in a sentence could be non-overlapping or totally nested .",1,0.8001895,41.73928158759463,30
510,"Different from traditional approaches regarding NER as a sequential labeling task and annotate entities consecutively , MGNER detects and recognizes entities on multiple granularities : it is able to recognize named entities without explicitly assuming non-overlapping or totally nested structures .",3,0.438721,83.94529115065525,41
510,MGNER consists of a Detector that examines all possible word segments and a Classifier that categorizes entities .,0,0.5372039,70.97679150694078,18
510,"In addition , contextual information and a self-attention mechanism are utilized throughout the framework to improve the NER performance .",2,0.4689961,27.492633985177648,20
510,Experimental results show that MGNER outperforms current state-of-the-art baselines up to 4.4 % in terms of the F1 score among nested / non-overlapping NER tasks .,3,0.9638435,14.978644124035156,32
511,"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text , and be fine-tuned to consistently improve the performance of various NLP tasks .",0,0.56491864,16.141710578218095,34
511,"However , the existing pre-trained language models rarely consider incorporating knowledge graphs ( KGs ) , which can provide rich structured knowledge facts for better language understanding .",0,0.89184654,64.04860316163746,28
511,We argue that informative entities in KGs can enhance language representation with external knowledge .,3,0.85923034,82.57187361012042,15
511,"In this paper , we utilize both large-scale textual corpora and KGs to train an enhanced language representation model ( ERNIE ) , which can take full advantage of lexical , syntactic , and knowledge information simultaneously .",1,0.6211135,41.51067893734564,38
511,"The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks , and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks .",3,0.9769577,18.564254777383333,37
511,The code and datasets will be available in the future .,3,0.81090903,11.884799821546785,11
512,Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments .,0,0.9190498,85.13256038770542,15
512,"In this paper , we propose a novel Multi-channel Graph Neural Network model ( MuGNN ) to learn alignment-oriented knowledge graph ( KG ) embeddings by robustly encoding two KGs via multiple channels .",1,0.8453029,33.68191672753598,35
512,"Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively , which are further combined via pooling techniques .",2,0.55259126,75.81677312946333,35
512,"Moreover , we also infer and transfer rule knowledge for completing two KGs consistently .",3,0.8726286,252.37431261555446,15
512,"MuGNN is expected to reconcile the structural differences of two KGs , and thus make better use of seed alignments .",3,0.76880187,55.310924986383256,21
512,Extensive experiments on five publicly available datasets demonstrate our superior performance ( 5 % Hits@1 up on average ) .,3,0.84531724,142.65528118313253,20
512,Source code and data used in the experiments can be accessed at https://github.com/thunlp/MuGNN .,3,0.56754524,7.730099000057717,14
513,Gazetteers were shown to be useful resources for named entity recognition ( NER ) .,0,0.84354883,34.76793535786281,15
513,"Many existing approaches to incorporating gazetteers into machine learning based NER systems rely on manually defined selection strategies or handcrafted templates , which may not always lead to optimal effectiveness , especially when multiple gazetteers are involved .",0,0.82012945,31.021838653000533,38
513,"This is especially the case for the task of Chinese NER , where the words are not naturally tokenized , leading to additional ambiguities .",0,0.6845311,33.81946840241159,25
513,"To automatically learn how to incorporate multiple gazetteers into an NER system , we propose a novel approach based on graph neural networks with a multi-digraph structure that captures the information that the gazetteers offer .",2,0.5200293,19.674833167757697,36
513,"Experiments on various datasets show that our model is effective in incorporating rich gazetteer information while resolving ambiguities , outperforming previous approaches .",3,0.93128073,26.091169352804055,23
514,Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling .,3,0.5476566,25.353585752333903,14
514,We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token .,2,0.5185466,26.275374599438486,24
514,"This biases the model towards retaining more contextual information , in turn improving its ability to predict the next token .",3,0.80015665,50.606300296057576,21
514,"With negligible overhead in the number of parameters and training time , our Past Decode Regularization ( PDR ) method improves perplexity on the Penn Treebank dataset by up to 1.8 points and by up to 2.3 points on the WikiText-2 dataset , over strong regularized baselines using a single softmax .",3,0.7554904,23.80136239787084,54
514,"With a mixture-of-softmax model , we show gains of up to 1.0 perplexity points on these datasets .",3,0.8783744,27.997051450615412,20
514,"In addition , our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling .",3,0.84184134,63.104590117091746,22
515,"In this paper , we study the problem of hybrid language modeling , that is using models which can predict both characters and larger units such as character ngrams or words .",1,0.89466566,56.59953267825834,32
515,"Using such models , multiple potential segmentations usually exist for a given string , for example one using words and one using characters only .",0,0.62722564,102.63787219026052,25
515,"Thus , the probability of a string is the sum of the probabilities of all the possible segmentations .",0,0.45858383,17.685089978312167,19
515,"Here , we show how it is possible to marginalize over the segmentations efficiently , in order to compute the true probability of a sequence .",1,0.5240597,40.48305083106159,26
515,"We apply our technique on three datasets , comprising seven languages , showing improvements over a strong character level language model .",3,0.50712836,106.99335026620011,22
516,Common language models typically predict the next word given the context .,0,0.87722766,33.42701370109262,12
516,"In this work , we propose a method that improves language modeling by learning to align the given context and the following phrase .",1,0.7290777,37.91070934604974,24
516,The model does not require any linguistic annotation of phrase segmentation .,2,0.48144755,48.16822602541114,12
516,"Instead , we define syntactic heights and phrase segmentation rules , enabling the model to automatically induce phrases , recognize their task-specific heads , and generate phrase embeddings in an unsupervised learning manner .",2,0.71643436,58.28836532210429,35
516,"Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment , and no change is required in the underlying language modeling network .",3,0.7856686,60.07675630237087,40
516,Experiments have shown that our model outperformed several strong baseline models on different data sets .,3,0.9453149,16.552930224017402,16
516,We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset .,3,0.9158848,12.981514603938816,21
516,"Additionally , visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation .",3,0.96402776,43.30453567540335,28
517,Many state-of-the-art neural models for NLP are heavily parameterized and thus memory inefficient .,0,0.87580836,12.524814898340717,18
517,This paper proposes a series of lightweight and memory efficient neural architectures for a potpourri of natural language processing ( NLP ) tasks .,1,0.82420254,22.31835387724328,24
517,"To this end , our models exploit computation using Quaternion algebra and hypercomplex spaces , enabling not only expressive inter-component interactions but also significantly ( 75 % ) reduced parameter size due to lesser degrees of freedom in the Hamilton product .",2,0.5412306,145.01852268222214,42
517,"We propose Quaternion variants of models , giving rise to new architectures such as the Quaternion attention Model and Quaternion Transformer .",2,0.4258478,38.341615520438204,22
517,"Extensive experiments on a battery of NLP tasks demonstrates the utility of proposed Quaternion-inspired models , enabling up to 75 % reduction in parameter size without significant loss in performance .",3,0.8897881,36.35140554072248,33
518,Sequence-to-sequence models are a powerful workhorse of NLP .,0,0.8651131,13.009747212319503,9
518,"Most variants employ a softmax transformation in both their attention mechanism and output layer , leading to dense alignments and strictly positive output probabilities .",3,0.51308024,107.15331038426406,25
518,"This density is wasteful , making models less interpretable and assigning probability mass to many implausible outputs .",0,0.49274492,226.00659373202475,18
518,1 .,4,0.8274833,81.77149873844907,2
518,"We provide fast algorithms to evaluate these transformations and their gradients , which scale well for large vocabulary sizes .",3,0.5602431,97.4745921878696,20
518,"Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs , sometimes rendering beam search exact .",3,0.725811,128.32169434009617,27
518,Experiments on morphological inflection and machine translation reveal consistent gains over dense models .,0,0.6404219,42.56367200453385,14
519,This work examines the robustness of self-attentive neural networks against adversarial input perturbations .,1,0.8781606,14.670444397539626,14
519,"Specifically , we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis , entailment and machine translation under adversarial attacks .",1,0.6103963,27.744764649164523,36
519,We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans .,3,0.63127166,39.81662141584499,22
519,"Experimental results show that , compared to recurrent neural models , self-attentive models are more robust against adversarial perturbation .",3,0.96138865,16.950506383472927,20
519,"In addition , we provide theoretical explanations for their superior robustness to support our claims .",3,0.55558974,73.17465105086863,16
520,"Many common character-level , string-to-string transduction tasks , e.g. , grapheme-to-phoneme conversion and morphological inflection , consist almost exclusively of monotonic transduction .",0,0.89194506,28.122944126168324,30
520,"Neural sequence-to-sequence models with soft attention , non-monotonic models , outperform popular monotonic models .",3,0.49602458,32.50770122944922,17
520,We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly .,2,0.6973302,49.698671522702256,18
520,"With the help of dynamic programming , we are able to compute the exact marginalization over all alignments .",3,0.55328184,41.111204452933954,19
520,Our models achieve state-of-the-art performance on morphological inflection .,3,0.8473505,8.440168570267863,14
520,"Furthermore , we find strong performance on two other character-level transduction tasks .",3,0.97104394,49.07265543131894,15
520,Code is available at https://github.com/shijie-wu/neural-transducer .,3,0.5072817,9.619277421826105,6
521,"Recurrent networks have achieved great success on various sequential tasks with the assistance of complex recurrent units , but suffer from severe computational inefficiency due to weak parallelization .",0,0.9207371,40.2000947400837,29
521,One direction to alleviate this issue is to shift heavy computations outside the recurrence .,0,0.50175315,48.83793493890668,15
521,"In this paper , we propose a lightweight recurrent network , or LRN .",1,0.80129427,82.0990985327473,14
521,"LRN uses input and forget gates to handle long-range dependencies as well as gradient vanishing and explosion , with all parameter related calculations factored outside the recurrence .",2,0.4029623,204.2948600146035,29
521,"The recurrence in LRN only manipulates the weight assigned to each token , tightly connecting LRN with self-attention networks .",3,0.65927374,76.77872632925265,20
521,We apply LRN as a drop-in replacement of existing recurrent units in several neural sequential models .,2,0.7107003,65.12957368652705,18
521,Extensive experiments on six NLP tasks show that LRN yields the best running efficiency with little or no loss in model performance .,3,0.8895167,26.35334573096814,23
522,Obstacles hindering the development of capsule networks for challenging NLP applications include poor scalability to large output spaces and less reliable routing processes .,0,0.8792118,81.52688057300878,24
522,"In this paper , we introduce : ( i ) an agreement score to evaluate the performance of routing processes at instance-level ;",1,0.78231484,113.73367590804,25
522,( ii ) an adaptive optimizer to enhance the reliability of routing ;,2,0.5195589,397.50975656356906,13
522,( iii ) capsule compression and partial routing to improve the scalability of capsule networks .,3,0.3277483,137.48677320272554,16
522,"We validate our approach on two NLP tasks , namely : multi-label text classification and question answering .",2,0.4272807,20.407958158193946,18
522,Experimental results show that our approach considerably improves over strong competitors on both tasks .,3,0.96939015,16.14770762957504,15
522,"In addition , we gain the best results in low-resource settings with few training instances .",3,0.9065685,31.018458781644096,16
523,"Transfer learning is effective for improving the performance of tasks that are related , and Multi-task learning ( MTL ) and Cross-lingual learning ( CLL ) are important instances .",0,0.8395197,28.198524053938005,30
523,"This paper argues that hard-parameter sharing , of hard-coding layers shared across different tasks or languages , cannot generalize well , when sharing with a loosely related task .",1,0.6423624,104.72817052837662,33
523,"Such case , which we call sparse transfer , might actually hurt performance , a phenomenon known as negative transfer .",0,0.57078564,153.80624262349275,21
523,"Our contribution is using adversarial training across tasks , to “ soft-code ” shared and private spaces , to avoid the shared space gets too sparse .",2,0.5617505,138.36325735494677,27
523,"In CLL , our proposed architecture considers another challenge of dealing with low-quality input .",3,0.7254213,96.03351696870118,15
524,"There has been an increased interest in multimodal language processing including multimodal dialog , question answering , sentiment analysis , and speech recognition .",0,0.96836275,20.109950506856496,24
524,"However , naturally occurring multimodal data is often imperfect as a result of imperfect modalities , missing entries or noise corruption .",0,0.94268394,110.17812448284268,22
524,"To address these concerns , we present a regularization method based on tensor rank minimization .",1,0.43597403,32.92761182955821,16
524,Our method is based on the observation that high-dimensional multimodal time series data often exhibit correlations across time and modalities which leads to low-rank tensor representations .,2,0.6986703,24.60048553745486,27
524,"However , the presence of noise or incomplete values breaks these correlations and results in tensor representations of higher rank .",3,0.66728,96.17145090338452,21
524,We design a model to learn such tensor representations and effectively regularize their rank .,2,0.67741424,79.01181790376637,15
524,Experiments on multimodal language data show that our model achieves good results across various levels of imperfection .,3,0.93858266,21.06826997500696,18
525,"A lot of work has been done in the field of image compression via machine learning , but not much attention has been given to the compression of natural language .",0,0.93460345,11.674898287638918,31
525,"Compressing text into lossless representations while making features easily retrievable is not a trivial task , yet has huge benefits .",0,0.90293944,71.82566937049903,21
525,Most methods designed to produce feature rich sentence embeddings focus solely on performing well on downstream tasks and are unable to properly reconstruct the original sequence from the learned embedding .,0,0.83647275,38.00025204665071,31
525,"In this work , we propose a near lossless method for encoding long sequences of texts as well as all of their sub-sequences into feature rich representations .",1,0.7836165,40.62074699935085,28
525,We test our method on sentiment analysis and show good performance across all sub-sentence and sentence embeddings .,3,0.69504833,17.623014912410255,18
526,Pinyin-to-character ( P2C ) conversion is the core component of pinyin-based Chinese input method engine ( IME ) .,0,0.9614372,46.761617741276645,19
526,"However , the conversion is seriously compromised by the ambiguities of Chinese characters corresponding to pinyin as well as the predefined fixed vocabularies .",0,0.61623174,30.689077303659122,24
526,"To alleviate such inconveniences , we propose a neural P2C conversion model augmented by an online updated vocabulary with a sampling mechanism to support open vocabulary learning during IME working .",2,0.52323437,142.43756872815584,31
526,Our experiments show that the proposed method outperforms commercial IMEs and state-of-the-art traditional models on standard corpus and true inputting history dataset in terms of multiple metrics and thus the online updated vocabulary indeed helps our IME effectively follows user inputting behavior .,3,0.95176345,103.7018570876837,49
527,"To ascertain the importance of phonetic information in the form of phonological distinctive features for the purpose of segment-level phonotactic acquisition , we compare the performance of two recurrent neural network models of phonotactic learning : one that has access to distinctive features at the start of the learning process , and one that does not .",1,0.6096936,21.157865514300884,59
527,"Though the predictions of both models are significantly correlated with human judgments of non-words , the feature-naive model significantly outperforms the feature-aware one in terms of probability assigned to a held-out test set of English words , suggesting that distinctive features are not obligatory for learning phonotactic patterns at the segment level .",3,0.9706317,33.196264453599845,59
528,"We incorporate morphological supervision into character language models ( CLMs ) via multitasking and show that this addition improves bits-per-character ( BPC ) performance across 24 languages , even when the morphology data and language modeling data are disjoint .",3,0.564138,55.905382290358595,42
528,"Analyzing the CLMs shows that inflected words benefit more from explicitly modeling morphology than uninflected words , and that morphological supervision improves performance even as the amount of language modeling data grows .",3,0.9679242,54.21006086924665,33
528,We then transfer morphological supervision across languages to improve performance in the low-resource setting .,2,0.73469526,24.751005622088403,15
529,"Training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normalization , albeit often outperformed by phrase-based models .",0,0.9188339,37.779092036012415,31
529,"Policy gradient training enables direct optimization for exact matches , and while the small datasets in historical text normalization are prohibitive of from-scratch reinforcement learning , we show that policy gradient fine-tuning leads to significant improvements across the board .",3,0.8990564,65.76025351703271,40
529,"Policy gradient training , in particular , leads to more accurate normalizations for long or unseen words .",3,0.64597934,184.9905030943906,18
530,"For unsegmented languages such as Japanese and Chinese , tokenization of a sentence has a significant impact on the performance of text classification .",0,0.8589138,22.575056628431796,24
530,Sentences are usually segmented with words or subwords by a morphological analyzer or byte pair encoding and then encoded with word ( or subword ) representations for neural networks .,0,0.79560214,43.23074670769099,30
530,"However , segmentation is potentially ambiguous , and it is unclear whether the segmented tokens achieve the best performance for the target task .",0,0.9219232,47.04239351295492,24
530,"In this paper , we propose a method to simultaneously learn tokenization and text classification to address these problems .",1,0.904303,30.155779618225612,20
530,Our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously .,2,0.81144625,29.27501016605594,20
530,"To make the model robust against infrequent tokens , we sampled segmentation for each sentence stochastically during training , which resulted in improved performance of text classification .",2,0.71372813,60.88047208989188,28
530,We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods .,3,0.5850909,14.069996715411731,23
531,"As Natural Language Processing ( NLP ) and Machine Learning ( ML ) tools rise in popularity , it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes .",0,0.9644049,24.71198883093397,35
531,"Although NLP models have shown success in modeling various applications , they propagate and may even amplify gender bias found in text corpora .",0,0.89049035,59.27757335063396,24
531,"While the study of bias in artificial intelligence is not new , methods to mitigate gender bias in NLP are relatively nascent .",0,0.8530446,29.811767048867456,23
531,"In this paper , we review contemporary studies on recognizing and mitigating gender bias in NLP .",1,0.9352252,28.83247652641507,17
531,We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias .,1,0.66415,82.72083903672014,18
531,"Furthermore , we discuss the advantages and drawbacks of existing gender debiasing methods .",1,0.5821768,26.274403614070927,14
531,"Finally , we discuss future studies for recognizing and mitigating gender bias in NLP .",3,0.72047335,29.03285128751584,15
532,"Word embeddings learnt from massive text collections have demonstrated significant levels of discriminative biases such as gender , racial or ethnic biases , which in turn bias the down-stream NLP applications that use those word embeddings .",0,0.9145708,33.19117575187859,37
532,"Taking gender-bias as a working example , we propose a debiasing method that preserves non-discriminative gender-related information , while removing stereotypical discriminative gender biases from pre-trained word embeddings .",2,0.6249004,19.94276214372275,30
532,"Specifically , we consider four types of information : feminine , masculine , gender-neutral and stereotypical , which represent the relationship between gender vs .",2,0.8240295,55.03560539957519,25
532,"bias , and propose a debiasing method that ( a ) preserves the gender-related information in feminine and masculine words , ( b ) preserves the neutrality in gender-neutral words , and ( c ) removes the biases from stereotypical words .",1,0.389219,28.944419618523135,42
532,Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information .,3,0.9247403,19.925405445348385,35
533,Gender stereotypes are manifest in most of the world ’s languages and are consequently propagated or amplified by NLP systems .,0,0.92002577,39.36516801749877,21
533,"Although research has focused on mitigating gender stereotypes in English , the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages .",0,0.90954995,37.73553114711985,25
533,We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages .,1,0.67548126,19.15419137770397,20
533,"For Spanish and Hebrew , our approach achieves F1 scores of 82 % and 73 % at the level of tags and accuracies of 90 % and 87 % at the level of forms .",3,0.9095809,26.25666908740826,35
533,"By evaluating our approach using four different languages , we show that , on average , it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality .",3,0.87221074,32.767333737135914,31
534,"Word embedding models have gained a lot of traction in the Natural Language Processing community , however , they suffer from unintended demographic biases .",0,0.9517004,29.833794772909755,25
534,Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test ( WEAT ) .,0,0.8229911,74.01281984860213,22
534,"While these approaches offer great geometric insights into unintended biases in the embedding vector space , they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications .",0,0.6258427,36.24854582375565,35
534,"In this work , we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias .",1,0.85733026,55.57975008234261,25
534,"Our metric ( Relative Negative Sentiment Bias , RNSB ) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups .",2,0.8010167,74.29211751402877,30
534,We show that our framework and metric enable useful analysis into the bias in word embeddings .,3,0.9197385,48.923592421318546,17
535,"We investigate how annotators ’ insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models , potentially amplifying harm against minority populations .",1,0.7620273,62.99098008051922,29
535,We first uncover unexpected correlations between surface markers of African American English ( AAE ) and ratings of toxicity in several widely-used hate speech datasets .,2,0.36221445,86.87393012828407,28
535,"Then , we show that models trained on these corpora acquire and propagate these biases , such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others .",3,0.8316718,41.03971445642269,42
535,"Finally , we propose * dialect * and * race priming * as ways to reduce the racial bias in annotation , showing that when annotators are made explicitly aware of an AAE tweet ’s dialect they are significantly less likely to label the tweet as offensive .",3,0.9097863,78.65860820663242,48
536,We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation ( MT ) .,1,0.60648245,38.06579393921758,22
536,"Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles ( e.g. , “ The doctor asked the nurse to help her in the operation ” ) .",2,0.88526523,47.948701996869694,37
536,"We devise an automatic gender bias evaluation method for eight target languages with grammatical gender , based on morphological analysis ( e.g. , the use of female inflection for the word “ doctor ” ) .",2,0.7020661,47.23099379982277,36
536,Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages .,3,0.9793356,39.55529497911428,35
536,Our data and code are publicly available at https://github.com/gabrielStanovsky/mt_gender .,3,0.59415984,16.696870550247777,10
537,"While word embeddings are now a de facto standard representation of words in most NLP tasks , recently the attention has been shifting towards vector representations which capture the different meanings , i.e. , senses , of words .",0,0.94508773,34.22144215721261,39
537,In this paper we explore the capabilities of a bidirectional LSTM model to learn representations of word senses from semantically annotated corpora .,1,0.8979598,15.054821324589895,23
537,"We show that the utilization of an architecture that is aware of word order , like an LSTM , enables us to create better representations .",3,0.94415325,48.643799308851214,26
537,"We assess our proposed model on various standard benchmarks for evaluating semantic representations , reaching state-of-the-art performance on the SemEval-2014 word-to-sense similarity task .",3,0.48969513,25.509340170357294,36
537,We release the code and the resulting word and sense embeddings at http://lcl.uniroma1.it/LSTMEmbed .,3,0.5247525,61.92238041303529,14
538,Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes .,0,0.90246314,48.840880927133405,15
538,"However , methods for measuring and removing such biases remain poorly understood .",0,0.95307404,76.22795915551752,13
538,"We show that for any embedding model that implicitly does matrix factorization , debiasing vectors post hoc using subspace projection ( Bolukbasi et al. , 2016 ) is , under certain conditions , equivalent to training on an unbiased corpus .",3,0.7115142,156.66713597895296,41
538,"We also prove that WEAT , the most common association test for word embeddings , systematically overestimates bias .",3,0.9295197,93.193367203895,19
538,"Given that the subspace projection method is provably effective , we use it to derive a new measure of association called the relational inner product association ( RIPA ) .",2,0.58889884,106.19970702538276,30
538,"Experiments with RIPA reveal that , on average , skipgram with negative sampling ( SGNS ) does not make most words any more gendered than they are in the training corpus .",3,0.94835824,105.87095585581632,32
538,"However , for gender-stereotyped words , SGNS actually amplifies the gender association in the corpus .",3,0.92634094,87.69024596805426,16
539,Studying the ways in which language is gendered has long been an area of interest in sociolinguistics .,0,0.96197903,10.296912483499844,18
539,"Studies have explored , for example , the speech of male and female characters in film and the language used to describe male and female politicians .",0,0.916055,46.993103566066296,27
539,"In this paper , we aim not to merely study this phenomenon qualitatively , but instead to quantify the degree to which the language used to describe men and women is different and , moreover , different in a positive or negative way .",1,0.9440218,37.91193861795304,44
539,"To that end , we introduce a generative latent-variable model that jointly represents adjective ( or verb ) choice , with its sentiment , given the natural gender of a head ( or dependent ) noun .",2,0.8366159,121.09975735853986,39
539,Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men .,3,0.6670915,13.15264610310648,20
540,"Given a small corpus D_T pertaining to a limited set of focused topics , our goal is to train embeddings that accurately capture the sense of words in the topic in spite of the limited size of D_T .",2,0.5195128,32.872358933593006,39
540,These embeddings may be used in various tasks involving D_T .,3,0.7550131,75.13577700325362,11
540,A popular strategy in limited data settings is to adapt pretrained embeddings E trained on a large corpus .,0,0.85681945,79.96739400003212,19
540,"To correct for sense drift , fine-tuning , regularization , projection , and pivoting have been proposed recently .",0,0.7740972,113.91908489579163,19
540,"Among these , regularization informed by a word ’s corpus frequency performed well , but we improve upon it using a new regularizer based on the stability of its cooccurrence with other words .",3,0.72192174,78.08221708388393,34
540,"However , a thorough comparison across ten topics , spanning three tasks , with standardized settings of hyper-parameters , reveals that even the best embedding adaptation strategies provide small gains beyond well-tuned baselines , which many earlier comparisons ignored .",3,0.89745384,95.46572845420475,42
540,"In a bold departure from adapting pretrained embeddings , we propose using D_T to probe , attend to , and borrow fragments from any large , topic-rich source corpus ( such as Wikipedia ) , which need not be the corpus used to pretrain embeddings .",2,0.5834552,92.95792121370083,47
540,This step is made scalable and practical by suitable indexing .,3,0.44579366,162.624524919445,11
540,"We reach the surprising conclusion that even limited corpus augmentation is more useful than adapting embeddings , which suggests that non-dominant sense information may be irrevocably obliterated from pretrained embeddings and cannot be salvaged by adaptation .",3,0.98171854,36.49464671772403,37
541,Lexical relations describe how meanings of terms relate to each other .,0,0.76635283,36.3279952521322,12
541,"Typical examples include hypernymy , synonymy , meronymy , etc .",0,0.830179,28.605017622489754,11
541,"Automatic distinction of lexical relations is vital for NLP applications , and also challenging due to the lack of contextual signals to discriminate between such relations .",0,0.9395512,37.532239048774066,27
541,"In this work , we present a neural representation learning model to distinguish lexical relations among term pairs based on Hyperspherical Relation Embeddings ( SphereRE ) .",1,0.71744215,64.45350424440926,27
541,"Rather than learning embeddings for individual terms , the model learns representations of relation triples by mapping them to the hyperspherical embedding space , where relation triples of different lexical relations are well separated .",2,0.46788526,38.19983993922113,35
541,Experiments over several benchmarks confirm SphereRE outperforms state-of-the-arts .,3,0.8899616,59.462521416340635,11
542,In this work we approach the task of learning multilingual word representations in an offline manner by fitting a generative latent variable model to a multilingual dictionary .,2,0.37344894,16.125893367925375,28
542,We model equivalent words in different languages as different views of the same word generated by a common latent variable representing their latent lexical meaning .,2,0.84232813,50.865003775597145,26
542,We explore the task of alignment by querying the fitted model for multilingual embeddings achieving competitive results across a variety of tasks .,2,0.63342303,33.97067576410212,23
542,The proposed model is robust to noise in the embedding space making it a suitable method for distributed representations learned from noisy corpora .,3,0.8341079,26.45976371841537,24
543,"In this work , we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks .",1,0.76258653,46.19564380785803,23
543,"We employ these in a data-driven and massively multilingual approach to the question , examining 106 languages .",2,0.7993156,80.75315231998003,18
543,We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation .,3,0.9806503,49.70586443876452,19
543,"Encouragingly , we also recover well-attested English examples of systematic affixes .",3,0.9523385,77.33244043517873,13
543,"Our approximate effect size ( measured in bits ) is quite small — despite some amount of systematicity between form and meaning , an arbitrary relationship and its resulting benefits dominate human language .",3,0.92969567,259.8218303122532,34
544,A large percentage of computational tools are concentrated in a very small subset of the planet ’s languages .,0,0.8135365,48.644924287654426,19
544,"Compounding the issue , many languages lack the high-quality linguistic annotation necessary for the construction of such tools with current machine learning methods .",0,0.8955895,59.34069608861309,24
544,"In this paper , we address both issues simultaneously : leveraging the high accuracy of English taggers and parsers , we project morphological information onto translations of the Bible in 26 varied test languages .",1,0.7298223,145.9851307270067,35
544,"Using an iterative discovery , constraint , and training process , we build inflectional lexica in the target languages .",2,0.7516445,123.71213827171398,20
544,"Through a combination of iteration , ensembling , and reranking , we see double-digit relative error reductions in lemmatization and morphological analysis over a strong initial system .",3,0.78492785,49.573040675000044,30
545,Morphological tagging is challenging for morphologically rich languages due to the large target space and the need for more training data to minimize model sparsity .,0,0.91874516,24.087300043185486,26
545,Dialectal variants of morphologically rich languages suffer more as they tend to be more noisy and have less resources .,0,0.72114915,56.822845243730825,20
545,In this paper we explore the use of multitask learning and adversarial training to address morphological richness and dialectal variations in the context of full morphological tagging .,1,0.88237625,28.6995002910156,28
545,"We use multitask learning for joint morphological modeling for the features within two dialects , and as a knowledge-transfer scheme for cross-dialectal modeling .",2,0.8763025,59.366932138640685,26
545,We use adversarial training to learn dialect invariant features that can help the knowledge-transfer scheme from the high to low-resource variants .,2,0.8312635,54.26535500602224,23
545,We work with two dialectal variants : Modern Standard Arabic ( high-resource “ dialect ’ ” ) and Egyptian Arabic ( low-resource dialect ) as a case study .,2,0.82925117,84.18492130848642,31
545,Our models achieve state-of-the-art results for both .,3,0.9213744,6.8311151148446285,13
545,"Furthermore , adversarial training provides more significant improvement when using smaller training datasets in particular .",3,0.9202443,68.07922704582619,16
546,The reordering model plays an important role in phrase-based statistical machine translation .,0,0.8485425,28.63345054929402,15
546,"However , there are few works that exploit the reordering information in neural machine translation .",0,0.89417547,27.72527748187347,16
546,"In this paper , we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information .",1,0.8575393,16.48958764334959,23
546,These learned reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation .,2,0.57104874,33.52617506481551,18
546,The reordering mechanism can be easily integrated into both the encoder and the decoder in the Transformer translation system .,3,0.7901828,15.81698052550599,20
546,"Experimental results on WMT’14 English-to-German , NIST Chinese-to-English , and WAT Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer .",3,0.92722017,8.12393803097128,35
547,We present a simple yet powerful data augmentation method for boosting Neural Machine Translation ( NMT ) performance by leveraging information retrieved from a Translation Memory ( TM ) .,1,0.55713207,22.111312670503615,30
547,We propose and test two methods for augmenting NMT training data with fuzzy TM matches .,1,0.421568,81.5550309652505,16
547,Tests on the DGT-TM data set for two language pairs show consistent and substantial improvements over a range of baseline systems .,3,0.8771822,40.40726747992481,23
547,"The results suggest that this method is promising for any translation environment in which a sizeable TM is available and a certain amount of repetition across translations is to be expected , especially considering its ease of implementation .",3,0.9889129,54.856133967508285,39
548,Transformer is the state-of-the-art model in recent machine translation evaluations .,0,0.7326716,10.717880623605382,16
548,Two strands of research are promising to improve models of this kind : the first uses wide networks ( a.k.a .,0,0.58152145,81.39341528023886,21
548,"Transformer-Big ) and has been the de facto standard for development of the Transformer system , and the other uses deeper language representation but faces the difficulty arising from learning deep networks .",0,0.62052065,156.48145548090397,35
548,"Here , we continue the line of research on the latter .",1,0.6121698,47.89495666473742,12
548,We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1 ) proper use of layer normalization and 2 ) a novel way of passing the combination of previous layers to the next .,3,0.83488566,68.58459006717649,40
548,"On WMT’16 English-German and NIST OpenMT’12 Chinese-English tasks , our deep system ( 30 / 25-layer encoder ) outperforms the shallow Transformer-Big / Base baseline ( 6-layer encoder ) by 0.4-2.4 BLEU points .",3,0.8298711,38.855657749298416,44
548,"As another bonus , the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big .",3,0.84644026,53.552224631141506,22
549,Users of machine translation systems may desire to obtain multiple candidates translated in different ways .,0,0.8793891,59.53364724778115,16
549,"In this work , we attempt to obtain diverse translations by using sentence codes to condition the sentence generation .",1,0.56978667,96.87495289559041,20
549,"We describe two methods to extract the codes , either with or without the help of syntax information .",2,0.51175207,44.25310662730385,19
549,"For diverse generation , we sample multiple candidates , each of which conditioned on a unique code .",2,0.80170745,167.45603904278755,18
549,"Experiments show that the sampled translations have much higher diversity scores when using reasonable sentence codes , where the translation quality is still on par with the baselines even under strong constraint imposed by the codes .",3,0.93471026,69.04183330786361,37
549,"In qualitative analysis , we show that our method is able to generate paraphrase translations with drastically different structures .",3,0.9585861,33.106255166053494,20
549,The proposed approach can be easily adopted to existing translation systems as no modification to the model is required .,3,0.91439223,31.884707095747206,20
550,We present a simple new method where an emergent NMT system is used for simultaneously selecting training data and learning internal NMT representations .,2,0.367695,48.85013924040787,24
550,"This is done in a self-supervised way without parallel data , in such a way that both tasks enhance each other during training .",2,0.5123937,21.104275913091257,24
550,"The method is language independent , introduces no additional hyper-parameters , and achieves BLEU scores of 29.21 ( en2 fr ) and 27.36 ( fr2en ) on newstest2014 using English and French Wikipedia data for training .",3,0.6253482,78.81208778306872,37
551,"Previous work on end-to-end translation from speech has primarily used frame-level features as speech representations , which creates longer , sparser sequences than text .",0,0.9121326,68.26051249373454,27
551,We show that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features .,3,0.9550117,42.11541830606094,28
551,"Specifically , we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter , higher-level source sequences for translation .",2,0.8461242,114.31697221128584,29
551,"We see improvements of up to 5 BLEU on both our high and low resource language pairs , with a reduction in training time of 60 % .",3,0.9385855,27.87786976408691,28
551,Our improvements hold across multiple data sizes and two language pairs .,3,0.930182,75.46147240338642,12
552,"We present the Visually Grounded Neural Syntax Learner ( VG-NSL ) , an approach for learning syntactic representations and structures without any explicit supervision .",1,0.5198203,41.86948961542945,27
552,The model learns by looking at natural images and reading paired captions .,2,0.53380007,78.43347434372376,13
552,"VG-NSL generates constituency parse trees of texts , recursively composes representations for constituents , and matches them with images .",2,0.39703873,123.27673020360272,22
552,"We define concreteness of constituents by their matching scores with images , and use it to guide the parsing of text .",2,0.7224996,77.70392339746311,22
552,"Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding , in terms of F1 scores against gold parse trees .",3,0.9314026,33.83292854371432,33
552,We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data .,3,0.9768893,33.05057642641455,23
552,We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists .,3,0.98473746,76.37997382424604,21
552,"Finally , we also apply VG-NSL to multiple languages in the Multi30 K data set , showing that our model consistently outperforms prior unsupervised approaches .",3,0.8880729,54.43835072191758,28
553,Advances in learning and representations have reinvigorated work that connects language to other modalities .,0,0.91817284,49.23886329682465,15
553,"A particularly exciting direction is Vision-and-Language Navigation ( VLN ) , in which agents interpret natural language instructions and visual scenes to move through environments and reach goals .",0,0.94314903,64.85886194419894,32
553,"Despite recent progress , current research leaves unclear how much of a role language under-standing plays in this task , especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions .",0,0.88901174,61.94544521012525,41
553,"Here , we highlight shortcomings of current metrics for the Room-to-Room dataset ( Anderson et al. ,2018 b ) and propose a new metric , Coverage weighted by Length Score ( CLS ) .",1,0.5448705,89.0266664081915,37
553,We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths .,3,0.95352155,57.22696501813057,27
553,"We join existing short paths to form more challenging extended paths to create a new data set , Room-for-Room ( R4 R ) .",2,0.78802127,197.81496191341563,24
553,"Using R4 R and CLS , we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion .",3,0.80475754,101.09659737449128,24
554,Describing images with text is a fundamental problem in vision-language research .,0,0.9264989,32.107089802558235,14
554,Current studies in this domain mostly focus on single image captioning .,0,0.89531565,27.3370009015939,12
554,"However , in various real applications ( e.g. , image editing , difference interpretation , and retrieval ) , generating relational captions for two images , can also be very useful .",0,0.7285276,124.13973110894042,32
554,This important problem has not been explored mostly due to lack of datasets and effective models .,0,0.93842447,36.73022645383558,17
554,"To push forward the research in this direction , we first introduce a new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions .",2,0.48236567,33.44073223331376,34
554,We then propose a new relational speaker model based on an encoder-decoder architecture with static relational attention and sequential multi-head attention .,2,0.46121874,29.046338419053598,22
554,"We also extend the model with dynamic relational attention , which calculates visual alignment while decoding .",2,0.524587,227.78309553838494,17
554,Our models are evaluated on our newly collected and two public datasets consisting of image pairs annotated with relationship sentences .,2,0.75503844,58.610579754210534,21
554,"Experimental results , based on both automatic and human evaluation , demonstrate that our model outperforms all baselines and existing methods on all the datasets .",3,0.93904656,18.29663751134221,26
555,"In this paper , we address a novel task , namely weakly-supervised spatio-temporally grounding natural sentence in video .",1,0.8489935,60.8406268467072,21
555,"Specifically , given a natural sentence and a video , we localize a spatio-temporal tube in the video that semantically corresponds to the given sentence , with no reliance on any spatio-temporal annotations during training .",2,0.78576285,32.62667063843024,36
555,"First , a set of spatio-temporal tubes , referred to as instances , are extracted from the video .",2,0.7685506,41.90082630908703,19
555,We then encode these instances and the sentence using our newly proposed attentive interactor which can exploit their fine-grained relationships to characterize their matching behaviors .,2,0.75894034,90.9413077601603,27
555,"Besides a ranking loss , a novel diversity loss is introduced to train our attentive interactor to strengthen the matching behaviors of reliable instance-sentence pairs and penalize the unreliable ones .",2,0.65861756,123.36164197808533,32
555,"We also contribute a dataset , called VID-sentence , based on the ImageNet video object detection dataset , to serve as a benchmark for our task .",2,0.56148845,59.30349871068122,29
555,Results from extensive experiments demonstrate the superiority of our model over the baseline approaches .,3,0.9445299,12.62642843781483,15
556,"This paper introduces the PhotoBook dataset , a large-scale collection of visually-grounded , task-oriented dialogues in English designed to investigate shared dialogue history accumulating during conversation .",1,0.7247891,62.76871301684481,31
556,"Taking inspiration from seminal work on dialogue analysis , we propose a data-collection task formulated as a collaborative game prompting two online participants to refer to images utilising both their visual context as well as previously established referring expressions .",2,0.5479831,81.76260911658231,40
556,"We provide a detailed description of the task setup and a thorough analysis of the 2,500 dialogues collected .",2,0.53031665,18.255965464755565,19
556,"To further illustrate the novel features of the dataset , we propose a baseline model for reference resolution which uses a simple method to take into account shared information accumulated in a reference chain .",2,0.50021386,47.02858891289077,35
556,Our results show that this information is particularly important to resolve later descriptions and underline the need to develop more sophisticated models of common ground in dialogue interaction .,3,0.9901904,52.965915326591194,29
557,Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task .,0,0.86714035,37.595100333037124,21
557,"Recently , this approach has shown promising performance improvements ( on language modeling and image classification ) with reasonable training speed , using a weight sharing strategy called Efficient Neural Architecture Search ( ENAS ) .",0,0.80310583,87.62562539576054,36
557,"In our work , we first introduce a novel continual architecture search ( CAS ) approach , so as to continually evolve the model parameters during the sequential training of several tasks , without losing performance on previously learned tasks ( via block-sparsity and orthogonality constraints ) , thus enabling life-long learning .",2,0.55008036,79.14118927183793,54
557,"Next , we explore a multi-task architecture search ( MAS ) approach over ENAS for finding a unified , single cell structure that performs well across multiple tasks ( via joint controller rewards ) , and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task .",2,0.59414667,142.58870187597134,52
557,We empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks ( GLUE ) and multimodal-generation based video captioning tasks .,3,0.7634086,62.7921529384387,37
557,"Further , we present several ablations and analyses on the learned cell structures .",3,0.6214715,63.713929343539746,14
558,Supervised models of NLP rely on large collections of text which closely resemble the intended testing setting .,0,0.85680103,92.03510375560627,18
558,"Unfortunately matching text is often not available in sufficient quantity , and moreover , within any domain of text , data is often highly heterogenous .",0,0.93864316,117.24117199464018,26
558,"In this paper we propose a method to distill the important domain signal as part of a multi-domain learning system , using a latent variable model in which parts of a neural model are stochastically gated based on the inferred domain .",1,0.83266634,28.873055950315766,42
558,"We compare the use of discrete versus continuous latent variables , operating in a domain-supervised or a domain semi-supervised setting , where the domain is known only for a subset of training inputs .",2,0.82069784,44.01600375153813,34
558,"We show that our model leads to substantial performance improvements over competitive benchmark domain adaptation methods , including methods using adversarial learning .",3,0.9147527,37.587132741161,23
559,"Modern entity linking systems rely on large collections of documents specifically annotated for the task ( e.g. , AIDA CoNLL ) .",0,0.9064952,55.51996613837782,22
559,"In contrast , we propose an approach which exploits only naturally occurring information : unlabeled documents and Wikipedia .",2,0.5222551,94.42166039643303,19
559,Our approach consists of two stages .,2,0.6922497,17.326332418944123,7
559,"First , we construct a high recall list of candidate entities for each mention in an unlabeled document .",2,0.8909225,32.95423596694454,19
559,"Second , we use the candidate lists as weak supervision to constrain our document-level entity linking model .",2,0.8027942,59.36124243110631,20
559,"The model treats entities as latent variables and , when estimated on a collection of unlabelled texts , learns to choose entities relying both on local context of each mention and on coherence with other entities in the document .",2,0.61326796,60.767533876749454,40
559,The resulting approach rivals fully-supervised state-of-the-art systems on standard test sets .,3,0.7333808,26.58179197116592,20
559,It also approaches their performance in the very challenging setting : when tested on a test set sampled from the data used to estimate the supervised systems .,3,0.5714384,87.33925301604894,28
559,"By comparing to Wikipedia-only training of our model , we demonstrate that modeling unlabeled documents is beneficial .",3,0.8748901,68.72349225322468,18
560,We consider the problem of learning to map from natural language instructions to state transitions ( actions ) in a data-efficient manner .,0,0.37696645,46.473054481955636,23
560,Our method takes inspiration from the idea that it should be easier to ground language to concepts that have already been formed through pre-linguistic observation .,2,0.6881011,29.066227436403878,26
560,We augment a baseline instruction-following learner with an initial environment-learning phase that uses observations of language-free state transitions to induce a suitable latent representation of actions before processing the instruction-following training data .,2,0.8862705,61.31505742958546,41
560,We show that mapping to pre-learned representations substantially improves performance over systems whose representations are learned from limited instructional data alone .,3,0.9317654,44.20288186678085,22
561,Supervised models suffer from the problem of domain shifting where distribution mismatch in the data across domains greatly affect model performance .,0,0.88111824,76.07777063931798,22
561,"To solve the problem , training data selection ( TDS ) has been proven to be a prospective solution for domain adaptation in leveraging appropriate data .",0,0.9419991,77.82785029395788,27
561,"However , conventional TDS methods normally requires a predefined threshold which is neither easy to set nor can be applied across tasks , and models are trained separately with the TDS process .",0,0.7652713,73.61123086895715,33
561,"To make TDS self-adapted to data and task , and to combine it with model training , in this paper , we propose a reinforcement learning ( RL ) framework that synchronously searches for training instances relevant to the target domain and learns better representations for them .",1,0.6643409,45.489803116524044,50
561,"A selection distribution generator ( SDG ) is designed to perform the selection and is updated according to the rewards computed from the selected data , where a predictor is included in the framework to ensure a task-specific model can be trained on the selected data and provides feedback to rewards .",2,0.7538479,43.83011425553996,53
561,"Experimental results from part-of-speech tagging , dependency parsing , and sentiment analysis , as well as ablation studies , illustrate that the proposed framework is not only effective in data selection and representation , but also generalized to accommodate different NLP tasks .",3,0.942135,24.357176026904042,44
562,Generating long and informative review text is a challenging natural language generation task .,0,0.9433342,30.914381153662813,14
562,"Previous work focuses on word-level generation , neglecting the importance of topical and syntactic characteristics from natural languages .",0,0.89371395,50.14042581526622,19
562,"In this paper , we propose a novel review generation model by characterizing an elaborately designed aspect-aware coarse-to-fine generation process .",1,0.89979166,34.41182217397711,24
562,"First , we model the aspect transitions to capture the overall content flow .",2,0.8395002,77.477790536275,14
562,"Then , to generate a sentence , an aspect-aware sketch will be predicted using an aspect-aware decoder .",2,0.7257116,47.798209568400715,19
562,"Finally , another decoder fills in the semantic slots by generating corresponding words .",2,0.49870375,154.64843579769547,14
562,"Our approach is able to jointly utilize aspect semantics , syntactic sketch , and context information .",3,0.6804937,115.94590663396373,17
562,Extensive experiments results have demonstrated the effectiveness of the proposed model .,3,0.9265016,15.435065226842195,12
563,We present a PaperRobot who performs as an automatic research assistant by ( 1 ) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs ( KGs ) ;,2,0.34036547,89.96658160068536,39
563,"( 2 ) creating new ideas by predicting links from the background KGs , by combining graph attention and contextual text attention ;",2,0.5709295,541.3022104700901,23
563,"( 3 ) incrementally writing some key elements of a new paper based on memory-attention networks : from the input title along with predicted related entities to generate a paper abstract , from the abstract to generate conclusion and future work , and finally from future work to generate a title for a follow-on paper .",2,0.58220667,65.94734375578258,59
563,"Turing Tests , where a biomedical domain expert is asked to compare a system output and a human-authored string , show PaperRobot generated abstracts , conclusion and future work sections , and new titles are chosen over human-written ones up to 30 % , 24 % and 12 % of the time , respectively .",3,0.709665,131.82923313764445,55
564,"Rhetoric is a vital element in modern poetry , and plays an essential role in improving its aesthetics .",0,0.9366509,30.72747084124223,19
564,"However , to date , it has not been considered in research on automatic poetry generation .",0,0.9264555,50.4973578335965,17
564,"In this paper , we propose a rhetorically controlled encoder-decoder for modern Chinese poetry generation .",1,0.9319485,39.1181532795526,16
564,"Our model relies on a continuous latent variable as a rhetoric controller to capture various rhetorical patterns in an encoder , and then incorporates rhetoric-based mixtures while generating modern Chinese poetry .",2,0.793099,89.83750416394362,34
564,"For metaphor and personification , an automated evaluation shows that our model outperforms state-of-the-art baselines by a substantial margin , while human evaluation shows that our model generates better poems than baseline methods in terms of fluency , coherence , meaningfulness , and rhetorical aesthetics .",3,0.9265266,21.84485751708244,52
565,"Automatic topic-to-essay generation is a challenging task since it requires generating novel , diverse , and topic-consistent paragraph-level text with a set of topics as input .",0,0.92180103,35.92753050737399,29
565,Previous work tends to perform essay generation based solely on the given topics while ignoring massive commonsense knowledge .,0,0.9193991,81.72289055519813,19
565,"However , this commonsense knowledge provides additional background information , which can help to generate essays that are more novel and diverse .",0,0.7531353,48.3503644055665,23
565,"Towards filling this gap , we propose to integrate commonsense from the external knowledge base into the generator through dynamic memory mechanism .",1,0.35183808,46.53758507336673,23
565,"Besides , the adversarial training based on a multi-label discriminator is employed to further improve topic-consistency .",2,0.5721144,28.039610299874994,17
565,We also develop a series of automatic evaluation metrics to comprehensively assess the quality of the generated essay .,2,0.5986041,17.369388700922077,19
565,"Experiments show that with external commonsense knowledge and adversarial training , the generated essays are more novel , diverse , and topic-consistent than existing methods in terms of both automatic and human evaluation .",3,0.94685274,44.61720491131044,34
566,"In this paper , we focus on the task of fine-grained text sentiment transfer ( FGST ) .",1,0.8761341,28.753723475304074,19
566,"This task aims to revise an input sequence to satisfy a given sentiment intensity , while preserving the original semantic content .",0,0.70159173,51.550743498717914,22
566,"Different from the conventional sentiment transfer task that only reverses the sentiment polarity ( positive / negative ) of text , the FTST task requires more nuanced and fine-grained control of sentiment .",3,0.45239556,52.13336819819334,34
566,"To remedy this , we propose a novel Seq2SentiSeq model .",1,0.3165636,45.527322714924324,11
566,"Specifically , the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output .",2,0.69187087,35.67462049997861,27
566,"Moreover , to tackle the problem of lacking parallel data , we propose a cycle reinforcement learning algorithm to guide the model training .",2,0.6442195,45.87630327382818,24
566,"In this framework , the elaborately designed rewards can balance both sentiment transformation and content preservation , while not requiring any ground truth output .",3,0.6392625,181.0840445333903,25
566,Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation .,3,0.9665209,5.34259894193359,22
567,Recent approaches to data-to-text generation have shown great promise thanks to the use of large-scale datasets and the application of neural network architectures which are trained end-to-end .,0,0.94304925,11.73450017295267,32
567,"These models rely on representation learning to select content appropriately , structure it coherently , and verbalize it grammatically , treating entities as nothing more than vocabulary tokens .",0,0.8380379,102.78166521606829,29
567,In this work we propose an entity-centric neural architecture for data-to-text generation .,1,0.8432124,11.513338592559618,17
567,Our model creates entity-specific representations which are dynamically updated .,2,0.5470528,59.932978305382804,11
567,Text is generated conditioned on the data input and entity memory representations using hierarchical attention at each time step .,2,0.7644593,63.63055724885595,20
567,We present experiments on the RotoWire benchmark and a ( five times larger ) new dataset on the baseball domain which we create .,2,0.69157284,136.79810400465297,24
567,Our results show that the proposed model outperforms competitive baselines in automatic and human evaluation .,3,0.98568535,7.43640100050542,16
568,A type description is a succinct noun compound which helps human and machines to quickly grasp the informative and distinctive information of an entity .,0,0.9151328,124.93773993316732,25
568,"Entities in most knowledge graphs ( KGs ) still lack such descriptions , thus calling for automatic methods to supplement such information .",0,0.9397511,120.09102170722872,23
568,"However , existing generative methods either overlook the grammatical structure or make factual mistakes in generated texts .",0,0.9423793,40.89849181099889,18
568,"To solve these problems , we propose a head-modifier template based method to ensure the readability and data fidelity of generated type descriptions .",2,0.39890608,49.14094803203064,24
568,We also propose a new dataset and two metrics for this task .,2,0.3759033,21.815430080647644,13
568,Experiments show that our method improves substantially compared with baselines and achieves state-of-the-art performance on both datasets .,3,0.95427644,6.224668443194483,24
569,Table-to-text generation aims to translate the structured data into the unstructured text .,0,0.8736209,10.469948156614613,16
569,"Most existing methods adopt the encoder-decoder framework to learn the transformation , which requires large-scale training samples .",0,0.7951619,21.635739252054904,18
569,"However , the lack of large parallel data is a major practical problem for many domains .",0,0.91719866,32.738550097195464,17
569,"In this work , we consider the scenario of low resource table-to-text generation , where only limited parallel data is available .",1,0.47032222,26.5325691531283,26
569,We propose a novel model to separate the generation into two stages : key fact prediction and surface realization .,2,0.32679954,84.15847155782791,20
569,"It first predicts the key facts from the tables , and then generates the text with the key facts .",2,0.6329341,26.141511566252184,20
569,"The training of key fact prediction needs much fewer annotated data , while surface realization can be trained with pseudo parallel corpus .",3,0.5945078,188.5001920511085,23
569,We evaluate our model on a biography generation dataset .,2,0.650047,76.77579750914195,10
569,"Our model can achieve 27.34 BLEU score with only 1,000 parallel data , while the baseline model only obtain the performance of 9.71 BLEU score .",3,0.8688152,23.892273824361446,26
570,The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabeled text corpora .,1,0.7085844,18.03158407839106,19
570,"The core framework is composed of a shared encoder and a pair of attentional-decoders , crucially assisted by discrimination-based losses and denoising .",2,0.44851804,45.603552907716214,27
570,The framework is trained using unlabeled text collected from en-Wikipedia dump .,2,0.7602184,57.146985421525194,12
570,"Our analysis ( both quantitative and qualitative involving human evaluators ) on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels , competitive to existing supervised methods .",3,0.9203358,45.70177408493104,36
570,It also outperforms viable unsupervised baselines .,3,0.86762077,35.647200830057535,7
570,Adding a few labeled pairs helps improve the performance further .,3,0.82244474,80.21899881647477,11
571,"We present a syntax-infused variational autoencoder ( SIVAE ) , that integrates sentences with their syntactic trees to improve the grammar of generated sentences .",2,0.36575103,41.10300126952601,27
571,"Distinct from existing VAE-based text generative models , SIVAE contains two separate latent spaces , for sentences and syntactic trees .",0,0.3958995,124.55056768940679,23
571,"The evidence lower bound objective is redesigned correspondingly , by optimizing a joint distribution that accommodates two encoders and two decoders .",2,0.49917826,123.67663106047814,22
571,SIVAE works with long short-term memory architectures to simultaneously generate sentences and syntactic trees .,0,0.5149092,73.35016057558349,15
571,"Two versions of SIVAE are proposed : one captures the dependencies between the latent variables through a conditional prior network , and the other treats the latent variables independently such that syntactically-controlled sentence generation can be performed .",2,0.513334,39.79058108163675,40
571,Experimental results demonstrate the generative superiority of SIVAE on both reconstruction and targeted syntactic evaluations .,3,0.9817362,66.13357872059859,16
571,"Finally , we show that the proposed models can be used for unsupervised paraphrasing given different syntactic tree templates .",3,0.9496226,21.17423092104066,20
572,Variational autoencoders ( VAEs ) have received much attention recently as an end-to-end architecture for text generation with latent variables .,0,0.951727,14.481743045716435,23
572,"However , previous works typically focus on synthesizing relatively short sentences ( up to 20 words ) , and the posterior collapse issue has been widely identified in text-VAEs .",0,0.8888194,86.72215841729128,32
572,"In this paper , we propose to leverage several multi-level structures to learn a VAE model for generating long , and coherent text .",1,0.85141706,49.545285302280874,24
572,"In particular , a hierarchy of stochastic layers between the encoder and decoder networks is employed to abstract more informative and semantic-rich latent codes .",2,0.54308647,46.25412054013722,27
572,"Besides , we utilize a multi-level decoder structure to capture the coherent long-term structure inherent in long-form texts , by generating intermediate sentence representations as high-level plan vectors .",2,0.7494308,53.255647490897,32
572,Extensive experimental results demonstrate that the proposed multi-level VAE model produces more coherent and less repetitive long text compared to baselines as well as can mitigate the posterior-collapse issue .,3,0.96753424,32.30135243296897,31
573,"Semantic parsing aims to transform natural language ( NL ) utterances into formal meaning representations ( MRs ) , whereas an NL generator achieves the reverse : producing an NL description for some given MRs .",0,0.8788605,75.06934631878352,36
573,"Despite this intrinsic connection , the two tasks are often studied separately in prior work .",0,0.87876546,59.70925752550047,16
573,"In this paper , we model the duality of these two tasks via a joint learning framework , and demonstrate its effectiveness of boosting the performance on both tasks .",1,0.64407915,22.7687798143145,30
573,"Concretely , we propose a novel method of dual information maximization ( DIM ) to regularize the learning process , where DIM empirically maximizes the variational lower bounds of expected joint distributions of NL and MRs .",2,0.6318043,56.94023454946381,37
573,"We further extend DIM to a semi-supervision setup ( SemiDIM ) , which leverages unlabeled data of both tasks .",2,0.69494826,63.31098743733204,20
573,"Experiments on three datasets of dialogue management and code generation ( and summarization ) show that performance on both semantic parsing and NL generation can be consistently improved by DIM , in both supervised and semi-supervised setups .",3,0.88283813,45.245826056303514,38
574,"We propose a data-to-text generation model with two modules , one for tracking and the other for text generation .",2,0.5355009,20.097033248415947,23
574,Our tracking module selects and keeps track of salient information and memorizes which record has been mentioned .,2,0.63395387,106.2783293420751,18
574,Our generation module generates a summary conditioned on the state of tracking module .,2,0.67534244,114.63581232157689,14
574,Our proposed model is considered to simulate the human-like writing process that gradually selects the information by determining the intermediate variables while writing the summary .,2,0.5097935,85.74787009998404,26
574,"In addition , we also explore the effectiveness of the writer information for generations .",3,0.43462187,130.99045454771186,15
574,Experimental results show that our proposed model outperforms existing models in all evaluation metrics even without writer information .,3,0.97185236,22.07106791515363,19
574,"Incorporating writer information further improves the performance , contributing to content planning and surface realization .",3,0.86633754,180.79759729448105,16
575,"This paper investigates a new task named Conversational Question Generation ( CQG ) which is to generate a question based on a passage and a conversation history ( i.e. , previous turns of question-answer pairs ) .",1,0.7686749,25.163192911002234,39
575,CQG is a crucial task for developing intelligent agents that can drive question-answering style conversations or test user understanding of a given passage .,0,0.8255735,49.636384391675215,26
575,"Towards that end , we propose a new approach named Reinforced Dynamic Reasoning network , which is based on the general encoder-decoder framework but incorporates a reasoning procedure in a dynamic manner to better understand what has been asked and what to ask next about the passage into the general encoder-decoder framework .",2,0.5133549,22.862418036201912,53
575,"To encourage producing meaningful questions , we leverage a popular question answering ( QA ) model to provide feedback and fine-tune the question generator using a reinforcement learning mechanism .",2,0.6862553,41.31355651386791,31
575,Empirical results on the recently released CoQA dataset demonstrate the effectiveness of our method in comparison with various baselines and model variants .,3,0.9064585,12.554858681438834,23
575,"Moreover , to show the applicability of our method , we also apply it to create multi-turn question-answering conversations for passages in SQuAD .",2,0.48651746,22.450173056252886,26
576,"Currently , no large-scale training data is available for the task of scientific paper summarization .",0,0.9227465,21.09398869387542,16
576,"In this paper , we propose a novel method that automatically generates summaries for scientific papers , by utilizing videos of talks at scientific conferences .",1,0.8993099,33.347038741706434,26
576,"We hypothesize that such talks constitute a coherent and concise description of the papers ’ content , and can form the basis for good summaries .",1,0.3311395,59.85986256685936,26
576,"We collected 1716 papers and their corresponding videos , and created a dataset of paper summaries .",2,0.8910339,75.73522138247547,17
576,A model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually .,3,0.76675856,25.23188933937187,20
576,"In addition , we validated the quality of our summaries by human experts .",2,0.578609,53.64795402090723,14
577,Comprehensive document encoding and salient information selection are two major difficulties for generating summaries with adequate salient information .,0,0.8604769,92.54731011945933,19
577,"To tackle the above difficulties , we propose a Transformer-based encoder-decoder framework with two novel extensions for abstractive document summarization .",2,0.49289837,15.109770204463079,23
577,"Specifically , ( 1 ) to encode the documents comprehensively , we design a focus-attention mechanism and incorporate it into the encoder .",2,0.75812364,40.45382574526727,23
577,"This mechanism models a Gaussian focal bias on attention scores to enhance the perception of local context , which contributes to producing salient and informative summaries .",3,0.64909333,101.06618357667935,27
577,"( 2 ) To distinguish salient information precisely , we design an independent saliency-selection network which manages the information flow from encoder to decoder .",2,0.73060274,94.42818906678244,26
577,This network effectively reduces the influences of secondary information on the generated summaries .,3,0.68777686,60.70095405187712,14
577,Experimental results on the popular CNN / Daily Mail benchmark demonstrate that our model outperforms other state-of-the-art baselines on the ROUGE metrics .,3,0.9218594,8.080950382010606,29
578,This paper focuses on the end-to-end abstractive summarization of a single product review without supervision .,1,0.90837,19.695676246367803,17
578,"We assume that a review can be described as a discourse tree , in which the summary is the root , and the child sentences explain their parent in detail .",2,0.7294043,48.18378963025212,31
578,"By recursively estimating a parent from its children , our model learns the latent discourse tree without an external parser and generates a concise summary .",2,0.5837977,57.35790570490835,26
578,We also introduce an architecture that ranks the importance of each sentence on the tree to support summary generation focusing on the main review point .,2,0.3811159,62.10888683052533,26
578,The experimental results demonstrate that our model is competitive with or outperforms other unsupervised approaches .,3,0.9720205,11.266653480437675,16
578,"In particular , for relatively long reviews , it achieves a competitive or better performance than supervised models .",3,0.7103226,127.34858874491414,19
578,"The induced tree shows that the child sentences provide additional information about their parent , and the generated summary abstracts the entire review .",3,0.8579367,102.16257902764168,24
579,The success of neural summarization models stems from the meticulous encodings of source articles .,0,0.9113141,51.71367956264678,15
579,"To overcome the impediments of limited and sometimes noisy training data , one promising direction is to make better use of the available training data by applying filters during summarization .",0,0.74485755,40.57799201579649,31
579,"In this paper , we propose a novel Bi-directional Selective Encoding with Template ( BiSET ) model , which leverages template discovered from training data to softly select key information from each source article to guide its summarization process .",1,0.82963026,50.572600564403594,40
579,Extensive experiments on a standard summarization dataset are conducted and the results show that the template-equipped BiSET model manages to improve the summarization performance significantly with a new state of the art .,3,0.93729025,21.895639304376758,35
580,Generating keyphrases that summarize the main points of a document is a fundamental task in natural language processing .,0,0.93480736,10.756464018390574,19
580,"Although existing generative models are capable of predicting multiple keyphrases for an input document as well as determining the number of keyphrases to generate , they still suffer from the problem of generating too few keyphrases .",0,0.9044681,12.207038720141885,37
580,"To address this problem , we propose a reinforcement learning ( RL ) approach for keyphrase generation , with an adaptive reward function that encourages a model to generate both sufficient and accurate keyphrases .",1,0.3376794,24.6474931513051,35
580,"Furthermore , we introduce a new evaluation method that incorporates name variations of the ground-truth keyphrases using the Wikipedia knowledge base .",2,0.60778654,45.115561356724385,24
580,"Thus , our evaluation method can more robustly evaluate the quality of predicted keyphrases .",3,0.9367358,42.3440499121139,15
580,Extensive experiments on five real-world datasets of different scales demonstrate that our RL approach consistently and significantly improves the performance of the state-of-the-art generative models with both conventional and new evaluation methods .,3,0.904131,13.497284509421062,39
581,"When writing a summary , humans tend to choose content from one or two sentences and merge them into a single summary sentence .",0,0.8979673,42.75973446069277,24
581,"However , the mechanisms behind the selection of one or multiple source sentences remain poorly understood .",0,0.95649296,42.911189682040714,17
581,Sentence fusion assumes multi-sentence input ; yet sentence selection methods only work with single sentences and not combinations of them .,0,0.7907651,86.202598740101,21
581,There is thus a crucial gap between sentence selection and fusion to support summarizing by both compressing single sentences and fusing pairs .,0,0.8031787,76.76459580314491,23
581,This paper attempts to bridge the gap by ranking sentence singletons and pairs together in a unified space .,1,0.8840112,38.88555462119226,19
581,"Our proposed framework attempts to model human methodology by selecting either a single sentence or a pair of sentences , then compressing or fusing the sentence ( s ) to produce a summary sentence .",2,0.70283747,63.65000906788811,35
581,We conduct extensive experiments on both single-and multi-document summarization datasets and report findings on sentence selection and abstraction .,2,0.46178758,30.3391545565663,21
582,"Transcripts of natural , multi-person meetings differ significantly from documents like news articles , which can make Natural Language Generation models for generating summaries unfocused .",0,0.82757777,151.6118567463214,26
582,We develop an abstractive meeting summarizer from both videos and audios of meeting recordings .,2,0.6892166,83.97279505218212,15
582,"Specifically , we propose a multi-modal hierarchical attention across three levels : segment , utterance and word .",2,0.6755339,56.446793834693494,18
582,"To narrow down the focus into topically-relevant segments , we jointly model topic segmentation and summarization .",2,0.7083887,70.28260832169171,19
582,"In addition to traditional text features , we introduce new multi-modal features derived from visual focus of attention , based on the assumption that the utterance is more important if the speaker receives more attention .",2,0.77065,37.34973452378758,36
582,Experiments show that our model significantly outperforms the state-of-the-art with both BLEU and ROUGE measures .,3,0.9664531,5.024979466385077,22
583,"A common issue in training a deep learning , abstractive summarization model is lack of a large set of training summaries .",0,0.9174826,34.26725309649568,22
583,This paper examines techniques for adapting from a labeled source domain to an unlabeled target domain in the context of an encoder-decoder model for text generation .,1,0.8957867,15.3268164878136,27
583,"In addition to adversarial domain adaptation ( ADA ) , we introduce the use of artificial titles and sequential training to capture the grammatical style of the unlabeled target domain .",2,0.7921846,57.08167773176406,31
583,Evaluation on adapting to / from news articles and Stack Exchange posts indicates that the use of these techniques can boost performance for both unsupervised adaptation as well as fine-tuning with limited target data .,3,0.92130506,31.77908668707001,35
584,"Most existing text summarization datasets are compiled from the news domain , where summaries have a flattened discourse structure .",0,0.9132066,61.187831936577325,20
584,"In such datasets , summary-worthy content often appears in the beginning of input articles .",0,0.7690968,131.08536735233017,17
584,"Moreover , large segments from input articles are present verbatim in their respective summaries .",3,0.5286805,87.21673144063443,15
584,These issues impede the learning and evaluation of systems that can understand an article ’s global content structure as well as produce abstractive summaries with high compression ratio .,0,0.85218394,51.751100804639066,29
584,"In this work , we present a novel dataset , BIGPATENT , consisting of 1.3 million records of U.S .",1,0.65313494,31.443742206793537,20
584,patent documents along with human written abstractive summaries .,0,0.3696221,119.00860310786682,9
584,"Compared to existing summarization datasets , BIGPATENT has the following properties : i ) summaries contain a richer discourse structure with more recurring entities , ii ) salient content is evenly distributed in the input , and iii ) lesser and shorter extractive fragments are present in the summaries .",3,0.5774232,79.0103108873099,50
584,"Finally , we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research .",3,0.7012188,66.25623713477393,27
585,"While recent progress on abstractive summarization has led to remarkably fluent summaries , factual errors in generated summaries still severely limit their use in practice .",0,0.9241142,24.943286544699657,26
585,"In this paper , we evaluate summaries produced by state-of-the-art models via crowdsourcing and show that such errors occur frequently , in particular with more abstractive models .",1,0.82866204,31.06626231881516,34
585,We study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries .,1,0.82683563,40.250233483185866,25
585,That leads to an interesting downstream application for entailment models .,3,0.76901025,65.39034531057885,11
585,"In our experiments , we find that out-of-the-box entailment models trained on NLI datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of NLI .",3,0.9607211,23.281694499405877,48
586,"Existing models for extractive summarization are usually trained from scratch with a cross-entropy loss , which does not explicitly capture the global context at the document level .",0,0.82573193,17.94238061270936,28
586,"In this paper , we aim to improve this task by introducing three auxiliary pre-training tasks that learn to capture the document-level context in a self-supervised fashion .",1,0.84818166,15.163712081452896,30
586,Experiments on the widely-used CNN / DM dataset validate the effectiveness of the proposed auxiliary tasks .,3,0.7687895,40.6225968269585,19
586,"Furthermore , we show that after pre-training , a clean model with simple building blocks is able to outperform previous state-of-the-art that are carefully designed .",3,0.9467042,17.70713467868654,32
587,Question understanding is one of the main challenges in question answering .,0,0.9490234,12.803531132137117,12
587,"In real world applications , users often submit natural language questions that are longer than needed and include peripheral information that increases the complexity of the question , leading to substantially more false positives in answer retrieval .",0,0.89843976,49.387462108873876,38
587,"In this paper , we study neural abstractive models for medical question summarization .",1,0.9087191,31.189198133905233,14
587,"We introduce the MeQSum corpus of 1,000 summarized consumer health questions .",2,0.61782,157.80045189463266,12
587,We explore data augmentation methods and evaluate state-of-the-art neural abstractive models on this new task .,2,0.5354236,12.284308494185355,22
587,"In particular , we show that semantic augmentation from question datasets improves the overall performance , and that pointer-generator networks outperform sequence-to-sequence attentional models on this task , with a ROUGE-1 score of 44.16 % .",3,0.95164114,37.34039448251742,42
587,We also present a detailed error analysis and discuss directions for improvement that are specific to question summarization .,3,0.57975,36.96095514684022,19
588,Multi-sentence compression ( MSC ) aims to generate a grammatical but reduced compression from multiple input sentences while retaining their key information .,0,0.91097534,51.50581616216761,23
588,Previous dominating approach for MSC is the extraction-based word graph approach .,0,0.83687806,68.84350203420544,14
588,A few variants further leveraged lexical substitution to yield more abstractive compression .,3,0.5039704,128.72500868364824,13
588,"However , two limitations exist .",0,0.7426523,158.85301421836954,6
588,"First , the word graph approach that simply concatenates fragments from multiple sentences may yield non-fluent or ungrammatical compression .",0,0.6597463,51.391200087344906,20
588,"Second , lexical substitution is often inappropriate without the consideration of context information .",0,0.8640551,72.37468162105421,14
588,"To tackle the above-mentioned issues , we present a neural rewriter for multi-sentence compression that does not need any parallel corpus .",2,0.38379717,26.36964920402376,22
588,Empirical studies have shown that our approach achieves comparable results upon automatic evaluation and improves the grammaticality of compression based on human evaluation .,3,0.70817876,23.67811628525721,24
588,"A parallel corpus with more than 140,000 ( sentence group , compression ) pairs is also constructed as a by-product for future research .",3,0.49011984,82.88888473451954,24
589,"This paper focuses on the topic of inferential machine comprehension , which aims to fully understand the meanings of given text to answer generic questions , especially the ones needed reasoning skills .",1,0.8967344,58.01542941900502,33
589,"In particular , we first encode the given document , question and options in a context aware way .",2,0.7487688,119.5036187075498,19
589,We then propose a new network to solve the inference problem by decomposing it into a series of attention-based reasoning steps .,2,0.48160404,16.585975856556328,24
589,The result of the previous step acts as the context of next step .,0,0.5509446,52.04474624264062,14
589,"To make each step can be directly inferred from the text , we design an operational cell with prior structure .",2,0.74000585,153.84269718719875,21
589,"By recursively linking the cells , the inferred results are synthesized together to form the evidence chain for reasoning , where the reasoning direction can be guided by imposing structural constraints to regulate interactions on the cells .",3,0.36535367,85.16682892967184,38
589,"Moreover , a termination mechanism is introduced to dynamically determine the uncertain reasoning depth , and the network is trained by reinforcement learning .",2,0.67066914,79.98203780783494,24
589,"Experimental results on 3 popular data sets , including MCTest , RACE and MultiRC , demonstrate the effectiveness of our approach .",3,0.8834436,38.23987949005649,22
590,Multi-passage reading comprehension requires the ability to combine cross-passage information and reason over multiple passages to infer the answer .,0,0.92672265,19.722888493279875,20
590,"In this paper , we introduce the Dynamic Self-attention Network ( DynSAN ) for multi-passage reading comprehension task , which processes cross-passage information at token-level and meanwhile avoids substantial computational costs .",1,0.8477053,38.67056241676737,34
590,"The core module of the dynamic self-attention is a proposed gated token selection mechanism , which dynamically selects important tokens from a sequence .",2,0.358298,44.19676979218153,24
590,These chosen tokens will attend to each other via a self-attention mechanism to model long-range dependencies .,2,0.4591059,30.157174451920014,18
590,"Besides , convolutional layers are combined with the dynamic self-attention to enhance the model ’s capacity of extracting local semantic .",2,0.43009645,41.43529337649893,21
590,"The experimental results show that the proposed DynSAN achieves new state-of-the-art performance on the SearchQA , Quasar-T and WikiHop datasets .",3,0.9735381,24.0188220768477,27
590,Further ablation study also validates the effectiveness of our model components .,3,0.97758657,35.710029414536685,12
591,"To bridge the gap between Machine Reading Comprehension ( MRC ) models and human beings , which is mainly reflected in the hunger for data and the robustness to noise , in this paper , we explore how to integrate the neural networks of MRC models with the general knowledge of human beings .",1,0.84507036,20.512935730713213,54
591,"On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage-question pair .",2,0.66443056,50.762475076946465,30
591,"On the other hand , we propose an end-to-end MRC model named as Knowledge Aided Reader ( KAR ) , which explicitly uses the above extracted general knowledge to assist its attention mechanisms .",2,0.53671646,50.80313227433466,35
591,"Based on the data enrichment method , KAR is comparable in performance with the state-of-the-art MRC models , and significantly more robust to noise than them .",3,0.92095745,25.59085203644692,33
591,"When only a subset ( 20 %-80 % ) of the training examples are available , KAR outperforms the state-of-the-art MRC models by a large margin , and is still reasonably robust to noise .",3,0.86045575,24.245013423140445,42
592,"This study tackles generative reading comprehension ( RC ) , which consists of answering questions based on textual evidence and natural language generation ( NLG ) .",1,0.82982063,35.1212185773685,27
592,"We propose a multi-style abstractive summarization model for question answering , called Masque .",1,0.4860285,72.4347555704529,14
592,The proposed model has two key characteristics .,3,0.6136125,30.592909332778465,8
592,"First , unlike most studies on RC that have focused on extracting an answer span from the provided passages , our model instead focuses on generating a summary from the question and multiple passages .",3,0.50283307,54.19665962692112,35
592,This serves to cover various answer styles required for real-world applications .,1,0.40419403,77.03811274270379,12
592,"Second , whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model , our approach learns multi-style answers within a model to improve the NLG capability for all styles involved .",2,0.45979905,88.26257354625126,41
592,This also enables our model to give an answer in the target style .,3,0.7381411,50.61845169528444,14
592,Experiments show that our model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA .,3,0.90383255,14.031499006756183,34
592,We observe that the transfer of the style-independent NLG capability to the target style is the key to its success .,3,0.96333563,29.791112184199246,22
593,This paper considers the reading comprehension task in which multiple documents are given as input .,1,0.64121556,25.71391805691189,16
593,"Prior work has shown that a pipeline of retriever , reader , and reranker can improve the overall performance .",0,0.84075326,36.7330376124633,20
593,"However , the pipeline system is inefficient since the input is re-encoded within each module , and is unable to leverage upstream components to help downstream training .",0,0.7245197,71.5984439629653,28
593,"In this work , we present RE3QA , a unified question answering model that combines context retrieving , reading comprehension , and answer reranking to predict the final answer .",1,0.8357353,53.19945424087423,30
593,"Unlike previous pipelined approaches , RE3QA shares contextualized text representation across different components , and is carefully designed to use high-quality upstream outputs ( e.g. , retrieved context or candidate answers ) for directly supervising downstream modules ( e.g. , the reader or the reranker ) .",0,0.56606317,82.23007432957824,47
593,"As a result , the whole network can be trained end-to-end to avoid the context inconsistency problem .",3,0.6259597,27.1694977110215,19
593,Experiments show that our model outperforms the pipelined baseline and achieves state-of-the-art results on two versions of TriviaQA and two variants of SQuAD .,3,0.9457279,6.357262949736294,30
594,This paper is concerned with the task of multi-hop open-domain Question Answering ( QA ) .,1,0.74649876,12.484623254448183,16
594,This task is particularly challenging since it requires the simultaneous performance of textual reasoning and efficient searching .,0,0.9187411,42.98250657477832,18
594,"We present a method for retrieving multiple supporting paragraphs , nested amidst a large knowledge base , which contain the necessary evidence to answer a given question .",1,0.3575833,84.99983917854726,28
594,Our method iteratively retrieves supporting paragraphs by forming a joint vector representation of both a question and a paragraph .,2,0.6048439,51.937697246582324,20
594,The retrieval is performed by considering contextualized sentence-level representations of the paragraphs in the knowledge source .,2,0.6434464,38.57128240365462,18
594,"Our method achieves state-of-the-art performance over two well-known datasets , SQuAD-Open and HotpotQA , which serve as our single-and multi-hop open-domain QA benchmarks , respectively .",3,0.68273073,18.727640676910003,36
595,Conversational machine reading systems help users answer high-level questions ( e.g .,0,0.9099217,37.385558177597176,13
595,determine if they qualify for particular government benefits ) when they do not know the exact rules by which the determination is made ( e.g .,0,0.671692,48.62481776577161,26
595,whether they need certain income levels or veteran status ) .,3,0.42246297,201.37351680571263,11
595,The key challenge is that these rules are only provided in the form of a procedural text ( e.g .,0,0.7922653,25.573841070644516,20
595,guidelines from government website ) which the system must read to figure out what to ask the user .,0,0.40526548,84.92655006572132,19
595,We present a new conversational machine reading model that jointly extracts a set of decision rules from the procedural text while reasoning about which are entailed by the conversational history and which still need to be edited to create questions for the user .,2,0.35547593,41.70469483739551,44
595,"On the recently introduced ShARC conversational machine reading dataset , our Entailment-driven Extract and Edit network ( E3 ) achieves a new state-of-the-art , outperforming existing systems as well as a new BERT-based baseline .",3,0.66856235,37.729260862477915,45
595,"In addition , by explicitly highlighting which information still needs to be gathered , E3 provides a more explainable alternative to prior work .",3,0.79022634,84.37863067311449,24
595,We release source code for our models and experiments at https://github.com/vzhong/e3 .,2,0.46353495,16.347824018766232,12
596,"The process of knowledge acquisition can be viewed as a question-answer game between a student and a teacher in which the student typically starts by asking broad , open-ended questions before drilling down into specifics ( Hintikka , 1981 ; Hakkarainen and Sintonen , 2002 ) .",0,0.91525096,40.362100383649356,49
596,This pedagogical perspective motivates a new way of representing documents .,0,0.68679994,63.69892281357087,11
596,"In this paper , we present SQUASH ( Specificity-controlled Question-Answer Hierarchies ) , a novel and challenging text generation task that converts an input document into a hierarchy of question-answer pairs .",1,0.7608984,26.29632529631784,38
596,"Users can click on high-level questions ( e.g. , “ Why did Frodo leave the Fellowship ? ” ) to reveal related but more specific questions ( e.g. , “ Who did Frodo leave with ? ” ) .",3,0.538756,17.329331726112425,40
596,"Using a question taxonomy loosely based on Lehnert ( 1978 ) , we classify questions in existing reading comprehension datasets as either GENERAL or SPECIFIC .",2,0.8501493,76.842639023009,26
596,We then use these labels as input to a pipelined system centered around a conditional neural language model .,2,0.7946193,46.045339861142125,19
596,We extensively evaluate the quality of the generated QA hierarchies through crowdsourced experiments and report strong empirical results .,3,0.62375593,34.87942273761733,19
597,Question answering ( QA ) using textual sources for purposes such as reading comprehension ( RC ) has attracted much attention .,0,0.967612,42.159069483550624,22
597,"This study focuses on the task of explainable multi-hop QA , which requires the system to return the answer with evidence sentences by reasoning and gathering disjoint pieces of the reference texts .",1,0.8823661,50.69221905544566,33
597,It proposes the Query Focused Extractor ( QFE ) model for evidence extraction and uses multi-task learning with the QA model .,2,0.5249602,51.379438921127885,22
597,QFE is inspired by extractive summarization models ;,2,0.36607277,190.53346754473404,8
597,"compared with the existing method , which extracts each evidence sentence independently , it sequentially extracts evidence sentences by using an RNN with an attention mechanism on the question sentence .",2,0.64933866,54.17017704354709,31
597,It enables QFE to consider the dependency among the evidence sentences and cover important information in the question sentence .,3,0.6356778,132.9232558936874,20
597,Experimental results show that QFE with a simple RC baseline model achieves a state-of-the-art evidence extraction score on HotpotQA .,3,0.97933745,25.975003610377794,26
597,"Although designed for RC , it also achieves a state-of-the-art evidence extraction score on FEVER , which is a recognizing textual entailment task on a large textual database .",3,0.7569465,50.215292344699854,35
598,Machine reading comprehension ( MRC ) is a crucial and challenging task in NLP .,0,0.96845746,17.078653518222826,15
598,"Recently , pre-trained language models ( LMs ) , especially BERT , have achieved remarkable success , presenting new state-of-the-art results in MRC .",0,0.8754897,20.373810528895607,30
598,"In this work , we investigate the potential of leveraging external knowledge bases ( KBs ) to further improve BERT for MRC .",1,0.94283205,36.047548807945574,23
598,"We introduce KT-NET , which employs an attention mechanism to adaptively select desired knowledge from KBs , and then fuses selected knowledge with BERT to enable context-and knowledge-aware predictions .",2,0.65426624,76.0782059610589,36
598,We believe this would combine the merits of both deep LMs and curated KBs towards better MRC .,3,0.9251739,154.77539807515797,18
598,"Experimental results indicate that KT-NET offers significant and consistent improvements over BERT , outperforming competitive baselines on ReCoRD and SQuAD1.1 benchmarks .",3,0.9754625,32.0312197477396,24
598,"Notably , it ranks the 1st place on the ReCoRD leaderboard , and is also the best single model on the SQuAD1.1 leaderboard at the time of submission ( March 4th , 2019 ) .",3,0.65441275,39.29425028961483,35
599,Open-domain question answering ( OpenQA ) aims to answer questions through text retrieval and reading comprehension .,0,0.9411424,23.64159678624103,17
599,"Recently , lots of neural network-based models have been proposed and achieved promising results in OpenQA .",0,0.85416085,16.76632908201088,19
599,"However , the success of these models relies on a massive volume of training data ( usually in English ) , which is not available in many other languages , especially for those low-resource languages .",0,0.8765927,21.330149390971883,36
599,"Therefore , it is essential to investigate cross-lingual OpenQA .",0,0.68035936,28.582431990795175,10
599,"In this paper , we construct a novel dataset XQA for cross-lingual OpenQA research .",1,0.8273469,21.475272579056096,15
599,It consists of a training set in English as well as development and test sets in eight other languages .,2,0.51093715,27.507386139803728,20
599,"Besides , we provide several baseline systems for cross-lingual OpenQA , including two machine translation-based methods and one zero-shot cross-lingual method ( multilingual BERT ) .",2,0.59983844,23.361657395092323,28
599,"Experimental results show that the multilingual BERT model achieves the best results in almost all target languages , while the performance of cross-lingual OpenQA is still much lower than that of English .",3,0.96846765,12.810633448193244,33
599,"Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are , but also how difficult the question set of the target language is .",3,0.9855613,21.39664917635498,36
599,The XQA dataset is publicly available at http://github.com/thunlp/XQA .,0,0.4319782,4.533215248511533,9
600,We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar .,2,0.38035926,74.24547711655178,23
600,"In contrast to traditional formulations which learn a single stochastic grammar , our context-free rule probabilities are modulated by a per-sentence continuous latent variable , which induces marginal dependencies beyond the traditional context-free assumptions .",2,0.4238614,63.099294391350256,37
600,"Inference in this context-dependent grammar is performed by collapsed variational inference , in which an amortized variational posterior is placed on the continuous variable , and the latent trees are marginalized with dynamic programming .",2,0.70462584,75.01767489723815,35
600,Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models .,3,0.912435,13.941749975711422,32
601,"During the past decades , due to the lack of sufficient labeled data , most studies on cross-domain parsing focus on unsupervised domain adaptation , assuming there is no target-domain training data .",0,0.9403029,29.058342247360077,35
601,"However , unsupervised approaches make limited progress so far due to the intrinsic difficulty of both domain adaptation and parsing .",0,0.9219286,34.26090563346237,21
601,"This paper tackles the semi-supervised domain adaptation problem for Chinese dependency parsing , based on two newly-annotated large-scale domain-aware datasets .",1,0.79751164,28.403112317253807,24
601,"We propose a simple domain embedding approach to merge the source-and target-domain training data , which is shown to be more effective than both direct corpus concatenation and multi-task learning .",3,0.46950555,26.22341784659108,35
601,"In order to utilize unlabeled target-domain data , we employ the recent contextualized word representations and show that a simple fine-tuning procedure can further boost cross-domain parsing accuracy by large margin .",2,0.47152048,25.95310228565664,33
602,Head-driven phrase structure grammar ( HPSG ) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings .,0,0.90838456,154.42685213766697,21
602,This paper makes the first attempt to formulate a simplified HPSG by integrating constituent and dependency formal representations into head-driven phrase structure .,1,0.5619255,124.16299670240821,25
602,"Then two parsing algorithms are respectively proposed for two converted tree representations , division span and joint span .",2,0.7968677,255.1611712338156,19
602,"As HPSG encodes both constituent and dependency structure information , the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees .",3,0.69463056,38.79911496318841,44
602,"Our parser achieves new state-of-the-art performance for both parsing tasks on Penn Treebank ( PTB ) and Chinese Penn Treebank , verifying the effectiveness of joint learning constituent and dependency structures .",3,0.89593977,27.60895094940817,37
602,"In details , we report 95.84 F1 of constituent parsing and 97.00 % UAS of dependency parsing on PTB .",3,0.95326805,91.66842458862372,20
603,"In this work , we explore the way to perform named entity recognition ( NER ) using only unlabeled data and named entity dictionaries .",1,0.82439643,25.105455296788286,25
603,"To this end , we formulate the task as a positive-unlabeled ( PU ) learning problem and accordingly propose a novel PU learning algorithm to perform the task .",2,0.48611397,31.16914202972677,30
603,We prove that the proposed algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data .,3,0.8802476,63.00975565311736,22
603,"A key feature of the proposed method is that it does not require the dictionaries to label every entity within a sentence , and it even does not require the dictionaries to label all of the words constituting an entity .",3,0.65557146,14.718353592546036,41
603,This greatly reduces the requirement on the quality of the dictionaries and makes our method generalize well with quite simple dictionaries .,3,0.7809138,39.506087836953974,22
603,Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method .,3,0.8377386,14.764979393792036,15
603,We have published the source code at https://github.com/v-mipeng/LexiconNER .,3,0.5817671,21.131897613250842,9
604,"In Semantic Dependency Parsing ( SDP ) , semantic relations form directed acyclic graphs , rather than trees .",0,0.8054843,68.71068041093666,19
604,We propose a new iterative predicate selection ( IPS ) algorithm for SDP .,1,0.6498243,132.01335597017038,14
604,Our IPS algorithm combines the graph-based and transition-based parsing approaches in order to handle multiple semantic head words .,2,0.58137107,62.80269327737554,22
604,We train the IPS model using a combination of multi-task learning and task-specific policy gradient training .,2,0.8367467,29.02885758486976,18
604,"Trained this way , IPS achieves a new state of the art on the SemEval 2015 Task 18 datasets .",3,0.7177225,59.08919941750885,20
604,"Furthermore , we observe that policy gradient training learns an easy-first strategy .",3,0.9613583,170.93910021446737,15
605,Current state-of-the-art systems for sequence labeling are typically based on the family of Recurrent Neural Networks ( RNNs ) .,0,0.93816996,9.678041460759811,25
605,"However , the shallow connections between consecutive hidden states of RNNs and insufficient modeling of global information restrict the potential performance of those models .",0,0.8216492,77.30936014189469,25
605,"In this paper , we try to address these issues , and thus propose a Global Context enhanced Deep Transition architecture for sequence labeling named GCDT .",1,0.9040223,80.49165810330224,27
605,"We deepen the state transition path at each position in a sentence , and further assign every token with a global representation learned from the entire sentence .",2,0.80295545,87.14547842438363,28
605,"Experiments on two standard sequence labeling tasks show that , given only training data and the ubiquitous word embeddings ( Glove ) , our GCDT achieves 91.96 F1 on the CoNLL03 NER task and 95.43 F1 on the CoNLL2000 Chunking task , which outperforms the best reported results under the same settings .",3,0.8978919,35.120967371736,53
605,"Furthermore , by leveraging BERT as an additional resource , we establish new state-of-the-art results with 93.47 F1 on NER and 97.30 F1 on Chunking .",3,0.89965844,17.130313921247208,32
606,Unsupervised PCFG inducers hypothesize sets of compact context-free rules as explanations for sentences .,0,0.452406,202.63621649296192,14
606,"PCFG induction not only provides tools for low-resource languages , but also plays an important role in modeling language acquisition ( Bannard et al. , 2009 ; Abend et al .",0,0.5528835,45.69814581033421,31
606,2017 ) .,4,0.74829686,133.12598158987706,3
606,"However , current PCFG induction models , using word tokens as input , are unable to incorporate semantics and morphology into induction , and may encounter issues of sparse vocabulary when facing morphologically rich languages .",0,0.8086114,138.8101121133366,36
606,"This paper describes a neural PCFG inducer which employs context embeddings ( Peters et al. , 2018 ) in a normalizing flow model ( Dinh et al. , 2015 ) to extend PCFG induction to use semantic and morphological information .",1,0.616144,53.79281170457225,41
606,Linguistically motivated sparsity and categorical distance constraints are imposed on the inducer as regularization .,2,0.49412856,75.83069302518514,15
606,Experiments show that the PCFG induction model with normalizing flow produces grammars with state-of-the-art accuracy on a variety of different languages .,3,0.95994896,22.928845445185956,28
606,"Ablation further shows a positive effect of normalizing flow , context embeddings and proposed regularizers .",3,0.9708501,125.76312496368098,16
607,"In unsupervised grammar induction , data likelihood is known to be only weakly correlated with parsing accuracy , especially at convergence after multiple runs .",0,0.8359093,89.55082038618485,25
607,"In order to find a better indicator for quality of induced grammars , this paper correlates several linguistically-and psycholinguistically-motivated predictors to parsing accuracy on a large multilingual grammar induction evaluation data set .",1,0.71706545,52.373324561858986,36
607,Results show that variance of average surprisal ( VAS ) better correlates with parsing accuracy than data likelihood and that using VAS instead of data likelihood for model selection provides a significant accuracy boost .,3,0.98776126,59.62157252412233,35
607,Further evidence shows VAS to be a better candidate than data likelihood for predicting word order typology classification .,3,0.9758303,93.41110102888258,19
607,"Analyses show that VAS seems to separate content words from function words in natural language grammars , and to better arrange words with different frequencies into separate classes that are more consistent with linguistic theory .",3,0.97055256,66.42294391525046,36
608,"Due to limitation of labeled resources , cross-domain named entity recognition ( NER ) has been a challenging task .",0,0.96625763,35.269718371748915,20
608,"Most existing work considers a supervised setting , making use of labeled data for both the source and target domains .",0,0.76079035,44.17865983145901,21
608,A disadvantage of such methods is that they cannot train for domains without NER data .,0,0.8236537,32.98181737230672,16
608,"To address this issue , we consider using cross-domain LM as a bridge cross-domains for NER domain adaptation , performing cross-domain and cross-task knowledge transfer by designing a novel parameter generation network .",2,0.44825816,49.77351926260636,35
608,"Results show that our method can effectively extract domain differences from cross-domain LM contrast , allowing unsupervised domain adaptation while also giving state-of-the-art results among supervised domain adaptation methods .",3,0.9880163,30.62647245894335,36
609,We investigate the problem of efficiently incorporating high-order features into neural graph-based dependency parsing .,1,0.78576016,30.54447962136206,17
609,"Instead of explicitly extracting high-order features from intermediate parse trees , we develop a more powerful dependency tree node representation which captures high-order information concisely and efficiently .",2,0.65590894,37.97944700195763,28
609,We use graph neural networks ( GNNs ) to learn the representations and discuss several new configurations of GNN ’s updating and aggregation functions .,2,0.7817464,40.73142946503566,25
609,"Experiments on PTB show that our parser achieves the best UAS and LAS on PTB ( 96.0 % , 94.3 % ) among systems without using any external resources .",3,0.9713624,36.31876351256035,30
610,"Minimalist Grammars ( Stabler , 1997 ) are a computationally oriented , and rigorous formalisation of many aspects of Chomsky ’s ( 1995 ) Minimalist Program .",0,0.7928778,139.14742747769483,27
610,This paper presents the first ever application of this formalism to the task of realistic wide-coverage parsing .,1,0.77322364,32.53998232834198,19
610,"The parser uses a linguistically expressive yet highly constrained grammar , together with an adaptation of the A* search algorithm currently used in CCG parsing ( Lewis and Steedman , 2014 ; Lewis et al. , 2016 ) , with supertag probabilities provided by a bi-LSTM neural network supertagger trained on MGbank , a corpus of MG derivation trees .",2,0.67463416,114.11050926857193,60
610,We report on some promising initial experimental results for overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies .,3,0.68045676,67.10203879538447,25
610,"Finally , although like other MG parsers , ours has a high order polynomial worst case time complexity , we show that in practice its expected time complexity is cubic in the length of the sentence .",3,0.9053631,73.83403479124446,37
610,The parser is publicly available .,0,0.3796592,73.06646001730135,6
611,Sarcasm is a subtle form of language in which people express the opposite of what is implied .,0,0.9377552,15.518087853979976,18
611,Previous works of sarcasm detection focused on texts .,0,0.85950226,129.72638401684915,9
611,"However , more and more social media platforms like Twitter allow users to create multi-modal messages , including texts , images , and videos .",0,0.9472793,27.623515250470092,25
611,It is insufficient to detect sarcasm from multi-model messages based only on texts .,0,0.6581027,58.10573931930182,14
611,"In this paper , we focus on multi-modal sarcasm detection for tweets consisting of texts and images in Twitter .",1,0.85506594,27.675118278960333,20
611,"We treat text features , image features and image attributes as three modalities and propose a multi-modal hierarchical fusion model to address this task .",2,0.7119095,33.651005870012604,25
611,"Our model first extracts image features and attribute features , and then leverages attribute features and bidirectional LSTM network to extract text features .",2,0.8172289,34.07735603012241,24
611,Features of three modalities are then reconstructed and fused into one feature vector for prediction .,2,0.59446496,52.931616002727154,16
611,We create a multi-modal sarcasm detection dataset based on Twitter .,2,0.8683274,22.286513802906374,11
611,Evaluation results on the dataset demonstrate the efficacy of our proposed model and the usefulness of the three modalities .,3,0.9695155,17.61752840628166,20
612,A huge volume of user-generated content is daily produced on social media .,0,0.966142,22.175785066340715,14
612,"To facilitate automatic language understanding , we study keyphrase prediction , distilling salient information from massive posts .",1,0.43562442,231.8782134285244,18
612,"While most existing methods extract words from source posts to form keyphrases , we propose a sequence-to-sequence ( seq2seq ) based neural keyphrase generation framework , enabling absent keyphrases to be created .",2,0.6448889,27.954930433772986,35
612,"Moreover , our model , being topic-aware , allows joint modeling of corpus-level latent topic representations , which helps alleviate data sparsity widely exhibited in social media language .",3,0.69785714,126.41800135162231,29
612,Experiments on three datasets collected from English and Chinese social media platforms show that our model significantly outperforms both extraction and generation models without exploiting latent topics .,3,0.87008053,23.225986930341502,28
612,"Further discussions show that our model learns meaningful topics , which interprets its superiority in social media keyphrase generation .",3,0.9767631,212.06858826342872,20
613,"The availability of large-scale online social data , coupled with computational methods can help us answer fundamental questions relat-ing to our social lives , particularly our health and well-being .",0,0.8921992,48.08071562134191,32
613,The # MeToo trend has led to people talking about personal experiences of harassment more openly .,0,0.85427195,50.82893826092835,17
613,This work at-tempts to aggregate such experiences of sex-ual abuse to facilitate a better understanding of social media constructs and to bring about social change .,3,0.6910441,63.33009997846534,26
613,It has been found that disclo-sure of abuse has positive psychological im-pacts .,0,0.78435326,139.03832343164038,13
613,"Hence , we contend that such informa-tion can leveraged to create better campaigns for social change by analyzing how users react to these stories and to obtain a better insight into the consequences of sexual abuse .",3,0.85788536,59.97048472799044,37
613,We use a three part Twitter-Specific Social Media Lan-guage Model to segregate personal recollec-tions of sexual harassment from Twitter posts .,2,0.9244067,115.4935459523875,22
613,An extensive comparison with state-of-the-art generic and specific models along with a de-tailed error analysis explores the merit of our proposed model .,3,0.7384765,38.11355241475837,27
614,"Hashtags are often employed on social media and beyond to add metadata to a textual utterance with the goal of increasing discoverability , aiding search , or providing additional semantics .",0,0.917817,88.23963917969265,31
614,"However , the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings .",0,0.8709412,79.08607404357734,32
614,"We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by framing it as a pairwise ranking problem between candidate segmentations .",2,0.7937779,39.646075337680166,32
614,Our novel neural approaches demonstrate 24.6 % error reduction in hashtag segmentation accuracy compared to the current state-of-the-art method .,3,0.9271555,26.524334164967517,26
614,"Finally , we demonstrate that a deeper understanding of hashtag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis , for which we achieved a 2.6 % increase in average recall on the SemEval 2017 sentiment analysis dataset .",3,0.9660501,25.71867590145509,43
615,"While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks , their potential usefulness for social-oriented tasks remains largely unexplored .",0,0.9226136,17.572725774952282,29
615,We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people .,3,0.77346826,23.198658811755493,18
615,"We evaluate our methodology quantitatively , on held-out affect lexicons , and qualitatively , through case examples .",2,0.61871636,187.23422309377008,20
615,"We find that contextualized word representations do encode meaningful affect information , but they are heavily biased towards their training data , which limits their usefulness to in-domain analyses .",3,0.9768048,47.98763184822096,31
615,We ultimately use our method to examine differences in portrayals of men and women .,3,0.45887506,27.32218378498377,15
616,"Claim verification is generally a task of verifying the veracity of a given claim , which is critical to many downstream applications .",0,0.9479595,17.26483740426176,23
616,"It is cumbersome and inefficient for human fact-checkers to find consistent pieces of evidence , from which solid verdict could be inferred against the claim .",0,0.8828248,63.753285107325254,27
616,"In this paper , we propose a novel end-to-end hierarchical attention network focusing on learning to represent coherent evidence as well as their semantic relatedness with the claim .",1,0.90304285,28.70432465478269,31
616,Our model consists of three main components : 1 ) A coherence-based attention layer embeds coherent evidence considering the claim and sentences from relevant articles ;,2,0.8074483,123.33970279548008,28
616,2 ) An entailment-based attention layer attends on sentences that can semantically infer the claim on top of the first attention ;,2,0.44027635,173.55294243076278,24
616,and 3 ) An output layer predicts the verdict based on the embedded evidence .,2,0.5879306,225.50430563193726,15
616,Experimental results on three public benchmark datasets show that our proposed model outperforms a set of state-of-the-art baselines .,3,0.929394,4.458640771403832,25
617,"The activities we do are linked to our interests , personality , political preferences , and decisions we make about the future .",0,0.83952576,77.53880957389447,23
617,"In this paper , we explore the task of predicting human activities from user-generated content .",1,0.87822706,20.884050407128893,17
617,We collect a dataset containing instances of social media users writing about a range of everyday activities .,2,0.88535184,52.690131840842234,18
617,We then use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and perform an automatic clustering of these activities .,2,0.83973706,20.218470448303826,31
617,We train a neural network model to make predictions about which clusters contain activities that were performed by a given user based on the text of their previous posts and self-description .,2,0.9102473,33.17363639551515,32
617,"Additionally , we explore the degree to which incorporating inferred user traits into our model helps with this prediction task .",3,0.42870623,62.35925884186626,21
618,"Inspired by Labov ’s seminal work on stylisticvariation as a function of social stratification , we develop and compare neural models thatpredict a person ’s presumed socio-economicstatus , obtained through distant supervision , from their writing style on social media .",2,0.49513763,70.02058781538209,41
618,Thefocus of our work is on identifying the mostimportant stylistic parameters to predict socio-economic group .,1,0.41645458,76.70707554327342,16
618,"In particular , we show theeffectiveness of morpho-syntactic features aspredictors of style , in contrast to lexical fea-tures , which are good predictors of topic .",3,0.83530873,38.52893918233172,26
619,Identifying the political perspective shaping the way news events are discussed in the media is an important and challenging task .,0,0.85917115,22.3232711246978,21
619,"In this paper , we highlight the importance of contextualizing social information , capturing how this information is disseminated in social networks .",1,0.89272374,26.06338420921194,23
619,"We use Graph Convolutional Networks , a recently proposed neural architecture for representing relational information , to capture the documents ’ social context .",2,0.85117555,51.94658893842187,24
619,"We show that social information can be used effectively as a source of distant supervision , and when direct supervision is available , even little social information can significantly improve performance .",3,0.95869654,40.66166625582096,32
620,This paper presents computational approaches for automatically detecting critical plot twists in reviews of media products .,1,0.8825178,109.64051559372328,17
620,"First , we created a large-scale book review dataset that includes fine-grained spoiler annotations at the sentence-level , as well as book and ( anonymized ) user information .",2,0.9317848,52.83790809583392,33
620,"Second , we carefully analyzed this dataset , and found that : spoiler language tends to be book-specific ;",3,0.91695875,199.1956846634181,19
620,spoiler distributions vary greatly across books and review authors ;,0,0.68064225,1239.2457239004623,10
620,and spoiler sentences tend to jointly appear in the latter part of reviews .,3,0.83376956,173.02699590563716,14
620,"Third , inspired by these findings , we developed an end-to-end neural network architecture to detect spoiler sentences in review corpora .",2,0.4436098,26.987220150995775,24
620,Quantitative and qualitative results demonstrate that the proposed method substantially outperforms existing baselines .,3,0.9717372,11.047656696692975,14
621,"Celebrities are among the most prolific users of social media , promoting their personas and rallying followers .",0,0.9096731,23.8128620908996,18
621,"This activity is closely tied to genuine writing samples , which makes them worthy research subjects in many respects , not least profiling .",0,0.76449543,167.274081654859,24
621,With this paper we introduce the Webis Celebrity Corpus 2019 .,1,0.6161532,327.1466728954838,11
621,"For its construction the Twitter feeds of 71,706 verified accounts have been carefully linked with their respective Wikidata items , crawling both .",2,0.7250032,192.46978757069982,23
621,"After cleansing , the resulting profiles contain an average of 29,968 words per profile and up to 239 pieces of personal information .",3,0.66261417,74.27773625781965,23
621,"A cross-evaluation that checked the correct association of Twitter account and Wikidata item revealed an error rate of only 0.6 % , rendering the profiles highly reliable .",3,0.94207096,104.97946133926943,28
621,"Our corpus comprises a wide cross-section of local and global celebrities , forming a unique combination of scale , profile comprehensiveness , and label reliability .",3,0.4469991,116.89356820720359,26
621,We further establish the state of the art ’s profiling performance by evaluating the winning approaches submitted to the PAN gender prediction tasks in a transfer learning experiment .,3,0.4877524,121.12840219844183,29
621,"They are only outperformed by our own deep learning approach , which we also use to exemplify celebrity occupation prediction for the first time .",3,0.8687376,64.27561692118336,25
622,"Ranking comments on an online news service is a practically important task for the service provider , and thus there have been many studies on this task .",0,0.9528912,37.092437040409806,28
622,"However , most of them considered users ’ positive feedback , such as “ Like ”-button clicks , as a quality measure .",3,0.5916005,116.39693092959278,25
622,"In this paper , we address directly evaluating the quality of comments on the basis of “ constructiveness , ” separately from user feedback .",1,0.8488682,87.99274700788803,25
622,Japanese comments with constructiveness scores ( C-scores ) .,2,0.6854311,122.32280746737786,9
622,"Our experiments clarify that C-scores are not always related to users ’ positive feedback , and the performance of pairwise ranking models tends to be enhanced by the variation of comments rather than articles .",3,0.9751813,67.64093663423652,35
623,"Accompanied by modern industrial developments , air pollution has already become a major concern for human health .",0,0.9560685,39.32171889898541,18
623,"Hence , air quality measures , such as the concentration of PM2.5 , have attracted increasing attention .",0,0.9447803,40.81094401663493,18
623,"Even some studies apply historical measurements into air quality forecast , the changes of air quality conditions are still hard to monitor .",0,0.893114,154.73835357323688,23
623,"In this paper , we propose to exploit social media and natural language processing techniques to enhance air quality prediction .",1,0.92507565,32.17576660089042,21
623,Social media users are treated as social sensors with their findings and locations .,0,0.76650846,132.03828607774966,14
623,"After filtering noisy tweets using word selection and topic modeling , a deep learning model based on convolutional neural networks and over-tweet-pooling is proposed to enhance air quality prediction .",2,0.64401764,56.83582534793327,32
623,We conduct experiments on 7-month real-world Twitter datasets in the five most heavily polluted states in the USA .,2,0.9139383,39.9597670856646,22
623,The results show that our approach significantly improves air quality prediction over the baseline that does not use social media by 6.9 % to 17.7 % in macro-F1 scores .,3,0.9873174,25.032280359801852,32
624,"In this paper , we investigate the importance of social network information compared to content information in the prediction of a Twitter user ’s occupational class .",1,0.94376075,34.21862740845014,27
624,"We show that the content information of a user ’s tweets , the profile descriptions of a user ’s follower / following community , and the user ’s social network provide useful information for classifying a user ’s occupational group .",3,0.9351913,29.274598366278987,41
624,"In our study , we extend an existing data set for this problem , and we achieve significantly better performance by using social network homophily that has not been fully exploited in previous work .",1,0.38015032,34.265586467268484,35
624,"In our analysis , we found that by using the graph convolutional network to exploit social homophily , we can achieve competitive performance on this data set with just a small fraction of the training data .",3,0.9638573,22.22807807624057,37
625,Domain adaptation is an essential task in dialog system building because there are so many new dialog tasks created for different needs every day .,0,0.8873054,45.77767425983248,25
625,Collecting and annotating training data for these new tasks is costly since it involves real user interactions .,0,0.8342169,28.86635871875827,18
625,We propose a domain adaptive dialog generation method based on meta-learning ( DAML ) .,2,0.40726164,84.31556598156327,15
625,DAML is an end-to-end trainable dialog system model that learns from multiple rich-resource tasks and then adapts to new domains with minimal training samples .,0,0.67377365,21.23686207965058,28
625,We train a dialog system model using multiple rich-resource single-domain dialog data by applying the model-agnostic meta-learning algorithm to dialog domain .,2,0.8980252,44.329401581550485,23
625,The model is capable of learning a competitive dialog system on a new domain with only a few training examples in an efficient manner .,3,0.71296567,21.960200119451745,25
625,The two-step gradient updates in DAML enable the model to learn general features across multiple tasks .,2,0.44890872,66.2830021795044,18
625,"We evaluate our method on a simulated dialog dataset and achieve state-of-the-art performance , which is generalizable to new tasks .",3,0.5361842,18.12437658557534,27
626,"Writers often rely on plans or sketches to write long stories , but most current language models generate word by word from left to right .",0,0.9238204,63.56347742378569,26
626,"We explore coarse-to-fine models for creating narrative texts of several hundred words , and introduce new models which decompose stories by abstracting over actions and entities .",2,0.51471764,72.31096798047396,28
626,"The model first generates the predicate-argument structure of the text , where different mentions of the same entity are marked with placeholder tokens .",2,0.67091244,50.581294731341494,26
626,"It then generates a surface realization of the predicate-argument structure , and finally replaces the entity placeholders with context-sensitive names and references .",2,0.48101443,75.56708389063378,25
626,Human judges prefer the stories from our models to a wide range of previous approaches to hierarchical text generation .,3,0.56809944,75.31868384976572,20
626,Extensive analysis shows that our methods can help improve the diversity and coherence of events and entities in generated stories .,3,0.97415394,26.578959218013186,21
627,Automatic argument generation is an appealing but challenging task .,0,0.92730254,27.99717827622996,10
627,"In this paper , we study the specific problem of counter-argument generation , and present a novel framework , CANDELA .",1,0.9080982,45.093720421051785,21
627,"It consists of a powerful retrieval system and a novel two-step generation model , where a text planning decoder first decides on the main talking points and a proper language style for each sentence , then a content realization decoder reflects the decisions and constructs an informative paragraph-level argument .",2,0.53732985,70.44570571001388,53
627,"Furthermore , our generation model is empowered by a retrieval system indexed with 12 million articles collected from Wikipedia and popular English news media , which provides access to high-quality content with diversity .",3,0.6016174,73.75764035294847,34
627,"Automatic evaluation on a large-scale dataset collected from Reddit shows that our model yields significantly higher BLEU , ROUGE , and METEOR scores than the state-of-the-art and non-trivial comparisons .",3,0.9149634,9.604745868025594,36
627,Human evaluation further indicates that our system arguments are more appropriate for refutation and richer in content .,3,0.9857372,90.0481703505774,18
628,"Recent neural language generation systems often hallucinate contents ( i.e. , producing irrelevant or contradicted facts ) , especially when trained on loosely corresponding pairs of the input structure and text .",0,0.92101276,157.7879616992083,32
628,"To mitigate this issue , we propose to integrate a language understanding module for data refinement with self-training iterations to effectively induce strong equivalence between the input data and the paired text .",3,0.36635914,37.474816504216435,33
628,Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50 % relative unaligned noise from the original data-text pairs .,3,0.9415343,42.627469317477996,27
628,A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator .,3,0.77353287,29.876346190529308,31
629,"Automatic commenting of online articles can provide additional opinions and facts to the reader , which improves user experience and engagement on social media platforms .",0,0.7355188,42.377348218013054,26
629,Previous work focuses on automatic commenting based solely on textual content .,0,0.9109119,68.95671734934488,12
629,"However , in real-scenarios , online articles usually contain multiple modal contents .",0,0.91256154,108.17008539112598,13
629,"For instance , graphic news contains plenty of images in addition to text .",0,0.8936718,159.342258607279,14
629,Contents other than text are also vital because they are not only more attractive to the reader but also may provide critical information .,0,0.8970209,29.99400115849803,24
629,"To remedy this , we propose a new task : cross-model automatic commenting ( CMAC ) , which aims to make comments by integrating multiple modal contents .",0,0.36357653,84.64669792411559,28
629,We construct a large-scale dataset for this task and explore several representative methods .,2,0.68765426,22.37499435092435,14
629,"Going a step further , an effective co-attention model is presented to capture the dependency between textual and visual information .",3,0.4442518,28.93037282135562,21
629,Evaluation results show that our proposed model can achieve better performance than competitive baselines .,3,0.97544724,8.861877434241926,15
630,"Recently , to incorporate external Knowledge Base ( KB ) information , one form of world knowledge , several end-to-end task-oriented dialog systems have been proposed .",0,0.90239006,57.172715028964966,31
630,"These models , however , tend to confound the dialog history with KB tuples and simply store them into one memory .",0,0.7785321,114.30563457286667,22
630,"Inspired by the psychological studies on working memory , we propose a working memory model ( WMM2Seq ) for dialog response generation .",2,0.36940217,42.823458087052195,23
630,"Our WMM2Seq adopts a working memory to interact with two separated long-term memories , which are the episodic memory for memorizing dialog history and the semantic memory for storing KB tuples .",2,0.7095195,68.2605450428911,32
630,"The working memory consists of a central executive to attend to the aforementioned memories , and a short-term storage system to store the “ activated ” contents from the long-term memories .",0,0.90382093,50.08061774168721,33
630,"Furthermore , we introduce a context-sensitive perceptual process for the token representations of dialog history , and then feed them into the episodic memory .",2,0.7548712,56.73769456539052,25
630,Extensive experiments on two task-oriented dialog datasets demonstrate that our WMM2Seq significantly outperforms the state-of-the-art results in several evaluation metrics .,3,0.9106477,13.394112626916984,29
631,We propose a new CogQA framework for multi-hop reading comprehension question answering in web-scale documents .,1,0.52164716,25.07181480187492,16
631,"Founded on the dual process theory in cognitive science , the framework gradually builds a cognitive graph in an iterative process by coordinating an implicit extraction module ( System 1 ) and an explicit reasoning module ( System 2 ) .",2,0.5330241,48.995602390540924,41
631,"While giving accurate answers , our framework further provides explainable reasoning paths .",3,0.907549,155.93086366110808,13
631,"Specifically , our implementation based on BERT and graph neural network efficiently handles millions of documents for multi-hop reasoning questions in the HotpotQA fullwiki dataset , achieving a winning joint F1 score of 34.9 on the leaderboard , compared to 23.1 of the best competitor .",3,0.8558289,54.20343091094917,46
632,Multi-hop reading comprehension ( RC ) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer .,0,0.9581697,36.50783111646055,29
632,"In this paper , we propose a new model to tackle the multi-hop RC problem .",1,0.87917954,17.91720207208572,16
632,"We introduce a heterogeneous graph with different types of nodes and edges , which is named as Heterogeneous Document-Entity ( HDE ) graph .",2,0.72140825,31.551271934181777,24
632,"The advantage of HDE graph is that it contains different granularity levels of information including candidates , documents and entities in specific document contexts .",0,0.536262,85.76177306899862,25
632,Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders .,3,0.78517884,59.5720115960814,22
632,We employ Graph Neural Networks ( GNN ) based message passing algorithms to accumulate evidences on the proposed HDE graph .,2,0.87536967,126.13999103575685,21
632,"Evaluated on the blind test set of the Qangaroo WikiHop data set , our HDE graph based single model delivers competitive result , and the ensemble model achieves the state-of-the-art performance .",3,0.8878492,61.155454444456446,38
633,Multi-hop reading comprehension requires the model to explore and connect relevant information from multiple sentences / documents in order to answer the question about the context .,0,0.8247395,27.621381482086534,27
633,"To achieve this , we propose an interpretable 3-module system called Explore-Propose-Assemble reader ( EPAr ) .",2,0.31134403,141.17069605847948,22
633,"First , the Document Explorer iteratively selects relevant documents and represents divergent reasoning chains in a tree structure so as to allow assimilating information from all chains .",2,0.7633018,76.29617911866794,28
633,The Answer Proposer then proposes an answer from every root-to-leaf path in the reasoning tree .,2,0.4574666,63.87061509116074,18
633,"Finally , the Evidence Assembler extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer .",2,0.6124436,55.084990667623444,25
633,"Intuitively , EPAr approximates the coarse-to-fine-grained comprehension behavior of human readers when facing multiple long documents .",3,0.62675357,88.94515494766803,19
633,We jointly optimize our 3 modules by minimizing the sum of losses from each stage conditioned on the previous stage ’s output .,2,0.8351449,92.1349540932341,23
633,"On two multi-hop reading comprehension datasets WikiHop and MedHop , our EPAr model achieves significant improvements over the baseline and competitive results compared to the state-of-the-art model .",3,0.933885,28.579474614858107,34
633,We also present multiple reasoning-chain-recovery tests and ablation studies to demonstrate our system ’s ability to perform interpretable and accurate reasoning .,3,0.69743663,42.24160266342287,25
634,Multi-hop question answering requires a model to connect multiple pieces of evidence scattered in a long context to answer the question .,0,0.89339364,22.796009154380133,22
634,"In this paper , we show that in the multi-hop HotpotQA ( Yang et al. , 2018 ) dataset , the examples often contain reasoning shortcuts through which models can directly locate the answer by word-matching the question with a sentence in the context .",1,0.49048975,46.63278119307824,45
634,We demonstrate this issue by constructing adversarial documents that create contradicting answers to the shortcut but do not affect the validity of the original answer .,3,0.3806928,53.612369396006684,26
634,"The performance of strong baseline models drops significantly on our adversarial test , indicating that they are indeed exploiting the shortcuts rather than performing multi-hop reasoning .",3,0.954832,68.79306520502533,27
634,"After adversarial training , the baseline ’s performance improves but is still limited on the adversarial test .",3,0.88567513,64.20115236373636,18
634,"Hence , we use a control unit that dynamically attends to the question at different reasoning hops to guide the model ’s multi-hop reasoning .",2,0.735597,69.81428479968824,25
634,We show that our 2-hop model trained on the regular data is more robust to the adversaries than the baseline .,3,0.9566289,30.296081223155838,23
634,"After adversarial training , it not only achieves significant improvements over its counterpart trained on regular data , but also outperforms the adversarially-trained baseline significantly .",3,0.8989352,21.186961586616732,28
634,"Finally , we sanity-check that these improvements are not obtained by exploiting potential new shortcuts in the adversarial data , but indeed due to robust multi-hop reasoning skills of the models .",3,0.9293624,64.0023201093777,34
635,"We propose a novel , path-based reasoning approach for the multi-hop reading comprehension task where a system needs to combine facts from multiple passages to answer a question .",1,0.5374618,23.60441887960398,31
635,"Although inspired by multi-hop reasoning over knowledge graphs , our proposed approach operates directly over unstructured text .",3,0.5747821,51.912999211398464,18
635,It generates potential paths through passages and scores them without any direct path supervision .,2,0.49463075,180.3434056607326,15
635,"The proposed model , named PathNet , attempts to extract implicit relations from text through entity pair representations , and compose them to encode each path .",2,0.60041046,123.61437060147533,27
635,"To capture additional context , PathNet also composes the passage representations along each path to compute a passage-based representation .",2,0.604583,116.98038636118966,22
635,"Unlike previous approaches , our model is then able to explain its reasoning via these explicit paths through the passages .",3,0.5708569,85.9862494358153,21
635,"We show that our approach outperforms prior models on the multi-hop Wikihop dataset , and also can be generalized to apply to the OpenBookQA dataset , matching state-of-the-art performance .",3,0.91604394,22.072020385334756,36
636,"For evaluating machine-generated texts , automatic methods hold the promise of avoiding collection of human judgments , which can be expensive and time-consuming .",0,0.9009127,60.258422145158924,28
636,"The most common automatic metrics , like BLEU and ROUGE , depend on exact word matching , an inflexible approach for measuring semantic similarity .",0,0.8271101,36.5084317074209,25
636,We introduce methods based on sentence mover ’s similarity ;,2,0.6580006,541.5632256312936,10
636,our automatic metrics evaluate text in a continuous space using word and sentence embeddings .,2,0.6372401,42.13002059370313,15
636,"We find that sentence-based metrics correlate with human judgments significantly better than ROUGE , both on machine-generated summaries ( average length of 3.4 sentences ) and human-authored essays ( average length of 7.5 ) .",3,0.9670911,22.922592435793856,39
636,We also show that sentence mover ’s similarity can be used as a reward when learning a generation model via reinforcement learning ;,3,0.9136782,84.48084886382635,23
636,"we present both automatic and human evaluations of summaries learned in this way , finding that our approach outperforms ROUGE .",3,0.7200455,33.41438423807887,21
637,Many complex discourse-level tasks can aid domain experts in their work but require costly expert annotations for data creation .,0,0.8506571,69.89396001764416,21
637,"To speed up and ease annotations , we investigate the viability of automatically generated annotation suggestions for such tasks .",1,0.66509813,82.68499187400997,20
637,"As an example , we choose a task that is particularly hard for both humans and machines : the segmentation and classification of epistemic activities in diagnostic reasoning texts .",2,0.54850876,55.595070632776384,30
637,We create and publish a new dataset covering two domains and carefully analyse the suggested annotations .,2,0.8121705,82.08488907455052,17
637,"We find that suggestions have positive effects on annotation speed and performance , while not introducing noteworthy biases .",3,0.9817625,81.34126946324288,19
637,"Envisioning suggestion models that improve with newly annotated texts , we contrast methods for continuous model adjustment and suggest the most effective setup for suggestions in future expert tasks .",3,0.59601814,171.1097044529838,30
638,Comparing between Deep Neural Network ( DNN ) models based on their performance on unseen data is crucial for the progress of the NLP field .,0,0.93157774,19.3151234165702,26
638,"However , these models have a large number of hyper-parameters and , being non-convex , their convergence point depends on the random values chosen at initialization and during training .",0,0.6792484,43.3946081602168,30
638,"Proper DNN comparison hence requires a comparison between their empirical score distributions on unseen data , rather than between single evaluation scores as is standard for more simple , convex models .",0,0.60019004,217.62583045030163,32
638,"In this paper , we propose to adapt to this problem a recently proposed test for the Almost Stochastic Dominance relation between two distributions .",1,0.83140534,57.420900849313206,25
638,"We define the criteria for a high quality comparison method between DNNs , and show , both theoretically and through analysis of extensive experimental results with leading DNN models for sequence tagging tasks , that the proposed test meets all criteria while previously proposed methods fail to do so .",3,0.44822294,64.4015850013023,50
638,We hope the test we propose here will set a new working practice in the NLP community .,3,0.9358429,48.04622335827384,18
639,It is standard practice in speech & language technology to rank systems according to their performance on a test set held out for evaluation .,0,0.9154139,33.92484880428612,25
639,"However , few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance , and few examine the stability of system ranking across multiple training-testing splits .",0,0.8733523,70.54106823420904,34
639,"We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018 , each of which claimed state-of-the-art performance on a widely-used “ standard split ” .",2,0.879315,31.413829328920535,39
639,"While we replicate results on the standard split , we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits .",3,0.94356436,147.06277089331917,28
639,We argue that randomly generated splits should be used in system evaluation .,3,0.84266436,83.74718537800462,13
640,Existing datasets for scoring text pairs in terms of semantic similarity contain instances whose resolution differs according to the degree of difficulty .,0,0.8770111,71.20961568272617,23
640,This paper proposes to distinguish obvious from non-obvious text pairs based on superficial lexical overlap and ground-truth labels .,1,0.8564713,41.713644654248846,21
640,We characterise existing datasets in terms of containing difficult cases and find that recently proposed models struggle to capture the non-obvious cases of semantic similarity .,3,0.6641233,37.87330793141029,26
640,We describe metrics that emphasise cases of similarity which require more complex inference and propose that these are used for evaluating systems for semantic similarity .,1,0.42478555,74.40428757525004,26
641,"Accurate , automatic evaluation of machine translation is critical for system tuning , and evaluating progress in the field .",0,0.89392936,99.31239790311112,20
641,"We proposed a simple unsupervised metric , and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences .",2,0.684004,48.81212729800382,25
641,"We find that these models rival or surpass all existing metrics in the WMT 2017 sentence-level and system-level tracks , and our trained model has a substantially higher correlation with human judgements than all existing metrics on the WMT 2017 to-English sentence level dataset .",3,0.96212476,32.252778748381836,49
642,"As the online world continues its exponential growth , interpersonal communication has come to play an increasingly central role in opinion formation and change .",0,0.9603462,31.49687168681276,25
642,"In order to help users better engage with each other online , we study a challenging problem of re-entry prediction foreseeing whether a user will come back to a conversation they once participated in .",1,0.3822034,33.83334799904948,35
642,We hypothesize that both the context of the ongoing conversations and the users ’ previous chatting history will affect their continued interests in future engagement .,1,0.36551422,53.18410913107922,26
642,"Specifically , we propose a neural framework with three main layers , each modeling context , user history , and interactions between them , to explore how the conversation context and user chatting history jointly result in their re-entry behavior .",2,0.6671763,80.47818735449057,41
642,We experiment with two large-scale datasets collected from Twitter and Reddit .,2,0.89844775,12.658692645318693,12
642,"Results show that our proposed framework with bi-attention achieves an F1 score of 61.1 on Twitter conversations , outperforming the state-of-the-art methods from previous work .",3,0.97752416,13.58356669844602,32
643,"Although there is an unprecedented effort to provide adequate responses in terms of laws and policies to hate content on social media platforms , dealing with hatred online is still a tough problem .",0,0.9053171,38.050187120352305,34
643,Tackling hate speech in the standard way of content deletion or user suspension may be charged with censorship and overblocking .,0,0.644676,75.66198279430621,21
643,"One alternate strategy , that has received little attention so far by the research community , is to actually oppose hate content with counter-narratives ( i.e .",0,0.6239327,52.85877373919752,28
643,informed textual responses ) .,2,0.49313623,1519.840543149537,5
643,"In this paper , we describe the creation of the first large-scale , multilingual , expert-based dataset of hate-speech / counter-narrative pairs .",1,0.915945,33.016489932227586,23
643,This dataset has been built with the effort of more than 100 operators from three different NGOs that applied their training and expertise to the task .,2,0.6148262,42.37723707914016,27
643,"Together with the collected data we also provide additional annotations about expert demographics , hate and response type , and data augmentation through translation and paraphrasing .",3,0.5868093,117.03087869354992,27
643,"Finally , we provide initial experiments to assess the quality of our data .",3,0.7515334,25.003077292588586,14
644,"Text in social media posts is frequently accompanied by images in order to provide content , supply context , or to express feelings .",0,0.93050945,64.50306621734902,24
644,This paper studies how the meaning of the entire tweet is composed through the relationship between its textual content and its image .,1,0.8998224,28.905848677714317,23
644,We build and release a data set of image tweets annotated with four classes which express whether the text or the image provides additional information to the other modality .,2,0.84004986,42.151109442801165,30
644,"We show that by combining the text and image information , we can build a machine learning approach that accurately distinguishes between the relationship types .",3,0.90445924,39.15528100940692,26
644,"Further , we derive insights into how these relationships are materialized through text and image content analysis and how they are impacted by user demographic traits .",3,0.64456326,36.14912281835878,27
644,"These methods can be used in several downstream applications including pre-training image tagging models , collecting distantly supervised data for image captioning , and can be directly used in end-user applications to optimize screen estate .",3,0.7364862,44.54622373651644,36
645,Understanding the structures of political debates ( which actors make what claims ) is essential for understanding democratic political decision making .,0,0.9132021,98.90097252129488,22
645,The vision of computational construction of such discourse networks from newspaper reports brings together political science and natural language processing .,0,0.8834747,86.13430995434318,21
645,"This paper presents three contributions towards this goal : ( a ) a requirements analysis , linking the task to knowledge base population ;",1,0.6306465,229.7049619921719,24
645,( b ) an annotated pilot corpus of migration claims based on German newspaper reports ;,2,0.52289104,608.8149009757828,16
645,( c ) initial modeling results .,3,0.7473553,1394.4121061094347,7
646,Research on social media has to date assumed that all posts from an account are authored by the same person .,0,0.9518466,20.522000135072382,21
646,"In this study , we challenge this assumption and study the linguistic differences between posts signed by the account owner or attributed to their staff .",1,0.82711244,72.93675843895495,26
646,We introduce a novel data set of tweets posted by U.S .,2,0.5728371,21.53201672561874,12
646,politicians who self-reported their tweets using a signature .,2,0.7134148,181.06487638049347,9
646,We analyze the linguistic topics and style features that distinguish the two types of tweets .,2,0.80158263,58.996179452554856,16
646,"Predictive results show that we are able to predict owner and staff attributed tweets with good accuracy , even when not using any training data from that account .",3,0.98146695,61.41999206957904,29
647,We investigate the impact of using author context on textual sarcasm detection .,1,0.7978602,69.71265775164517,13
647,We define author context as the embedded representation of their historical posts on Twitter and suggest neural models that extract these representations .,2,0.52666336,103.31647794188757,23
647,"We experiment with two tweet datasets , one labelled manually for sarcasm , and the other via tag-based distant supervision .",2,0.8793387,90.60931186754875,21
647,"We achieve state-of-the-art performance on the second dataset , but not on the one labelled manually , indicating a difference between intended sarcasm , captured by distant supervision , and perceived sarcasm , captured by manual labelling .",3,0.9233745,43.10156646869941,43
648,"We consider open domain event extraction , the task of extracting unconstraint types of events from news clusters .",2,0.6151051,101.11284430883383,19
648,"A novel latent variable neural model is constructed , which is scalable to very large corpus .",2,0.56827086,87.8931526611101,17
648,"A dataset is collected and manually annotated , with task-specific evaluation metrics being designed .",2,0.7873374,66.47779238629045,16
648,Results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction .,3,0.9864759,12.17449750264034,26
649,This paper presents a multi-level matching and aggregation network ( MLMAN ) for few-shot relation classification .,1,0.8414752,68.95688175516656,17
649,"Previous studies on this topic adopt prototypical networks , which calculate the embedding vector of a query instance and the prototype vector of the support set for each relation candidate independently .",0,0.89288443,111.7144118601856,32
649,"On the contrary , our proposed MLMAN model encodes the query instance and each support set in an interactive way by considering their matching information at both local and instance levels .",3,0.56356335,108.10047545177594,32
649,"The final class prototype for each support set is obtained by attentive aggregation over the representations of support instances , where the weights are calculated using the query instance .",2,0.64155585,99.02045288997243,30
649,"Experimental results demonstrate the effectiveness of our proposed methods , which achieve a new state-of-the-art performance on the FewRel dataset .",3,0.9431591,9.7374964715836,27
650,We introduce a conceptually simple and effective method to quantify the similarity between relations in knowledge bases .,1,0.3574244,22.27522012411003,18
650,"Specifically , our approach is based on the divergence between the conditional probability distributions over entity pairs .",2,0.72898674,27.316491040535023,18
650,"In this paper , these distributions are parameterized by a very simple neural network .",2,0.49428496,51.859830010862595,15
650,"Although computing the exact similarity is in-tractable , we provide a sampling-based method to get a good approximation .",2,0.38916254,47.55264376284226,21
650,We empirically show the outputs of our approach significantly correlate with human judgments .,3,0.91809434,26.222855159126087,14
650,"By applying our method to various tasks , we also find that ( 1 ) our approach could effectively detect redundant relations extracted by open information extraction ( Open IE ) models , that ( 2 ) even the most competitive models for relational classification still make mistakes among very similar relations , and that ( 3 ) our approach could be incorporated into negative sampling and softmax classification to alleviate these mistakes .",3,0.9150873,66.31996012507064,74
651,"General purpose relation extractors , which can model arbitrary relations , are a core aspiration in information extraction .",0,0.80870634,124.03470543636863,19
651,"Efforts have been made to build general purpose extractors that represent relations with their surface forms , or which jointly embed surface forms with relations from an existing knowledge graph .",0,0.88208073,57.37311453261336,31
651,"However , both of these approaches are limited in their ability to generalize .",0,0.8740169,15.795074525535815,14
651,"In this paper , we build on extensions of Harris ’ distributional hypothesis to relations , as well as recent advances in learning text representations ( specifically , BERT ) , to build task agnostic relation representations solely from entity-linked text .",1,0.72984433,96.84516261965372,44
651,We show that these representations significantly outperform previous work on exemplar based relation extraction ( FewRel ) even without using any of that task ’s training data .,3,0.948417,79.89368181829202,28
651,"We also show that models initialized with our task agnostic representations , and then tuned on supervised relation extraction datasets , significantly outperform the previous methods on SemEval 2010 Task 8 , KBP37 , and TACRED .",3,0.9351115,81.1489607688156,37
652,We present a novel semantic framework for modeling temporal relations and event durations that maps pairs of events to real-valued scales .,1,0.42879415,44.846938243922914,22
652,"We use this framework to construct the largest temporal relations dataset to date , covering the entirety of the Universal Dependencies English Web Treebank .",2,0.7565757,39.25162810446133,25
652,We use this dataset to train models for jointly predicting fine-grained temporal relations and event durations .,2,0.83093363,27.60240873271773,19
652,We report strong results on our data and show the efficacy of a transfer-learning approach for predicting categorical relations .,3,0.9712776,34.02859415366927,20
653,"We present FIESTA , a model selection approach that significantly reduces the computational resources required to reliably identify state-of-the-art performance from large collections of candidate models .",1,0.4431907,34.050116763300906,33
653,"Despite being known to produce unreliable comparisons , it is still common practice to compare model evaluations based on single choices of random seeds .",0,0.9393918,78.16935249022269,25
653,We show that reliable model selection also requires evaluations based on multiple train-test splits ( contrary to common practice in many shared tasks ) .,3,0.9483511,92.73080779873833,25
653,"Using bandit theory from the statistics literature , we are able to adaptively determine appropriate numbers of data splits and random seeds used to evaluate each model , focusing computational resources on the evaluation of promising models whilst avoiding wasting evaluations on models with lower performance .",2,0.52433753,94.85673372059705,47
653,"Furthermore , our user-friendly Python implementation produces confidence guarantees of correctly selecting the optimal model .",3,0.8906863,124.1368306154321,16
653,We evaluate our algorithms by selecting between 8 target-dependent sentiment analysis methods using dramatically fewer model evaluations than current model selection approaches .,2,0.5069883,121.67655422232633,24
654,Attention mechanisms have recently boosted performance on a range of NLP tasks .,0,0.9392498,19.165424222540928,13
654,"Because attention layers explicitly weight input components ’ representations , it is also often assumed that attention can be used to identify information that models found important ( e.g. , specific contextualized word tokens ) .",0,0.8804075,108.68156701450926,36
654,We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions .,2,0.49984968,44.41814266386456,26
654,"While we observe some ways in which higher attention weights correlate with greater impact on model predictions , we also find many ways in which this does not hold , i.e. , where gradient-based rankings of attention weights better predict their effects than their magnitudes .",3,0.9461498,43.40986096332445,48
654,"We conclude that while attention noisily predicts input components ’ overall importance to a model , it is by no means a fail-safe indicator .",3,0.98636186,80.31744176992642,27
655,Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP .,0,0.8889352,21.50235451049527,30
655,Here we present two methods based on Representational Similarity Analysis ( RSA ) and Tree Kernels ( TK ) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees .,1,0.69275105,29.731260651542545,46
655,"We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics , and show that they exhibit the expected pattern of results .",3,0.5499362,49.47948606665377,34
655,We then our methods to correlate neural representations of English sentences with their constituency parse trees .,2,0.7334619,98.15846254271408,17
656,The success of neural networks comes hand in hand with a desire for more interpretability .,0,0.90116847,19.616135222930733,16
656,We focus on text classifiers and make them more interpretable by having them provide a justification –a rationale –for their predictions .,2,0.40365717,33.2302196388916,23
656,We approach this problem by jointly training two neural network models : a latent model that selects a rationale ( i.e .,2,0.74714637,43.98334732423533,22
656,"a short and informative part of the input text ) , and a classifier that learns from the words in the rationale alone .",2,0.6453812,86.27719474598801,24
656,Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation .,0,0.78358966,206.38638371024308,26
656,We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE .,2,0.59435016,116.93688558727902,27
656,"In our formulation , we can tractably compute the expected value of penalties such as L0 , which allows us to directly optimise the model towards a pre-specified text selection rate .",3,0.5339371,69.95124104495868,32
656,"We show that our approach is competitive with previous work on rationale extraction , and explore further uses in attention mechanisms .",3,0.88507956,67.24053545305588,22
657,"Transformers have a potential of learning longer-term dependency , but are limited by a fixed-length context in the setting of language modeling .",0,0.73058504,58.954278299310516,27
657,We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence .,1,0.44803008,74.33609324973497,22
657,It consists of a segment-level recurrence mechanism and a novel positional encoding scheme .,0,0.48900706,41.4871606302115,15
657,"Our method not only enables capturing longer-term dependency , but also resolves the context fragmentation problem .",3,0.85300595,80.08106823271041,19
657,"As a result , Transformer-XL learns dependency that is 80 % longer than RNNs and 450 % longer than vanilla Transformers , achieves better performance on both short and long sequences , and is up to 1,800 + times faster than vanilla Transformers during evaluation .",3,0.92894495,52.13727122724945,48
657,"Notably , we improve the state-of-the-art results of bpc / perplexity to 0.99 on enwiki8 , 1.08 on text8 , 18.3 on WikiText-103 , 21.8 on One Billion Word , and 54.5 on Penn Treebank ( without finetuning ) .",3,0.90994465,47.28043181120152,48
657,"When trained only on WikiText-103 , Transformer-XL manages to generate reasonably coherent , novel text articles with thousands of tokens .",3,0.855069,158.353039137372,25
657,"Our code , pretrained models , and hyperparameters are available in both Tensorflow and PyTorch .",2,0.45549238,38.467240244042614,16
658,It has been previously noted that neural machine translation ( NMT ) is very sensitive to domain shift .,0,0.9545026,19.296302402682027,19
658,"In this paper , we argue that this is a dual effect of the highly lexicalized nature of NMT , resulting in failure for sentences with large numbers of unknown words , and lack of supervision for domain-specific words .",1,0.701018,39.47272105166036,40
658,"To remedy this problem , we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus .",2,0.48215416,11.899455506999404,28
658,"Specifically , we perform lexicon induction to extract an in-domain lexicon , and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences .",2,0.90149224,24.954725137942408,31
658,"In five domains over twenty pairwise adaptation settings and two model architectures , our method achieves consistent improvements without using any in-domain parallel sentences , improving up to 14 BLEU over unadapted models , and up to 2 BLEU over strong back-translation baselines .",3,0.82170653,35.35264392374819,47
659,Neural Machine Translation ( NMT ) has achieved notable success in recent years .,0,0.96171343,13.130863833718218,14
659,Such a framework usually generates translations in isolation .,0,0.761364,147.737373126115,9
659,"In contrast , human translators often refer to reference data , either rephrasing the intricate sentence fragments with common terms in source language , or just accessing to the golden translation directly .",0,0.9006835,149.98792494613474,33
659,"In this paper , we propose a Reference Network to incorporate referring process into translation decoding of NMT .",1,0.90222806,103.36654344634869,19
659,"To construct a reference book , an intuitive way is to store the detailed translation history with extra memory , which is computationally expensive .",0,0.78594965,89.87855220886625,25
659,"Instead , we employ Local Coordinates Coding ( LCC ) to obtain global context vectors containing monolingual and bilingual contextual information for NMT decoding .",2,0.78389895,46.21261934562517,25
659,Experimental results on Chinese-English and English-German tasks demonstrate that our proposed model is effective in improving the translation quality with lightweight computation cost .,3,0.9609892,13.125376965163813,26
660,"Non-Autoregressive Transformer ( NAT ) aims to accelerate the Transformer model through discarding the autoregressive mechanism and generating target words independently , which fails to exploit the target sequential information .",0,0.88741714,49.878188699533325,31
660,"Over-translation and under-translation errors often occur for the above reason , especially in the long sentence translation scenario .",0,0.8798814,53.59947372259043,19
660,"In this paper , we propose two approaches to retrieve the target sequential information for NAT to enhance its translation ability while preserving the fast-decoding property .",1,0.8793929,50.149906557879895,28
660,"Firstly , we propose a sequence-level training method based on a novel reinforcement algorithm for NAT ( Reinforce-NAT ) to reduce the variance and stabilize the training procedure .",2,0.7513333,33.68314540033895,33
660,"Secondly , we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder .",2,0.5852273,30.994913447178856,26
660,Experimental results on three translation tasks show that the Reinforce-NAT surpasses the baseline NAT system by a significant margin on BLEU without decelerating the decoding speed and the FS-decoder achieves comparable translation performance to the autoregressive Transformer with considerable speedup .,3,0.9520212,21.96347792458992,45
661,"Simultaneous translation , which translates sentences before they are finished , is use-ful in many scenarios but is notoriously dif-ficult due to word-order differences .",0,0.8783709,50.6693696433641,25
661,"While the conventional seq-to-seq framework is only suitable for full-sentence translation , we pro-pose a novel prefix-to-prefix framework for si-multaneous translation that implicitly learns to anticipate in a single translation model .",2,0.46173227,57.01976165449515,34
661,"Within this framework , we present a very sim-ple yet surprisingly effective “ wait-k ” policy trained to generate the target sentence concur-rently with the source sentence , but always k words behind .",2,0.43024924,118.95555581208077,35
661,Experiments show our strat-egy achieves low latency and reasonable qual-ity ( compared to full-sentence translation ) on 4 directions : zh↔en and de↔en .,3,0.94229585,142.25898450218995,24
662,Soft-attention based Neural Machine Translation ( NMT ) models have achieved promising results on several translation tasks .,0,0.9235579,10.983265487566118,18
662,"These models attend all the words in the source sequence for each target token , which makes them ineffective for long sequence translation .",0,0.76059043,64.63712330601388,24
662,"In this work , we propose a hard-attention based NMT model which selects a subset of source tokens for each target token to effectively handle long sequence translation .",1,0.7046635,38.016898849548646,29
662,"Due to the discrete nature of the hard-attention mechanism , we design a reinforcement learning algorithm coupled with reward shaping strategy to efficiently train it .",2,0.7268412,41.080389739500696,26
662,Experimental results show that the proposed model performs better on long sequences and thereby achieves significant BLEU score improvement on English-German ( EN-DE ) and English-French ( ENFR ) translation tasks compared to the soft attention based NMT .,3,0.95682216,25.038434289815488,45
663,"Neural machine translation ( NMT ) is notoriously sensitive to noises , but noises are almost inevitable in practice .",0,0.9628313,60.25072207129541,20
663,"One special kind of noise is the homophone noise , where words are replaced by other words with similar pronunciations .",0,0.87706196,21.376895636694343,21
663,"We propose to improve the robustness of NMT to homophone noises by 1 ) jointly embedding both textual and phonetic information of source sentences , and 2 ) augmenting the training dataset with homophone noises .",2,0.4595931,26.732341602199906,36
663,"Interestingly , to achieve better translation quality and more robustness , we found that most ( though not all ) weights should be put on the phonetic rather than textual information .",3,0.9846364,53.63767130862638,32
663,"Experiments show that our method not only significantly improves the robustness of NMT to homophone noises , but also surprisingly improves the translation quality on some clean test sets .",3,0.9579482,26.46939855054628,30
664,Automatic post-editing ( APE ) seeks to automatically refine the output of a black-box machine translation ( MT ) system through human post-edits .,0,0.94812405,33.94594159404003,26
664,"APE systems are usually trained by complementing human post-edited data with large , artificial data generated through back-translations , a time-consuming process often no easier than training a MT system from scratch .",0,0.91312975,88.98876553636043,37
664,"in this paper , we propose an alternative where we fine-tune pre-trained BERT models on both the encoder and decoder of an APE system , exploring several parameter sharing strategies .",1,0.8264604,23.98805595439175,32
664,By only training on a dataset of 23 K sentences for 3 hours on a single GPU we obtain results that are competitive with systems that were trained on 5 M artificial sentences .,3,0.7372203,46.21964933049294,34
664,When we add this artificial data our method obtains state-of-the-art results .,3,0.91247797,12.656381016694347,17
665,"Given a rough , word-by-word gloss of a source language sentence , target language natives can uncover the latent , fully-fluent rendering of the translation .",0,0.66450727,123.28572431931902,31
665,"In this work we explore this intuition by breaking translation into a two step process : generating a rough gloss by means of a dictionary and then ‘ translating ’ the resulting pseudo-translation , or ‘ Translationese ’ into a fully fluent translation .",1,0.46416998,60.16792232294589,44
665,"We build our Translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques , resulting in rapidly generated unsupervised neural MT systems for many source languages .",2,0.61401993,82.88599949621835,43
665,"We apply this process to 14 test languages , obtaining better or comparable translation results on high-resource languages than previously published unsupervised MT studies , and obtaining good quality results for low-resource languages that have never been used in an unsupervised MT scenario .",2,0.57713664,31.864431658337217,45
666,This paper proposes a novel method to inject custom terminology into neural machine translation at run time .,1,0.86752504,22.83221318061757,18
666,Previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms .,0,0.86337984,50.71950442321684,24
666,"While being effective , these constrained decoding methods add , however , significant computational overhead to the inference step , and , as we show in this paper , can be brittle when tested in realistic conditions .",3,0.5661091,98.38146333280005,38
666,In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input .,1,0.67151344,35.61511472618103,26
666,"Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding , but is also as fast as constraint-free decoding .",3,0.9528322,14.73549513417601,36
667,Self-attention networks have received increasing research attention .,0,0.9481602,9.082449175591442,8
667,"By default , the hidden states of each word are hierarchically calculated by attending to all words in the sentence , which assembles global information .",0,0.45546186,69.19868563600534,26
667,"However , several studies pointed out that taking all signals into account may lead to overlooking neighboring information ( e.g .",0,0.8160887,58.91614321029274,21
667,phrase pattern ) .,0,0.4294747,1570.6769591694126,4
667,"To address this argument , we propose a hybrid attention mechanism to dynamically leverage both of the local and global information .",2,0.38177457,33.68498447801886,22
667,"Specifically , our approach uses a gating scalar for integrating both sources of the information , which is also convenient for quantifying their contributions .",2,0.6650074,55.96824996124284,25
667,Experiments on various neural machine translation tasks demonstrate the effectiveness of the proposed method .,3,0.8267081,5.547945410150247,15
667,"The extensive analyses verify that the two types of contexts are complementary to each other , and our method gives highly effective improvements in their integration .",3,0.9359582,44.15886220369459,27
668,The training objective of neural machine translation ( NMT ) is to minimize the loss between the words in the translated sentences and those in the references .,0,0.9112995,23.712730331543195,28
668,"In NMT , there is a natural correspondence between the source sentence and the target sentence .",0,0.6580561,18.474362898216828,17
668,"However , this relationship has only been represented using the entire neural network and the training objective is computed in word-level .",0,0.5933748,50.0064288962894,23
668,"In this paper , we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence .",1,0.86836857,22.553784991165042,26
668,The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences .,3,0.90189266,25.106221466172425,29
668,Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance .,3,0.95245034,10.83105467026931,26
669,"In this paper , we propose a multilingual unsupervised NMT scheme which jointly trains multiple languages with a shared encoder and multiple decoders .",1,0.8706141,16.901684990542996,24
669,Our approach is based on denoising autoencoding of each language and back-translating between English and multiple non-English languages .,2,0.7161341,17.90815242707083,21
669,"This results in a universal encoder which can encode any language participating in training into an inter-lingual representation , and language-specific decoders .",3,0.4521883,47.182181183543754,24
669,Our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 BLEU points on WMT test sets .,3,0.95785964,18.582933302606612,32
669,"We also observe that even if we do not train the network for all possible translation directions , the network is still able to translate in a many-to-many fashion leveraging encoder ’s ability to generate interlingual representation .",3,0.9659166,24.976141235368164,41
670,Neural machine translation ( NMT ) takes deterministic sequences for source representations .,0,0.72726655,83.11638319020581,13
670,"However , either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes .",0,0.8416965,57.47527671129901,28
670,We hypothesize that the diversity in segmentations may affect the NMT performance .,3,0.48591083,33.1349591893087,13
670,"To integrate different segmentations with the state-of-the-art NMT model , Transformer , we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training .",2,0.6911145,33.87116873550747,35
670,We propose two methods : 1 ) lattice positional encoding and 2 ) lattice-aware self-attention .,2,0.64394397,32.52657127497209,18
670,These two methods can be used together and show complementary to each other to further improve translation performance .,3,0.91489756,22.900310835829522,19
670,Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder .,3,0.9841777,20.66071363522902,21
671,Modern NLP applications have enjoyed a great boost utilizing neural networks models .,0,0.95282537,79.71361346824952,13
671,"Such deep neural models , however , are not applicable to most human languages due to the lack of annotated training data for various NLP tasks .",0,0.8894724,21.127550061998587,27
671,Cross-lingual transfer learning ( CLTL ) is a viable method for building NLP models for a low-resource target language by leveraging labeled data from other ( source ) languages .,0,0.94398856,27.99314683784227,30
671,"In this work , we focus on the multilingual transfer setting where training data in multiple source languages is leveraged to further boost target language performance .",1,0.6856693,29.156915064757946,27
671,"Unlike most existing methods that rely only on language-invariant features for CLTL , our approach coherently utilizes both language-invariant and language-specific features at instance level .",3,0.60271466,28.029129921766536,31
671,"Our model leverages adversarial networks to learn language-invariant features , and mixture-of-experts models to dynamically exploit the similarity between the target language and each individual source language .",2,0.7743647,28.758934068004713,32
671,This enables our model to learn effectively what to share between various languages in the multilingual setup .,3,0.7472142,41.86277195596597,18
671,"Moreover , when coupled with unsupervised multilingual embeddings , our model can operate in a zero-resource setting where neither target language training data nor cross-lingual resources are available .",3,0.8583852,17.928034342653767,29
671,"Our model achieves significant performance gains over prior art , as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale industry dataset .",3,0.8359647,32.92999848459403,32
672,"Recently , a variety of unsupervised methods have been proposed that map pre-trained word embeddings of different languages into the same space without any parallel data .",0,0.9343128,12.026898251760953,27
672,These methods aim to find a linear transformation based on the assumption that monolingual word embeddings are approximately isomorphic between languages .,0,0.69957745,22.750314120344484,22
672,"However , it has been demonstrated that this assumption holds true only on specific conditions , and with limited resources , the performance of these methods decreases drastically .",0,0.87307703,48.89251695293186,29
672,"To overcome this problem , we propose a new unsupervised multilingual embedding method that does not rely on such assumption and performs well under resource-poor scenarios , namely when only a small amount of monolingual data ( i.e. , 50 k sentences ) are available , or when the domains of monolingual data are different across languages .",2,0.52014226,22.669670878257506,60
672,"Our proposed model , which we call ‘ Multilingual Neural Language Models ’ , shares some of the network parameters among multiple languages , and encodes sentences of multiple languages into the same space .",2,0.4913398,48.39103943904689,35
672,"The model jointly learns word embeddings of different languages in the same space , and generates multilingual embeddings without any parallel data or pre-training .",2,0.61373943,18.045234473858383,25
672,"Our experiments on word alignment tasks have demonstrated that , on the low-resource condition , our model substantially outperforms existing unsupervised and even supervised methods trained with 500 bilingual pairs of words .",3,0.9587396,42.321118901528955,33
672,Our model also outperforms unsupervised methods given different-domain corpora across languages .,3,0.89706796,33.17328048291803,14
672,Our code is publicly available .,3,0.42964724,8.875162332047664,6
673,"Cross-lingual transfer , where a high-resource transfer language is used to improve the accuracy of a low-resource task language , is now an invaluable tool for improving performance of natural language processing ( NLP ) on low-resource languages .",0,0.9459515,18.011308200590758,40
673,"However , given a particular task language , it is not clear which language to transfer from , and the standard strategy is to select languages based on ad hoc criteria , usually the intuition of the experimenter .",0,0.92374104,53.246062016112546,39
673,"Since a large number of features contribute to the success of cross-lingual transfer ( including phylogenetic similarity , typological properties , lexical overlap , or size of available data ) , even the most enlightened experimenter rarely considers all these factors for the particular task at hand .",0,0.8472851,41.49013802132137,48
673,"In this paper , we consider this task of automatically selecting optimal transfer languages as a ranking problem , and build models that consider the aforementioned features to perform this prediction .",1,0.8204996,68.39725805904884,32
673,"In experiments on representative NLP tasks , we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation , and glean insights on what features are most informative for each different NLP tasks , which may inform future ad hoc selection even without use of our method .",3,0.8953201,69.67191547785849,57
674,"This paper introduces CogNet , a new , large-scale lexical database that provides cognates-words of common origin and meaning-across languages .",1,0.7286095,85.4610325618901,24
674,The database currently contains 3.1 million cognate pairs across 338 languages using 35 writing systems .,0,0.7864526,95.0538296835247,16
674,"The paper also describes the automated method by which cognates were computed from publicly available wordnets , with an accuracy evaluated to 94 % .",3,0.4555889,126.54417170602515,25
674,"Finally , it presents statistics about the cognate data and some initial insights into it , hinting at a possible future exploitation of the resource by various fields of lingustics .",3,0.6320268,126.61575632808552,31
675,In this paper we propose a novel neural approach for automatic decipherment of lost languages .,1,0.90232426,19.547678285544364,16
675,"To compensate for the lack of strong supervision signal , our model design is informed by patterns in language change documented in historical linguistics .",2,0.6005156,57.5145634283583,25
675,The model utilizes an expressive sequence-to-sequence model to capture character-level correspondences between cognates .,2,0.6477674,24.589619698507594,18
675,"To effectively train the model in unsupervised manner , we innovate the training procedure by formalizing it as a minimum-cost flow problem .",2,0.7403456,38.95767593318589,23
675,"When applied to decipherment of Ugaritic , we achieve 5 % absolute improvement over state-of-the-art results .",3,0.95586205,38.84874748645707,23
675,"We also report first automatic results in deciphering Linear B , a syllabic language related to ancient Greek , where our model correctly translates 67.3 % of cognates .",3,0.82140905,118.70301240086624,29
676,"Previous cross-lingual knowledge graph ( KG ) alignment studies rely on entity embeddings derived only from monolingual KG structural information , which may fail at matching entities that have different facts in two KGs .",0,0.91532665,43.3221323780187,35
676,"In this paper , we introduce the topic entity graph , a local sub-graph of an entity , to represent entities with their contextual information in KG .",1,0.76280725,39.33502431480122,28
676,"From this view , the KB-alignment task can be formulated as a graph matching problem ;",0,0.4142284,85.76287722491051,18
676,"and we further propose a graph-attention based solution , which first matches all entities in two topic entity graphs , and then jointly model the local matching information to derive a graph-level matching vector .",2,0.6851928,65.60371817296902,36
676,Experiments show that our model outperforms previous state-of-the-art methods by a large margin .,3,0.9502718,3.1265618294101265,20
677,Abstractive Sentence Summarization ( ASSUM ) targets at grasping the core idea of the source sentence and presenting it as the summary .,0,0.92436796,57.30484317541856,23
677,It is extensively studied using statistical models or neural models based on the large-scale monolingual source-summary parallel corpus .,0,0.83880055,41.985636667385315,19
677,"But there is no cross-lingual parallel corpus , whose source sentence language is different to the summary language , to directly train a cross-lingual ASSUM system .",0,0.85070664,52.77817915576345,27
677,We propose to solve this zero-shot problem by using resource-rich monolingual ASSUM system to teach zero-shot cross-lingual ASSUM system on both summary word generation and attention .,2,0.3645756,41.211588143303054,29
677,This teaching process is along with a back-translation process which simulates source-summary pairs .,2,0.5520587,67.7124486357151,15
677,"Experiments on cross-lingual ASSUM task show that our proposed method is significantly better than pipeline baselines and previous works , and greatly enhances the cross-lingual performances closer to the monolingual performances .",3,0.93483883,30.66701762768673,32
678,"In this paper , we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations .",1,0.890734,28.798531111894533,21
678,"We match queries and documents in both source and target languages with four components , each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input .",2,0.8451496,25.63901242159111,36
678,"By including query likelihood scores as extra features , our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs .",3,0.5094267,42.85780777673406,31
678,"Due to the shared cross-lingual word embedding space , the model can also be directly applied to another language pair without any training label .",3,0.5840444,23.79857061560902,25
678,"Experimental results on the Material dataset show that our model outperforms the competitive translation-based baselines on English-Swahili , English-Tagalog , and English-Somali cross-lingual information retrieval tasks .",3,0.95193,13.857077329202564,33
679,"Cross-lingual word embeddings ( CLWE ) underlie many multilingual natural language processing systems , often through orthogonal transformations of pre-trained monolingual embeddings .",0,0.9159527,20.95662777605793,23
679,"However , orthogonal mapping only works on language pairs whose embeddings are naturally isomorphic .",0,0.6843195,43.11531823666348,15
679,"For non-isomorphic pairs , our method ( Iterative Normalization ) transforms monolingual embeddings to make orthogonal alignment easier by simultaneously enforcing that ( 1 ) individual word vectors are unit length , and ( 2 ) each language ’s average vector is zero .",2,0.67973614,63.7322518224811,44
679,"Iterative Normalization consistently improves word translation accuracy of three CLWE methods , with the largest improvement observed on English-Japanese ( from 2 % to 44 % test accuracy ) .",3,0.94076174,122.91586199452215,32
680,The task of unsupervised bilingual lexicon induction ( UBLI ) aims to induce word translations from monolingual corpora in two languages .,0,0.8783065,14.856775873597764,22
680,"Previous work has shown that morphological variation is an intractable challenge for the UBLI task , where the induced translation in failure case is usually morphologically related to the correct translation .",0,0.90807116,41.21871232501362,32
680,"To tackle this challenge , we propose a morphology-aware alignment model for the UBLI task .",1,0.40195262,45.96006447172725,18
680,The proposed model aims to alleviate the adverse effect of morphological variation by introducing grammatical information learned by the pre-trained denoising language model .,2,0.44235197,20.995476158317867,24
680,"Results show that our approach can substantially outperform several state-of-the-art unsupervised systems , and even achieves competitive performance compared to supervised methods .",3,0.97774386,9.773412694994093,29
681,"Machine translation is highly sensitive to the size and quality of the training data , which has led to an increasing interest in collecting and filtering large parallel corpora .",0,0.9565314,19.796980434448784,30
681,"In this paper , we propose a new method for this task based on multilingual sentence embeddings .",1,0.86656237,11.75410338666117,18
681,"In contrast to previous approaches , which rely on nearest neighbor retrieval with a hard threshold over cosine similarity , our proposed method accounts for the scale inconsistencies of this measure , considering the margin between a given sentence pair and its closest candidates instead .",2,0.5153742,93.47557537552711,46
681,Our experiments show large improvements over existing methods .,3,0.9646885,39.993087889880506,9
681,"We outperform the best published results on the BUCC mining task and the UN reconstruction task by more than 10 F1 and 30 precision points , respectively .",3,0.9526295,67.49559650963717,28
681,"Filtering the English-German ParaCrawl corpus with our approach , we obtain 31.2 BLEU points on newstest2014 , an improvement of more than one point over the best official filtered version .",3,0.88738865,51.65445791176652,33
682,Viable cross-lingual transfer critically depends on the availability of parallel texts .,0,0.84591055,29.077622472150676,12
682,Shortage of such resources imposes a development and evaluation bottleneck in multilingual processing .,0,0.8852673,97.73527186169052,14
682,"We introduce JW300 , a parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average .",2,0.7481883,89.69851725572359,23
682,"In this paper , we present the resource and showcase its utility in experiments with cross-lingual word embedding induction and multi-source part-of-speech projection .",1,0.70814586,25.055686485793533,25
683,Cross-lingual transfer is an effective way to build syntactic analysis tools in low-resource languages .,0,0.58183724,8.684857912266356,15
683,"However , transfer is difficult when transferring to typologically distant languages , especially when neither annotated target data nor parallel corpora are available .",0,0.8163983,51.26039860698068,24
683,"In this paper , we focus on methods for cross-lingual transfer to distant languages and propose to learn a generative model with a structured prior that utilizes labeled source data and unlabeled target data jointly .",1,0.8544072,22.86436951491368,36
683,The parameters of source model and target model are softly shared through a regularized log likelihood objective .,2,0.6298424,99.0815228187323,18
683,An invertible projection is employed to learn a new interlingual latent embedding space that compensates for imperfect cross-lingual word embedding input .,2,0.6426475,22.919575859400915,22
683,We evaluate our method on two syntactic tasks : part-of-speech ( POS ) tagging and dependency parsing .,2,0.6329307,35.73371451754736,18
683,"On the Universal Dependency Treebanks , we use English as the only source corpus and transfer to a wide range of target languages .",2,0.56804466,24.63998423020565,24
683,"On the 10 languages in this dataset that are distant from English , our method yields an average of 5.2 % absolute improvement on POS tagging and 8.3 % absolute improvement on dependency parsing over a direct transfer method using state-of-the-art discriminative models .",3,0.8795999,17.25622833967808,50
684,State-of-the-art methods for unsupervised bilingual word embeddings ( BWE ) train a mapping function that maps pre-trained monolingual word embeddings into a bilingual space .,0,0.860742,12.729479409637374,29
684,"Despite its remarkable results , unsupervised mapping is also well-known to be limited by the original dissimilarity between the word embedding spaces to be mapped .",0,0.8573802,30.586017356722724,27
684,"In this work , we propose a new approach that trains unsupervised BWE jointly on synthetic parallel data generated through unsupervised machine translation .",1,0.8262124,34.807323641316785,24
684,We demonstrate that existing algorithms that jointly train BWE are very robust to noisy training data and show that unsupervised BWE jointly trained significantly outperform unsupervised mapped BWE in several cross-lingual NLP tasks .,3,0.9159471,20.620006761283094,34
685,We consider the task of inferring “ is-a ” relationships from large text corpora .,0,0.42746753,31.82372218253555,17
685,"For this purpose , we propose a new method combining hyperbolic embeddings and Hearst patterns .",1,0.55952364,31.104386138115927,16
685,This approach allows us to set appropriate constraints for inferring concept hierarchies from distributional contexts while also being able to predict missing “ is-a ”-relationships and to correct wrong extractions .,3,0.65316385,72.11070688288638,35
685,Moreover – and in contrast with other methods – the hierarchical nature of hyperbolic space allows us to learn highly efficient representations and to improve the taxonomic consistency of the inferred hierarchies .,3,0.7372376,25.682266481851787,33
685,"Experimentally , we show that our approach achieves state-of-the-art performance on several commonly-used benchmarks .",3,0.87718785,6.349081221783704,23
686,"Segmenting a chunk of text into words is usually the first step of processing Chinese text , but its necessity has rarely been explored .",0,0.94018364,39.82804315957499,25
686,"In this paper , we ask the fundamental question of whether Chinese word segmentation ( CWS ) is necessary for deep learning-based Chinese Natural Language Processing .",1,0.9205726,28.56889462156122,29
686,"We benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end NLP benchmark tasks : language modeling , machine translation , sentence matching / paraphrase and text classification .",2,0.7812988,44.13268609984072,44
686,"Through direct comparisons between these two types of models , we find that char-based models consistently outperform word-based models .",3,0.92298967,17.568875895233774,22
686,"Based on these observations , we conduct comprehensive experiments to study why word-based models underperform char-based models in these deep learning-based NLP tasks .",3,0.42137048,26.46041350181846,27
686,"We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary ( OOV ) words , and thus more prone to overfitting .",3,0.92761195,20.133587311440802,35
686,We hope this paper could encourage researchers in the community to rethink the necessity of word segmentation in deep learning-based Chinese Natural Language Processing .,3,0.90129185,21.15991869620665,27
687,A surprising property of word vectors is that word analogies can often be solved with vector arithmetic .,0,0.9001328,51.56888769785982,18
687,"However , it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling ( SGNS ) .",0,0.8963611,76.93572794217961,23
687,We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution .,3,0.3798232,39.29768867195198,26
687,Our theory has several implications .,3,0.9245746,34.58583332157897,6
687,Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios ;,0,0.92903394,95.85276171995112,19
687,we prove that this holds for SGNS .,2,0.43930477,188.78560951727363,8
687,"We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word , as weighting schemes do ad hoc .",3,0.8330472,170.21958031001898,29
687,"Lastly , we offer an information theoretic interpretation of Euclidean distance in vector spaces , justifying its use in capturing word dissimilarity .",3,0.6052031,49.03517179029704,23
688,The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations .,0,0.8008976,21.846633592262023,29
688,Prediction of ( non )-compositionality is a task that has been frequently addressed with distributional semantic models .,0,0.9172826,52.20419019467328,20
688,We introduce a novel technique to blend hierarchical information with distributional information for predicting compositionality .,2,0.3699393,41.36326939348016,16
688,"In particular , we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincaré embeddings in addition to the distributional information to detect compositionality for noun phrases .",2,0.7976402,35.22534701763368,36
688,"Using a weighted average of the distributional similarity and a Poincaré similarity function , we obtain consistent and substantial , statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only .",3,0.7528933,22.83232749700406,43
688,"Unlike traditional approaches that solely use an unsupervised setting , we have also framed the problem as a supervised task , obtaining comparable improvements .",3,0.44510347,74.53894462028174,25
688,"Further , we publicly release our Poincaré embeddings , which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus .",2,0.71205205,23.887722864626642,25
689,Biomedical concepts are often mentioned in medical documents under different name variations ( synonyms ) .,0,0.9286349,140.11444924958758,16
689,"This mismatch between surface forms is problematic , resulting in difficulties pertaining to learning effective representations .",0,0.9138601,135.72505429246144,17
689,"Consequently , this has tremendous implications such as rendering downstream applications inefficacious and / or potentially unreliable .",0,0.85315144,61.819947860308595,18
689,This paper proposes a new framework for learning robust representations of biomedical names and terms .,1,0.82429236,39.176123047767724,16
689,"The idea behind our approach is to consider and encode contextual meaning , conceptual meaning , and the similarity between synonyms during the representation learning process .",2,0.5236294,51.665074862390256,27
689,"Via extensive experiments , we show that our proposed method outperforms other baselines on a battery of retrieval , similarity and relatedness benchmarks .",3,0.8095804,34.87667859777568,24
689,"Moreover , our proposed method is also able to compute meaningful representations for unseen names , resulting in high practical utility in real-world applications .",3,0.9340705,39.71846135236098,25
690,"While word embeddings have been shown to implicitly encode various forms of attributional knowledge , the extent to which they capture relational information is far more limited .",0,0.90839624,21.596236511909275,28
690,"In previous work , this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding .",0,0.83876795,31.456556801243234,23
690,"Such strategies may not be optimal , however , as they are limited by the coverage of available resources and conflate similarity with other forms of relatedness .",0,0.77565545,40.1136115680631,28
690,"As an alternative , in this paper we propose to encode relational knowledge in a separate word embedding , which is aimed to be complementary to a given standard word embedding .",1,0.42775026,26.656502911998523,32
690,"This relational word embedding is still learned from co-occurrence statistics , and can thus be used even when no external knowledge base is available .",3,0.65136087,35.350317671598454,25
690,Our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings .,3,0.98173887,20.839132118443278,23
691,"Discriminating antonyms and synonyms is an important NLP task that has the difficulty that both , antonyms and synonyms , contains similar distributional information .",0,0.9456571,35.98907697599903,25
691,"Consequently , pairs of antonyms and synonyms may have similar word vectors .",3,0.49649876,47.624463900003775,13
691,We present an approach to unravel antonymy and synonymy from word vectors based on a siamese network inspired approach .,1,0.47085798,51.699773754360294,20
691,The model consists of a two-phase training of the same base network : a pre-training phase according to a siamese model supervised by synonyms and a training phase on antonyms through a siamese-like model that supports the antitransitivity present in antonymy .,2,0.654405,44.66886956856402,46
691,The approach makes use of the claim that the antonyms in common of a word tend to be synonyms .,2,0.41350085,36.89422795200638,20
691,"We show that our approach outperforms distributional and pattern-based approaches , relaying on a simple feed forward network as base network of the training phases .",3,0.92245,97.65216601906229,28
692,Word embeddings have been widely adopted across several NLP applications .,0,0.93408644,10.901779460911065,11
692,Most existing word embedding methods utilize sequential context of a word to learn its embedding .,0,0.8621891,36.622906817217796,16
692,"While there have been some attempts at utilizing syntactic context of a word , such methods result in an explosion of the vocabulary size .",0,0.93434995,45.87025508958814,25
692,"In this paper , we overcome this problem by proposing SynGCN , a flexible Graph Convolution based method for learning word embeddings .",1,0.8126265,35.25208079331357,23
692,SynGCN utilizes the dependency context of a word without increasing the vocabulary size .,3,0.43374816,93.10710843537514,14
692,Word embeddings learned by SynGCN outperform existing methods on various intrinsic and extrinsic tasks and provide an advantage when used with ELMo .,3,0.8925,31.815657833766934,23
692,"We also propose SemGCN , an effective framework for incorporating diverse semantic knowledge for further enhancing learned word representations .",3,0.64329857,106.46128021604824,20
692,We make the source code of both models available to encourage reproducible research .,3,0.6181788,31.702283466400186,14
693,Word embedding models typically learn two types of vectors : target word vectors and context word vectors .,0,0.75576687,56.381291582799946,18
693,"These vectors are normally learned such that they are predictive of some word co-occurrence statistic , but they are otherwise unconstrained .",0,0.67977315,47.40492018226646,22
693,"However , the words from a given language can be organized in various natural groupings , such as syntactic word classes ( e.g .",0,0.90253216,61.75177327551528,24
693,"nouns , adjectives , verbs ) and semantic themes ( e.g .",2,0.4323946,89.14161356001983,12
693,"sports , politics , sentiment ) .",0,0.4303226,1449.1601378982168,7
693,Our hypothesis in this paper is that embedding models can be improved by explicitly imposing a cluster structure on the set of context word vectors .,1,0.59243286,36.1576821236657,26
693,"To this end , our model relies on the assumption that context word vectors are drawn from a mixture of von Mises-Fisher ( vMF ) distributions , where the parameters of this mixture distribution are jointly optimized with the word vectors .",2,0.7959026,40.71841862992226,44
693,We show that this results in word vectors which are qualitatively different from those obtained with existing word embedding models .,3,0.9293503,18.450967109222134,21
693,We furthermore show that our embedding model can also be used to learn high-quality document representations .,3,0.9397859,19.66212039379178,17
694,Unsupervised word embeddings have become a popular approach of word representation in NLP tasks .,0,0.9417664,13.700464443931727,15
694,"However there are limitations to the semantics represented by unsupervised embeddings , and inadequate fine-tuning of embeddings can lead to suboptimal performance .",0,0.8341432,13.380668755215728,23
694,"We propose a novel learning technique called Delta Embedding Learning , which can be applied to general NLP tasks to improve performance by optimized tuning of the word embeddings .",2,0.38542956,20.625341520355875,30
694,A structured regularization is applied to the embeddings to ensure they are tuned in an incremental way .,2,0.5979753,27.830499842327406,18
694,"As a result , the tuned word embeddings become better word representations by absorbing semantic information from supervision without “ forgetting .",3,0.8800063,92.28053237007016,22
694,We apply the method to various NLP tasks and see a consistent improvement in performance .,3,0.6822304,14.941592534601787,16
694,Evaluation also confirms the tuned word embeddings have better semantic properties .,3,0.973131,53.21515902649165,12
695,We present the first annotated resource for the aspectual classification of German verb tokens in their clausal context .,3,0.43178037,47.85364895525323,19
695,We use aspectual features compatible with the plurality of aspectual classifications in previous work and treat aspectual ambiguity systematically .,2,0.7492753,125.00847542341974,20
695,"We evaluate our corpus by using it to train supervised classifiers to automatically assign aspectual categories to verbs in context , permitting favourable comparisons to previous work .",2,0.5254411,121.57952563307406,28
696,"In neural network models of language , words are commonly represented using context-invariant representations ( word embeddings ) which are then put in context in the hidden layers .",0,0.82101154,32.33092332924482,29
696,"Since words are often ambiguous , representing the contextually relevant information is not trivial .",0,0.8862474,55.75026310321043,15
696,"We investigate how an LSTM language model deals with lexical ambiguity in English , designing a method to probe its hidden representations for lexical and contextual information about words .",1,0.7967311,32.60424427744099,30
696,"We find that both types of information are represented to a large extent , but also that there is room for improvement for contextual information .",3,0.9823496,24.56332817837817,26
697,"Graph measures , such as node distances , are inefficient to compute .",0,0.7819555,175.59445931689206,13
697,We explore dense vector representations as an effective way to approximate the same information .,2,0.47069913,51.96883721877144,15
697,We introduce a simple yet efficient and effective approach for learning graph embeddings .,2,0.33139265,14.71242786694134,14
697,"Instead of directly operating on the graph structure , our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures , such as e.g .",2,0.621357,75.23712972898802,35
697,the shortest path distance or distance measures that take information beyond the graph structure into account .,0,0.5969372,105.87479264897308,17
697,"We demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures , while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks .",3,0.92547834,35.29362472348259,47
698,"Due to the ubiquitous use of embeddings as input representations for a wide range of natural language tasks , imputation of embeddings for rare and unseen words is a critical problem in language processing .",0,0.952727,15.125921408719126,35
698,"Embedding imputation involves learning representations for rare or unseen words during the training of an embedding model , often in a post-hoc manner .",0,0.74024904,42.054232178607435,24
698,"In this paper , we propose an approach for embedding imputation which uses grounded information in the form of a knowledge graph .",1,0.88924694,20.6379635951535,23
698,This is in contrast to existing approaches which typically make use of vector space properties or subword information .,0,0.51899517,28.297000072503668,19
698,We propose an online method to construct a graph from grounded information and design an algorithm to map from the resulting graphical structure to the space of the pre-trained embeddings .,2,0.5742396,31.228872548069713,31
698,"Finally , we evaluate our approach on a range of rare and unseen word tasks across various domains and show that our model can learn better representations .",3,0.813699,31.319780675275048,28
698,"For example , on the Card-660 task our method improves Pearson ’s and Spearman ’s correlation coefficients upon the state-of-the-art by 11 % and 17.8 % respectively using GloVe embeddings .",3,0.9219365,34.23386245059976,39
699,"Hypernymy modeling has largely been separated according to two paradigms , pattern-based methods and distributional methods .",0,0.922148,42.583729117884,19
699,"However , recent works utilizing a mix of these strategies have yielded state-of-the-art results .",0,0.893501,17.204852716880445,21
699,This paper evaluates the contribution of both paradigms to hybrid success by evaluating the benefits of hybrid treatment of baseline models from each paradigm .,1,0.92658556,61.56578144953688,25
699,"Even with a simple methodology for each individual system , utilizing a hybrid approach establishes new state-of-the-art results on two domain-specific English hypernym discovery tasks and outperforms all non-hybrid approaches in a general English hypernym discovery task .",3,0.91701555,23.784542896551628,44
700,"Previous studies on lexical substitution tend to obtain substitute candidates by finding the target word ’s synonyms from lexical resources ( e.g. , WordNet ) and then rank the candidates based on its contexts .",0,0.8665777,59.83777404640709,35
700,These approaches have two limitations : ( 1 ) They are likely to overlook good substitute candidates that are not the synonyms of the target words in the lexical resources ;,0,0.7060281,58.94733514533618,31
700,( 2 ) They fail to take into account the substitution ’s influence on the global context of the sentence .,0,0.50394595,58.613514336414624,21
700,"To address these issues , we propose an end-to-end BERT-based lexical substitution approach which can propose and validate substitute candidates without using any annotated data or manually curated resources .",1,0.53109914,22.520792883448674,33
700,"Our approach first applies dropout to the target word ’s embedding for partially masking the word , allowing BERT to take balanced consideration of the target word ’s semantics and contexts for proposing substitute candidates , and then validates the candidates based on their substitution ’s influence on the global contextualized representation of the sentence .",2,0.74909633,49.2341442601254,56
700,"Experiments show our approach performs well in both proposing and ranking substitute candidates , achieving the state-of-the-art results in both LS07 and LS14 benchmarks .",3,0.9492526,36.06374430473599,31
701,Word embeddings are now pervasive across NLP subfields as the de-facto method of forming text representataions .,0,0.942906,100.2260916758529,17
701,"In this work , we show that existing embedding models are inadequate at constructing representations that capture salient aspects of mathematical meaning for numbers , which is important for language understanding .",1,0.66141915,43.45361055039694,32
701,Numbers are ubiquitous and frequently appear in text .,0,0.90455824,37.424958377437385,9
701,"Inspired by cognitive studies on how humans perceive numbers , we develop an analysis framework to test how well word embeddings capture two essential properties of numbers : magnitude ( e.g .",2,0.5570513,44.521346508774,32
701,3< 4 ) and numeration ( e.g .,2,0.4761769,167.51745438453656,9
701,3 =three ) .,3,0.6852491,3610.765887970268,4
701,"Our experiments reveal that most models capture an approximate notion of magnitude , but are inadequate at capturing numeration .",3,0.9726261,103.64396859043586,20
701,We hope that our observations provide a starting point for the development of methods which better capture numeracy in NLP systems .,3,0.97245747,20.96662805659673,22
702,"There has been substantial progress in summarization research enabled by the availability of novel , often large-scale , datasets and recent advances on neural network-based approaches .",0,0.94490314,36.667613584583854,29
702,"However , manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers .",0,0.50495505,49.01945005610071,23
702,"To address this issue , we propose a novel approach for manual evaluation , Highlight-based Reference-less Evaluation of Summarization ( HighRES ) , in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter .",1,0.45596343,71.70959016064961,46
702,"Thus summary assessment on the source document by human judges is facilitated , while the highlights can be used for evaluating multiple systems .",3,0.7474644,135.9409333861902,24
702,To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems .,2,0.7430515,46.34364285744941,27
702,"We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly , while they help emphasize differences among systems that would be ignored under other evaluation approaches .",3,0.97273886,190.84261976947698,32
703,"We present the first sentence simplification model that learns explicit edit operations ( ADD , DELETE , and KEEP ) via a neural programmer-interpreter approach .",1,0.37435234,73.52027133512003,28
703,Most current neural sentence simplification systems are variants of sequence-to-sequence models adopted from machine translation .,0,0.8559081,28.307216163474987,18
703,These methods learn to simplify sentences as a byproduct of the fact that they are trained on complex-simple sentence pairs .,0,0.6314118,59.50904006422232,22
703,"By contrast , our neural programmer-interpreter is directly trained to predict explicit edit operations on targeted parts of the input sentence , resembling the way that humans perform simplification and revision .",3,0.46939233,118.16002978414386,34
703,"Our model outperforms previous state-of-the-art neural sentence simplification models ( without external knowledge ) by large margins on three benchmark text simplification corpora in terms of SARI ( + 0.95 WikiLarge , + 1.89 WikiSmall , + 1.41 Newsela ) , and is judged by humans to produce overall better and simpler output sentences .",3,0.85852486,45.3101865385414,60
704,"Paraphrasing exists at different granularity levels , such as lexical level , phrasal level and sentential level .",0,0.77636456,29.682878620142965,18
704,"This paper presents Decomposable Neural Paraphrase Generator ( DNPG ) , a Transformer-based model that can learn and generate paraphrases of a sentence at different levels of granularity in a disentangled way .",1,0.79339963,15.473229837415293,35
704,"Specifically , the model is composed of multiple encoders and decoders with different structures , each of which corresponds to a specific granularity .",2,0.5490921,16.118766845972843,24
704,The empirical study shows that the decomposition mechanism of DNPG makes paraphrase generation more interpretable and controllable .,3,0.98619854,40.610608341232606,18
704,"Based on DNPG , we further develop an unsupervised domain adaptation method for paraphrase generation .",2,0.5654375,40.32062700810584,16
704,"Experimental results show that the proposed model achieves competitive in-domain performance compared to state-of-the-art neural models , and significantly better performance when adapting to a new domain .",3,0.96763426,8.242236811215928,35
705,"We present an approach for recursively splitting and rephrasing complex English sentences into a novel semantic hierarchy of simplified sentences , with each of them presenting a more regular structure that may facilitate a wide variety of artificial intelligence tasks , such as machine translation ( MT ) or information extraction ( IE ) .",1,0.51291585,40.46863344321523,55
705,"Using a set of hand-crafted transformation rules , input sentences are recursively transformed into a two-layered hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations .",2,0.59149593,49.74386077305441,35
705,"In this way , the semantic relationship of the decomposed constituents is preserved in the output , maintaining its interpretability for downstream applications .",3,0.6273158,48.505867990558464,24
705,Both a thorough manual analysis and automatic evaluation across three datasets from two different domains demonstrate that the proposed syntactic simplification approach outperforms the state of the art in structural text simplification .,3,0.9298367,17.738500178047858,33
705,"Moreover , an extrinsic evaluation shows that when applying our framework as a preprocessing step the performance of state-of-the-art Open IE systems can be improved by up to 346 % in precision and 52 % in recall .",3,0.9593487,27.728629066616236,43
705,"To enable reproducible research , all code is provided online .",0,0.37086573,111.01531441350069,11
706,A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases .,0,0.7717241,43.6194294959491,31
706,"We study this issue within natural language inference ( NLI ) , the task of determining whether one sentence entails another .",1,0.6020723,52.27584374446559,22
706,"We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics : the lexical overlap heuristic , the subsequence heuristic , and the constituent heuristic .",1,0.35915816,40.24945618200995,27
706,"To determine whether models have adopted these heuristics , we introduce a controlled evaluation set called HANS ( Heuristic Analysis for NLI Systems ) , which contains many examples where the heuristics fail .",2,0.605465,48.09140063749529,34
706,"We find that models trained on MNLI , including BERT , a state-of-the-art model , perform very poorly on HANS , suggesting that they have indeed adopted these heuristics .",3,0.97472024,26.093116479366397,36
706,"We conclude that there is substantial room for improvement in NLI systems , and that the HANS dataset can motivate and measure progress in this area .",3,0.9860049,30.465698745587943,27
707,"We present the zero-shot entity linking task , where mentions must be linked to unseen entities without in-domain labeled data .",2,0.34293914,49.126937556112935,23
707,"The goal is to enable robust transfer to highly specialized domains , and so no metadata or alias tables are assumed .",0,0.4167386,179.8437833820842,22
707,"In this setting , entities are only identified by text descriptions , and models must rely strictly on language understanding to resolve the new entities .",0,0.78740734,83.82725090133096,26
707,"First , we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities .",3,0.5542159,12.33539618169955,23
707,"Second , we propose a simple and effective adaptive pre-training strategy , which we term domain-adaptive pre-training ( DAP ) , to address the domain shift problem associated with linking unseen entities in a new domain .",2,0.60381037,28.987706716965793,38
707,"We present experiments on a new dataset that we construct for this task and show that DAP improves over strong pre-training baselines , including BERT .",3,0.63647443,29.544782796568022,26
707,The data and code are available at https://github.com/lajanugen/zeshel .,3,0.57406855,20.9405703461044,9
708,We propose a new neural transfer method termed Dual Adversarial Transfer Network ( DATNet ) for addressing low-resource Named Entity Recognition ( NER ) .,1,0.55952483,34.25572723614584,25
708,"Specifically , two variants of DATNet , i.e. , DATNet-F and DATNet-P , are investigated to explore effective feature fusion between high and low resource .",2,0.6326039,36.527647126983766,29
708,"To address the noisy and imbalanced training data , we propose a novel Generalized Resource-Adversarial Discriminator ( GRAD ) .",2,0.57736033,32.23565841730974,21
708,"Additionally , adversarial training is adopted to boost model generalization .",2,0.6826571,35.89574835785095,11
708,"In experiments , we examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data , without augmenting any additional hand-crafted features and pre-trained language model .",3,0.72425205,34.59417093658808,41
709,"Prior work has shown that , on small amounts of training data , syntactic neural language models learn structurally sensitive generalisations more successfully than sequential language models .",0,0.8959172,49.63832524436236,28
709,"However , their computational complexity renders scaling difficult , and it remains an open question whether structural biases are still necessary when sequential models have access to ever larger amounts of training data .",0,0.8796687,48.69104760863077,34
709,"To answer this question , we introduce an efficient knowledge distillation ( KD ) technique that transfers knowledge from a syntactic language model trained on a small corpus to an LSTM language model , hence enabling the LSTM to develop a more structurally sensitive representation of the larger training data it learns from .",2,0.38483098,26.639304379735385,54
709,"On targeted syntactic evaluations , we find that , while sequential LSTMs perform much better than previously reported , our proposed technique substantially improves on this baseline , yielding a new state of the art .",3,0.94803673,46.56253427391141,36
709,"Our findings and analysis affirm the importance of structural biases , even in models that learn from large amounts of data .",3,0.98961455,39.91461486507386,22
710,"Recently , there has been an increasing interest in unsupervised parsers that optimize semantically oriented objectives , typically using reinforcement learning .",0,0.95513916,40.1193981027023,22
710,"Unfortunately , the learned trees often do not match actual syntax trees well .",0,0.843691,120.4983990535722,14
710,Shen et al .,4,0.89735776,14.367157709330709,4
710,"( 2018 ) propose a structured attention mechanism for language modeling ( PRPN ) , which induces better syntactic structures but relies on ad hoc heuristics .",0,0.5416848,98.79115153510286,27
710,"Also , their model lacks interpretability as it is not grounded in parsing actions .",0,0.68631077,76.70147949946946,15
710,"In our work , we propose an imitation learning approach to unsupervised parsing , where we transfer the syntactic knowledge induced by PRPN to a Tree-LSTM model with discrete parsing actions .",2,0.5544135,47.16297161757554,33
710,Its policy is then refined by Gumbel-Softmax training towards a semantically oriented objective .,2,0.61215466,135.34652540892364,16
710,"We evaluate our approach on the All Natural Language Inference dataset and show that it achieves a new state of the art in terms of parsing F-score , outperforming our base models , including PRPN .",3,0.76602226,35.41011550592144,36
711,"Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender , but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences .",0,0.89312065,22.364658256400634,43
711,"To address this , we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles ’ authors , and build taggers and parsers trained on this data that show performance differences in text written by men and women .",2,0.8110767,39.71089584730811,46
711,Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data .,3,0.9658274,46.028546485584656,26
711,"The results underscore the importance of accounting for gendered differences in syntactic tasks , and outline future venues for developing more accurate taggers and parsers .",3,0.9892202,61.01578625541999,26
711,We release our data to the research community .,3,0.40331545,26.766557237635705,9
712,We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions .,3,0.9479599,18.59596358753369,21
712,"We first compare the benefits of no pre-training , fastText , ELMo , and BERT for English and find that BERT outperforms ELMo , in large part due to increased model capacity , whereas ELMo in turn outperforms the non-contextual fastText embeddings .",3,0.7950551,35.122181548938286,43
712,We also find that pre-training is beneficial across all 11 languages tested ;,3,0.9825216,66.41185931551165,13
712,"however , large model sizes ( more than 100 million parameters ) make it computationally expensive to train separate models for each language .",0,0.7225526,44.65955188814659,24
712,"To address this shortcoming , we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model .",3,0.5754028,28.204622521495978,30
712,The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2 % relative error increase in aggregate .,3,0.95102715,54.2443091756044,25
712,We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages .,3,0.87948096,19.519344177788724,27
712,"Finally , we demonstrate new state-of-the-art results for 11 languages , including English ( 95.8 F1 ) and Chinese ( 91.8 F1 ) .",3,0.9231576,10.526133750411157,30
713,We present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world ’s languages .,1,0.57791907,28.465115047788867,28
713,"We evaluate our method on Parallel Bible Corpus + ( PBC + ) , a parallel corpus of 1593 languages .",2,0.8000022,131.93790184250182,21
713,The key idea is to use Byte Pair Encodings ( BPEs ) as basic units for multilingual embeddings .,0,0.711706,24.02834719012564,19
713,"Through zero-shot transfer from English sentiment , we learn a seed lexicon for each language in the domain of PBC + .",2,0.70273113,128.0717991799893,22
713,"Through domain adaptation , we then generalize the domain-specific lexicon to a general one .",2,0.6946165,49.007495561997494,15
713,We show – across typologically diverse languages in PBC + – good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation .,3,0.93287355,130.79066902575846,32
713,"We make freely available our code , seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages .",2,0.66872066,115.9576281375385,25
714,"Tree-LSTMs have been used for tree-based sentiment analysis over Stanford Sentiment Treebank , which allows the sentiment signals over hierarchical phrase structures to be calculated simultaneously .",0,0.6115419,52.57464767740937,29
714,"However , traditional tree-LSTMs capture only the bottom-up dependencies between constituents .",0,0.86538255,86.1109841948182,15
714,"In this paper , we propose a tree communication model using graph convolutional neural network and graph recurrent neural network , which allows rich information exchange between phrases constituent tree .",1,0.7667487,46.303938053865124,31
714,"Experiments show that our model outperforms existing work on bidirectional tree-LSTMs in both accuracy and efficiency , providing more consistent predictions on phrase-level sentiments .",3,0.96515334,35.69486931613555,27
715,Multilingual writers and speakers often alternate between two languages in a single discourse .,0,0.9187183,78.53916307326762,14
715,This practice is called “ code-switching ” .,0,0.84509087,29.050244486986397,8
715,Existing sentiment detection methods are usually trained on sentiment-labeled monolingual text .,0,0.84221923,23.047032114521656,14
715,"Manually labeled code-switched text , especially involving minority languages , is extremely rare .",0,0.9008862,130.94492830732048,14
715,"Consequently , the best monolingual methods perform relatively poorly on code-switched text .",3,0.62784356,34.26144475450909,13
715,"We present an effective technique for synthesizing labeled code-switched text from labeled monolingual text , which is relatively readily available .",1,0.49135965,36.521377278941856,21
715,The idea is to replace carefully selected subtrees of constituency parses of sentences in the resource-rich language with suitable token spans selected from automatic translations to the resource-poor language .,2,0.43414626,63.189683511196094,34
715,"By augmenting the scarce labeled code-switched text with plentiful synthetic labeled code-switched text , we achieve significant improvements in sentiment labeling accuracy ( 1.5 % , 5.11 % 7.20 % ) for three different language pairs ( English-Hindi , English-Spanish and English-Bengali ) .",3,0.8644335,29.917888332292375,48
715,The improvement is even significant in hatespeech detection whereby we achieve a 4 % improvement using only synthetic code-switched data ( 6 % with data augmentation ) .,3,0.96860284,86.49266760665569,28
716,Aspect term extraction ( ATE ) aims at identifying all aspect terms in a sentence and is usually modeled as a sequence labeling problem .,0,0.92582995,21.97390627340483,25
716,"However , sequence labeling based methods cannot make full use of the overall meaning of the whole sentence and have the limitation in processing dependencies between labels .",0,0.8934692,60.85777480615431,28
716,"To tackle these problems , we first explore to formalize ATE as a sequence-to-sequence ( Seq2Seq ) learning task where the source sequence and target sequence are composed of words and labels respectively .",2,0.6029714,20.640064744484334,34
716,"At the same time , to make Seq2Seq learning suit to ATE where labels correspond to words one by one , we design the gated unit networks to incorporate corresponding word representation into the decoder , and position-aware attention to pay more attention to the adjacent words of a target word .",2,0.79522055,61.26882146595085,54
716,The experimental results on two datasets show that Seq2Seq learning is effective in ATE accompanied with our proposed gated unit networks and position-aware attention mechanism .,3,0.9705732,38.83310667335638,28
717,"In the literature , existing studies on aspect sentiment classification ( ASC ) focus on individual non-interactive reviews .",0,0.90806496,85.81429755644663,19
717,"This paper extends the research to interactive reviews and proposes a new research task , namely Aspect Sentiment Classification towards Question-Answering ( ASC-QA ) , for real-world applications .",1,0.7701221,54.42434805886113,33
717,This new task aims to predict sentiment polarities for specific aspects from interactive QA style reviews .,1,0.65160644,137.2423907639701,17
717,"In particular , a high-quality annotated corpus is constructed for ASC-QA to facilitate corresponding research .",2,0.5295144,50.50418469623297,18
717,"On this basis , a Reinforced Bidirectional Attention Network ( RBAN ) approach is proposed to address two inherent challenges in ASC-QA , i.e. , semantic matching between question and answer , and data noise .",2,0.45980984,49.58047546770187,38
717,Experimental results demonstrate the great advantage of the proposed approach to ASC-QA against several state-of-the-art baselines .,3,0.9675907,13.620143740207522,24
718,"We introduce the first large-scale corpus for long form question answering , a task requiring elaborate and in-depth answers to open-ended questions .",1,0.430077,30.78002116758564,24
718,The dataset comprises 270 K threads from the Reddit forum “ Explain Like I ’m Five ” ( ELI5 ) where an online community provides answers to questions which are comprehensible by five year olds .,2,0.8267888,103.28165336134977,36
718,"Compared to existing datasets , ELI5 comprises diverse questions requiring multi-sentence answers .",3,0.60370266,111.33800530737663,13
718,We provide a large set of web documents to help answer the question .,0,0.56915283,31.5881080923133,14
718,"Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq , language modeling , as well as a strong extractive baseline .",3,0.91342384,32.902436879678056,29
718,"However , our best model is still far from human performance since raters prefer gold responses in over 86 % of cases , leaving ample opportunity for future improvement .",3,0.95136,60.62588960277867,30
719,"In this work , we introduce a novel algorithm for solving the textbook question answering ( TQA ) task which describes more realistic QA problems compared to other recent tasks .",1,0.8309927,43.325479038881284,31
719,We mainly focus on two related issues with analysis of the TQA dataset .,3,0.5402664,47.141016165036596,14
719,"First , solving the TQA problems requires to comprehend multi-modal contexts in complicated input data .",0,0.6560392,62.12005300021404,16
719,"To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features , we establish a context graph from texts and images , and propose a new module f-GCN based on graph convolutional networks ( GCN ) .",2,0.5727387,63.19643325802641,44
719,"Second , scientific terms are not spread over the chapters and subjects are split in the TQA dataset .",3,0.635685,162.65663189082386,19
719,"To overcome this so called ‘ out-of-domain ’ issue , before learning QA problems , we introduce a novel self-supervised open-set learning process without any annotations .",2,0.59412956,56.41821635479224,28
719,The experimental results show that our model significantly outperforms prior state-of-the-art methods .,3,0.9775475,4.118494838973749,18
719,"Moreover , ablation studies validate that both methods of incorporating f-GCN for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for TQA problems .",3,0.9738023,50.31798749690189,30
720,Visual question answering ( VQA ) and image captioning require a shared body of general knowledge connecting language and vision .,0,0.9350232,71.66720238485254,21
720,We present a novel approach to better VQA performance that exploits this connection by jointly generating captions that are targeted to help answer a specific visual question .,1,0.48813224,45.219695624063085,28
720,The model is trained using an existing caption dataset by automatically determining question-relevant captions using an online gradient-based method .,2,0.8029599,48.20339199211964,24
720,Experimental results on the VQA v2 challenge demonstrates that our approach obtains state-of-the-art VQA performance ( e.g .,3,0.92512095,11.262810213207537,23
720,68.4 % in the Test-standard set using a single model ) by simultaneously generating question-relevant captions .,3,0.81803477,130.16861921197003,19
721,Attention mechanisms are widely used in Visual Question Answering ( VQA ) to search for visual clues related to the question .,0,0.9460446,13.066633528924099,22
721,"Most approaches train attention models from a coarse-grained association between sentences and images , which tends to fail on small objects or uncommon concepts .",0,0.8219996,78.2060760549155,25
721,"To address this problem , this paper proposes a multi-grained attention method .",1,0.5920467,28.895430318327925,13
721,It learns explicit word-object correspondence by two types of word-level attention complementary to the sentence-image association .,2,0.4980511,74.72292830200487,17
721,"Evaluated on the VQA benchmark , the multi-grained attention model achieves competitive performance with state-of-the-art models .",3,0.8580183,11.444066542277236,22
721,And the visualized attention maps demonstrate that addition of object-level groundings leads to a better understanding of the images and locates the attended objects more precisely .,3,0.9690795,43.92727012404167,28
722,We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering ( VQA ) .,1,0.73993164,18.225457766182796,22
722,"Motivated by evidence from psycholinguistics , we devise a set of linguistically-informed VQA tasks , which differ by the types of questions involved ( Wh-questions and polar questions ) .",2,0.5827176,50.80811071901068,32
722,"We test what impact task difficulty has on continual learning , and whether the order in which a child acquires question types facilitates computational models .",1,0.6337413,110.71773593776776,26
722,Our results show that dramatic forgetting is at play and that task difficulty and order matter .,3,0.99011874,100.8367607645573,17
722,Two well-known current continual learning methods mitigate the problem only to a limiting degree .,0,0.7800793,79.16168336558074,17
723,Paragraph-style image captions describe diverse aspects of an image as opposed to the more common single-sentence captions that only provide an abstract description of the image .,0,0.8806479,17.712741997067905,29
723,These paragraph captions can hence contain substantial information of the image for tasks such as visual question answering .,3,0.5796876,94.6533212021877,19
723,"Moreover , this textual information is complementary with visual information present in the image because it can discuss both more abstract concepts and more explicit , intermediate symbolic information about objects , events , and scenes that can directly be matched with the textual question and copied into the textual answer ( i.e. , via easier modality match ) .",3,0.5435681,85.27719906863638,60
723,"Hence , we propose a combined Visual and Textual Question Answering ( VTQA ) model which takes as input a paragraph caption as well as the corresponding image , and answers the given question based on both inputs .",2,0.36898157,24.301090848759152,39
723,"In our model , the inputs are fused to extract related information by cross-attention ( early fusion ) , then fused again in the form of consensus ( late fusion ) , and finally expected answers are given an extra score to enhance the chance of selection ( later fusion ) .",2,0.76282513,68.11835584594874,52
723,"Empirical results show that paragraph captions , even when automatically generated ( via an RL-based encoder-decoder model ) , help correctly answer more visual questions .",3,0.97130233,54.457160707375444,29
723,"Overall , our joint model , when trained on the Visual Genome dataset , significantly improves the VQA performance over a strong baseline model .",3,0.96654797,34.42359752768398,25
724,"Word embedding is central to neural machine translation ( NMT ) , which has attracted intensive research interest in recent years .",0,0.9539935,24.858318945046804,22
724,"In NMT , the source embedding plays the role of the entrance while the target embedding acts as the terminal .",0,0.4757143,30.091199103178287,21
724,These layers occupy most of the model parameters for representation learning .,3,0.47674924,65.67492399916883,12
724,"Furthermore , they indirectly interface via a soft-attention mechanism , which makes them comparatively isolated .",0,0.5393904,117.47402340637933,16
724,"In this paper , we propose shared-private bilingual word embeddings , which give a closer relationship between the source and target embeddings , and which also reduce the number of model parameters .",1,0.694073,32.668274851717406,35
724,"For similar source and target words , their embeddings tend to share a part of the features and they cooperatively learn these common representation units .",3,0.48677132,85.34985472250712,26
724,Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters .,3,0.89915293,19.719531334656047,37
725,In this work we present a new dataset of literary events — events that are depicted as taking place within the imagined space of a novel .,1,0.8084602,18.98473481201079,27
725,"While previous work has focused on event detection in the domain of contemporary news , literature poses a number of complications for existing systems , including complex narration , the depiction of a broad array of mental states , and a strong emphasis on figurative language .",0,0.8823423,76.43469747481589,47
725,We outline the annotation decisions of this new dataset and compare several models for predicting events ;,3,0.449531,189.93360004987179,17
725,"the best performing model , a bidirectional LSTM with BERT token representations , achieves an F1 score of 73.9 .",3,0.9127269,26.240410441366496,20
725,We then apply this model to a corpus of novels split across two dimensions — prestige and popularity — and demonstrate that there are statistically significant differences in the distribution of events for prestige .,3,0.52619857,30.191785535745588,35
726,"Self-attention networks ( SAN ) have attracted a lot of interests due to their high parallelization and strong performance on a variety of NLP tasks , e.g .",0,0.94867086,16.699999786946908,28
726,machine translation .,0,0.53410447,268.0022327779235,3
726,"Due to the lack of recurrence structure such as recurrent neural networks ( RNN ) , SAN is ascribed to be weak at learning positional information of words for sequence modeling .",0,0.8964455,64.29934362729118,32
726,"However , neither this speculation has been empirically confirmed , nor explanations for their strong performances on machine translation tasks when “ lacking positional information ” have been explored .",0,0.867815,79.58653203262288,30
726,"To this end , we propose a novel word reordering detection task to quantify how well the word order information learned by SAN and RNN .",1,0.6322545,46.726991126888315,26
726,"Specifically , we randomly move one word to another position , and examine whether a trained model can detect both the original and inserted positions .",2,0.85265356,57.418573560846866,26
726,Experimental results reveal that : 1 ) SAN trained on word reordering detection indeed has difficulty learning the positional information even with the position embedding ;,3,0.9819904,259.4507890678839,26
726,"and 2 ) SAN trained on machine translation learns better positional information than its RNN counterpart , in which position embedding plays a critical role .",3,0.79361856,83.05616288691354,26
726,"Although recurrence structure make the model more universally-effective on learning word order , learning objectives matter more in the downstream tasks such as machine translation .",3,0.78211206,90.73123442334949,28
727,Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data .,0,0.9483887,31.35087465965258,24
727,These models have obtained notable gains in accuracy across many NLP tasks .,0,0.8544097,25.403582885208067,13
727,"However , these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption .",0,0.5865681,86.21402657694065,21
727,"As a result these models are costly to train and develop , both financially , due to the cost of hardware and electricity or cloud compute time , and environmentally , due to the carbon footprint required to fuel modern tensor processing hardware .",0,0.85449404,70.9568938060547,44
727,In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP .,1,0.85420024,18.609362388015608,34
727,"Based on these findings , we propose actionable recommendations to reduce costs and improve equity in NLP research and practice .",3,0.9806543,25.9360978665549,21
728,BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks .,0,0.6906054,41.791051099775686,18
728,This result indicates the possibility that BERT networks capture structural information about language .,3,0.9884471,52.99298417003716,14
728,"In this work , we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT .",1,0.58930546,30.60361138191144,29
728,Our findings are fourfold .,3,0.97522867,101.84282861098221,5
728,BERT ’s phrasal representation captures the phrase-level information in the lower layers .,3,0.4504329,39.74893680279717,13
728,"The intermediate layers of BERT compose a rich hierarchy of linguistic information , starting with surface features at the bottom , syntactic features in the middle followed by semantic features at the top .",0,0.6083946,33.488212588077715,34
728,BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem .,3,0.6274908,133.55337481148416,16
728,"Finally , the compositional scheme underlying BERT mimics classical , tree-like structures .",3,0.53431386,153.18155323018718,13
729,Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse .,0,0.96264136,31.53138143356139,22
729,"However , current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions .",0,0.9192136,66.21988306732315,24
729,"In this position paper , we argue that the community needs to make three substantive changes : ( 1 ) expanding our scope of problems to tackle both more subtle and more serious forms of abuse , ( 2 ) developing proactive technologies that counter or inhibit abuse before it harms , and ( 3 ) reframing our effort within a framework of justice to promote healthy communities .",1,0.7032156,43.391007873451414,69
730,"The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed , leaving a vast store of potential training signal untapped .",0,0.79982746,63.666734494871825,31
730,"In this work , we propose the self-feeding chatbot , a dialogue agent with the ability to extract new training examples from the conversations it participates in .",1,0.62644404,38.493580073786156,28
730,"As our agent engages in conversation , it also estimates user satisfaction in its responses .",2,0.46916962,134.12922634354342,16
730,"When the conversation appears to be going well , the user ’s responses become new training examples to imitate .",0,0.54426813,62.810210338315784,20
730,"When the agent believes it has made a mistake , it asks for feedback ;",0,0.38458765,67.19758504332495,15
730,learning to predict the feedback that will be given improves the chatbot ’s dialogue abilities further .,3,0.86548907,81.60992105872509,17
730,"On the PersonaChat chit-chat dataset with over 131 k training examples , we find that learning from dialogue with a self-feeding chatbot significantly improves performance , regardless of the amount of traditional supervision .",3,0.94746685,45.70311433128352,35
731,"It is desirable for dialog systems to have capability to express specific emotions during a conversation , which has a direct , quantifiable impact on improvement of their usability and user satisfaction .",0,0.68572056,47.70259049171103,33
731,"After a careful investigation of real-life conversation data , we found that there are at least two ways to express emotions with language .",3,0.9327484,20.79426086445041,24
731,One is to describe emotional states by explicitly using strong emotional words ;,0,0.6509654,313.16593337015644,13
731,another is to increase the intensity of the emotional experiences by implicitly combining neutral words in distinct ways .,3,0.49214813,89.49080267821722,19
731,"We propose an emotional dialogue system ( EmoDS ) that can generate the meaningful responses with a coherent structure for a post , and meanwhile express the desired emotion explicitly or implicitly within a unified framework .",1,0.5743693,84.52529337013463,37
731,"Experimental results showed EmoDS performed better than the baselines in BLEU , diversity and the quality of emotional expression .",3,0.98154,70.73373222748927,20
732,Semantically controlled neural response generation on limited-domain has achieved great performance .,0,0.74776846,183.89027648504924,14
732,"However , moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains .",0,0.71912014,38.242003832832985,27
732,"To alleviate such scalability issue , we exploit the structure of dialog acts to build a multi-layer hierarchical graph , where each act is represented as a root-to-leaf route on the graph .",2,0.72135794,36.57674586758561,35
732,"Then , we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network , where we disentangle attention heads to model designated nodes on the dialog act graph .",2,0.871936,56.35226358747746,35
732,"By activating different ( disentangled ) heads at each layer , combinatorially many dialog act semantics can be modeled to control the neural response generation .",3,0.35476717,219.77920407064403,26
732,"On the large-scale Multi-Domain-WOZ dataset , our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics .",3,0.94291085,14.360548213279356,24
733,Clarifying user needs is essential for existing task-oriented dialogue systems .,0,0.72529256,25.11554307072064,13
733,"However , in real-world applications , developers can never guarantee that all possible user demands are taken into account in the design phase .",0,0.85610855,37.684517523536975,24
733,"Consequently , existing systems will break down when encountering unconsidered user needs .",0,0.810172,139.97628385199533,13
733,"To address this problem , we propose a novel incremental learning framework to design task-oriented dialogue systems , or for short Incremental Dialogue System ( IDS ) , without pre-defining the exhaustive list of user needs .",1,0.47710717,61.93646634234504,39
733,"Specifically , we introduce an uncertainty estimation module to evaluate the confidence of giving correct responses .",2,0.726271,60.99812839235757,17
733,"If there is high confidence , IDS will provide responses to users .",0,0.48375055,146.8759364415913,13
733,"Otherwise , humans will be involved in the dialogue process , and IDS can learn from human intervention through an online learning module .",3,0.43538785,103.58769301842906,24
733,"To evaluate our method , we propose a new dataset which simulates unanticipated user needs in the deployment stage .",2,0.58606577,48.378419253765884,20
733,"Experiments show that IDS is robust to unconsidered user actions , and can update itself online by smartly selecting only the most effective training data , and hence attains better performance with less annotation cost .",3,0.9350352,71.64478801886041,36
734,"In multi-turn dialogue generation , response is usually related with only a few contexts .",0,0.8435137,86.90550149875291,15
734,"Therefore , an ideal model should be able to detect these relevant contexts and produce a suitable response accordingly .",0,0.63163674,39.646425076577884,20
734,"However , the widely used hierarchical recurrent encoder-decoder models just treat all the contexts indiscriminately , which may hurt the following response generation process .",0,0.8313211,72.3787885391375,25
734,"Some researchers try to use the cosine similarity or the traditional attention mechanism to find the relevant contexts , but they suffer from either insufficient relevance assumption or position bias problem .",0,0.88260955,77.29863346922893,32
734,"In this paper , we propose a new model , named ReCoSa , to tackle this problem .",1,0.8619995,33.8623433465649,18
734,"Firstly , a word level LSTM encoder is conducted to obtain the initial representation of each context .",2,0.8006358,24.654069742862525,18
734,"Then , the self-attention mechanism is utilized to update both the context and masked response representation .",2,0.55490196,28.633559777447875,17
734,"Finally , the attention weights between each context and response representations are computed and used in the further decoding process .",2,0.62689084,53.04728983997923,21
734,"Experimental results on both Chinese customer services dataset and English Ubuntu dialogue dataset show that ReCoSa significantly outperforms baseline models , in terms of both metric-based and human evaluations .",3,0.94190663,36.30234097465372,30
734,"Further analysis on attention shows that the detected relevant contexts by ReCoSa are highly coherent with human ’s understanding , validating the correctness and interpretability of ReCoSa .",3,0.9747858,77.6068704566878,28
735,Consistency is a long standing issue faced by dialogue models .,0,0.91314733,46.01462250909616,11
735,"In this paper , we frame the consistency of dialogue agents as natural language inference ( NLI ) and create a new natural language inference dataset called Dialogue NLI .",1,0.8182244,31.786285390742393,30
735,"We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model , and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model ’s consistency .",2,0.3798368,30.221226460090637,51
736,"This paper presents a new approach that extends Deep Dyna-Q ( DDQ ) by incorporating a Budget-Conscious Scheduling ( BCS ) to best utilize a fixed , small amount of user interactions ( budget ) for learning task-oriented dialogue agents .",1,0.7441158,76.20001241898451,45
736,BCS consists of ( 1 ) a Poisson-based global scheduler to allocate budget over different stages of training ;,2,0.42124483,124.24294989996682,21
736,( 2 ) a controller to decide at each training step whether the agent is trained using real or simulated experiences ;,2,0.7001709,124.63588142130169,22
736,( 3 ) a user goal sampling module to generate the experiences that are most effective for policy learning .,2,0.6878121,166.00788288790847,20
736,Experiments on a movie-ticket booking task with simulated and real users show that our approach leads to significant improvements in success rate over the state-of-the-art baselines given the fixed budget .,3,0.87217754,13.572081094085602,37
737,"While conditional language models have greatly improved in their ability to output high quality natural language , many NLP applications benefit from being able to generate a diverse set of candidate sequences .",0,0.9173258,25.620130773371454,33
737,"Diverse decoding strategies aim to , within a given-sized candidate list , cover as much of the space of high-quality outputs as possible , leading to improvements for tasks that rerank and combine candidate outputs .",0,0.7555169,91.1377860456406,36
737,"Standard decoding methods , such as beam search , optimize for generating high likelihood sequences rather than diverse ones , though recent work has focused on increasing diversity in these methods .",0,0.8857493,87.7946339007434,32
737,"In this work , we perform an extensive survey of decoding-time strategies for generating diverse outputs from a conditional language model .",1,0.86175144,51.80097205169161,24
737,"In addition , we present a novel method where we over-sample candidates , then use clustering to remove similar sequences , thus achieving high diversity without sacrificing quality .",2,0.5550773,70.20952020104488,29
738,"Dialogue systems are usually built on either generation-based or retrieval-based approaches , yet they do not benefit from the advantages of different models .",0,0.8950622,21.422462305366615,28
738,"In this paper , we propose a Retrieval-Enhanced Adversarial Training ( REAT ) method for neural response generation .",1,0.8946652,17.799488172486022,21
738,"Distinct from existing approaches , the REAT method leverages an encoder-decoder framework in terms of an adversarial training paradigm , while taking advantage of N-best response candidates from a retrieval-based system to construct the discriminator .",2,0.5221453,32.20875474964524,38
738,An empirical study on a large scale public available benchmark dataset shows that the REAT method significantly outperforms the vanilla Seq2Seq model as well as the conventional adversarial training approach .,3,0.9097134,18.733553293211134,31
739,We study the task of response generation .,1,0.497962,31.201514720774615,8
739,"Conventional methods employ a fixed vocabulary and one-pass decoding , which not only make them prone to safe and general responses but also lack further refining to the first generated raw sequence .",0,0.7794821,114.16923506400812,34
739,"To tackle the above two problems , we present a Vocabulary Pyramid Network ( VPN ) which is able to incorporate multi-pass encoding and decoding with multi-level vocabularies into response generation .",2,0.46118796,44.60955715356952,32
739,"Specifically , the dialogue input and output are represented by multi-level vocabularies which are obtained from hierarchical clustering of raw words .",2,0.53948355,34.43577920478762,22
739,"Then , multi-pass encoding and decoding are conducted on the multi-level vocabularies .",2,0.7922154,32.81346680714917,13
739,"Since VPN is able to leverage rich encoding and decoding information with multi-level vocabularies , it has the potential to generate better responses .",3,0.52932674,33.71535581842202,24
739,Experiments on English Twitter and Chinese Weibo datasets demonstrate that VPN remarkably outperforms strong baselines .,3,0.85396254,24.131560678952898,16
740,A challenging problem in on-device text classification is to build highly accurate neural models that can fit in small memory footprint and have low latency .,0,0.9105339,31.38392246005383,27
740,"To address this challenge , we propose an on-device neural network SGNN ++ which dynamically learns compact projection vectors from raw text using structured and context-dependent partition projections .",2,0.39400235,100.91247136552134,29
740,We show that this results in accelerated inference and performance improvements .,3,0.92754966,45.472930470688354,12
740,"We conduct extensive evaluation on multiple conversational tasks and languages such as English , Japanese , Spanish and French .",2,0.6877781,36.7717325944256,20
740,"Our SGNN ++ model significantly outperforms all baselines , improves upon existing on-device neural models and even surpasses RNN , CNN and BiLSTM models on dialog act and intent prediction .",3,0.9389882,82.73551364376715,32
740,Through a series of ablation studies we show the impact of the partitioned projections and structured information leading to 10 % improvement .,3,0.7658078,68.07172857391848,23
740,We study the impact of the model size on accuracy and introduce quatization-aware training for SGNN + + to further reduce the model size while preserving the same quality .,1,0.3468981,66.17203099628102,32
740,"Finally , we show fast inference on mobile phones .",3,0.8961099,109.69939943676162,10
741,"Though great progress has been made for human-machine conversation , current dialogue system is still in its infancy : it usually converses passively and utters words more as a matter of response , rather than on its own initiatives .",0,0.927012,45.49313284194771,40
741,"In this paper , we take a radical step towards building a human-like conversational agent : endowing it with the ability of proactively leading the conversation ( introducing a new topic or maintaining the current topic ) .",1,0.83576316,27.594289063210123,38
741,"To facilitate the development of such conversation systems , we create a new dataset named Konv where one acts as a conversation leader and the other acts as the follower .",2,0.64791095,35.866995724523164,31
741,"The leader is provided with a knowledge graph and asked to sequentially change the discussion topics , following the given conversation goal , and meanwhile keep the dialogue as natural and engaging as possible .",2,0.6509124,78.76249708931725,35
741,Konv enables a very challenging task as the model needs to both understand dialogue and plan over the given knowledge graph .,0,0.6088355,78.7374881750124,22
741,We establish baseline results on this dataset ( about 270 K utterances and 30 k dialogues ) using several state-of-the-art models .,3,0.56954896,33.465122226728134,28
741,Experimental results show that dialogue models that plan over the knowledge graph can make full use of related knowledge to generate more diverse multi-turn conversations .,3,0.97796065,25.23261725576725,26
741,The baseline systems along with the dataset are publicly available .,2,0.4013412,55.77334261494382,11
742,We study learning of a matching model for response selection in retrieval-based dialogue systems .,1,0.6193628,55.87420140563204,17
742,"The problem is equally important with designing the architecture of a model , but is less explored in existing literature .",0,0.743836,58.57914702080344,21
742,"To learn a robust matching model from noisy training data , we propose a general co-teaching framework with three specific teaching strategies that cover both teaching with loss functions and teaching with data curriculum .",2,0.6720169,61.518212378706075,35
742,"Under the framework , we simultaneously learn two matching models with independent training sets .",2,0.66125816,70.38741563740632,15
742,"In each iteration , one model transfers the knowledge learned from its training set to the other model , and at the same time receives the guide from the other model on how to overcome noise in training .",2,0.5480449,33.477427654607745,39
742,"Through being both a teacher and a student , the two models learn from each other and get improved together .",0,0.6724887,32.255562526039206,21
742,Evaluation results on two public data sets indicate that the proposed learning approach can generally and significantly improve the performance of existing matching models .,3,0.95307904,21.707217848378793,25
743,Neural generative models for open-domain chit-chat conversations have become an active area of research in recent years .,0,0.9259084,12.063519437270656,18
743,A critical issue with most existing generative models is that the generated responses lack informativeness and diversity .,0,0.90549695,26.19388058088016,18
743,"A few researchers attempt to leverage the results of retrieval models to strengthen the generative models , but these models are limited by the quality of the retrieval results .",0,0.8839372,24.88786916675282,30
743,"In this work , we propose a memory-augmented generative model , which learns to abstract from the training corpus and saves the useful information to the memory to assist the response generation .",1,0.68107915,31.2225221452985,35
743,"Our model clusters query-response samples , extracts characteristics of each cluster , and learns to utilize these characteristics for response generation .",2,0.801209,133.24296203827075,22
743,Experimental results show that our model outperforms other competitive baselines .,3,0.9671331,6.471408177641752,11
744,"Due to its potential applications , open-domain dialogue generation has become popular and achieved remarkable progress in recent years , but sometimes suffers from generic responses .",0,0.9447075,36.05561985800472,27
744,"Previous models are generally trained based on 1-to-1 mapping from an input query to its response , which actually ignores the nature of 1-to-n mapping in dialogue that there may exist multiple valid responses corresponding to the same query .",0,0.8620694,29.127302605296787,47
744,"In this paper , we propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1-to-n mapping with a novel two-step generation architecture .",1,0.7598799,54.4679901054685,35
744,"The first generation phase extracts the common features of different responses which , combined with distinctive features obtained in the second phase , can generate multiple diverse and appropriate responses .",0,0.45997438,105.01465808814007,31
744,Experimental results show that our proposed model can effectively improve the quality of response and outperform existing neural dialogue models on both automatic and human evaluations .,3,0.9774301,10.279367010931399,27
745,This paper examines various unsupervised pretraining objectives for learning dialog context representations .,1,0.87131083,36.69885416788119,13
745,"Two novel methods of pretraining dialog context encoders are proposed , and a total of four methods are examined .",2,0.6220208,33.101637997455576,20
745,Each pretraining objective is fine-tuned and evaluated on a set of downstream dialog tasks using the MultiWoz dataset and strong performance improvement is observed .,2,0.4741744,27.715866124502003,25
745,"Further evaluation shows that our pretraining objectives result in not only better performance , but also better convergence , models that are less data hungry and have better domain generalizability .",3,0.9743007,43.77426373182947,31
746,"Disentangling conversations mixed together in a single stream of messages is a difficult task , made harder by the lack of large manually annotated datasets .",0,0.9285787,38.921709927934806,26
746,"We created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure .",2,0.8841333,62.452220866198815,24
746,"Our data is 16 times larger than all previously released datasets combined , the first to include adjudication of annotation disagreements , and the first to include context .",3,0.9069459,66.83307071132624,29
746,"We use our data to re-examine prior work , in particular , finding that 89 % of conversations in a widely used dialogue corpus are either missing messages or contain extra messages .",3,0.52773803,48.93504809841675,33
746,"Our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement , which will help advance dialogue research .",3,0.9616173,35.15650568933455,24
747,"The sequential order of utterances is often meaningful in coherent dialogues , and the order changes of utterances could lead to low-quality and incoherent conversations .",0,0.84353805,38.8335881201148,26
747,"We consider the order information as a crucial supervised signal for dialogue learning , which , however , has been neglected by many previous dialogue systems .",2,0.42426515,114.9977022262089,27
747,"Therefore , in this paper , we introduce a self-supervised learning task , inconsistent order detection , to explicitly capture the flow of conversation in dialogues .",1,0.76205605,55.41221582576481,27
747,"Given a sampled utterance pair triple , the task is to predict whether it is ordered or misordered .",0,0.4343257,54.971740725550404,19
747,Then we propose a sampling-based self-supervised network SSN to perform the prediction with sampled triple references from previous dialogue history .,2,0.7088228,61.91653436441609,23
747,"Furthermore , we design a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training .",2,0.596503,57.55236760880497,26
747,"We demonstrate that the proposed methods can be applied to both open-domain and task-oriented dialogue scenarios , and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets .",3,0.913859,14.327279872868262,38
748,The cognitive mechanisms needed to account for the English past tense have long been a subject of debate in linguistics and cognitive science .,0,0.93649125,25.11589636591893,24
748,"Neural network models were proposed early on , but were shown to have clear flaws .",0,0.86038804,75.67897765020358,16
748,"Recently , however , Kirov and Cotterell ( 2018 ) showed that modern encoder-decoder ( ED ) models overcome many of these flaws .",0,0.89400584,53.86983084860045,24
748,They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task .,3,0.7985119,97.47775284562381,15
748,"Here , we look more closely at the behaviour of their model in this task .",1,0.33083278,30.372960673258046,16
748,"We find that ( 1 ) the model exhibits instability across multiple simulations in terms of its correlation with human data , and ( 2 ) even when results are aggregated across simulations ( treating each simulation as an individual human participant ) , the fit to the human data is not strong — worse than an older rule-based model .",3,0.96049327,43.54178186237603,62
748,These findings hold up through several alternative training regimes and evaluation measures .,3,0.9827439,100.10501335405866,13
748,"Although other neural architectures might do better , we conclude that there is still insufficient evidence to claim that neural nets are a good cognitive model for this task .",3,0.961105,27.96405627349784,30
749,"We propose an unsupervised approach for assessing conceptual complexity of texts , based on spreading activation .",1,0.5125353,83.81390133374401,17
749,"Using DBpedia knowledge graph as a proxy to long-term memory , mentioned concepts become activated and trigger further activation as the text is sequentially traversed .",3,0.49516547,89.96512303259935,26
749,"Drawing inspiration from psycholinguistic theories of reading comprehension , we model memory processes such as semantic priming , sentence wrap-up , and forgetting .",2,0.5488998,53.614708586879324,26
749,We show that our models capture various aspects of conceptual text complexity and significantly outperform current state of the art .,3,0.9526317,24.058488132104593,21
750,End-to-end training with Deep Neural Networks ( DNN ) is a currently popular method for metaphor identification .,0,0.9311545,30.13376561198844,20
750,"However , standard sequence tagging models do not explicitly take advantage of linguistic theories of metaphor identification .",0,0.8999315,101.79859783119812,18
750,We experiment with two DNN models which are inspired by two human metaphor identification procedures .,2,0.87042296,98.72913070800855,16
750,"By testing on three public datasets , we find that our models achieve state-of-the-art performance in end-to-end metaphor identification .",3,0.88268703,11.369197516954317,28
751,Diachronic word embeddings have been widely used in detecting temporal changes .,0,0.93860537,26.02900001941725,12
751,"However , existing methods face the meaning conflation deficiency by representing a word as a single vector at each time period .",0,0.88660026,83.09541998786878,22
751,"To address this issue , this paper proposes a sense representation and tracking framework based on deep contextualized embeddings , aiming at answering not only what and when , but also how the word meaning changes .",1,0.80662763,43.19981612438736,37
751,"The experiments show that our framework is effective in representing fine-grained word senses , and it brings a significant improvement in word change detection task .",3,0.97473425,33.45412937453179,28
751,"Furthermore , we model the word change from an ecological viewpoint , and sketch two interesting sense behaviors in the process of language evolution , i.e .",2,0.6255605,118.26978072446578,27
751,sense competition and sense cooperation .,0,0.43725875,584.3031724859526,6
752,"Recent research studies communication emergence in communities of deep network agents assigned a joint task , hoping to gain insights on human language evolution .",0,0.85031223,299.04567847746347,25
752,"We propose here a new task capturing crucial aspects of the human environment , such as natural object affordances , and of human conversation , such as full symmetry among the participants .",3,0.4136389,135.4217333607923,33
752,"By conducting a thorough pragmatic and semantic analysis of the emergent protocol , we show that the agents solve the shared task through genuine bilateral , referential communication .",3,0.6539274,123.75019302464271,29
752,"However , the agents develop multiple idiolects , which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge .",3,0.95380193,117.50713351542763,27
753,"Lake and Baroni ( 2018 ) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations , such as inferring the meaning of “ jump around ” 0-shot from the component words .",0,0.609804,98.29138676261911,39
753,Recurrent networks ( RNNs ) were found to completely fail the most challenging generalization cases .,0,0.5062436,60.016857015569634,16
753,"We test here a convolutional network ( CNN ) on these tasks , reporting hugely improved performance with respect to RNNs .",2,0.5185158,81.00780881220732,22
753,"Despite the big improvement , the CNN has however not induced systematic rules , suggesting that the difference between compositional and non-compositional behaviour is not clear-cut .",3,0.9158647,59.07883158644609,27
754,"The study of linguistic typology is rooted in the implications we find between linguistic features , such as the fact that languages with object-verb word ordering tend to have postpositions .",0,0.8044708,48.43722229153053,33
754,"Uncovering such implications typically amounts to time-consuming manual processing by trained and experienced linguists , which potentially leaves key linguistic universals unexplored .",0,0.8676269,65.50778303679343,25
754,"In this paper , we present a computational model which successfully identifies known universals , including Greenberg universals , but also uncovers new ones , worthy of further linguistic investigation .",1,0.81655586,87.38653470193037,31
754,"Our approach outperforms baselines previously used for this problem , as well as a strong baseline from knowledge base population .",3,0.8295191,49.961977882162984,21
755,"When learning language , infants need to break down the flow of input speech into minimal word-like units , a process best described as unsupervised bottom-up segmentation .",0,0.9090783,51.53264254250705,31
755,"Proposed strategies include several segmentation algorithms , but only cross-linguistically robust algorithms could be plausible candidates for human word learning , since infants have no initial knowledge of the ambient language .",3,0.7378693,146.98263986414818,32
755,We report on the stability in performance of 11 conceptually diverse algorithms on a selection of 8 typologically distinct languages .,1,0.60465795,79.65392147032081,21
755,"The results consist evidence that some segmentation algorithms are cross-linguistically valid , thus could be considered as potential strategies employed by all infants .",3,0.9882583,193.06210647999652,24
756,Embedding a clause inside another ( “ the girl [ who likes cars [ that run fast ] ] has arrived ” ) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness .,0,0.8961251,110.50417791933118,40
756,"As such , it plays a central role in fundamental debates on what makes human language unique , and how they might have evolved .",0,0.8067789,40.892953640548505,25
756,Empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size .,0,0.87938964,43.46846954269213,26
756,"We introduce here a collection of large , dependency-parsed written corpora in 17 languages , that allow us , for the first time , to capture clausal embedding through dependency graphs and assess their distribution .",2,0.33114994,80.29856290708264,38
756,Our results indicate that there is no evidence for hard constraints on embedding depth : the tail of depth distributions is heavy .,3,0.9897031,96.11547443405736,23
756,"Moreover , although deeply embedded clauses tend to be shorter , suggesting processing load issues , complex sentences with many embeddings do not display a bias towards less deep embeddings .",3,0.938316,106.15277415002504,31
756,"Taken together , the results suggest that deep embeddings are not disfavoured in written language .",3,0.98841804,40.947773579383586,16
756,"More generally , our study illustrates how resources and methods from latest-generation big-data NLP can provide new perspectives on fundamental questions in theoretical linguistics .",3,0.98614234,60.357576586501935,27
757,"In this paper , we present a novel approach for incorporating external knowledge in Recurrent Neural Networks ( RNNs ) .",1,0.898008,13.393972117820748,21
757,We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures .,3,0.45114893,16.84561660475416,17
757,"This form of conditioning on the attention distribution , enforces the contribution of the most salient words for the task at hand .",3,0.42478827,45.368645708030684,23
757,"We introduce three methods , namely attentional concatenation , feature-based gating and affine transformation .",2,0.7098174,79.28026064786732,17
757,Experiments on six benchmark datasets show the effectiveness of our methods .,3,0.7930597,8.326880985457558,12
757,Attentional feature-based gating yields consistent performance improvement across tasks .,3,0.8261779,59.96553779713252,12
757,Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture .,3,0.6516955,17.126650798922636,30
758,"We introduce a new benchmark for coreference resolution and NLI , KnowRef , that targets common-sense understanding and world knowledge .",1,0.47491658,131.6219547094465,21
758,"Previous coreference resolution tasks can largely be solved by exploiting the number and gender of the antecedents , or have been handcrafted and do not reflect the diversity of naturally occurring text .",0,0.8771386,41.76075232137694,33
758,"We present a corpus of over 8,000 annotated text passages with ambiguous pronominal anaphora .",2,0.57428384,23.13657921183257,15
758,These instances are both challenging and realistic .,0,0.8473148,53.03346796778926,8
758,"We show that various coreference systems , whether rule-based , feature-rich , or neural , perform significantly worse on the task than humans , who display high inter-annotator agreement .",3,0.96162546,58.33933410248731,32
758,"To explain this performance gap , we show empirically that state-of-the art models often fail to capture context , instead relying on the gender or number of candidate antecedents to make a decision .",3,0.40397143,28.637983867858146,38
758,We then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models .,2,0.5676605,56.90467755836067,20
758,"Finally , we show that antecedent switching yields promising results on other tasks as well : we use it to achieve state-of-the-art results on the GAP coreference task .",3,0.95244694,15.810910280767786,35
759,"Wikipedia can easily be justified as a behemoth , considering the sheer volume of content that is added or removed every minute to its several projects .",0,0.72219795,67.67803857477084,27
759,"This creates an immense scope , in the field of natural language processing toward developing automated tools for content moderation and review .",0,0.7603419,72.78160088556747,23
759,In this paper we propose Self Attentive Revision Encoder ( StRE ) which leverages orthographic similarity of lexical units toward predicting the quality of new edits .,1,0.81848365,60.85350912614382,27
759,"In contrast to existing propositions which primarily employ features like page reputation , editor activity or rule based heuristics , we utilize the textual content of the edits which , we believe contains superior signatures of their quality .",2,0.5981086,251.21472705638615,39
759,"More specifically , we deploy deep encoders to generate representations of the edits from its text content , which we then leverage to infer quality .",2,0.7964058,73.41867572388918,26
759,We further contribute a novel dataset containing ∼ 21 M revisions across 32 K Wikipedia pages and demonstrate that StRE outperforms existing methods by a significant margin – at least 17 % and at most 103 % .,3,0.8348682,77.31606967658966,38
759,Our pre-trained model achieves such result after retraining on a set as small as 20 % of the edits in a wikipage .,3,0.86822754,41.78197509734879,23
759,"This , to the best of our knowledge , is also the first attempt towards employing deep language models to the enormous domain of automated content moderation and review in Wikipedia .",3,0.9436322,42.69937341748081,32
760,Most current NLP systems have little knowledge about quantitative attributes of objects and events .,0,0.9197186,41.511668640852754,15
760,"We propose an unsupervised method for collecting quantitative information from large amounts of web data , and use it to create a new , very large resource consisting of distributions over physical quantities associated with objects , adjectives , and verbs which we call Distributions over Quantitative ( DoQ ) .",2,0.5022441,52.41885863422361,51
760,This contrasts with recent work in this area which has focused on making only relative comparisons such as “ Is a lion bigger than a wolf ? ” .,0,0.61615396,35.67689153871446,29
760,"Our evaluation shows that DoQ compares favorably with state of the art results on existing datasets for relative comparisons of nouns and adjectives , and on a new dataset we introduce .",3,0.95429975,41.335084194725596,32
761,Sentence function is an important linguistic feature referring to a user ’s purpose in uttering a specific sentence .,0,0.9081305,54.17569210469744,19
761,The use of sentence function has shown promising results to improve the performance of conversation models .,0,0.6097403,32.38345944129735,17
761,"However , there is no large conversation dataset annotated with sentence functions .",0,0.87460595,79.97810964831558,13
761,"In this work , we collect a new Short-Text Conversation dataset with manually annotated SEntence FUNctions ( STC-Sefun ) .",2,0.5610494,126.8106762743653,24
761,Classification models are trained on this dataset to ( i ) recognize the sentence function of new data in a large corpus of short-text conversations ;,2,0.68562984,122.8051373886278,28
761,( ii ) estimate a proper sentence function of the response given a test query .,2,0.6485673,180.12253532029374,16
761,"We later train conversation models conditioned on the sentence functions , including information retrieval-based and neural generative models .",2,0.74819833,92.64139893730453,21
761,Experimental results demonstrate that the use of sentence functions can help improve the quality of the returned responses .,3,0.97339225,22.583276916536988,19
762,"While the vast majority of existing work on automated essay scoring has focused on holistic scoring , researchers have recently begun work on scoring specific dimensions of essay quality .",0,0.9296963,29.436721277677997,30
762,"Nevertheless , progress on dimension-specific essay scoring is limited in part by the lack of annotated corpora .",0,0.88013417,36.49475112989518,19
762,"To facilitate advances in this area , we design a scoring rubric for scoring a core , yet unexplored dimension of persuasive essay quality , thesis strength , and annotate a corpus of essays with thesis strength scores .",1,0.46936995,109.79846460428817,39
762,"We additionally identify the attributes that could impact thesis strength and annotate the essays with the values of these attributes , which , when predicted by computational models , could provide further feedback to students on why her essay receives a particular thesis strength score .",2,0.54298073,91.54965163381048,46
763,Sentiment analysis has a range of corpora available across multiple languages .,0,0.90205264,55.279390311378464,12
763,"For emotion analysis , the situation is more limited , which hinders potential research on crosslingual modeling and the development of predictive models for other languages .",0,0.6478662,55.67640910898661,27
763,"In this paper , we fill this gap for German by constructing deISEAR , a corpus designed in analogy to the well-established English ISEAR emotion dataset .",1,0.76587784,105.98874817103281,29
763,"Motivated by Scherer ’s appraisal theory , we implement a crowdsourcing experiment which consists of two steps .",2,0.76564527,46.66213114786758,18
763,"In step 1 , participants create descriptions of emotional events for a given emotion .",2,0.7740693,122.32648219034674,15
763,"In step 2 , five annotators assess the emotion expressed by the texts .",2,0.7868427,130.74920230971196,14
763,"We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not , on average , cause a performance drop .",3,0.925946,96.74020764051075,32
764,"This paper presents a new multilingual corpus with semantic annotation of collocations in English , Portuguese , and Spanish .",1,0.747995,41.12398782412802,20
764,"The whole resource contains 155 k tokens and 1,526 collocations labeled in context .",3,0.5082008,168.43657299060104,14
764,"The annotated examples belong to three syntactic relations ( adjective-noun , verb-object , and nominal compounds ) , and represent 58 lexical functions in the Meaning-Text Theory ( e.g. , Oper , Magn , Bon , etc. ) .",3,0.5808865,93.36461087480684,42
764,Each collocation was annotated by three linguists and the final resource was revised by a team of experts .,2,0.84846073,39.43663684367407,19
764,"The resulting corpus can serve as a basis to evaluate different approaches for collocation identification , which in turn can be useful for different NLP tasks such as natural language understanding or natural language generation .",3,0.816164,19.38331124591821,36
765,Introducing common sense to natural language understanding systems has received increasing research attention .,0,0.95210344,30.514272726300284,14
765,It remains a fundamental question on how to evaluate whether a system has the sense-making capability .,0,0.9288501,30.235135892654657,17
765,Existing benchmarks measure common sense knowledge indirectly or without reasoning .,0,0.8382515,272.1354382653676,11
765,"In this paper , we release a benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense .",1,0.78262967,20.721677649942638,30
765,"In addition , a system is asked to identify the most crucial reason why a statement does not make sense .",2,0.56276697,39.51698710619262,21
765,"We evaluate models trained over large-scale language modeling tasks as well as human performance , showing that there are different challenges for system sense-making .",3,0.6301529,55.828926836212986,25
766,The task of humor recognition has attracted a lot of attention recently due to the urge to process large amounts of user-generated texts and rise of conversational agents .,0,0.9682608,25.78382291075926,29
766,We collected a dataset of jokes and funny dialogues in Russian from various online resources and complemented them carefully with unfunny texts with similar lexical properties .,2,0.93088484,75.07657742949311,27
766,"The dataset comprises of more than 300,000 short texts , which is significantly larger than any previous humor-related corpus .",3,0.46530744,37.127084460468616,20
766,"Manual annotation of 2,000 items proved the reliability of the corpus construction approach .",3,0.8569556,79.91418024408998,14
766,"Further , we applied language model fine-tuning for text classification and obtained an F1 score of 0.91 on a test set , which constitutes a considerable gain over baseline methods .",3,0.86041045,22.207016903229718,31
766,The dataset is freely available for research community .,3,0.68771863,40.23301162435504,9
767,"When a bilingual student learns to solve word problems in math , we expect the student to be able to solve these problem in both languages the student is fluent in , even if the math lessons were only taught in one language .",0,0.57409626,20.130256233465,44
767,"However , current representations in machine learning are language dependent .",0,0.9072833,135.5301320858163,11
767,"In this work , we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion .",1,0.7317247,33.73018182643358,41
767,"We learn these representations by taking inspiration from linguistics , specifically the Universal Grammar hypothesis and learn universal latent representations that are language agnostic .",2,0.6139093,68.61710523121168,25
767,We demonstrate the capabilities of these representations by showing that models trained on a single language using language agnostic representations achieve very similar accuracies in other languages .,3,0.79727507,26.783505867586825,28
768,Short texts such as tweets often contain insufficient word co-occurrence information for training conventional topic models .,0,0.8979482,41.67699241598961,17
768,"To deal with the insufficiency , we propose a generative model that aggregates short texts into clusters by leveraging the associated meta information .",2,0.53884786,32.324618560551585,24
768,Our model can generate more interpretable topics as well as document clusters .,3,0.89416987,58.05132061096215,13
768,We develop an effective Gibbs sampling algorithm favoured by the fully local conjugacy in the model .,2,0.55173504,177.15935474651147,17
768,Extensive experiments demonstrate that our model achieves better performance in terms of document clustering and topic coherence .,3,0.90866953,17.561819275045327,18
769,Encoder-decoder models for unsupervised sentence representation learning using the distributional hypothesis effectively constrain the learnt representation of a sentence to only that needed to reproduce the next sentence .,0,0.52942216,42.66975894923743,29
769,"While the decoder is important to constrain the representation , these models tend to discard the decoder after training since only the encoder is needed to map the input sentence into a vector representation .",0,0.741948,21.68187834531856,35
769,"However , parameters learnt in the decoder also contain useful information about the language .",0,0.6944283,61.27893077662069,15
769,"In order to utilise the decoder after learning , we present two types of decoding functions whose inverse can be easily derived without expensive inverse calculation .",2,0.58344966,76.06823046334658,27
769,"Therefore , the inverse of the decoding function serves as another encoder that produces sentence representations .",3,0.45913738,57.57665988419065,17
769,"We show that , with careful design of the decoding functions , the model learns good sentence representations , and the ensemble of the representations produced from the encoder and the inverse of the decoder demonstrate even better generalisation ability and solid transferability .",3,0.9501942,54.113741309731004,44
770,"There exist few text-specific methods for unsupervised anomaly detection , and for those that do exist , none utilize pre-trained models for distributed vector representations of words .",0,0.88948786,41.53081414853044,29
770,In this paper we introduce a new anomaly detection method — Context Vector Data Description ( CVDD ) — which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism .,1,0.827108,28.0428728497219,40
770,Modeling multiple contexts enables us to perform contextual anomaly detection of sentences and phrases with respect to the multiple themes and concepts present in an unlabeled text corpus .,0,0.4246917,48.086550813373194,29
770,These contexts in combination with the self-attention weights make our method highly interpretable .,3,0.88268447,33.58251090212825,14
770,"We demonstrate the effectiveness of CVDD quantitatively as well as qualitatively on the well-known Reuters , 20 Newsgroups , and IMDB Movie Reviews datasets .",3,0.76660967,72.7322324618889,26
771,Bilingual Lexicon Induction ( BLI ) is the task of translating words from corpora in two languages .,0,0.8956708,24.87435579889903,18
771,Recent advances in BLI work by aligning the two word embedding spaces .,0,0.7096238,55.123432067179145,13
771,"Following that , a key step is to retrieve the nearest neighbor ( NN ) in the target space given the source word .",0,0.5024654,35.40716922063128,24
771,"However , a phenomenon called hubness often degrades the accuracy of NN .",0,0.8713859,125.05682741652177,13
771,"Hubness appears as some data points , called hubs , being extra-ordinarily close to many of the other data points .",0,0.5204423,96.34545626788471,21
771,Reducing hubness is necessary for retrieval tasks .,0,0.5667728,262.5300177755685,8
771,"One successful example is Inverted SoFtmax ( ISF ) , recently proposed to improve NN .",0,0.8237103,209.22096486298975,16
771,"This work proposes a new method , Hubless Nearest Neighbor ( HNN ) , to mitigate hubness .",1,0.710881,128.96443230734744,18
771,HNN differs from NN by imposing an additional equal preference assumption .,3,0.3698194,283.4818591762723,12
771,"Moreover , the HNN formulation explains why ISF works as well as it does .",3,0.7782963,93.51427242408941,15
771,"Empirical results demonstrate that HNN outperforms NN , ISF and other state-of-the-art .",3,0.968679,28.868106869852593,17
771,"For reproducibility and follow-ups , we have published all code .",2,0.5274211,45.293607289400946,12
772,"Accurate entity linkers have been produced for domains and languages where annotated data ( i.e. , texts linked to a knowledge base ) is available .",0,0.8212296,53.8898319969583,26
772,"However , little progress has been made for the settings where no or very limited amounts of labeled data are present ( e.g. , legal or most scientific domains ) .",0,0.8517206,64.00640974374718,31
772,"In this work , we show how we can learn to link mentions without having any labeled examples , only a knowledge base and a collection of unannotated texts from the corresponding domain .",1,0.6024542,43.494137616474234,34
772,"In order to achieve this , we frame the task as a multi-instance learning problem and rely on surface matching to create initial noisy labels .",2,0.67975867,36.436909085818165,26
772,"As the learning signal is weak and our surrogate labels are noisy , we introduce a noise detection component in our model : it lets the model detect and disregard examples which are likely to be noisy .",2,0.64385027,56.40635368694436,38
772,"Our method , jointly learning to detect noise and link entities , greatly outperforms the surface matching baseline .",3,0.81485856,261.8922298216882,19
772,"For a subset of entity categories , it even approaches the performance of supervised learning .",3,0.8384053,85.23195278187339,16
773,Heuristic-based active learning ( AL ) methods are limited when the data distribution of the underlying learning problems vary .,0,0.92103356,103.87918393119588,22
773,Recent data-driven AL policy learning methods are also restricted to learn from closely related domains .,0,0.8799251,131.48633266895257,16
773,We introduce a new sample-efficient method that learns the AL policy directly on the target domain of interest by using wake and dream cycles .,2,0.64682156,125.1315683152564,27
773,Our approach interleaves between querying the annotation of the selected datapoints to update the underlying student learner and improving AL policy using simulation where the current student learner acts as an imperfect annotator .,2,0.7190406,77.99581105696774,34
773,We evaluate our method on cross-domain and cross-lingual text classification and named entity recognition tasks .,2,0.4788542,9.72822609492826,16
773,Experimental results show that our dream-based AL policy training strategy is more effective than applying the pretrained policy without further fine-tuning and better than the existing strong baseline methods that use heuristics or reinforcement learning .,3,0.9716745,32.00790533379175,37
774,"Existing approaches for learning word embedding often assume there are sufficient occurrences for each word in the corpus , such that the representation of words can be accurately estimated from their contexts .",0,0.83999354,29.796375790327257,33
774,"However , in real-world scenarios , out-of-vocabulary ( a.k.a .",0,0.82990366,21.35738947234401,10
774,OOV ) words that do not appear in training corpus emerge frequently .,3,0.6185654,156.80151264685904,13
774,How to learn accurate representations of these words to augment a pre-trained embedding by only a few observations is a challenging research problem .,0,0.8554147,33.88805076955118,24
774,"In this paper , we formulate the learning of OOV embedding as a few-shot regression problem by fitting a representation function to predict an oracle embedding vector ( defined as embedding trained with abundant observations ) based on limited contexts .",1,0.5359942,72.12879569522767,41
774,"Specifically , we propose a novel hierarchical attention network-based embedding framework to serve as the neural regression function , in which the context information of a word is encoded and aggregated from K observations .",2,0.7312815,38.40438567214504,37
774,"Furthermore , we propose to use Model-Agnostic Meta-Learning ( MAML ) for adapting the learned model to the new corpus fast and robustly .",2,0.48966828,39.21758795628263,24
774,Experiments show that the proposed approach significantly outperforms existing methods in constructing an accurate embedding for OOV words and improves downstream tasks when the embedding is utilized .,3,0.94722074,26.404061606799118,28
775,"Language usage can change across periods of time , but document classifiers models are usually trained and tested on corpora spanning multiple years without considering temporal variations .",0,0.89322764,54.56428974795736,28
775,This paper describes two complementary ways to adapt classifiers to shifts across time .,1,0.8179258,67.2125504661993,14
775,"First , we show that diachronic word embeddings , which were originally developed to study language change , can also improve document classification , and we show a simple method for constructing this type of embedding .",1,0.36383706,32.268531020961774,37
775,"Second , we propose a time-driven neural classification model inspired by methods for domain adaptation .",2,0.6537606,40.25012792288606,16
775,Experiments on six corpora show how these methods can make classifiers more robust over time .,3,0.83450496,22.455895648986512,16
776,Learning representations such that the source and target distributions appear as similar as possible has benefited transfer learning tasks across several applications .,0,0.872522,72.33783332331033,23
776,Generally it requires labeled data from the source and only unlabeled data from the target to learn such representations .,0,0.8345091,25.217990767644867,20
776,While these representations act like a bridge to transfer knowledge learned in the source to the target ;,0,0.74324995,58.81012990906791,18
776,they may lead to negative transfer when the source specific characteristics detract their ability to represent the target data .,0,0.6402682,94.33718861376285,20
776,We present a novel neural network architecture to simultaneously learn a two-part representation which is based on the principle of segregating source specific representation from the common representation .,1,0.443313,30.521148536070978,30
776,The first part captures the source specific characteristics while the second part captures the truly common representation .,2,0.51101875,63.15621662568442,18
776,Our architecture optimizes an objective function which acts adversarial for the source specific part if it contributes towards the cross-domain learning .,2,0.5195805,131.26169130190286,22
776,"We empirically show that two parts of the representation , in different arrangements , outperforms existing learning algorithms on the source learning as well as cross-domain tasks on multiple datasets .",3,0.9157729,65.92137435543589,31
777,"Direct comparison on point estimation of the precision ( P ) , recall ( R ) , and F1 measure of two natural language processing ( NLP ) models on a common test corpus is unreasonable and results in less replicable conclusions due to a lack of a statistical test .",0,0.7676561,56.928316411510686,51
777,"However , the existing t-tests in cross-validation ( CV ) for model comparison are inappropriate because the distributions of P , R , F1 are skewed and an interval estimation of P , R , and F1 based on a t-test may exceed [ 0,1 ] .",0,0.81635034,56.511887137823074,47
777,"In this study , we propose to use a block-regularized 3×2 CV ( 3×2 BCV ) in model comparison because it could regularize the difference in certain frequency distributions over linguistic units between training and validation sets and yield stable estimators of P , R , and F1 .",1,0.6573795,82.88469524337299,50
777,"On the basis of the 3×2 BCV , we calibrate the posterior distributions of P , R , and F1 and derive an accurate interval estimation of P , R , and F1 .",2,0.6542293,28.89153129000038,34
777,"Furthermore , we formulate the comparison into a hypothesis testing problem and propose a novel Bayes test .",2,0.66242814,43.08586727315933,18
777,The test could directly compute the probabilities of the hypotheses on the basis of the posterior distributions and provide more informative decisions than the existing significance t-tests .,3,0.65296024,42.69489430989042,28
777,"Three experiments with regard to NLP chunking tasks are conducted , and the results illustrate the validity of the Bayes test .",3,0.74031967,58.56423285041588,22
778,"Text infilling aims at filling in the missing part of a sentence or paragraph , which has been applied to a variety of real-world natural language generation scenarios .",0,0.9006824,19.74846207596797,29
778,"Given a well-trained sequential generative model , it is challenging for its unidirectional decoder to generate missing symbols conditioned on the past and future information around the missing part .",0,0.83870596,46.43869690562506,32
778,"In this paper , we propose an iterative inference algorithm based on gradient search , which could be the first inference algorithm that can be broadly applied to any neural sequence generative models for text infilling tasks .",1,0.8458455,26.556384036056535,38
778,"Extensive experimental comparisons show the effectiveness and efficiency of the proposed method on three different text infilling tasks with various mask ratios and different mask strategies , comparing with five state-of-the-art methods .",3,0.73214656,23.976551687089703,39
779,"We introduce the Scratchpad Mechanism , a novel addition to the sequence-to-sequence ( seq2seq ) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks .",2,0.37289587,12.999565001284497,38
779,"By enabling the decoder at each time step to write to all of the encoder output layers , Scratchpad can employ the encoder as a “ scratchpad ” memory to keep track of what has been generated so far and thereby guide future generation .",3,0.4237609,32.68394173054036,45
779,"We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation , Question Generation , and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task .",2,0.6188518,15.635931418819457,44
779,"Qualitative assessments in the form of human judgements ( question generation ) , attention visualization ( MT ) , and sample output ( summarization ) provide further evidence of the ability of Scratchpad to generate fluent and expressive output .",3,0.87317926,67.63090647354015,40
780,The common practice in coreference resolution is to identify and evaluate the maximum span of mentions .,0,0.90699697,30.046020607852135,17
780,The use of maximum spans tangles coreference evaluation with the challenges of mention boundary detection like prepositional phrase attachment .,3,0.74379826,331.5640776030758,20
780,"To address this problem , minimum spans are manually annotated in smaller corpora .",0,0.7873793,92.66649365679793,14
780,"However , this additional annotation is costly and therefore , this solution does not scale to large corpora .",0,0.7690894,55.426723733982904,19
780,"In this paper , we propose the MINA algorithm for automatically extracting minimum spans to benefit from minimum span evaluation in all corpora .",1,0.8969671,68.1456131616971,24
780,We show that the extracted minimum spans by MINA are consistent with those that are manually annotated by experts .,3,0.95179445,47.72574053637072,20
780,"Our experiments show that using minimum spans is in particular important in cross-dataset coreference evaluation , in which detected mention boundaries are noisier due to domain shift .",3,0.9803017,98.33521903452286,28
780,We have integrated MINA into https://github.com/ns-moosavi/coval for reporting standard coreference scores based on both maximum and automatically detected minimum spans .,2,0.62277645,139.39227754767174,21
781,Recognizing coreferring events and entities across multiple texts is crucial for many NLP applications .,0,0.929153,39.285323143041865,15
781,"Despite the task ’s importance , research focus was given mostly to within-document entity coreference , with rather little attention to the other variants .",0,0.4927792,208.34448075880078,25
781,We propose a neural architecture for cross-document coreference resolution .,1,0.51088375,30.23223816703041,10
781,Inspired by Lee et al .,0,0.6446903,17.642521375423854,6
781,"( 2012 ) , we jointly model entity and event coreference .",2,0.6045103,104.10416048644414,12
781,"We represent an event ( entity ) mention using its lexical span , surrounding context , and relation to entity ( event ) mentions via predicate-arguments structures .",2,0.8163504,145.22113593638485,30
781,"Our model outperforms the previous state-of-the-art event coreference model on ECB + , while providing the first entity coreference results on this corpus .",3,0.93801993,33.89470896526635,30
781,"Our analysis confirms that all our representation elements , including the mention span itself , its context , and the relation to other mentions contribute to the model ’s success .",3,0.9814906,71.86321624718049,31
782,We propose an efficient neural framework for sentence-level discourse analysis in accordance with Rhetorical Structure Theory ( RST ) .,1,0.42351273,33.486104822428096,22
782,"Our framework comprises a discourse segmenter to identify the elementary discourse units ( EDU ) in a text , and a discourse parser that constructs a discourse tree in a top-down fashion .",2,0.74937856,32.363977957949444,33
782,Both the segmenter and the parser are based on Pointer Networks and operate in linear time .,3,0.42112237,32.91930699045472,17
782,"Our segmenter yields an F1 score of 95.4 % , and our parser achieves an F1 score of 81.7 % on the aggregated labeled ( relation ) metric , surpassing previous approaches by a good margin and approaching human agreement on both tasks ( 98.3 and 83.0 F1 ) .",3,0.921821,34.73550608591805,50
783,It has been shown that implicit connectives can be exploited to improve the performance of the models for implicit discourse relation recognition ( IDRR ) .,0,0.88823146,42.83433301970354,26
783,An important property of the implicit connectives is that they can be accurately mapped into the discourse relations conveying their functions .,0,0.72321117,53.516205890066374,22
783,"In this work , we explore this property in a multi-task learning framework for IDRR in which the relations and the connectives are simultaneously predicted , and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives .",1,0.46147907,28.42569179577295,47
783,"We propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for IDRR on several settings of the benchmark dataset ( i.e. , the Penn Discourse Treebank dataset ) .",2,0.47573826,42.326426648523565,39
784,"When a speaker , Mary , asks “ Do you know that Florence is packed with visitors ? ” , we take her to believe that Florence is packed with visitors , but not if she asks “ Do you think that Florence is packed with visitors ? ” .",0,0.6894413,18.909043195494984,50
784,Inferring speaker commitment ( aka event factuality ) is crucial for information extraction and question answering .,0,0.88179123,118.80125751729193,17
784,"Here , we explore the hypothesis that linguistic deficits drive the error patterns of existing speaker commitment models by analyzing the linguistic correlates of model error on a challenging naturalistic dataset .",1,0.8503077,75.16795701478495,32
784,"We evaluate two state-of-the-art speaker commitment models on the CommitmentBank , an English dataset of naturally occurring discourses .",2,0.7929844,43.32086195409353,23
784,"The CommitmentBank is annotated with speaker commitment towards the content of the complement ( “ Florence is packed with visitors ” in our example ) of clause-embedding verbs ( “ know ” , “ think ” ) under four entailment-canceling environments ( negation , modal , question , conditional ) .",2,0.50485337,114.62619210502716,55
784,"A breakdown of items by linguistic features reveals asymmetrical error patterns : while the models achieve good performance on some classes ( e.g. , negation ) , they fail to generalize to the diverse linguistic constructions ( e.g. , conditionals ) in natural language , highlighting directions for improvement .",3,0.86834204,45.861791158075995,50
785,Modeling script knowledge can be useful for a wide range of NLP tasks .,3,0.47288242,23.799052912314234,14
785,"Current statistical script learning approaches embed the events , such that their relationships are indicated by their similarity in the embedding .",0,0.8382196,104.04689083999754,22
785,"While intuitive , these approaches fall short of representing nuanced relations , needed for downstream tasks .",0,0.71126294,194.8598715984101,17
785,"In this paper , we suggest to view learning event embedding as a multi-relational problem , which allows us to capture different aspects of event pairs .",1,0.78464997,37.157210454846606,27
785,"We model a rich set of event relations , such as Cause and Contrast , derived from the Penn Discourse Tree Bank .",2,0.8284962,77.89776208585732,23
785,"We evaluate our model on three types of tasks , the popular Mutli-Choice Narrative Cloze and its variants , several multi-relational prediction tasks , and a related downstream task — implicit discourse sense classification .",2,0.69253397,95.27345698249577,36
786,"In this paper , we propose a method for why-question answering ( why-QA ) that uses an adversarial learning framework .",1,0.879096,17.15164255023307,24
786,Existing why-QA methods retrieve “ answer passages ” that usually consist of several sentences .,0,0.88175035,144.64800588653452,16
786,"These multi-sentence passages contain not only the reason sought by a why-question and its connection to the why-question , but also redundant and / or unrelated parts .",0,0.6310183,53.244183211394045,30
786,We use our proposed “ Adversarial networks for Generating compact-answer Representation ” ( AGR ) to generate from a passage a vector representation of the non-redundant reason sought by a why-question and exploit the representation for judging whether the passage actually answers the why-question .,2,0.8450419,49.18995753183316,49
786,"Through a series of experiments using Japanese why-QA datasets , we show that these representations improve the performance of our why-QA neural model as well as that of a BERT-based why-QA model .",3,0.7791463,14.37844197185529,40
786,"We show that they also improve a state-of-the-art distantly supervised open-domain QA ( DS-QA ) method on publicly available English datasets , even though the target task is not a why-QA .",3,0.886099,28.457949287317007,42
787,Machine reading comprehension with unanswerable questions is a challenging task .,0,0.9551078,20.05173755624817,11
787,"In this work , we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding paragraph that contains the answer .",1,0.7163282,23.924827905537796,31
787,"We introduce a pair-to-sequence model for unanswerable question generation , which effectively captures the interactions between the question and the paragraph .",2,0.6749075,26.89098128612973,22
787,We also present a way to construct training data for our question generation models by leveraging the existing reading comprehension dataset .,3,0.4739635,24.458413752861453,22
787,Experimental results show that the pair-to-sequence model performs consistently better compared with the sequence-to-sequence baseline .,3,0.96921456,9.52523994935116,18
787,"We further use the automatically generated unanswerable questions as a means of data augmentation on the SQuAD 2.0 dataset , yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model .",3,0.686518,14.380180117371385,41
788,Multi-hop reading comprehension ( RC ) questions are challenging because they require reading and reasoning over multiple paragraphs .,0,0.94780713,27.62591264366582,19
788,We argue that it can be difficult to construct large multi-hop RC datasets .,3,0.79168195,38.68154473457245,14
788,"For example , even highly compositional questions can be answered with a single hop if they target specific entity types , or the facts needed to answer them are redundant .",0,0.73232675,78.04361652318265,31
788,"Our analysis is centered on HotpotQA , where we show that single-hop reasoning can solve much more of the dataset than previously thought .",3,0.52965164,32.12564304850439,26
788,We introduce a single-hop BERT-based RC model that achieves 67 F1 — comparable to state-of-the-art multi-hop models .,2,0.6146261,15.464496501202195,27
788,We also design an evaluation setting where humans are not shown all of the necessary paragraphs for the intended multi-hop reasoning but can still answer over 80 % of questions .,2,0.70022476,56.050829318960076,31
788,"Together with detailed error analysis , these results suggest there should be an increasing focus on the role of evidence in multi-hop reasoning and possibly even a shift towards information retrieval style evaluations with large and diverse evidence collections .",3,0.9898207,37.75726480032861,40
789,"We propose a new end-to-end question answering model , which learns to aggregate answer evidence from an incomplete knowledge base ( KB ) and a set of retrieved text snippets .",2,0.4337826,40.444693014671174,32
789,"Under the assumptions that structured data is easier to query and the acquired knowledge can help the understanding of unstructured text , our model first accumulates knowledge ofKB entities from a question-related KB sub-graph ;",2,0.54016614,76.13173323305833,37
789,then reformulates the question in the latent space and reads the text with the accumulated entity knowledge at hand .,2,0.7041812,50.71058096382733,20
789,The evidence from KB and text are finally aggregated to predict answers .,2,0.598252,127.8009988454489,13
789,"On the widely-used KBQA benchmark WebQSP , our model achieves consistent improvements across settings with different extents of KB incompleteness .",3,0.89548904,41.509906984990515,23
790,Neural semantic parsers utilize the encoder-decoder framework to learn an end-to-end model for semantic parsing that transduces a natural language sentence to the formal semantic representation .,0,0.79761434,12.149957341566559,29
790,"To keep the model aware of the underlying grammar in target sequences , many constrained decoders were devised in a multi-stage paradigm , which decode to the sketches or abstract syntax trees first , and then decode to target semantic tokens .",0,0.55832976,99.70798934887897,42
790,We instead to propose an adaptive decoding method to avoid such intermediate representations .,2,0.46938124,85.45051945164252,14
790,The decoder is guided by model uncertainty and automatically uses deeper computations when necessary .,2,0.4288495,74.07262875522764,15
790,Thus it can predict tokens adaptively .,3,0.54172873,165.6955792054015,7
790,Our model outperforms the state-of-the-art neural models and does not need any expertise like predefined grammar or sketches in the meantime .,3,0.89554626,25.241299728650727,27
791,The non-indexed parts of the Internet ( the Darknet ) have become a haven for both legal and illegal anonymous activity .,0,0.9234119,50.103213600938965,22
791,"Given the magnitude of these networks , scalably monitoring their activity necessarily relies on automated tools , and notably on NLP tools .",0,0.84778804,178.9666709314277,23
791,"However , little is known about what characteristics texts communicated through the Darknet have , and how well do off-the-shelf NLP tools do on this domain .",0,0.9387805,36.132337639883204,29
791,"This paper tackles this gap and performs an in-depth investigation of the characteristics of legal and illegal text in the Darknet , comparing it to a clear net website with similar content as a control condition .",1,0.86069214,42.94807742069764,38
791,"Taking drugs-related websites as a test case , we find that texts for selling legal and illegal drugs have several linguistic characteristics that distinguish them from one another , as well as from the control condition , among them the distribution of POS tags , and the coverage of their named entities in Wikipedia .",3,0.8841812,56.07256267842859,55
792,Cognitive task analysis ( CTA ) is a type of analysis in applied psychology aimed at eliciting and representing the knowledge and thought processes of domain experts .,0,0.9585496,50.67586937949667,28
792,"In CTA , often heavy human labor is involved to parse the interview transcript into structured knowledge ( e.g. , flowchart for different actions ) .",0,0.7584668,143.4920318791382,26
792,"To reduce human efforts and scale the process , automated CTA transcript parsing is desirable .",0,0.89891756,329.67525721166186,16
792,"However , this task has unique challenges as ( 1 ) it requires the understanding of long-range context information in conversational text ;",0,0.87481457,49.39505749597738,24
792,"and ( 2 ) the amount of labeled data is limited and indirect — i.e. , context-aware , noisy , and low-resource .",0,0.56856984,57.78638293536974,25
792,"In this paper , we propose a weakly-supervised information extraction framework for automated CTA transcript parsing .",1,0.9153336,36.8669080630428,19
792,"We partition the parsing process into a sequence labeling task and a text span-pair relation extraction task , with distant supervision from human-curated protocol files .",2,0.88338685,63.18209090772356,26
792,"To model long-range context information for extracting sentence relations , neighbor sentences are involved as a part of input .",2,0.5315704,131.4230860377218,21
792,Different types of models for capturing context dependency are then applied .,2,0.6754258,68.4744018640272,12
792,We manually annotate real-world CTA transcripts to facilitate the evaluation of the parsing tasks .,2,0.85660547,34.34032121902456,15
793,"As Massive Open Online Courses ( MOOCs ) become increasingly popular , it is promising to automatically provide extracurricular knowledge for MOOC users .",0,0.9386223,20.13331370068707,24
793,"Suffering from semantic drifts and lack of knowledge guidance , existing methods can not effectively expand course concepts in complex MOOC environments .",0,0.8483548,134.43587023215755,23
793,"In this paper , we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results .",1,0.56855816,53.89020460002442,30
793,"In addition , to involve human efforts in our model , we design an interactive optimization mechanism based on a game .",2,0.7797865,113.3387375378666,22
793,Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements ( + 0.19 by MAP ) over existing methods .,3,0.947061,57.362199875423336,28
794,"We show that the imperceptibility of several existing linguistic steganographic systems ( Fang et al. , 2017 ; Yang et al. , 2018 ) relies on implicit assumptions on statistical behaviors of fluent text .",3,0.7999817,62.49723396445818,35
794,We formally analyze them and empirically evaluate these assumptions .,2,0.5355793,79.45715253794332,10
794,"Furthermore , based on these observations , we propose an encoding algorithm called patient-Huffman with improved near-imperceptible guarantees .",3,0.6954934,91.32876597243008,21
795,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents , which require local , non-local , syntactic and semantic dependencies .",0,0.8154298,44.00438816844141,25
795,Existing methods do not fully exploit such dependencies .,0,0.8786863,36.678604227500976,9
795,We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph .,2,0.38995692,20.734581136217734,24
795,The graph is constructed using various inter-and intra-sentence dependencies to capture local and non-local dependency information .,2,0.6981789,27.293100717469784,17
795,"In order to predict the relation of an entity pair , we utilise multi-instance learning with bi-affine pairwise scoring .",2,0.7756357,57.17216979006565,20
795,Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets .,3,0.9639403,6.455060193585742,25
795,Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction .,3,0.9864558,26.020108631004472,17
796,"Legal judgment prediction is the task of automatically predicting the outcome of a court case , given a text describing the case ’s facts .",0,0.9250847,27.507169717681812,25
796,Previous work on using neural models for this task has focused on Chinese ;,0,0.8972766,79.64260365348945,14
796,"only feature-based models ( e.g. , using bags of words and topics ) have been considered in English .",0,0.48238653,84.65800023205828,21
796,"We release a new English legal judgment prediction dataset , containing cases from the European Court of Human Rights .",2,0.83166105,56.99054081774273,20
796,"We evaluate a broad variety of neural models on the new dataset , establishing strong baselines that surpass previous feature-based models in three tasks : ( 1 ) binary violation classification ;",2,0.50365895,104.02952759062848,34
796,( 2 ) multi-label classification ; ( 3 ) case importance prediction .,2,0.6509404,185.90476166398628,13
796,We also explore if models are biased towards demographic information via data anonymization .,2,0.4810655,77.03653316960323,14
796,"As a side-product , we propose a hierarchical version of BERT , which bypasses BERT ’s length limitation .",2,0.42314997,45.084303370147275,19
797,Neural machine translation ( NMT ) often suffers from the vulnerability to noisy perturbations in the input .,0,0.9524719,24.572231449316178,18
797,"We propose an approach to improving the robustness of NMT models , which consists of two parts : ( 1 ) attack the translation model with adversarial source examples ;",2,0.48333097,39.90428140797725,30
797,( 2 ) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs .,2,0.53053826,50.36656174654802,21
797,"For the generation of adversarial inputs , we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs .",2,0.7243357,44.01010638765795,28
797,Experimental results on Chinese-English and English-German translation tasks demonstrate that our approach achieves significant improvements ( 2.8 and 1.6 BLEU points ) over Transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data .,3,0.92160356,16.60207809532886,40
798,Neural Machine Translation ( NMT ) generates target words sequentially in the way of predicting the next word conditioned on the context words .,0,0.7985666,43.87102384727506,24
798,"At training time , it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch .",2,0.3956351,72.77351507634042,26
798,This discrepancy of the fed context leads to error accumulation among the way .,3,0.5392143,347.6296323209807,14
798,"Furthermore , word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations .",0,0.53832686,68.75004101091122,27
798,"In this paper , we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training , where the predicted sequence is selected with a sentence-level optimum .",1,0.53936285,35.190645230592445,44
798,Experiment results on Chinese->English and WMT’14 English-> German translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets .,3,0.9405464,15.271802845558131,24
799,"While most neural machine translation ( NMT ) systems are still trained using maximum likelihood estimation , recent work has demonstrated that optimizing systems to directly improve evaluation metrics such as BLEU can significantly improve final translation accuracy .",0,0.9187342,26.96780855498259,39
799,"However , training with BLEU has some limitations : it does n’t assign partial credit , it has a limited range of output values , and it can penalize semantically correct hypotheses if they differ lexically from the reference .",0,0.56385374,56.14619252738792,40
799,"In this paper , we introduce an alternative reward function for optimizing NMT systems that is based on recent work in semantic similarity .",1,0.86832803,32.46124731397779,24
799,"We evaluate on four disparate languages trans-lated to English , and find that training with our proposed metric results in better translations as evaluated by BLEU , semantic similarity , and human evaluation , and also that the optimization procedure converges faster .",3,0.87981856,81.63922904610709,43
799,"Analysis suggests that this is because the proposed metric is more conducive to optimization , assigning partial credit and providing more diversity in scores than BLEU .",3,0.9812184,77.91009505181822,27
800,The process of extracting knowledge from natural language text poses a complex problem that requires both a combination of machine learning techniques and proper feature selection .,0,0.93952763,25.316763624583015,27
800,"Recent advances in Automatic Machine Learning ( AutoML ) provide effective tools to explore large sets of algorithms , hyper-parameters and features to find out the most suitable combination of them .",0,0.93080056,49.82001186776513,32
800,"This paper proposes a novel AutoML strategy based on probabilistic grammatical evolution , which is evaluated on the health domain by facing the knowledge discovery challenge in Spanish text documents .",1,0.83186704,78.18023727113903,31
800,Our approach achieves state-of-the-art results and provides interesting insights into the best combination of parameters and algorithms to use when dealing with this challenge .,3,0.9303579,15.776143707850508,30
800,Source code is provided for the research community .,0,0.32590336,27.29561260541684,9
801,Event detection systems rely on discrimination knowledge to distinguish ambiguous trigger words and generalization knowledge to detect unseen / sparse trigger words .,0,0.85860723,91.16438615106753,23
801,"Current neural event detection approaches focus on trigger-centric representations , which work well on distilling discrimination knowledge , but poorly on learning generalization knowledge .",0,0.8745087,139.50438667246303,25
801,"To address this problem , this paper proposes a Delta-learning approach to distill discrimination and generalization knowledge by effectively decoupling , incrementally learning and adaptively fusing event representation .",1,0.7348051,63.89754376651612,29
801,"Experiments show that our method significantly outperforms previous approaches on unseen / sparse trigger words , and achieves state-of-the-art performance on both ACE2005 and KBP2017 datasets .",3,0.92546785,16.4507281312962,33
802,"Chinese relation extraction is conducted using neural networks with either character-based or word-based inputs , and most existing methods typically suffer from segmentation errors and ambiguity of polysemy .",0,0.79611075,34.502452447715896,33
802,"To address the issues , we propose a multi-grained lattice framework ( MG lattice ) for Chinese relation extraction to take advantage of multi-grained language information and external linguistic knowledge .",1,0.4401435,36.93419344196641,31
802,"In this framework , ( 1 ) we incorporate word-level information into character sequence inputs so that segmentation errors can be avoided .",2,0.6605285,55.46111920914214,23
802,"( 2 ) We also model multiple senses of polysemous words with the help of external linguistic knowledge , so as to alleviate polysemy ambiguity .",2,0.7389895,47.675711860501686,26
802,"Experiments on three real-world datasets in distinct domains show consistent and significant superiority and robustness of our model , as compared with other baselines .",3,0.9036557,21.076222990760336,25
802,We will release the source code of this paper in the future .,3,0.85988224,7.94183809660483,13
803,State-of-the-art models for knowledge graph completion aim at learning a fixed embedding representation of entities in a multi-relational graph which can generalize to infer unseen entity relationships at test time .,0,0.86431026,24.10393710826781,35
803,This can be sub-optimal as it requires memorizing and generalizing to all possible entity relationships using these fixed representations .,0,0.52054006,48.136585966381105,20
803,We thus propose a novel attention-based method to learn query-dependent representation of entities which adaptively combines the relevant graph neighborhood of an entity leading to more accurate KG completion .,1,0.41055292,57.38600142478634,34
803,"The proposed method is evaluated on two benchmark datasets for knowledge graph completion , and experimental results show that the proposed model performs competitively or better than existing state-of-the-art , including recent methods for explicit multi-hop reasoning .",3,0.7273754,19.55064260716246,44
803,"Qualitative probing offers insight into how the model can reason about facts involving multiple hops in the knowledge graph , through the use of neighborhood attention .",3,0.64792585,61.126067024930315,27
804,Event factuality prediction ( EFP ) is the task of assessing the degree to which an event mentioned in a sentence has happened .,0,0.95645624,33.612084632441466,24
804,"For this task , both syntactic and semantic information are crucial to identify the important context words .",0,0.6942712,45.3196831862804,18
804,The previous work for EFP has only combined these information in a simple way that cannot fully exploit their coordination .,0,0.8721799,108.2092929272803,21
804,"In this work , we introduce a novel graph-based neural network for EFP that can integrate the semantic and syntactic information more effectively .",1,0.85718066,21.646977082462318,26
804,Our experiments demonstrate the advantage of the proposed model for EFP .,3,0.9627263,53.465575601845245,12
805,Data-driven models have demonstrated state-of-the-art performance in inferring the temporal ordering of events in text .,0,0.9505413,9.057675918349775,21
805,"However , these models often overlook explicit temporal signals , such as dates and time windows .",0,0.93715096,94.81323126986355,17
805,"Rule-based methods can be used to identify the temporal links between these time expressions ( timexes ) , but they fail to capture timexes ’ interactions with events and are hard to integrate with the distributed representations of neural net models .",0,0.71171534,44.803142419713005,43
805,"In this paper , we introduce a framework to infuse temporal awareness into such models by learning a pre-trained model to embed timexes .",1,0.79186785,40.692739289951696,24
805,"We generate synthetic data consisting of pairs of timexes , then train a character LSTM to learn embeddings and classify the timexes ’ temporal relation .",2,0.9106807,70.36311997672637,26
805,"We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering , and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions .",3,0.7444921,42.29555820620631,46
806,"We consider a novel question answering ( QA ) task where the machine needs to read from large streaming data ( long documents or videos ) without knowing when the questions will be given , which is difficult to solve with existing QA methods due to their lack of scalability .",0,0.39009184,30.187114211747858,51
806,"To tackle this problem , we propose a novel end-to-end deep network model for reading comprehension , which we refer to as Episodic Memory Reader ( EMR ) that sequentially reads the input contexts into an external memory , while replacing memories that are less important for answering unseen questions .",2,0.44610497,33.82474213955663,53
806,"Specifically , we train an RL agent to replace a memory entry when the memory is full , in order to maximize its QA accuracy at a future timepoint , while encoding the external memory using either the GRU or the Transformer architecture to learn representations that considers relative importance between the memory entries .",2,0.8653952,52.69385041473849,55
806,"We validate our model on a synthetic dataset ( bAbI ) as well as real-world large-scale textual QA ( TriviaQA ) and video QA ( TVQA ) datasets , on which it achieves significant improvements over rule based memory scheduling policies or an RL based baseline that independently learns the query-specific importance of each memory .",3,0.5329249,58.22350231197908,57
807,"Natural Language Sentence Matching ( NLSM ) has gained substantial attention from both academics and the industry , and rich public datasets contribute a lot to this process .",0,0.96416646,45.182946619571446,29
807,"However , biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results .",0,0.7615055,41.546016066769695,19
807,"For many NLSM datasets , the providers select some pairs of sentences into the datasets , and this sampling procedure can easily bring unintended pattern , i.e. , selection bias .",0,0.46297556,150.03749636849207,31
807,"One example is the QuoraQP dataset , where some content-independent naive features are unreasonably predictive .",0,0.6061252,113.29529439224817,17
807,Such features are the reflection of the selection bias and termed as the “ leakage features .,0,0.67742175,123.47074881028361,17
807,"In this paper , we investigate the problem of selection bias on six NLSM datasets and find that four out of them are significantly biased .",1,0.79529464,30.986490261854982,26
807,We further propose a training and evaluation framework to alleviate the bias .,3,0.42066494,23.322789909016066,13
807,"Experimental results on QuoraQP suggest that the proposed framework can improve the generalization ability of trained models , and give more trustworthy evaluation results for real-world adoptions .",3,0.9769291,29.071501590550568,28
808,"Existing open-domain question answering ( QA ) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query , which is computationally prohibitive .",0,0.9365797,22.46684169311088,36
808,"In this paper , we introduce query-agnostic indexable representations of document phrases that can drastically speed up open-domain QA .",1,0.87675905,38.34984362132965,21
808,"In particular , our dense-sparse phrase encoding effectively captures syntactic , semantic , and lexical information of the phrases and eliminates the pipeline filtering of context documents .",3,0.71301854,91.70851625867857,28
808,"Leveraging strategies for optimizing training and inference time , our model can be trained and deployed even in a single 4-GPU server .",3,0.82138455,64.34194491304694,25
808,"Moreover , by representing phrases as pointers to their start and end tokens , our model indexes phrases in the entire English Wikipedia ( up to 60 billion phrases ) using under 2TB .",3,0.43722457,129.61928956658645,34
808,"Our experiments on SQuAD-Open show that our model is on par with or more accurate than previous models with 6000x reduced computational cost , which translates into at least 68x faster end-to-end inference benchmark on CPUs .",3,0.9555183,37.886845871118666,41
808,Code and demo are available at nlp.cs.washington.edu/denspi .,3,0.5000099,25.71367896166704,8
809,"Sequential recurrent neural networks have achieved superior performance on language modeling , but overlook the structure information in natural language .",0,0.9298825,39.01456141500935,21
809,Recent works on structure-aware models have shown promising results on language modeling .,0,0.8751159,17.27640796469996,15
809,"However , how to incorporate structure knowledge on corpus without syntactic annotations remains an open problem .",0,0.8606264,72.66013084720095,17
809,"In this work , we propose neural variational language model ( NVLM ) , which enables the sharing of grammar knowledge among different corpora .",1,0.6937344,44.93556900375887,25
809,Experimental results demonstrate the effectiveness of our framework on two popular benchmark datasets .,3,0.8974032,8.378390985133908,14
809,"With the help of shared grammar , our language model converges significantly faster to a lower perplexity on new training corpus .",3,0.8943315,53.25798381513067,22
810,"We consider a zero-shot semantic parsing task : parsing instructions into compositional logical forms , in domains that were not seen during training .",2,0.7867223,74.92129766412536,24
810,"We present a new dataset with 1,390 examples from 7 application domains ( e.g .",2,0.71531403,33.87095877274425,15
810,"a calendar or a file manager ) , each example consisting of a triplet : ( a ) the application ’s initial state , ( b ) an instruction , to be carried out in the context of that state , and ( c ) the state of the application after carrying out the instruction .",2,0.5640609,35.3968971292255,56
810,"We introduce a new training algorithm that aims to train a semantic parser on examples from a set of source domains , so that it can effectively parse instructions from an unknown target domain .",2,0.37272045,28.08759061552719,35
810,"We integrate our algorithm into the floating parser of Pasupat and Liang ( 2015 ) , and further augment the parser with features and a logical form candidate filtering logic , to support zero-shot adaptation .",2,0.5939655,168.98111586421308,36
810,Our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser .,3,0.96107316,26.773214120546296,16
811,"Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo ( Peters et al. , 2018a ) and BERT ( Devlin et al. , 2019 ) which are pretrained on variants of language modeling .",0,0.8476276,20.247722852687705,43
811,"We conduct the first large-scale systematic study of candidate pretraining tasks , comparing 19 different tasks both as alternatives and complements to language modeling .",1,0.43588406,65.276292909742,25
811,"Our primary results support the use language modeling , especially when combined with pretraining on additional labeled-data tasks .",3,0.98986304,98.72451719942849,21
811,"In ELMo ’s pretrain-then-freeze paradigm , random baselines are worryingly strong and results vary strikingly across target tasks .",3,0.61627203,105.43472298531182,22
811,"In addition , fine-tuning BERT on an intermediate task often negatively impacts downstream transfer .",0,0.5386289,59.628595093627915,15
811,"In a more positive trend , we see modest gains from multitask training , suggesting the development of more sophisticated multitask and transfer learning techniques as an avenue for further research .",3,0.9849458,38.2368891989239,32
812,"In this work , we focus on complex question semantic parsing and propose a novel Hierarchical Semantic Parsing ( HSP ) method , which utilizes the decompositionality of complex questions for semantic parsing .",1,0.88067555,20.224304045105303,34
812,Our model is designed within a three-stage parsing architecture based on the idea of decomposition-integration .,2,0.72738093,27.55407470566931,19
812,"In the first stage , we propose a question decomposer which decomposes a complex question into a sequence of sub-questions .",2,0.6144929,12.882349713672156,21
812,"In the second stage , we design an information extractor to derive the type and predicate information of these questions .",2,0.83500457,43.25920342586818,21
812,"In the last stage , we integrate the generated information from previous stages and generate a logical form for the complex question .",2,0.7506815,37.244982318560865,23
812,"We conduct experiments on COMPLEXWEBQUESTIONS which is a large scale complex question semantic parsing dataset , results show that our model achieves significant improvement compared to state-of-the-art methods .",3,0.76866883,25.205848567951048,35
813,"In this paper , we present a Multi-Task Deep Neural Network ( MT-DNN ) for learning representations across multiple natural language understanding ( NLU ) tasks .",1,0.86847055,22.423779159351344,29
813,"MT-DNN not only leverages large amounts of cross-task data , but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains .",3,0.5960762,28.083124336164296,35
813,MT-DNN extends the model proposed in Liu et al .,3,0.52327365,47.54594381153169,11
813,"( 2015 ) by incorporating a pre-trained bidirectional transformer language model , known as BERT ( Devlin et al. , 2018 ) .",2,0.4994686,36.02214410753232,23
813,"MT-DNN obtains new state-of-the-art results on ten NLU tasks , including SNLI , SciTail , and eight out of nine GLUE tasks , pushing the GLUE benchmark to 82.7 % ( 2.2 % absolute improvement ) as of February 25 , 2019 on the latest GLUE test set .",3,0.8936412,30.25294642319827,56
813,We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations .,3,0.93123454,33.33287386447331,32
813,Our code and pre-trained models will be made publicly available .,3,0.53266263,10.585926625381408,11
814,Learning effective representations of sentences is one of the core missions of natural language understanding .,0,0.9504208,26.583300361112343,16
814,"Existing models either train on a vast amount of text , or require costly , manually curated sentence relation datasets .",0,0.84877163,159.36566226124663,21
814,"We show that with dependency parsing and rule-based rubrics , we can curate a high quality sentence relation task by leveraging explicit discourse relations .",3,0.93101466,82.15345380464035,25
814,"We show that our curated dataset provides an excellent signal for learning vector representations of sentence meaning , representing relations that can only be determined when the meanings of two sentences are combined .",3,0.96852523,43.437358518012246,34
814,We demonstrate that the automatically curated corpus allows a bidirectional LSTM sentence encoder to yield high quality sentence embeddings and can serve as a supervised fine-tuning dataset for larger models such as BERT .,3,0.9588952,20.842262468382184,34
814,"Our fixed sentence embeddings achieve high performance on a variety of transfer tasks , including SentEval , and we achieve state-of-the-art results on Penn Discourse Treebank ’s implicit relation prediction task .",3,0.828541,18.188317378635663,38
815,"We present SParC , a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences ( 12 k + individual questions annotated with SQL queries ) .",2,0.6864,112.374071367533,28
815,It is obtained from controlled user interactions with 200 complex databases over 138 domains .,2,0.5250733,208.5444616108588,15
815,We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets .,3,0.8154281,19.95420053643544,20
815,"SParC demonstrates complex contextual dependencies , ( 2 ) has greater semantic diversity , and ( 3 ) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time .",3,0.6507514,88.48743505774144,36
815,"We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent , cross-domain setup .",2,0.83688277,17.71389070303808,26
815,"The best model obtains an exact match accuracy of 20.2 % over all questions and less than 10 % over all interaction sequences , indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research .",3,0.96848226,34.69071502445489,43
815,"The dataset , baselines , and leaderboard are released at https://yale-lily.github.io/sparc .",3,0.46607915,37.10587278939219,12
816,We present a neural approach called IRNet for complex and cross-domain Text-to-SQL .,1,0.36144686,44.82442582127142,15
816,IRNet aims to address two challenges : 1 ) the mismatch between intents expressed in natural language ( NL ) and the implementation details in SQL ;,0,0.83651537,88.23820860937045,27
816,2 ) the challenge in predicting columns caused by the large number of out-of-domain words .,3,0.57047325,47.72162162354508,18
816,"Instead of end-to-end synthesizing a SQL query , IRNet decomposes the synthesis process into three phases .",2,0.39948064,37.1126411546573,18
816,"In the first phase , IRNet performs a schema linking over a question and a database schema .",2,0.56560725,120.76230545396568,18
816,"Then , IRNet adopts a grammar-based neural model to synthesize a SemQL query which is an intermediate representation that we design to bridge NL and SQL .",2,0.8447635,89.16320924062377,27
816,"Finally , IRNet deterministically infers a SQL query from the synthesized SemQL query with domain knowledge .",2,0.60989785,152.1463275635969,17
816,"On the challenging Text-to-SQL benchmark Spider , IRNet achieves 46.7 % accuracy , obtaining 19.5 % absolute improvement over previous state-of-the-art approaches .",3,0.9245824,28.912947991482465,31
816,"At the time of writing , IRNet achieves the first position on the Spider leaderboard .",0,0.6983137,57.823621321130275,16
817,"Distributed representation of words , or word embeddings , have motivated methods for calculating semantic representations of word sequences such as phrases , sentences and paragraphs .",0,0.93280053,51.453039463052335,27
817,"Most of the existing methods to do so either use algorithms to learn such representations , or improve on calculating weighted averages of the word vectors .",0,0.7884967,62.88615012204484,27
817,"In this work , we experiment with spectral methods of signal representation and summarization as mechanisms for constructing such word-sequence embeddings in an unsupervised fashion .",1,0.60284764,37.91457807170561,26
817,"In particular , we explore an algorithm rooted in fluid-dynamics , known as higher-order Dynamic Mode Decomposition , which is designed to capture the eigenfrequencies , and hence the fundamental transition dynamics , of periodic and quasi-periodic systems .",2,0.5239675,50.1762300935252,40
817,"It is empirically observed that this approach , which we call EigenSent , can summarize transitions in a sequence of words and generate an embedding that can represent well the sequence itself .",3,0.5318192,50.97500676735128,33
817,"To the best of the authors ’ knowledge , this is the first application of a spectral decomposition and signal summarization technique on text , to create sentence embeddings .",3,0.8355641,41.48781346180048,30
817,"We test the efficacy of this algorithm in creating sentence embeddings on three public datasets , where it performs appreciably well .",2,0.61342376,44.86604953231916,22
817,"Moreover it is also shown that , due to the positive combination of their complementary properties , concatenating the embeddings generated by EigenSent with simple word vector averaging achieves state-of-the-art results .",3,0.9529251,39.82681822724795,38
818,Evaluating AMR parsing accuracy involves comparing pairs of AMR graphs .,0,0.51362354,69.50351686525178,11
818,"The major evaluation metric , SMATCH ( Cai and Knight , 2013 ) , searches for one-to-one mappings between the nodes of two AMRs with a greedy hill-climbing algorithm , which leads to search errors .",0,0.4921658,87.58957395196838,42
818,"We propose SEMBLEU , a robust metric that extends BLEU ( Papineni et al. , 2002 ) to AMRs .",1,0.33169937,143.19395212009096,20
818,It does not suffer from search errors and considers non-local correspondences in addition to local ones .,0,0.4043438,31.902751524008373,17
818,SEMBLEU is fully content-driven and punishes situations where a system ’s output does not preserve most information from the input .,3,0.446469,66.27589116633649,21
818,Preliminary experiments on both sentence and corpus levels show that SEMBLEU has slightly higher consistency with human judgments than SMATCH .,3,0.93026906,61.278989216790976,21
818,Our code is available at http://github.com/ freesunshine0316 / sembleu .,3,0.47898188,49.41561211194928,10
819,Semantic parsing considers the task of transducing natural language ( NL ) utterances into machine executable meaning representations ( MRs ) .,0,0.8791071,48.42271975591289,22
819,"While neural network-based semantic parsers have achieved impressive improvements over previous methods , results are still far from perfect , and cursory manual inspection can easily identify obvious problems such as lack of adequacy or coherence of the generated MRs .",0,0.71072215,35.923838841228424,43
819,"This paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted MRs , using features that are designed to fix observed problems with baseline models .",1,0.7896364,54.27376527242637,41
819,"We implement our reranker in a competitive neural semantic parser and test on four semantic parsing ( GEO , ATIS ) and Python code generation ( Django , CoNaLa ) tasks , improving the strong baseline parser by up to 5.7 % absolute in BLEU ( CoNaLa ) and 2.9 % in accuracy ( Django ) , outperforming the best published neural parser results on all four datasets .",3,0.6509554,51.01612610356065,69
820,"Research on parsing language to SQL has largely ignored the structure of the database ( DB ) schema , either because the DB was very simple , or because it was observed at both training and test time .",0,0.9432676,65.93495513883644,39
820,"DBs are given at test time , and so the structure of the DB schema can inform the predicted SQL query .",3,0.41244298,65.23363288750816,22
820,"In this paper , we present an encoder-decoder semantic parser , where the structure of the DB schema is encoded with a graph neural network , and this representation is later used at both encoding and decoding time .",1,0.7307573,25.324231241402774,39
820,"Evaluation shows that encoding the schema structure improves our parser accuracy from 33.8 % to 39.4 % , dramatically above the current state of the art , which is at 19.7 % .",3,0.9484153,30.286966980809954,33
821,"The GLUE benchmark ( Wang et al. , 2019 b ) is a suite of language understanding tasks which has seen dramatic progress in the past year , with average performance moving from 70.0 at launch to 83.9 , state of the art at the time of writing ( May 24 , 2019 ) .",0,0.8417621,34.516587654562834,55
821,"Here , we measure human performance on the benchmark , in order to learn whether significant headroom remains for further progress .",2,0.53737587,47.10678247217384,22
821,Our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples .,2,0.61724263,45.05511871140308,19
821,"In spite of limited training , these annotators robustly outperform the state of the art on six of the nine GLUE tasks and achieve an average score of 87.1 .",3,0.94564515,23.582244781772598,30
821,"Given the fast pace of progress however , the headroom we observe is quite limited .",0,0.4875719,71.5883389997374,16
821,"To reproduce the data-poor setting that our annotators must learn in , we also train the BERT model ( Devlin et al. , 2019 ) in limited-data regimes , and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding .",3,0.67093015,65.02992852967473,50
822,Most semantic parsers that map sentences to graph-based meaning representations are hand-designed for specific graphbanks .,0,0.8462537,70.43465509503467,18
822,"We present a compositional neural semantic parser which achieves , for the first time , competitive accuracies across a diverse range of graphbanks .",3,0.41245687,43.795507361365964,24
822,"Incorporating BERT embeddings and multi-task learning improves the accuracy further , setting new states of the art on DM , PAS , PSD , AMR 2015 and EDS .",3,0.85487115,59.8380023098355,29
823,"Our work involves enriching the Stack-LSTM transition-based AMR parser ( Ballesteros and Al-Onaizan , 2017 ) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs .",2,0.7359842,118.51004337244983,37
823,"In addition , we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification , named entities and contextualized embeddings .",2,0.605157,55.90085065421821,33
823,We achieve a highly competitive performance that is comparable to the best published results .,3,0.9080551,26.515671844879673,15
823,We show an in-depth study ablating each of the new components of the parser .,3,0.71353614,49.106655218415455,16
824,Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks .,0,0.92883027,6.239204853658687,16
824,"We focus on one such model , BERT , and aim to quantify where linguistic information is captured within the network .",1,0.6678407,71.52061113718538,22
824,"We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way , and that the regions responsible for each step appear in the expected sequence : POS tagging , parsing , NER , semantic roles , then coreference .",3,0.95383525,104.18500697434706,47
824,"Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically , revising lower-level decisions on the basis of disambiguating information from higher-level representations .",3,0.9536669,42.343605707682464,32
825,"We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext , removing the time-consuming intermediate step of creating para-phrase corpora .",2,0.377548,41.76067266913906,27
825,"Further , we show that the resulting model can be applied to cross lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines .",3,0.9080733,14.117348516252806,37
826,Semantic dependency parsing aims to identify semantic relationships between words in a sentence that form a graph .,0,0.8892304,15.584147406691763,18
826,"In this paper , we propose a second-order semantic dependency parser , which takes into consideration not only individual dependency edges but also interactions between pairs of edges .",1,0.8756455,29.72804974127212,29
826,We show that second-order parsing can be approximated using mean field ( MF ) variational inference or loopy belief propagation ( LBP ) .,3,0.7327538,90.32955446009426,24
826,We can unfold both algorithms as recurrent layers of a neural network and therefore can train the parser in an end-to-end manner .,3,0.76101696,25.111004567669088,25
826,Our experiments show that our approach achieves state-of-the-art performance .,3,0.95562834,4.465761262686699,15
827,"Sarcasm is often expressed through several verbal and non-verbal cues , e.g. , a change of tone , overemphasis in a word , a drawn-out syllable , or a straight looking face .",0,0.8475558,34.05863379961588,34
827,Most of the recent work in sarcasm detection has been carried out on textual data .,0,0.9083153,10.509663426892939,16
827,"In this paper , we argue that incorporating multimodal cues can improve the automatic classification of sarcasm .",1,0.85004646,21.34351828304294,18
827,"As a first step towards enabling the development of multimodal approaches for sarcasm detection , we propose a new sarcasm dataset , Multimodal Sarcasm Detection Dataset ( MUStARD ) , compiled from popular TV shows .",1,0.44478285,19.93001404748306,36
827,MUStARD consists of audiovisual utterances annotated with sarcasm labels .,2,0.39544138,48.251788558137136,10
827,"Each utterance is accompanied by its context of historical utterances in the dialogue , which provides additional information on the scenario where the utterance occurs .",0,0.44145897,29.290530165692335,26
827,Our initial results show that the use of multimodal information can reduce the relative error rate of sarcasm detection by up to 12.9 % in F-score when compared to the use of individual modalities .,3,0.9879376,12.970989566370367,35
827,The full dataset is publicly available for use at https://github.com/soujanyaporia/MUStARD .,3,0.5407723,24.077585083977713,11
828,Systems for automatic argument generation and debate require the ability to ( 1 ) determine the stance of any claims employed in the argument and ( 2 ) assess the specificity of each claim relative to the argument context .,0,0.9328423,38.07861083309593,40
828,"Existing work on understanding claim specificity and stance , however , has been limited to the study of argumentative structures that are relatively shallow , most often consisting of a single claim that directly supports or opposes the argument thesis .",0,0.9194897,56.72736063371408,41
828,"In this paper , we tackle these tasks in the context of complex arguments on a diverse set of topics .",1,0.8755125,30.25204482857066,21
828,"In particular , our dataset consists of manually curated argument trees for 741 controversial topics covering 95,312 unique claims ;",2,0.63006026,222.65668708166757,20
828,lines of argument are generally of depth 2 to 6 .,0,0.6392996,155.11067717289416,11
828,"We find that as the distance between a pair of claims increases along the argument path , determining the relative specificity of a pair of claims becomes easier and determining their relative stance becomes harder .",3,0.98104024,37.17044811248009,36
829,Neural models have been investigated for sentiment classification over constituent trees .,0,0.8462191,55.49394831136621,12
829,"They learn phrase composition automatically by encoding tree structures but do not explicitly model sentiment composition , which requires to encode sentiment class labels .",0,0.40101323,127.20821031346608,25
829,"To this end , we investigate two formalisms with deep sentiment representations that capture sentiment subtype expressions by latent variables and Gaussian mixture vectors , respectively .",2,0.63408756,70.36563640298621,27
829,Experiments on Stanford Sentiment Treebank ( SST ) show the effectiveness of sentiment grammar over vanilla neural encoders .,3,0.67313284,32.90002084629439,19
829,"Using ELMo embeddings , our method gives the best results on this benchmark .",3,0.92466754,24.393405049225898,14
830,Text classification approaches have usually required task-specific model architectures and huge labeled datasets .,0,0.90531224,91.21212933072847,14
830,"Recently , thanks to the rise of text-based transfer learning techniques , it is possible to pre-train a language model in an unsupervised manner and leverage them to perform effective on downstream tasks .",0,0.9430759,16.47408546696546,36
830,In this work we focus on Japanese and show the potential use of transfer learning techniques in text classification .,1,0.71038955,26.815127276604283,20
830,"Specifically , we perform binary and multi-class sentiment classification on the Rakuten product review and Yahoo movie review datasets .",2,0.8402584,67.31127107595405,20
830,We show that transfer learning-based approaches perform better than task-specific models trained on 3 times as much data .,3,0.9695087,20.49422280575815,22
830,"Furthermore , these approaches perform just as well for language modeling pre-trained on only 1/30 of the data .",3,0.88526684,30.62304074879817,19
830,We release our pre-trained models and code as open source .,2,0.5520684,13.776058520270288,11
831,Task reaches just three points below the average untrained human baseline .,3,0.9019618,199.0144428933856,12
831,"However , we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset .",3,0.9695198,34.75570247150346,21
831,We analyze the nature of these cues and demonstrate that a range of models all exploit them .,3,0.5796963,47.58078025677674,18
831,This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy .,2,0.44903636,68.44410830987184,17
831,Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work .,3,0.98635405,28.195391285721723,22
832,We identify agreement and disagreement between utterances that express stances towards a topic of discussion .,3,0.6089841,72.29245429206794,16
832,"Existing methods focus mainly on conversational settings , where dialogic features are used for ( dis ) agreement inference .",0,0.79870933,186.69138418099564,20
832,"We extend this scope and seek to detect stance ( dis ) agreement in a broader setting , where independent stance-bearing utterances , which prevail in many stance corpora and real-world scenarios , are compared .",2,0.3806155,159.1081075414014,38
832,"To cope with such non-dialogic utterances , we find that the reasons uttered to back up a specific stance can help predict stance ( dis ) agreements .",3,0.9027891,99.02479691534093,28
832,We propose a reason comparing network ( RCN ) to leverage reason information for stance comparison .,1,0.5263851,490.367282720865,17
832,"Empirical results on a well-known stance corpus show that our method can discover useful reason information , enabling it to outperform several baselines in stance ( dis ) agreement detection .",3,0.9190492,69.1112342117509,33
833,"In sentiment detection , the natural language processing community has focused on determining holders , facets , and valences , but has paid little attention to the reasons for sentiment decisions .",0,0.917752,94.38038263719017,32
833,Our work considers human motives as the driver for human sentiments and addresses the problem of motive detection as the first step .,3,0.29104733,54.15982006210106,23
833,"Following a study in psychology , we define six basic motives that cover a wide range of topics appearing in review texts , annotate 1,600 texts in restaurant and laptop domains with the motives , and report the performance of baseline methods on this new dataset .",2,0.7688771,84.95862906149024,47
833,"We also show that cross-domain transfer learning boosts detection performance , which indicates that these universal motives exist across different domains .",3,0.96695745,66.85015440584928,22
834,Attention-based neural models were employed to detect the different aspects and sentiment polarities of the same target in targeted aspect-based sentiment analysis ( TABSA ) .,2,0.7979556,56.479371505809226,29
834,"However , existing methods do not specifically pre-train reasonable embeddings for targets and aspects in TABSA .",0,0.8275506,76.53798483082423,17
834,This may result in targets or aspects having the same vector representations in different contexts and losing the context-dependent information .,3,0.54768115,48.95996353675029,21
834,"To address this problem , we propose a novel method to refine the embeddings of targets and aspects .",1,0.36211473,21.399404087387957,19
834,Such pivotal embedding refinement utilizes a sparse coefficient vector to adjust the embeddings of target and aspect from the context .,2,0.44619462,112.17464763516325,21
834,Hence the embeddings of targets and aspects can be refined from the highly correlative words instead of using context-independent or randomly initialized vectors .,3,0.74609464,92.38923922904216,25
834,Experiment results on two benchmark datasets show that our approach yields the state-of-the-art performance in TABSA task .,3,0.9553847,8.169212809404183,24
835,Political debates offer a rare opportunity for citizens to compare the candidates ’ positions on the most controversial topics of the campaign .,0,0.9157844,30.78879929964043,23
835,Thus they represent a natural application scenario for Argument Mining .,3,0.7985991,110.80947767800701,11
835,"As existing research lacks solid empirical investigation of the typology of argument components in political debates , we fill this gap by proposing an Argument Mining approach to political debates .",1,0.66446584,46.49737044804266,31
835,"We address this task in an empirical manner by annotating 39 political debates from the last 50 years of US presidential campaigns , creating a new corpus of 29 k argument components , labeled as premises and claims .",2,0.8110374,68.38525702969486,39
835,"We then propose two tasks : ( 1 ) identifying the argumentative components in such debates , and ( 2 ) classifying them as premises and claims .",2,0.43742564,57.47481080527795,28
835,We show that feature-rich SVM learners and Neural Network architectures outperform standard baselines in Argument Mining over such complex data .,3,0.9426504,79.057192555524,23
835,We release the new corpus USElecDeb60To16 and the accompanying software under free licenses to the research community .,2,0.44452047,233.22173437826112,18
836,"For several natural language processing ( NLP ) tasks , span representation design is attracting considerable attention as a promising new technique ;",0,0.9436508,109.36831490309453,23
836,a common basis for an effective design has been established .,0,0.86748,67.72019815391742,11
836,"With such basis , exploring task-dependent extensions for argumentation structure parsing ( ASP ) becomes an interesting research direction .",0,0.85439074,230.2504200700535,22
836,This study investigates ( i ) span representation originally developed for other NLP tasks and ( ii ) a simple task-dependent extension for ASP .,1,0.9340282,77.59395649325536,27
836,Our extensive experiments and analysis show that these representations yield high performance for ASP and provide some challenging types of instances to be parsed .,3,0.9618389,60.80176442787389,25
837,"In this paper , we present a fast and strong neural approach for general purpose text matching applications .",1,0.886117,44.02411654126364,19
837,"We explore what is sufficient to build a fast and well-performed text matching model and propose to keep three key features available for inter-sequence alignment : original point-wise features , previous aligned features , and contextual features while simplifying all the remaining components .",1,0.37395614,80.58317299616867,46
837,"We conduct experiments on four well-studied benchmark datasets across tasks of natural language inference , paraphrase identification and answer selection .",2,0.85710937,34.55766853753073,22
837,The performance of our model is on par with the state-of-the-art on all datasets with much fewer parameters and the inference speed is at least 6 times faster compared with similarly performed ones .,3,0.91294444,12.964574164238881,40
838,"The recent proliferation of knowledge graphs ( KGs ) coupled with incomplete or partial information , in the form of missing relations ( links ) between entities , has fueled a lot of research on knowledge base completion ( also known as relation prediction ) .",0,0.96013933,47.57133152477573,46
838,Several recent works suggest that convolutional neural network ( CNN ) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction .,0,0.91996855,31.841808014461225,29
838,"However , we observe that these KG embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple .",3,0.88994306,43.455724072003804,33
838,"To this effect , our paper proposes a novel attention-based feature embedding that captures both entity and relation features in any given entity ’s neighborhood .",1,0.6585019,48.09007061341211,28
838,"Additionally , we also encapsulate relation clusters and multi-hop relations in our model .",2,0.53897643,43.9667923405133,14
838,Our empirical study offers insights into the efficacy of our attention-based model and we show marked performance gains in comparison to state-of-the-art methods on all datasets .,3,0.9526413,18.517071042229457,35
839,"We present a monolingual alignment system for long , sentence-or clause-level alignments , and demonstrate that systems designed for word-or short phrase-based alignment are ill-suited for these longer alignments .",3,0.40270528,45.848584421876765,38
839,Our system is capable of aligning semantically similar spans of arbitrary length .,3,0.8313995,30.93814560706683,13
839,We achieve significantly higher recall on aligning phrases of four or more words and outperform state-of-the-art aligners on the long alignments in the MSR RTE corpus .,3,0.94905245,27.187038540027217,33
840,Link prediction and entailment graph induction are often treated as different problems .,0,0.8542036,80.76589884617961,13
840,"In this paper , we show that these two problems are actually complementary .",1,0.8253298,25.574682508968174,14
840,We train a link prediction model on a knowledge graph of assertions extracted from raw text .,2,0.8966426,47.10127953887038,17
840,"We propose an entailment score that exploits the new facts discovered by the link prediction model , and then form entailment graphs between relations .",2,0.55701107,138.76557351174205,25
840,We further use the learned entailments to predict improved link prediction scores .,2,0.5957806,144.4827018547763,13
840,Our results show that the two tasks can benefit from each other .,3,0.98952174,15.910851973200227,13
840,The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores .,3,0.94172686,31.96770612510071,34
841,We present a latent variable model for predicting the relationship between a pair of text sequences .,2,0.52053875,17.343050048521235,17
841,"Unlike previous auto-encoding –based approaches that consider each sequence separately , our proposed framework utilizes both sequences within a single model by generating a sequence that has a given relationship with a source sequence .",2,0.541098,46.434611573373665,35
841,We further extend the cross-sentence generating framework to facilitate semi-supervised training .,3,0.54229945,24.10071335423506,12
841,We also define novel semantic constraints that lead the decoder network to generate semantically plausible and diverse sequences .,3,0.5555889,62.75760980028166,19
841,"We demonstrate the effectiveness of the proposed model from quantitative and qualitative experiments , while achieving state-of-the-art results on semi-supervised natural language inference and paraphrase identification .",3,0.8189502,13.159885602394551,33
842,"We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs : ATOMIC ( Sap et al. , 2019 ) and ConceptNet ( Speer et al. , 2017 ) .",1,0.3911176,48.232752990592275,36
842,"Contrary to many conventional KBs that store knowledge with canonical templates , commonsense KBs only store loosely structured open-text descriptions of knowledge .",0,0.88380957,125.27700250646787,25
842,"We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge , and propose COMmonsEnse Transformers ( COMET ) that learn to generate rich and diverse commonsense descriptions in natural language .",1,0.6539013,57.24587868770871,40
842,"Despite the challenges of commonsense modeling , our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs .",3,0.97161555,33.05045822855386,31
842,"Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality , with up to 77.5 % ( ATOMIC ) and 91.7 % ( ConceptNet ) precision at top 1 , which approaches human performance for these resources .",3,0.96261907,72.56910082170546,45
842,Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods .,3,0.9898127,51.79413042949504,23
843,Recognizing the internal structure of events is a challenging language processing task of great importance for text understanding .,0,0.954043,30.08059735444428,19
843,We present a supervised model for automatically identifying when one event is a subevent of another .,1,0.42735493,36.090623900279134,17
843,"Building on prior work , we introduce several novel features , in particular discourse and narrative features , that significantly improve upon prior state-of-the-art performance .",3,0.3960409,30.54664984514333,32
843,Error analysis further demonstrates the utility of these features .,3,0.95386744,38.17766038504957,10
843,We evaluate our model on the only two annotated corpora with event hierarchies : HiEve and the Intelligence Community corpus .,2,0.6480996,118.39747245639043,21
843,No prior system has been evaluated on both corpora .,0,0.62004983,52.41137308850551,10
843,"Our model outperforms previous systems on both corpora , achieving 0.74 BLANC F1 on the Intelligence Community corpus and 0.70 F1 on the HiEve corpus , respectively a 15 and 5 percentage point improvement over previous models .",3,0.9160002,44.88549000978546,38
844,Recent work by Zellers et al .,0,0.68521196,34.526184463922824,7
844,"( 2018 ) introduced a new task of commonsense natural language inference : given an event description such as “ A woman sits at a piano , ” a machine must select the most likely followup : “ She sets her fingers on the keys .",0,0.74997294,43.420118745007734,46
844,"With the introduction of BERT , near human-level performance was reached .",3,0.67263556,61.79250988817453,12
844,"In this paper , we show that commonsense inference still proves difficult for even state-of-the-art models , by presenting HellaSwag , a new challenge dataset .",1,0.7812773,36.23528227932773,32
844,"Though its questions are trivial for humans ( > 95 % accuracy ) , state-of-the-art models struggle ( < 48 % ) .",0,0.7500269,50.92710847352189,28
844,"We achieve this via Adversarial Filtering ( AF ) , a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers .",2,0.733256,46.924618472486635,31
844,AF proves to be surprisingly robust .,3,0.7767188,56.097113059298216,7
844,"The key insight is to scale up the length and complexity of the dataset examples towards a critical ‘ Goldilocks ’ zone wherein generated text is ridiculous to humans , yet often misclassified by state-of-the-art models .",3,0.6288948,67.96262359188529,43
844,"Our construction of HellaSwag , and its resulting difficulty , sheds light on the inner workings of deep pretrained models .",3,0.8488133,52.933698323996865,21
844,"More broadly , it suggests a new path forward for NLP research , in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way , so as to present ever-harder challenges .",3,0.87702084,29.415638955684113,39
845,Semantic parsing over multiple knowledge bases enables a parser to exploit structural similarities of programs across the multiple domains .,0,0.7094498,92.29562656027954,20
845,"However , the fundamental challenge lies in obtaining high-quality annotations of ( utterance , program ) pairs across various domains needed for training such models .",0,0.9025008,68.41780818872182,26
845,"To overcome this , we propose a novel framework to build a unified multi-domain enabled semantic parser trained only with weak supervision ( denotations ) .",2,0.43253478,58.35093551294046,26
845,Weakly supervised training is particularly arduous as the program search space grows exponentially in a multi-domain setting .,0,0.69888496,54.72218855493063,18
845,"To solve this , we incorporate a multi-policy distillation mechanism in which we first train domain-specific semantic parsers ( teachers ) using weak supervision in the absence of the ground truth programs , followed by training a single unified parser ( student ) from the domain specific policies obtained from these teachers .",2,0.8326836,63.4919874065252,53
845,"The resultant semantic parser is not only compact but also generalizes better , and generates more accurate programs .",3,0.86592215,65.09501730568954,19
845,It further does not require the user to provide a domain label while querying .,3,0.51991427,58.03781384145828,15
845,"On the standard Overnight dataset ( containing multiple domains ) , we demonstrate that the proposed model improves performance by 20 % in terms of denotation accuracy in comparison to baseline techniques .",3,0.88161284,43.177512866154814,33
846,We introduce the use of Poincaré embeddings to improve existing state-of-the-art approaches to domain-specific taxonomy induction from text as a signal for both relocating wrong hyponym terms within a ( pre-induced ) taxonomy as well as for attaching disconnected terms in a taxonomy .,2,0.4906501,44.67358771829056,50
846,This method substantially improves previous state-of-the-art results on the SemEval-2016 Task 13 on taxonomy extraction .,3,0.85193044,19.919672311970213,22
846,"We demonstrate the superiority of Poincaré embeddings over distributional semantic representations , supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the Euclidean space .",3,0.9483902,18.637810801891145,31
847,Researchers illustrate improvements in contextual encoding strategies via resultant performance on a battery of shared Natural Language Understanding ( NLU ) tasks .,0,0.4729847,110.18348338958825,23
847,"Many of these tasks are of a categorical prediction variety : given a conditioning context ( e.g. , an NLI premise ) , provide a label based on an associated prompt ( e.g. , an NLI hypothesis ) .",0,0.8071993,54.95030302560103,39
847,The categorical nature of these tasks has led to common use of a cross entropy log-loss objective during training .,0,0.83180714,50.462539545325356,20
847,"We suggest this loss is intuitively wrong when applied to plausibility tasks , where the prompt by design is neither categorically entailed nor contradictory given the context .",3,0.95533526,120.91361563220957,28
847,"Log-loss naturally drives models to assign scores near 0.0 or 1.0 , in contrast to our proposed use of a margin-based loss .",3,0.76273733,47.00681929873255,24
847,"Following a discussion of our intuition , we describe a confirmation study based on an extreme , synthetically curated task derived from MultiNLI .",1,0.41720006,163.64932368142718,24
847,We find that a margin-based loss leads to a more plausible model of plausibility .,3,0.9804243,44.30813138386876,15
847,"Finally , we illustrate improvements on the Choice Of Plausible Alternative ( COPA ) task through this change in loss .",3,0.73398525,101.61934698780419,21
848,Lexical entailment ( LE ; also known as hyponymy-hypernymy or is-a relation ) is a core asymmetric lexical relation that supports tasks like taxonomy induction and text generation .,0,0.9180807,59.3246827991513,32
848,"In this work , we propose a simple and effective method for fine-tuning distributional word vectors for LE .",1,0.8489924,20.973617590840348,19
848,Our Generalized Lexical ENtailment model ( GLEN ) is decoupled from the word embedding model and applicable to any distributional vector space .,2,0.5935981,50.37607323721132,23
848,Yet – unlike existing retrofitting models – it captures a general specialization function allowing for LE-tuning of the entire distributional space and not only the vectors of words seen in lexical constraints .,3,0.6987878,78.46774022198564,35
848,"Coupled with a multilingual embedding space , GLEN seamlessly enables cross-lingual LE detection .",3,0.58478266,58.156825719709744,14
848,We demonstrate the effectiveness of GLEN in graded LE and report large improvements ( over 20 % in accuracy ) over state-of-the-art in cross-lingual LE detection .,3,0.94291663,45.182149465012536,33
849,The recently introduced BERT model exhibits strong performance on several language understanding benchmarks .,0,0.53281456,22.916510506434978,14
849,"In this paper , we describe a simple re-implementation of BERT for commonsense reasoning .",1,0.89066726,17.73914725561505,15
849,We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem and Winograd Schema Challenge .,3,0.9453019,17.875523740267656,25
849,Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful .,3,0.8805101,32.00319718076084,15
849,Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin .,3,0.94304335,16.532314580490898,29
849,"While results suggest that BERT seems to implicitly learn to establish complex relationships between entities , solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora .",3,0.9858246,48.57309387587101,32
850,The Winograd Schema Challenge ( WSC ) dataset WSC273 and its inference counterpart WNLI are popular benchmarks for natural language understanding and commonsense reasoning .,0,0.9381036,50.30151866432255,25
850,"In this paper , we show that the performance of three language models on WSC273 consistently and robustly improves when fine-tuned on a similar pronoun disambiguation problem dataset ( denoted WSCR ) .",1,0.7159789,39.272474611133156,33
850,We additionally generate a large unsupervised WSC-like dataset .,2,0.7742218,47.801582897868286,11
850,"By fine-tuning the BERT language model both on the introduced and on the WSCR dataset , we achieve overall accuracies of 72.5 % and 74.7 % on WSC273 and WNLI , improving the previous state-of-the-art solutions by 8.8 % and 9.6 % , respectively .",3,0.8402358,17.481419643444152,52
850,"Furthermore , our fine-tuned models are also consistently more accurate on the “ complex ” subsets of WSC273 , introduced by Trichelair et al .",3,0.9690629,77.51537199036142,25
850,( 2018 ) .,4,0.83046687,158.48024670856745,4
851,Automatic article commenting is helpful in encouraging user engagement on online news platforms .,0,0.8059671,66.9024206362741,14
851,"However , the news documents are usually too long for models under traditional encoder-decoder frameworks , which often results in general and irrelevant comments .",0,0.81307256,88.30656528987926,25
851,"In this paper , we propose to generate comments with a graph-to-sequence model that models the input news as a topic interaction graph .",1,0.8078767,26.32948693727748,26
851,"By organizing the article into graph structure , our model can better understand the internal structure of the article and the connection between topics , which makes it better able to generate coherent and informative comments .",3,0.8121372,34.40545613472301,37
851,We collect and release a large scale news-comment corpus from a popular Chinese online news platform Tencent Kuaibao .,2,0.79853904,52.06529866708707,20
851,Extensive experiment results show that our model can generate much more coherent and informative comments compared with several strong baseline models .,3,0.9771641,21.648117702687426,22
852,We study the problem of generating interconnected questions in question-answering style conversations .,1,0.6466155,34.797117721304495,15
852,"Compared with previous works which generate questions based on a single sentence ( or paragraph ) , this setting is different in two major aspects : ( 1 ) Questions are highly conversational .",0,0.4668676,50.94097648980176,34
852,Almost half of them refer back to conversation history using coreferences .,0,0.72197855,70.47318861399404,12
852,"( 2 ) In a coherent conversation , questions have smooth transitions between turns .",3,0.50924724,247.05809898393125,15
852,We propose an end-to-end neural model with coreference alignment and conversation flow modeling .,2,0.41693437,22.311953499563664,15
852,"The coreference alignment modeling explicitly aligns coreferent mentions in conversation history with corresponding pronominal references in generated questions , which makes generated questions interconnected to conversation history .",2,0.49991506,94.52702971166372,28
852,The conversation flow modeling builds a coherent conversation by starting questioning on the first few sentences in a text passage and smoothly shifting the focus to later parts .,2,0.522507,86.61772396766933,29
852,Extensive experiments show that our system outperforms several baselines and can generate highly conversational questions .,3,0.9277544,14.94655215329224,16
852,The code implementation is released at https://github.com/Evan-Gao/conversaional-QG .,3,0.61387044,21.067164929250044,8
853,Automatic question generation ( QG ) is a challenging problem in natural language understanding .,0,0.9497715,12.107022059096552,15
853,QG systems are typically built assuming access to a large number of training instances where each instance is a question and its corresponding answer .,0,0.8328131,24.194988962409063,25
853,"For a new language , such training instances are hard to obtain making the QG problem even more challenging .",0,0.69118494,84.95688708780541,20
853,"Using this as our motivation , we study the reuse of an available large QG dataset in a secondary language ( e.g .",2,0.5776593,69.1729861323289,23
853,English ) to learn a QG model for a primary language ( e.g .,2,0.73083305,38.48816567965549,14
853,Hindi ) of interest .,4,0.57804394,304.428642952172,5
853,"For the primary language , we assume access to a large amount of monolingual text but only a small QG dataset .",2,0.73643166,35.01664656383238,22
853,We propose a cross-lingual QG model which uses the following training regime : ( i ) Unsupervised pretraining of language models in both primary and secondary languages and ( ii ) joint supervised training for QG in both languages .,2,0.6852724,23.61432576634341,40
853,"We demonstrate the efficacy of our proposed approach using two different primary languages , Hindi and Chinese .",3,0.71560544,34.18697132018041,18
853,"Our proposed framework clearly outperforms a number of baseline models , including a fully-supervised transformer-based model trained on the QG datasets in the primary language .",3,0.87658215,32.25940791793066,30
853,We also create and release a new question answering dataset for Hindi consisting of 6555 sentences .,2,0.6435971,53.75754113166603,17
854,"Unsupervised text style transfer aims to alter text styles while preserving the content , without aligned data for supervision .",0,0.86811435,86.39365882934696,20
854,"Existing seq2seq methods face three challenges : 1 ) the transfer is weakly interpretable , 2 ) generated outputs struggle in content preservation , and 3 ) the trade-off between content and style is intractable .",0,0.8991151,39.43854558252912,37
854,"To address these challenges , we propose a hierarchical reinforced sequence operation method , named Point-Then-Operate ( PTO ) , which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence .",2,0.44470575,57.71288623417573,41
854,"We provide comprehensive training objectives to control the fluency , style , and content of the outputs and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents .",2,0.49710652,54.672815708086645,36
854,Experimental results on two text style transfer datasets show that our method significantly outperforms recent methods and effectively addresses the aforementioned challenges .,3,0.9243845,15.19982859398706,23
855,"Automatically constructed datasets for generating text from semi-structured data ( tables ) , such as WikiBio , often contain reference texts that diverge from the information in the corresponding semi-structured data .",0,0.9140772,31.865312930935946,32
855,"We show that metrics which rely solely on the reference texts , such as BLEU and ROUGE , show poor correlation with human judgments when those references diverge .",3,0.9474251,28.801291418329097,29
855,"We propose a new metric , PARENT , which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall .",2,0.4654428,44.62297084711216,28
855,"Through a large scale human evaluation study of table-to-text models for WikiBio , we show that PARENT correlates with human judgments better than existing text generation metrics .",3,0.7895542,43.65028586269878,32
855,"We also adapt and evaluate the information extraction based evaluation proposed by Wiseman et al ( 2017 ) , and show that PARENT has comparable correlation to it , while being easier to use .",3,0.7757517,90.5239340788344,35
855,We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge .,3,0.9139771,65.61335382601256,23
856,"Obtaining training data for Question Answering ( QA ) is time-consuming and resource-intensive , and existing QA datasets are only available for limited domains and languages .",0,0.9622072,13.00083269225876,29
856,"In this work , we explore to what extent high quality training data is actually required for Extractive QA , and investigate the possibility of unsupervised Extractive QA .",1,0.9305506,25.53043483706185,29
856,"We approach this problem by first learning to generate context , question and answer triples in an unsupervised manner , which we then use to synthesize Extractive QA training data automatically .",2,0.6387502,39.1052848342011,32
856,"To generate such triples , we first sample random context paragraphs from a large corpus of documents and then random noun phrases or Named Entity mentions from these paragraphs as answers .",2,0.8952185,75.7330907237503,32
856,Next we convert answers in context to “ fill-in-the-blank ” cloze questions and finally translate them into natural questions .,2,0.8395183,34.93640822179073,25
856,"We propose and compare various unsupervised ways to perform cloze-to-natural question translation , including training an unsupervised NMT model using non-aligned corpora of natural questions and cloze questions as well as a rule-based approach .",2,0.5208238,32.43703213962909,36
856,We find that modern QA models can learn to answer human questions surprisingly well using only synthetic training data .,3,0.97126836,31.163071246816674,20
856,"We demonstrate that , without using the SQuAD training data at all , our approach achieves 56.4 F1 on SQuAD v1 ( 64.5 F1 when the answer is a Named Entity mention ) , outperforming early supervised models .",3,0.91805506,34.65631682920279,39
857,"A large number of reading comprehension ( RC ) datasets has been created recently , but little analysis has been done on whether they generalize to one another , and the extent to which existing datasets can be leveraged for improving performance on new ones .",0,0.94470745,29.541444117310704,46
857,"In this paper , we conduct such an investigation over ten RC datasets , training on one or more source RC datasets , and evaluating generalization , as well as transfer to a target RC dataset .",1,0.77885777,52.20913166872342,37
857,"We analyze the factors that contribute to generalization , and show that training on a source RC dataset and transferring to a target dataset substantially improves performance , even in the presence of powerful contextual representations from BERT ( Devlin et al. , 2019 ) .",3,0.66441166,39.414162047492965,46
857,"We also find that training on multiple source RC datasets leads to robust generalization and transfer , and can reduce the cost of example collection for a new RC dataset .",3,0.9715001,53.86641456770246,31
857,"Following our analysis , we propose MultiQA , a BERT-based model , trained on multiple RC datasets , which leads to state-of-the-art performance on five RC datasets .",3,0.5962145,21.157850381021596,36
857,We share our infrastructure for the benefit of the research community .,3,0.5563967,37.91098954422631,12
858,This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens .,1,0.7977346,54.144972484213525,20
858,"We propose a curriculum learning ( CL ) based Pointer-Generator framework for reading / sampling over large documents , enabling diverse training of the neural model based on the notion of alternating contextual difficulty .",2,0.4940745,128.91217212049332,37
858,This can be interpreted as a form of domain randomization and / or generative pretraining during training .,3,0.6587513,30.664334392735576,18
858,"To this end , the usage of the Pointer-Generator softens the requirement of having the answer within the context , enabling us to construct diverse training samples for learning .",3,0.45216307,60.185483349459176,32
858,"Additionally , we propose a new Introspective Alignment Layer ( IAL ) , which reasons over decomposed alignments using block-based self-attention .",2,0.54631084,51.059201979792434,24
858,"We evaluate our proposed method on the NarrativeQA reading comprehension benchmark , achieving state-of-the-art performance , improving existing baselines by 51 % relative improvement on BLEU-4 and 17 % relative improvement on Rouge-L .",3,0.6117666,25.50478523349221,43
858,Extensive ablations confirm the effectiveness of our proposed IAL and CL components .,3,0.9786214,117.46662952978447,13
859,"Deep learning models perform poorly on tasks that require commonsense reasoning , which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input .",0,0.8216567,29.17681023468491,32
859,We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations ( CoS-E ) .,2,0.8115473,47.717582703438694,31
859,We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation ( CAGE ) framework .,2,0.8204944,24.304874515288642,30
859,CAGE improves the state-of-the-art by 10 % on the challenging CommonsenseQA task .,3,0.9081628,23.399055442007295,17
859,We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks .,3,0.6430786,35.47607001450417,22
859,Empirical results indicate that we can effectively leverage language models for commonsense reasoning .,3,0.98187536,12.200707365250755,14
860,Interpretability of machine learning ( ML ) models becomes more relevant with their increasing adoption .,0,0.94188124,53.054246398784656,16
860,"In this work , we address the interpretability of ML based question answering ( QA ) models on a combination of knowledge bases ( KB ) and text documents .",1,0.8794211,40.495156137541784,30
860,We adapt post hoc explanation methods such as LIME and input perturbation ( IP ) and compare them with the self-explanatory attention mechanism of the model .,2,0.8137747,60.90547211289587,27
860,"For this purpose , we propose an automatic evaluation paradigm for explanation methods in the context of QA .",1,0.848689,28.330421867093296,19
860,We also conduct a study with human annotators to evaluate whether explanations help them identify better QA models .,2,0.49420214,34.64638647498889,19
860,"Our results suggest that IP provides better explanations than LIME or attention , according to both automatic and human evaluation .",3,0.99135476,75.11260016646311,21
860,"We obtain the same ranking of methods in both experiments , which supports the validity of our automatic evaluation paradigm .",3,0.9562371,72.26267685305952,21
861,Cross-lingual word embeddings encode the meaning of words from different languages into a shared low-dimensional space .,0,0.80615556,7.8848960387701945,17
861,"An important requirement for many downstream tasks is that word similarity should be independent of language — i.e. , word vectors within one language should not be more similar to each other than to words in another language .",0,0.8769786,19.83717867831793,39
861,"We measure this characteristic using modularity , a network measurement that measures the strength of clusters in a graph .",2,0.85087377,82.06426429309225,20
861,"Modularity has a moderate to strong correlation with three downstream tasks , even though modularity is based only on the structure of embeddings and does not require any external resources .",3,0.8994519,27.99758545708615,31
861,"We show through experiments that modularity can serve as an intrinsic validation metric to improve unsupervised cross-lingual word embeddings , particularly on distant language pairs in low-resource settings .",3,0.84917516,20.48357364987171,29
862,"Grounded in cognitive linguistics , graded lexical entailment ( GR-LE ) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale .",0,0.93365926,71.71988323225747,31
862,"In this paper , we present the first work on cross-lingual generalisation of GR-LE relation .",1,0.8262173,35.25278679946829,18
862,"Starting from HyperLex , the only available GR-LE dataset in English , we construct new monolingual GR-LE datasets for three other languages , and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX .",2,0.86907613,49.44114943560397,46
862,"We next present a novel method dubbed CLEAR ( Cross-Lingual Lexical Entailment Attract-Repel ) for effectively capturing graded ( and binary ) LE , both monolingually in different languages as well as across languages ( i.e. , on CL-HYPERLEX ) .",2,0.5049379,82.32603881072029,44
862,"Coupled with a bilingual dictionary , CLEAR leverages taxonomic LE knowledge in a resource-rich language ( e.g. , English ) and propagates it to other languages .",3,0.35088128,58.55848045885763,28
862,"Supported by cross-lingual LE transfer , CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets .",3,0.8595998,46.93890734157417,27
862,"In addition , we show that CLEAR outperforms current state-of-the-art on binary cross-lingual LE detection by a wide margin for diverse language pairs .",3,0.9364013,17.524986668295604,30
863,"In prior work ( Cotterell et al. , 2018 ) we attempted to address this question for language modeling , and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus .",0,0.58649796,31.93657142889154,45
863,We speculated that inflectional morphology may be the primary culprit for the discrepancy .,3,0.84582824,40.61486878705433,14
863,"In this paper , we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus .",1,0.5424434,73.90945106430026,23
863,"Methodologically , we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora .",2,0.86593217,60.43602624772307,24
863,"In other words , the model is aware of inter-sentence variation and can handle missing data .",3,0.5895197,27.002286945490383,17
863,"Exploiting this model , we show that “ translationese ” is not any easier to model than natively written language in a fair comparison .",3,0.7459749,68.70190027206905,25
863,"Trying to answer the question of what features difficult languages have in common , we try and fail to reproduce our earlier ( Cotterell et al. , 2018 ) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample .",3,0.4257492,71.19929399205584,53
864,"Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods , which independently train word embeddings in different languages and map them to a shared space through linear transformations .",0,0.9392841,19.249185498889986,33
864,"While several authors have questioned the underlying isomorphism assumption , which states that word embeddings in different languages have approximately the same structure , it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings .",0,0.89019847,21.65244331547235,47
864,"So as to answer this question , we experiment with parallel corpora , which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces .",2,0.6665021,71.51133553640854,32
864,"We observe that , under these ideal conditions , joint learning yields to more isomorphic embeddings , is less sensitive to hubness , and obtains stronger results in bilingual lexicon induction .",3,0.9677068,97.26278313542664,32
864,"We thus conclude that current mapping methods do have strong limitations , calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal .",3,0.9871242,36.81580471500145,27
865,"In this paper , we show that Multilingual BERT ( M-BERT ) , released by Devlin et al .",1,0.83267915,36.71356534231499,20
865,"( 2018 ) as a single language model pre-trained from monolingual corpora in 104 languages , is surprisingly good at zero-shot cross-lingual model transfer , in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language .",3,0.59794265,21.497561710366806,47
865,"To understand why , we present a large number of probing experiments , showing that transfer is possible even to languages in different scripts , that transfer works best between typologically similar languages , that monolingual corpora can train models for code-switching , and that the model can find translation pairs .",3,0.48669037,61.91674103349105,52
865,"From these results , we can conclude that M-BERT does create multilingual representations , but that these representations exhibit systematic deficiencies affecting certain language pairs .",3,0.9897608,76.87646650239763,26
866,A recent research line has obtained strong results on bilingual lexicon induction by aligning independently trained word embeddings in two languages and using the resulting cross-lingual embeddings to induce word translation pairs through nearest neighbor or related retrieval methods .,0,0.8652754,33.66454343549993,40
866,"In this paper , we propose an alternative approach to this problem that builds on the recent work on unsupervised machine translation .",1,0.8994175,11.752301583743884,23
866,"This way , instead of directly inducing a bilingual lexicon from cross-lingual embeddings , we use them to build a phrase-table , combine it with a language model , and use the resulting machine translation system to generate a synthetic parallel corpus , from which we extract the bilingual lexicon using statistical word alignment techniques .",2,0.6707219,36.92395371868962,56
866,"As such , our method can work with any word embedding and cross-lingual mapping technique , and it does not require any additional resource besides the monolingual corpus used to train the embeddings .",3,0.79371727,21.490833089609772,34
866,"When evaluated on the exact same cross-lingual embeddings , our proposed method obtains an average improvement of 6 accuracy points over nearest neighbor and 4 points over CSLS retrieval , establishing a new state-of-the-art in the standard MUSE dataset .",3,0.9108155,26.718087913388597,46
867,Complaining is a basic speech act regularly used in human and computer mediated communication to express a negative mismatch between reality and expectations in a particular situation .,0,0.9528829,75.62688660974507,28
867,Automatically identifying complaints in social media is of utmost importance for organizations or brands to improve the customer experience or in developing dialogue systems for handling and responding to complaints .,0,0.7921019,46.106121573934004,31
867,"In this paper , we introduce the first systematic analysis of complaints in computational linguistics .",1,0.90871394,25.354413898785804,16
867,We collect a new annotated data set of written complaints expressed on Twitter .,2,0.86472714,65.2994860197978,14
867,We present an extensive linguistic analysis of complaining as a speech act in social media and train strong feature-based and neural models of complaints across nine domains achieving a predictive performance of up to 79 F1 using distant supervision .,2,0.41554987,69.93023032987855,42
868,"With social media becoming increasingly popular on which lots of news and real-time events are reported , developing automated question answering systems is critical to the effective-ness of many applications that rely on real-time knowledge .",0,0.9531585,32.37089238804434,38
868,"While previous datasets have concentrated on question answering ( QA ) for formal text like news and Wikipedia , we present the first large-scale dataset for QA over social media data .",0,0.58789593,29.65952679941433,32
868,"To ensure that the tweets we collected are useful , we only gather tweets used by journalists to write news articles .",2,0.80589074,65.67232480234163,22
868,We then ask human annotators to write questions and answers upon these tweets .,2,0.8449456,89.98322811127903,14
868,"Unlike other QA datasets like SQuAD in which the answers are extractive , we allow the answers to be abstractive .",3,0.41396302,31.064699528785265,21
868,We show that two recently proposed neural models that perform well on formal texts are limited in their performance when applied to our dataset .,3,0.9315703,30.43375571343696,25
868,"In addition , even the fine-tuned BERT model is still lagging behind human performance with a large margin .",3,0.95379037,25.39811428632149,20
868,Our results thus point to the need of improved QA systems targeting social media text .,3,0.9901472,54.03272759643261,16
869,Teaching machines to ask questions is an important yet challenging task .,0,0.94645524,16.95129849984741,12
869,Most prior work focused on generating questions with fixed answers .,0,0.82053393,37.96958738067385,11
869,"As contents are highly limited by given answers , these questions are often not worth discussing .",0,0.84752667,150.61711343082916,17
869,"In this paper , we take the first step on teaching machines to ask open-answered questions from real-world news for open discussion ( openQG ) .",1,0.83037794,82.3630263769049,27
869,"To generate high-qualified questions , effective ways for question evaluation are required .",0,0.7824732,212.5503751772629,13
869,"We take the perspective that the more answers a question receives , the better it is for open discussion , and analyze how language use affects the number of answers .",1,0.3670558,53.76693666451534,31
869,"Compared with other factors , e.g .",3,0.7290068,45.66884688470494,7
869,"topic and post time , linguistic factors keep our evaluation from being domain-specific .",3,0.7211268,319.78585156868166,14
869,"We carefully perform variable control on 11.5 M questions from online forums to get a dataset , OQRanD , and further perform question analysis .",2,0.8265438,322.227733064859,25
869,"Based on these conclusions , several models are built for question evaluation .",3,0.7912044,226.43818643879817,13
869,"For openQG task , we construct OQGenD , the first dataset as far as we know , and propose a model based on conditional generative adversarial networks and our question evaluation model .",2,0.74481773,95.85577837388111,33
869,Experiments show that our model can generate questions with higher quality compared with commonly-used text generation methods .,3,0.97393894,17.51493242771726,20
870,Learning from social-media conversations has gained significant attention recently because of its applications in areas like rumor detection .,0,0.95848775,38.00398493601825,19
870,"In this research , we propose a new way to represent social-media conversations as binarized constituency trees that allows comparing features in source-posts and their replies effectively .",1,0.8042367,82.92897226840485,28
870,"Moreover , we propose to use convolution units in Tree LSTMs that are better at learning patterns in features obtained from the source and reply posts .",3,0.5395728,89.61327124605526,27
870,Our Tree LSTM models employ multi-task ( stance + rumor ) learning and propagate the useful stance signal up in the tree for rumor classification at the root node .,2,0.64486915,205.666768186646,30
870,"The proposed models achieve state-of-the-art performance , outperforming the current best model by 12 % and 15 % on F1-macro for rumor-veracity classification and stance classification tasks respectively .",3,0.8552096,34.74799696896778,36
871,"Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels , which are created heuristically using rule-based methods .",0,0.80559915,43.274439587745135,31
871,Training the hierarchical encoder with these inaccurate labels is challenging .,0,0.61968267,101.43107668804062,11
871,"Inspired by the recent work on pre-training transformer sentence encoders ( Devlin et al. , 2018 ) , we propose Hibert ( as shorthand for HIerachical Bidirectional Encoder Representations from Transformers ) for document encoding and a method to pre-train it using unlabeled data .",2,0.51530683,50.261035222239016,45
871,We apply the pre-trained Hibert to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN / Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset .,3,0.67245317,25.524713763972716,38
871,We also achieve the state-of-the-art performance on these two datasets .,3,0.8600392,6.853044378746299,16
872,"In this paper , we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner .",1,0.87921566,29.17045287500483,32
872,We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence .,2,0.79840237,42.778048089737446,28
872,"Our model learns latent dependencies among textual units , but can also take advantage of explicit graph representations focusing on similarity or discourse relations .",3,0.46929833,150.86070762743978,25
872,Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines .,3,0.93150175,10.466231929581362,19
873,This work proposes a novel framework for enhancing abstractive text summarization based on the combination of deep learning techniques along with semantic data transformations .,1,0.7878297,26.654126096293712,25
873,"Initially , a theoretical model for semantic-based text generalization is introduced and used in conjunction with a deep encoder-decoder architecture in order to produce a summary in generalized form .",2,0.63931143,26.165792666054493,32
873,"Subsequently , a methodology is proposed which transforms the aforementioned generalized summary into human-readable form , retaining at the same time important informational aspects of the original text and addressing the problem of out-of-vocabulary or rare words .",2,0.42257214,44.18682367098037,41
873,The overall approach is evaluated on two popular datasets with encouraging results .,3,0.6203332,38.32974274087986,13
874,"In summarization , automatic evaluation metrics are usually compared based on their ability to correlate with human judgments .",0,0.8662064,28.81511752976335,19
874,"Unfortunately , the few existing human judgment datasets have been created as by-products of the manual evaluations performed during the DUC / TAC shared tasks .",0,0.880372,90.38241986664887,27
874,"However , modern systems are typically better than the best systems submitted at the time of these shared tasks .",0,0.8504481,84.95761628174573,20
874,"We show that , surprisingly , evaluation metrics which behave similarly on these datasets ( average-scoring range ) strongly disagree in the higher-scoring range in which current systems now operate .",3,0.9621192,164.1569406746576,35
874,It is problematic because metrics disagree yet we ca n’t decide which one to trust .,0,0.86568034,207.38057377674903,16
874,This is a call for collecting human judgments for high-scoring summaries as this would resolve the debate over which metrics to trust .,0,0.52350116,59.59692899820059,23
874,This would also be greatly beneficial to further improve summarization systems and metrics alike .,3,0.80858696,67.72920808364225,15
875,We propose an unsupervised method for sentence summarization using only language modeling .,1,0.37051544,23.853911176154032,13
875,"The approach employs two language models , one that is generic ( i.e .",2,0.698034,43.11491733830377,14
875,"pretrained ) , and the other that is specific to the target domain .",3,0.3871937,83.81565983782271,14
875,We show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency .,3,0.94474167,117.58734233717536,22
875,Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data .,3,0.94494784,33.75772854124801,24
876,Existing neural generation approaches create multi-sentence text as a single sequence .,0,0.8417857,45.80793855511035,12
876,In this paper we propose a structured convolutional decoder that is guided by the content structure of target summaries .,1,0.8470099,25.088946437464873,20
876,We compare our model with existing sequential decoders on three data sets representing different domains .,2,0.61333704,33.069312189372276,16
876,Automatic and human evaluation demonstrate that our summaries have better content coverage .,3,0.94573843,54.7590711972421,13
877,We present a study of morphological irregularity .,1,0.8049891,60.621697989535114,8
877,"Following recent work , we define an information-theoretic measure of irregularity based on the predictability of forms in a language .",0,0.40940377,35.35743176494064,23
877,"Using a neural transduction model , we estimate this quantity for the forms in 28 languages .",2,0.7267817,95.50274469497458,17
877,We first present several validatory and exploratory analyses of irregularity .,1,0.32522088,121.69895196581928,11
877,We then show that our analyses provide evidence for a correlation between irregularity and frequency : higher frequency items are more likely to be irregular and irregular items are more likely be highly frequent .,3,0.93113756,23.109141414697284,35
877,"To our knowledge , this result is the first of its breadth and confirms longstanding proposals from the linguistics literature .",3,0.9846948,46.32210195350842,21
877,The correlation is more robust when aggregated at the level of whole paradigms — providing support for models of linguistic structure in which inflected forms are unified by abstract underlying stems or lexemes .,3,0.9678141,78.18377887936099,34
878,We examine the benefits of visual context in training neural language models to perform next-word prediction .,1,0.69968414,29.192055455562947,18
878,"A multi-modal neural architecture is introduced that outperform its equivalent trained on language alone with a 2 % decrease in perplexity , even when no visual context is available at test .",2,0.4107184,50.15398394791214,32
878,Fine-tuning the embeddings of a pre-trained state-of-the-art bidirectional language model ( BERT ) in the language modeling framework yields a 3.5 % improvement .,3,0.7870648,16.183524539560263,29
878,"The advantage for training with visual context when testing without is robust across different languages ( English , German and Spanish ) and different models ( GRU , LSTM , Delta-RNN , as well as those that use BERT embeddings ) .",3,0.8904481,53.83888673936978,43
878,"Thus , language models perform better when they learn like a baby , i.e , in a multi-modal environment .",3,0.6944876,35.873914440494104,20
878,This finding is compatible with the theory of situated cognition : language is inseparable from its physical context .,3,0.9715407,61.54816985765407,19
879,"We investigate these questions using sentences with simple syntax and semantics ( e.g. , The bone was eaten by the dog .",2,0.63604504,97.14959273048206,22
879,) .,4,0.4099943,82.56537726498657,2
879,"We consider multiple neural network architectures , including recently proposed ELMo and BERT .",2,0.6221425,39.02754888384463,14
879,We use magnetoencephalography ( MEG ) brain recording data collected from human subjects when they were reading these simple sentences .,2,0.8969846,32.618247280739205,21
879,"Overall , we find that BERT ’s activations correlate the best with MEG brain data .",3,0.98619473,82.90702846560224,16
879,We also find that the deep network representation can be used to generate brain data from new sentences to augment existing brain data .,3,0.9784777,40.33845379574157,24
879,"To the best of our knowledge , this is the first work showing that the MEG brain recording when reading a word in a sentence can be used to distinguish earlier words in the sentence .",3,0.94221365,19.365134794713658,36
879,Our exploration is also the first to use deep neural network representations to generate synthetic brain data and to show that it helps in improving subsequent stimuli decoding task accuracy .,3,0.94825584,52.384264117903314,31
880,Recent work shows that distributional semantic models can be used to decode patterns of brain activity associated with individual words and sentence meanings .,0,0.9161534,28.205489996113247,24
880,"However , it is yet unclear to what extent such models can be used to study and decode fMRI patterns associated with specific aspects of semantic composition such as the negation function .",0,0.876589,29.339328915327123,33
880,"In this paper , we apply lexical and compositional semantic models to decode fMRI patterns associated with negated and affirmative sentences containing hand-action verbs .",1,0.7653785,67.02282964399694,25
880,"Our results show reduced decoding ( correlation ) of sentences where the verb is in the negated context , as compared to the affirmative one , within brain regions implicated in action-semantic processing .",3,0.9897483,100.79335141976746,34
880,"This supports behavioral and brain imaging studies , suggesting that negation involves reduced access to aspects of the affirmative mental representation .",3,0.8889155,164.58794593695958,22
880,The results pave the way for testing alternate semantic models of negation against human semantic processing in the brain .,3,0.9884183,113.2047333536851,20
881,Sequence-processing neural networks led to remarkable progress on many NLP tasks .,0,0.9172745,22.556398481882855,13
881,"As a consequence , there has been increasing interest in understanding to what extent they process language as humans do .",0,0.96445304,29.817524824921318,21
881,We aim here to uncover which biases such models display with respect to “ natural ” word-order constraints .,1,0.94673437,91.47219807683348,19
881,"We train models to communicate about paths in a simple gridworld , using miniature languages that reflect or violate various natural language trends , such as the tendency to avoid redundancy or to minimize long-distance dependencies .",2,0.8384875,137.47418648888126,37
881,We study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations .,1,0.7045629,150.70310631385397,21
881,The results draw a mixed picture .,3,0.9813478,72.50058336970056,7
881,"On the one hand , neural networks show a strong tendency to avoid long-distance dependencies .",0,0.81284183,23.42238073812436,16
881,"On the other hand , there is no clear preference for the efficient , non-redundant encoding of information that is widely attested in natural language .",0,0.81327534,23.68927403175523,26
881,"We thus suggest inoculating a notion of “ effort ” into neural networks , as a possible way to make their linguistic behavior more human-like .",3,0.9284291,59.6813903413052,27
882,Named entity recognition ( NER ) is widely used in natural language processing applications and downstream tasks .,0,0.9660206,15.98599487190833,18
882,"However , most NER tools target flat annotation from popular datasets , eschewing the semantic information available in nested entity mentions .",0,0.87325597,180.4023214715295,22
882,"We describe NNE — a fine-grained , nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank ( PTB ) .",2,0.3547855,69.96271622682283,26
882,"Our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting .",3,0.7083974,153.7272018468015,17
882,We hope the public release of this large dataset for English newswire will encourage development of new techniques for nested NER .,3,0.9641021,55.655306987338626,22
883,"Sequential labeling-based NER approaches restrict each word belonging to at most one entity mention , which will face a serious problem when recognizing nested entity mentions .",0,0.6807306,103.99917367153512,29
883,"In this paper , we propose to resolve this problem by modeling and leveraging the head-driven phrase structures of entity mentions , i.e. , although a mention can nest other mentions , they will not share the same head word .",1,0.7286266,43.69405898870658,42
883,"Specifically , we propose Anchor-Region Networks ( ARNs ) , a sequence-to-nuggets architecture for nested mention detection .",2,0.41694164,64.10335558208564,20
883,"ARNs first identify anchor words ( i.e. , possible head words ) of all mentions , and then recognize the mention boundaries for each anchor word by exploiting regular phrase structures .",2,0.44920945,110.17833463113818,32
883,"Furthermore , we also design Bag Loss , an objective function which can train ARNs in an end-to-end manner without using any anchor word annotation .",2,0.7524731,60.230269937762294,28
883,Experiments show that ARNs achieve the state-of-the-art performance on three standard nested entity mention detection benchmarks .,3,0.91423696,17.107037641573562,22
884,Constituting highly informative network embeddings is an essential tool for network analysis .,0,0.85005665,38.97519748107354,13
884,"It encodes network topology , along with other useful side information , into low dimensional node-based feature representations that can be exploited by statistical modeling .",0,0.4481931,106.6204448642271,27
884,This work focuses on learning context-aware network embeddings augmented with text data .,1,0.7527756,33.09857602543083,15
884,"We reformulate the network embedding problem , and present two novel strategies to improve over traditional attention mechanisms : ( i ) a content-aware sparse attention module based on optimal transport ;",2,0.63593364,102.03726375975681,34
884,and ( ii ) a high-level attention parsing module .,2,0.42768633,87.62892632711518,11
884,Our approach yields naturally sparse and self-normalized relational inference .,3,0.6547119,242.7136429111399,10
884,"It can capture long-term interactions between sequences , thus addressing the challenges faced by existing textual network embedding schemes .",3,0.59979063,70.06597738071626,20
884,Extensive experiments are conducted to demonstrate our model can consistently outperform alternative state-of-the-art methods .,3,0.65590686,9.69692536600548,20
885,"While the fast-paced inception of novel tasks and new datasets helps foster active research in a community towards interesting directions , keeping track of the abundance of research activity in different areas on different datasets is likely to become increasingly difficult .",0,0.8771142,71.7694885018626,42
885,"The community could greatly benefit from an automatic system able to summarize scientific results , e.g. , in the form of a leaderboard .",3,0.48837578,41.102138903286594,24
885,"In this paper we build two datasets and develop a framework ( TDMS-IE ) aimed at automatically extracting task , dataset , metric and score from NLP papers , towards the automatic construction of leaderboards .",1,0.7667833,100.18710137770846,38
885,Experiments show that our model outperforms several baselines by a large margin .,3,0.94665325,4.708711374529886,13
885,"Our model is a first step towards automatic leaderboard construction , e.g. , in the NLP domain .",3,0.81579983,49.96020304598555,18
886,Supplementing product information by extracting attribute values from title is a crucial task in e-Commerce domain .,0,0.9286009,50.91813634897761,17
886,"Previous studies treat each attribute only as an entity type and build one set of NER tags ( e.g. , BIO ) for each of them , leading to a scalability issue which unfits to the large sized attribute system in real world e-Commerce .",0,0.8753086,74.00883195295854,45
886,"In this work , we propose a novel approach to support value extraction scaling up to thousands of attributes without losing performance : ( 1 ) We propose to regard attribute as a query and adopt only one global set of BIO tags for any attributes to reduce the burden of attribute tag or model explosion ;",1,0.53729004,102.22846282600615,57
886,"( 2 ) We explicitly model the semantic representations for attribute and title , and develop an attention mechanism to capture the interactive semantic relations in-between to enforce our framework to be attribute comprehensive .",2,0.7029152,97.84163298286121,37
886,We conduct extensive experiments in real-life datasets .,2,0.7951986,25.260215352430553,8
886,"The results show that our model not only outperforms existing state-of-the-art NER tagging models , but also is robust and generates promising results for up to 8,906 attributes .",3,0.98212975,19.901780541880733,35
887,"Keyphrases , that concisely describe the high-level topics discussed in a document , are very useful for a wide range of natural language processing tasks .",0,0.84937006,18.19670163505921,27
887,"Though existing keyphrase generation methods have achieved remarkable performance on this task , they generate many overlapping phrases ( including sub-phrases or super-phrases ) of keyphrases .",0,0.8852451,31.88562693970913,27
887,"In this paper , we propose the parallel Seq2Seq network with the coverage attention to alleviate the overlapping phrase problem .",1,0.8430488,42.67118322986945,21
887,"Specifically , we integrate the linguistic constraints of keyphrase into the basic Seq2Seq network on the source side , and employ the multi-task learning framework on the target side .",2,0.8449993,30.39036693091592,30
887,"In addition , in order to prevent from generating overlapping phrases of keyphrases with correct syntax , we introduce the coverage vector to keep track of the attention history and to decide whether the parts of source text have been covered by existing generated keyphrases .",2,0.7520441,48.60498608546393,46
887,"Experimental results show that our method can outperform the state-of-the-art CopyRNN on scientific datasets , and is also more effective in news domain .",3,0.9686658,15.698442392042399,29
888,The mining of adverse drug reaction ( ADR ) has a crucial role in the pharmacovigilance .,0,0.97690487,46.56664196319494,17
888,"The traditional ways of identifying ADR are reliable but time-consuming , non-scalable and offer a very limited amount of ADR relevant information .",0,0.9172814,53.538883894772525,25
888,"With the unprecedented growth of information sources in the forms of social media texts ( Twitter , Blogs , Reviews etc. ) , biomedical literature , and Electronic Medical Records ( EMR ) , it has become crucial to extract the most pertinent ADR related information from these free-form texts .",0,0.9597316,41.12827271118216,51
888,"In this paper , we propose a neural network inspired multi-task learning framework that can simultaneously extract ADRs from various sources .",1,0.88018095,23.586062731154826,22
888,We adopt a novel adversarial learning-based approach to learn features across multiple ADR information sources .,2,0.8563309,40.12747195215358,18
888,"Unlike the other existing techniques , our approach is capable to extracting fine-grained information ( such as ‘ Indications ’ , ‘ Symptoms ’ , ‘ Finding ’ , ‘ Disease ’ , ‘ Drug ’ ) which provide important cues in pharmacovigilance .",3,0.8525963,30.559586994549502,46
888,"We evaluate our proposed approach on three publicly available real-world benchmark pharmacovigilance datasets , a Twitter dataset from PSB 2016 Social Me-dia Shared Task , CADEC corpus and Medline ADR corpus .",2,0.7239407,165.59874148796317,32
888,Experiments show that our unified framework achieves state-of-the-art performance on individual tasks associated with the different benchmark datasets .,3,0.95621264,12.10822003204506,25
888,"This establishes the fact that our proposed approach is generic , which enables it to achieve high performance on the diverse datasets .",3,0.9478231,31.114102436621035,23
889,An arithmetic word problem typically includes a textual description containing several constant quantities .,0,0.9158395,209.7752901724363,14
889,"The key to solving the problem is to reveal the underlying mathematical relations ( such as addition and subtraction ) among quantities , and then generate equations to find solutions .",0,0.8164432,38.82534879678526,31
889,"This work presents a novel approach , Quantity Tagger , that automatically discovers such hidden relations by tagging each quantity with a sign corresponding to one type of mathematical operation .",1,0.6561305,115.90748831521171,31
889,"For each quantity , we assume there exists a latent , variable-sized quantity span surrounding the quantity token in the text , which conveys information useful for determining its sign .",2,0.76408565,80.90149840456996,32
889,"Empirical results show that our method achieves 5 and 8 points of accuracy gains on two datasets respectively , compared to prior approaches .",3,0.94900465,19.677915294814657,24
890,Multi-label classification ( MLC ) aims to predict a set of labels for a given instance .,0,0.86811686,18.11371503034143,17
890,"Based on a pre-defined label order , the sequence-to-sequence ( Seq2Seq ) model trained via maximum likelihood estimation method has been successfully applied to the MLC task and shows powerful ability to capture high-order correlations between labels .",3,0.39226875,26.61390492439438,38
890,"However , the output labels are essentially an unordered set rather than an ordered sequence .",0,0.57713825,44.44015436562132,16
890,"This inconsistency tends to result in some intractable problems , e.g. , sensitivity to the label order .",0,0.6648758,61.693263278554596,18
890,"To remedy this , we propose a simple but effective sequence-to-set model .",2,0.38356262,30.104948203891386,16
890,"The proposed model is trained via reinforcement learning , where reward feedback is designed to be independent of the label order .",2,0.6701286,46.440579157952094,22
890,"In this way , we can reduce the dependence of the model on the label order , as well as capture high-order correlations between labels .",3,0.717333,25.308200007984237,26
890,"Extensive experiments show that our approach can substantially outperform competitive baselines , as well as effectively reduce the sensitivity to the label order .",3,0.90865856,21.040791641227493,24
891,Being able to recognize words as slots and detect the intent of an utterance has been a keen issue in natural language understanding .,0,0.9615719,32.42231072378855,24
891,"The existing works either treat slot filling and intent detection separately in a pipeline manner , or adopt joint models which sequentially label slots while summarizing the utterance-level intent without explicitly preserving the hierarchical relationship among words , slots , and intents .",0,0.70780176,91.34383515192819,45
891,"To exploit the semantic hierarchy for effective modeling , we propose a capsule-based neural network model which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema .",2,0.59689564,66.56143544066981,34
891,A re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation .,3,0.54218936,79.77441529511708,18
891,"Experiments on two real-world datasets show the effectiveness of our model when compared with other alternative model architectures , as well as existing natural language understanding services .",3,0.8456152,18.729221359801983,28
892,Lack of labeled training data is a major bottleneck for neural network based aspect and opinion term extraction on product reviews .,0,0.83733875,64.29250674026417,22
892,"To alleviate this problem , we first propose an algorithm to automatically mine extraction rules from existing training examples based on dependency parsing results .",2,0.45760614,47.82309347045237,25
892,The mined rules are then applied to label a large amount of auxiliary data .,2,0.59372085,56.075049319338284,15
892,"Finally , we study training procedures to train a neural model which can learn from both the data automatically labeled by the rules and a small amount of data accurately annotated by human .",2,0.51089764,46.45153095072532,34
892,"Experimental results show that although the mined rules themselves do not perform well due to their limited flexibility , the combination of human annotated data and rule labeled auxiliary data can improve the neural model and allow it to achieve performance better than or comparable with the current state-of-the-art .",3,0.9708518,24.190351504845598,55
893,"In supervised event detection , most of the mislabeling occurs between a small number of confusing type pairs , including trigger-NIL pairs and sibling sub-types of the same coarse type .",3,0.55990434,114.19117654597882,33
893,"To address this label confusion problem , this paper proposes cost-sensitive regularization , which can force the training procedure to concentrate more on optimizing confusing type pairs .",1,0.558913,111.66131470862135,28
893,"Specifically , we introduce a cost-weighted term into the training loss , which penalizes more on mislabeling between confusing label pairs .",2,0.78293955,85.82109045328833,22
893,"Furthermore , we also propose two estimators which can effectively measure such label confusion based on instance-level or population-level statistics .",3,0.42295003,46.599027679840255,25
893,Experiments on TAC-KBP 2017 datasets demonstrate that the proposed method can significantly improve the performances of different models in both English and Chinese event detection .,3,0.94922835,20.91076565872056,27
894,"Traditional approaches to the task of ACE event extraction usually depend on manually annotated data , which is often laborious to create and limited in size .",0,0.8613643,36.99924625854845,27
894,"Therefore , in addition to the difficulty of event extraction itself , insufficient training data hinders the learning process as well .",0,0.54338384,40.007631553836156,22
894,"To promote event extraction , we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles .",2,0.5735215,61.0404343537578,28
894,"Moreover , to address the problem of insufficient training data , we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality .",2,0.62214136,48.87416080663655,33
894,Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods .,3,0.93279594,27.06303187664946,17
894,"Besides , incorporating our generation method exhibits further significant improvement .",3,0.9349121,252.67667097305156,11
894,"It obtains new state-of-the-art results on the event extraction task , including pushing the F1 score of trigger classification to 81.1 % , and the F1 score of argument classification to 58.9 % .",3,0.8955731,17.105973150800608,40
895,Open information extraction ( IE ) is the task of extracting open-domain assertions from natural language sentences .,0,0.94275856,34.77374665801889,18
895,"A key step in open IE is confidence modeling , ranking the extractions based on their estimated quality to adjust precision and recall of extracted assertions .",0,0.6693184,173.35584571264323,27
895,"We found that the extraction likelihood , a confidence measure used by current supervised open IE systems , is not well calibrated when comparing the quality of assertions extracted from different sentences .",3,0.9822281,86.28015688924623,33
895,"We propose an additional binary classification loss to calibrate the likelihood to make it more globally comparable , and an iterative learning process , where extractions generated by the open IE model are incrementally included as training samples to help the model learn from trial and error .",2,0.5915333,74.21864648995233,48
895,Experiments on OIE2016 demonstrate the effectiveness of our method .,3,0.8620062,21.666256992234377,10
895,Code and data are available at https://github.com/jzbjyb/oie_rank .,3,0.52260023,26.08684016081676,8
896,"Most of the recently proposed neural models for named entity recognition have been purely data-driven , with a strong emphasis on getting rid of the efforts for collecting external resources or designing hand-crafted features .",0,0.92868835,31.496030640829886,36
896,"This could increase the chance of overfitting since the models cannot access any supervision signal beyond the small amount of annotated data , limiting their power to generalize beyond the annotated entities .",3,0.67784524,50.77328395399402,33
896,"In this work , we show that properly utilizing external gazetteers could benefit segmental neural NER models .",1,0.66595465,70.76308208546018,18
896,We add a simple module on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results .,3,0.6308554,67.19172155725138,19
897,Relation Extraction is the task of identifying entity mention spans in raw text and then identifying relations between pairs of the entity mentions .,0,0.80102044,23.400745870239422,24
897,Recent approaches for this span-level task have been token-level models which have inherent limitations .,0,0.9089621,50.658003149972274,17
897,"They cannot easily define and implement span-level features , cannot model overlapping entity mentions and have cascading errors due to the use of sequential decoding .",0,0.644285,103.64846602589765,26
897,"To address these concerns , we present a model which directly models all possible spans and performs joint entity mention detection and relation extraction .",1,0.41170618,49.78214726970691,25
897,We report a new state-of-the-art performance of 62.83 F1 ( prev best was 60.49 ) on the ACE2005 dataset .,3,0.9211169,40.234162716517204,25
898,Most of the unsupervised dependency parsers are based on probabilistic generative models that learn the joint distribution of the given sentence and its parse .,0,0.9012508,16.72534190955614,25
898,"Probabilistic generative models usually explicit decompose the desired dependency tree into factorized grammar rules , which lack the global features of the entire sentence .",0,0.8437825,105.46494270667999,25
898,"In this paper , we propose a novel probabilistic model called discriminative neural dependency model with valence ( D-NDMV ) that generates a sentence and its parse from a continuous latent representation , which encodes global contextual information of the generated sentence .",1,0.8105325,38.99261535050024,45
898,We propose two approaches to model the latent representation : the first deterministically summarizes the representation from the sentence and the second probabilistically models the representation conditioned on the sentence .,2,0.7343873,24.598426933897823,31
898,Our approach can be regarded as a new type of autoencoder model to unsupervised dependency parsing that combines the benefits of both generative and discriminative techniques .,3,0.9181113,15.572785682065478,27
898,"In particular , our approach breaks the context-free independence assumption in previous generative approaches and therefore becomes more expressive .",3,0.6685003,60.24411458352083,21
898,Our extensive experimental results on seventeen datasets from various sources show that our approach achieves competitive accuracy compared with both generative and discriminative state-of-the-art unsupervised dependency parsers .,3,0.93538535,15.67279897728768,34
899,"We propose two neural network architectures for nested named entity recognition ( NER ) , a setting in which named entities may overlap and also be labeled with more than one label .",1,0.45145515,42.09201900890353,33
899,We encode the nested labels using a linearized scheme .,2,0.81902033,136.31514453082553,10
899,"In our first proposed approach , the nested labels are modeled as multilabels corresponding to the Cartesian product of the nested labels in a standard LSTM-CRF architecture .",2,0.7005604,40.07688415595501,30
899,"In the second one , the nested NER is viewed as a sequence-to-sequence problem , in which the input sequence consists of the tokens and output sequence of the labels , using hard attention on the word whose label is being predicted .",2,0.5156088,42.50382080398757,45
899,"The proposed methods outperform the nested NER state of the art on four corpora : ACE-2004 , ACE-2005 , GENIA and Czech CNEC .",3,0.8544466,63.85429282404088,28
899,"We also enrich our architectures with the recently published contextual embeddings : ELMo , BERT and Flair , reaching further improvements for the four nested entity corpora .",3,0.5914342,105.85868914751902,28
899,"In addition , we report flat NER state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003 English .",3,0.7101097,22.553526885056133,28
900,Probabilistic finite automata ( PFAs ) are com-mon statistical language model in natural lan-guage and speech processing .,0,0.9138345,115.34148327560035,18
900,A typical task for PFAs is to compute the probability of all strings that match a query pattern .,0,0.85179204,48.63512508837789,19
900,"An impor-tant special case of this problem is computing the probability of a string appearing as a pre-fix , suffix , or infix .",0,0.8524341,58.03737104976787,24
900,These problems find use in many natural language processing tasks such word prediction and text error correction .,0,0.85640824,41.42302554477426,18
900,"Recently , we gave the first incremental algorithm to efficiently compute the infix probabilities of each prefix of a string ( Cognetta et al. , 2018 ) .",0,0.62455404,80.52117882240331,28
900,"We develop an asymptotic improvement of that algorithm and solve the open problem of computing the infix probabilities of PFAs from streaming data , which is crucial when process-ing queries online and is the ultimate goal of the incremental approach .",2,0.34388193,121.26357475038648,42
901,There are many different ways in which external information might be used in a NLP task .,0,0.8645677,10.623475226750282,17
901,This paper investigates how external syntactic information can be used most effectively in the Semantic Role Labeling ( SRL ) task .,1,0.90955025,28.932373176638965,22
901,We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model .,2,0.72450763,21.6597234452099,34
901,"We show that using a constituency representation as input features improves performance the most , achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL’05 and CoNLL ’12 benchmarks .",3,0.9270134,23.47013069786023,40
902,The Penn Treebank ( PTB ) represents syntactic structures as graphs due to nonlocal dependencies .,0,0.84328884,47.33705380630489,16
902,This paper proposes a method that approximates PTB graph-structured representations by trees .,1,0.82576376,43.02199934086084,13
902,"By our approximation method , we can reduce nonlocal dependency identification and constituency parsing into single tree-based parsing .",3,0.79156876,142.72958201196317,21
902,An experimental result demonstrates that our approximation method with an off-the-shelf tree-based constituency parser significantly outperforms the previous methods in nonlocal dependency identification .,3,0.9515042,32.7118662407709,25
903,We use parsing as sequence labeling as a common framework to learn across constituency and dependency syntactic abstractions .,2,0.79601175,82.4541525802167,19
903,"To do so , we cast the problem as multitask learning ( MTL ) .",2,0.6305316,38.75376812516762,15
903,"First , we show that adding a parsing paradigm as an auxiliary loss consistently improves the performance on the other paradigm .",3,0.6796568,46.88669631053776,22
903,"Secondly , we explore an MTL sequence labeling model that parses both representations , at almost no cost in terms of performance and speed .",2,0.6382033,63.31156103152571,25
903,"The results across the board show that on average MTL models with auxiliary losses for constituency parsing outperform single-task ones by 1.05 F1 points , and for dependency parsing by 0.62 UAS points .",3,0.9809069,45.95739086257836,36
904,"Processing has been perplexed for many years by the problem that multiple semantics are mixed inside a word , even with the help of context .",0,0.95873004,42.702855230217956,26
904,"To solve this problem , we propose a prism module to disentangle the semantic aspects of words and reduce noise at the input layer of a model .",2,0.47233286,30.430664825994427,28
904,"In the prism module , some words are selectively replaced with task-related semantic aspects , then these denoised word representations can be fed into downstream tasks to make them easier .",2,0.389456,94.8010704329075,33
904,"Besides , we also introduce a structure to train this module jointly with the downstream model without additional data .",2,0.5510372,44.165621885238224,20
904,This module can be easily integrated into the downstream model and significantly improve the performance of baselines on named entity recognition ( NER ) task .,3,0.8032803,19.90998151203646,26
904,The ablation analysis demonstrates the rationality of the method .,3,0.9559417,36.42830105181413,10
904,"As a side effect , the proposed method also provides a way to visualize the contribution of each word .",3,0.8312923,20.62047872118468,20
905,"Retrieve-and-edit based approaches to structured prediction , where structures associated with retrieved neighbors are edited to form new structures , have recently attracted increased interest .",0,0.9510857,113.75916801261687,26
905,"However , much recent work merely conditions on retrieved structures ( e.g. , in a sequence-to-sequence framework ) , rather than explicitly manipulating them .",0,0.91129893,62.61959731205216,28
905,We show we can perform accurate sequence labeling by explicitly ( and only ) copying labels from retrieved neighbors .,3,0.9160323,118.20020923913812,20
905,"Moreover , because this copying is label-agnostic , we can achieve impressive performance in zero-shot sequence-labeling tasks .",3,0.65824336,56.05013441819534,22
905,"We additionally consider a dynamic programming approach to sequence labeling in the presence of retrieved neighbors , which allows for controlling the number of distinct ( copied ) segments used to form a prediction , and leads to both more interpretable and accurate predictions .",2,0.66092986,80.6361785670083,45
906,"One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly , a key communicative skill .",0,0.8431921,84.95401088385005,21
906,"While it is straightforward for humans to recognize and acknowledge others ’ feelings in a conversation , this is a significant challenge for AI systems due to the paucity of suitable publicly-available datasets for training and evaluation .",0,0.93234724,24.878928641281863,40
906,"This work proposes a new benchmark for empathetic dialogue generation and EmpatheticDialogues , a novel dataset of 25 k conversations grounded in emotional situations .",1,0.61658317,53.05196960639216,25
906,"Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators , compared to models merely trained on large-scale Internet conversation data .",3,0.98302907,25.818861959889936,31
906,"We also present empirical comparisons of dialogue model adaptations for empathetic responding , leveraging existing models or datasets without requiring lengthy re-training of the full model .",3,0.4947841,85.7588696157369,27
907,"In this paper , a novel Generation-Evaluation framework is developed for multi-turn conversations with the objective of letting both participants know more about each other .",1,0.8056648,24.447925421206747,28
907,"For the sake of rational knowledge utilization and coherent conversation flow , a dialogue strategy which controls knowledge selection is instantiated and continuously adapted via reinforcement learning .",2,0.60190135,100.71475262447323,28
907,"Under the deployed strategy , knowledge grounded conversations are conducted with two dialogue agents .",2,0.78604525,225.9995888983578,15
907,"The generated dialogues are comprehensively evaluated on aspects like informativeness and coherence , which are aligned with our objective and human instinct .",2,0.48720992,56.278712361988504,23
907,These assessments are integrated as a compound reward to guide the evolution of dialogue strategy via policy gradient .,2,0.547937,179.9974383362619,19
907,"Comprehensive experiments have been carried out on the publicly available dataset , demonstrating that the proposed method outperforms the other state-of-the-art approaches significantly .",3,0.8218461,9.58164291465988,30
908,"Despite their popularity in the chatbot literature , retrieval-based models have had modest impact on task-oriented dialogue systems , with the main obstacle to their application being the low-data regime of most task-oriented dialogue tasks .",0,0.9116027,36.46935297380329,42
908,"Inspired by the recent success of pretraining in language modelling , we propose an effective method for deploying response selection in task-oriented dialogue .",1,0.41773224,25.205914673006337,26
908,"To train response selection models for task-oriented dialogue tasks , we propose a novel method which : 1 ) pretrains the response selection model on large general-domain conversational corpora ;",2,0.64888674,37.43706858329032,34
908,"and then 2 ) fine-tunes the pretrained model for the target dialogue domain , relying only on the small in-domain dataset to capture the nuances of the given dialogue domain .",2,0.7826542,34.95525460697011,35
908,"Our evaluation on five diverse application domains , ranging from e-commerce to banking , demonstrates the effectiveness of the proposed training method .",3,0.8617993,24.818488006236375,23
909,We wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios .,1,0.5756383,33.841310570157766,19
909,"Since computer games allow us to simulate such tasks without the need for physical robots , we define a Minecraft-based collaborative building task in which one player ( A , the Architect ) is shown a target structure and needs to instruct the other player ( B , the Builder ) to build this structure .",2,0.5768037,27.03795684285608,58
909,Both players interact via a chat interface .,2,0.6262968,64.16314165957253,8
909,A can observe B but cannot place blocks .,3,0.5106391,927.9238195057422,9
909,"We present the Minecraft Dialogue Corpus , a collection of 509 conversations and game logs .",2,0.49727756,114.51670973552184,16
909,"As a first step towards our goal of developing fully interactive agents for this task , we consider the subtask of Architect utterance generation , and show how challenging it is .",1,0.6433189,43.94079294791547,32
910,We present open domain dialogue generation with meta-words .,2,0.33702603,128.0264937058656,9
910,"A meta-word is a structured record that describes attributes of a response , and thus allows us to explicitly model the one-to-many relationship within open domain dialogues and perform response generation in an explainable and controllable manner .",0,0.637832,34.38794739985603,38
910,"To incorporate meta-words into generation , we propose a novel goal-tracking memory network that formalizes meta-word expression as a goal in response generation and manages the generation process to achieve the goal with a state memory panel and a state controller .",2,0.60740703,62.38709721202926,43
910,"Experimental results from both automatic evaluation and human judgment on two large-scale data sets indicate that our model can significantly outperform state-of-the-art generation models in terms of response relevance , response diversity , and accuracy of meta-word expression .",3,0.9502079,17.88829682512966,45
911,"Although neural conversational models are effective in learning how to produce fluent responses , their primary challenge lies in knowing what to say to make the conversation contentful and non-vacuous .",0,0.9194704,27.686146171987872,31
911,We present a new end-to-end approach to contentful neural conversation that jointly models response generation and on-demand machine reading .,1,0.5770308,28.222717244573733,21
911,The key idea is to provide the conversation model with relevant long-form text on the fly as a source of external knowledge .,0,0.37130034,24.834451795941696,25
911,"The model performs QA-style reading comprehension on this text in response to each conversational turn , thereby allowing for more focused integration of external knowledge than has been possible in prior approaches .",2,0.48631722,43.22893271142659,35
911,"To support further research on knowledge-grounded conversation , we introduce a new large-scale conversation dataset grounded in external web pages ( 2.8 M turns , 7.4 M sentences of grounding ) .",2,0.66404563,61.18185101936532,33
911,"Both human evaluation and automated metrics show that our approach results in more contentful responses compared to a variety of previous methods , improving both the informativeness and diversity of generated output .",3,0.9611803,27.288494021223727,33
912,Multimodal dialogue systems have opened new frontiers in the traditional goal-oriented dialogue systems .,0,0.9459891,18.04698990545504,14
912,"The state-of-the-art dialogue systems are primarily based on unimodal sources , predominantly the text , and hence cannot capture the information present in the other sources such as videos , audios , images etc .",0,0.89145005,29.521610002139244,40
912,"With the availability of large scale multimodal dialogue dataset ( MMD ) ( Saha et al. , 2018 ) on the fashion domain , the visual appearance of the products is essential for understanding the intention of the user .",0,0.7601094,45.06480901693383,40
912,"Without capturing the information from both the text and image , the system will be incapable of generating correct and desirable responses .",0,0.5610144,31.515192535791307,23
912,"In this paper , we propose a novel position and attribute aware attention mechanism to learn enhanced image representation conditioned on the user utterance .",1,0.86842537,46.98774834537115,25
912,Our evaluation shows that the proposed model can generate appropriate responses while preserving the position and attribute information .,3,0.9835554,30.56382772522971,19
912,"Experimental results also prove that our proposed approach attains superior performance compared to the baseline models , and outperforms the state-of-the-art approaches on text similarity based evaluation metrics .",3,0.96594065,13.82715426805331,35
913,Dialogue contexts are proven helpful in the spoken language understanding ( SLU ) system and they are typically encoded with explicit memory representations .,0,0.94032043,73.33725553665313,24
913,"However , most of the previous models learn the context memory with only one objective to maximizing the SLU performance , leaving the context memory under-exploited .",0,0.858264,49.006116834291774,27
913,"In this paper , we propose a new dialogue logistic inference ( DLI ) task to consolidate the context memory jointly with SLU in the multi-task framework .",1,0.8909274,56.08443537696227,28
913,DLI is defined as sorting a shuffled dialogue session into its original logical order and shares the same memory encoder and retrieval mechanism as the SLU model .,0,0.5183403,78.8988713965234,28
913,"Our experimental results show that various popular contextual SLU models can benefit from our approach , and improvements are quite impressive , especially in slot filling .",3,0.9719925,82.00914682245455,27
914,Existing personalized dialogue models use human designed persona descriptions to improve dialogue consistency .,0,0.8469275,108.68866705352806,14
914,Collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs .,0,0.8038874,48.866191141039586,15
914,"In this paper , we propose to extend Model-Agnostic Meta-Learning ( MAML ) ( Finn et al. , 2017 ) to personalized dialogue learning without using any persona descriptions .",1,0.75793576,55.03287619299329,30
914,"Our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user , which is fundamentally different from conditioning the response on the persona descriptions .",3,0.5120072,44.183273532180046,35
914,"Empirical results on Persona-chat dataset ( Zhang et al. , 2018 ) indicate that our solution outperforms non-meta-learning baselines using automatic evaluation metrics , and in terms of human-evaluated fluency and consistency .",3,0.8972157,35.95834629911938,34
915,"Comprehending multi-turn spoken conversations is an emerging research area , presenting challenges different from reading comprehension of passages due to the interactive nature of information exchange from at least two speakers .",0,0.94311607,46.855585248693025,32
915,"Unlike passages , where sentences are often the default semantic modeling unit , in multi-turn conversations , a turn is a topically coherent unit embodied with immediately relevant context , making it a linguistically intuitive segment for computationally modeling verbal interactions .",0,0.7049846,210.06527342575546,42
915,"Therefore , in this work , we propose a hierarchical attention neural network architecture , combining turn-level and word-level attention mechanisms , to improve spoken dialogue comprehension performance .",1,0.8665441,44.486830223784025,29
915,"Experiments are conducted on a multi-turn conversation dataset , where nurses inquire and discuss symptom information with patients .",2,0.7786149,58.29450813268971,19
915,"We empirically show that the proposed approach outperforms standard attention baselines , achieves more efficient learning outcomes , and is more robust to lengthy and out-of-distribution test samples .",3,0.9134005,33.262211052671304,30
916,"A spoken language understanding ( SLU ) system includes two main tasks , slot filling ( SF ) and intent detection ( ID ) .",0,0.8599477,64.84661596272059,25
916,The joint model for the two tasks is becoming a tendency in SLU .,3,0.49010798,108.1809176318498,14
916,But the bi-directional interrelated connections between the intent and slots are not established in the existing joint models .,0,0.60408056,40.349573084959005,19
916,"In this paper , we propose a novel bi-directional interrelated model for joint intent detection and slot filling .",1,0.9136608,26.694694455275542,19
916,We introduce an SF-ID network to establish direct connections for the two tasks to help them promote each other mutually .,2,0.700903,108.62271156330014,23
916,"Besides , we design an entirely new iteration mechanism inside the SF-ID network to enhance the bi-directional interrelated connections .",2,0.60181534,78.52414690202485,22
916,"The experimental results show that the relative improvement in the sentence-level semantic frame accuracy of our model is 3.79 % and 5.42 % on ATIS and Snips datasets , respectively , compared to the state-of-the-art model .",3,0.9510598,18.07743160391119,45
917,Natural language understanding ( NLU ) and natural language generation ( NLG ) are both critical research topics in the NLP and dialogue fields .,0,0.9470598,16.47091609533923,25
917,"Natural language understanding is to extract the core semantic meaning from the given utterances , while natural language generation is opposite , of which the goal is to construct corresponding sentences based on the given semantics .",0,0.8772365,39.75755222483659,37
917,"However , such dual relationship has not been investigated in literature .",0,0.903897,53.5729891722266,12
917,"This paper proposes a novel learning framework for natural language understanding and generation on top of dual supervised learning , providing a way to exploit the duality .",1,0.8179236,41.44324668943813,28
917,"The preliminary experiments show that the proposed approach boosts the performance for both tasks , demonstrating the effectiveness of the dual relationship .",3,0.9686008,33.77655916187587,23
918,"In goal-oriented dialog systems , belief trackers estimate the probability distribution of slot-values at every dialog turn .",0,0.67059344,78.17184989159338,18
918,"Previous neural approaches have modeled domain-and slot-dependent belief trackers , and have difficulty in adding new slot-values , resulting in lack of flexibility of domain ontology configurations .",0,0.8711875,194.90271077977437,29
918,"In this paper , we propose a new approach to universal and scalable belief tracker , called slot-utterance matching belief tracker ( SUMBT ) .",1,0.85883707,128.8052584815385,25
918,The model learns the relations between domain-slot-types and slot-values appearing in utterances through attention mechanisms based on contextual semantic vectors .,2,0.6510772,77.57135298063015,21
918,"Furthermore , the model predicts slot-value labels in a non-parametric way .",3,0.6703818,39.42545894249406,12
918,"From our experiments on two dialog corpora , WOZ 2.0 and MultiWOZ , the proposed model showed performance improvement in comparison with slot-dependent methods and achieved the state-of-the-art joint accuracy .",3,0.86353534,22.4227901257338,37
919,"Task-oriented dialog systems increasingly rely on deep learning-based slot filling models , usually needing extensive labeled training data for target domains .",0,0.8970011,60.059857050096724,26
919,"Often , however , little to no target domain training data may be available , or the training and target domain schemas may be misaligned , as is common for web forms on similar websites .",0,0.78435534,53.094485846422494,36
919,"Prior zero-shot slot filling models use slot descriptions to learn concepts , but are not robust to misaligned schemas .",0,0.7547796,67.43260884663123,20
919,"We propose utilizing both the slot description and a small number of examples of slot values , which may be easily available , to learn semantic representations of slots which are transferable across domains and robust to misaligned schemas .",2,0.42359915,64.89282880811523,40
919,"Our approach outperforms state-of-the-art models on two multi-domain datasets , especially in the low-data setting .",3,0.855166,12.602716932457348,21
920,Identifying the unknown ( novel ) user intents that have never appeared in the training set is a challenging task in the dialogue system .,0,0.6113017,46.45293748533636,25
920,"In this paper , we present a two-stage method for detecting unknown intents .",1,0.91357565,18.14068343853492,15
920,We use bidirectional long short-term memory ( BiLSTM ) network with the margin loss as the feature extractor .,2,0.90051705,42.505989471231885,19
920,"With margin loss , we can learn discriminative deep features by forcing the network to maximize inter-class variance and to minimize intra-class variance .",3,0.50263745,40.66046415600054,24
920,"Then , we feed the feature vectors to the density-based novelty detection algorithm , local outlier factor ( LOF ) , to detect unknown intents .",2,0.83270717,123.15158613472298,28
920,Experiments on two benchmark datasets show that our method can yield consistent improvements compared with the baseline methods .,3,0.90978587,8.874969777809591,19
921,"Multi-turn conversations consist of complex semantic structures , and it is still a challenge to generate coherent and diverse responses given previous utterances .",0,0.93563503,33.71919837690973,24
921,"It ’s practical that a conversation takes place under a background , meanwhile , the query and response are usually most related and they are consistent in topic but also different in content .",0,0.68808025,128.4499484072357,34
921,"However , little work focuses on such hierarchical relationship among utterances .",0,0.9075345,114.58913995196886,12
921,"To address this problem , we propose a Conversational Semantic Relationship RNN ( CSRR ) model to construct the dependency explicitly .",1,0.3301509,69.90159255489165,22
921,The model contains latent variables in three hierarchies .,2,0.7226147,84.74208830850046,9
921,"The discourse-level one captures the global background , the pair-level one stands for the common topic information between query and response , and the utterance-level ones try to represent differences in content .",0,0.52229273,63.45445708288133,36
921,"Experimental results show that our model significantly improves the quality of responses in terms of fluency , coherence , and diversity compared to baseline methods .",3,0.97734094,14.234084566464183,26
922,The Air Travel Information Service ( ATIS ) corpus has been the most common benchmark for evaluating Spoken Language Understanding ( SLU ) tasks for more than three decades since it was released .,0,0.9653134,35.87757531457339,34
922,Recent state-of-the-art neural models have obtained F1-scores near 98 % on the task of slot filling .,0,0.76670504,15.609158246256714,23
922,We developed a rule-based grammar for the ATIS domain that achieves a 95.82 % F1-score on our evaluation set .,2,0.50936896,35.452166868943124,22
922,"In the process , we furthermore discovered numerous shortcomings in the ATIS corpus annotation , which we have fixed .",3,0.90402687,110.09467477074058,20
922,"This paper presents a detailed account of these shortcomings , our proposed repairs , our rule-based grammar and the neural slot-filling architectures associated with ATIS .",1,0.8168445,191.07982428274795,27
922,We also rationally reappraise the motivations for choosing a neural architecture in view of this account .,3,0.5013649,99.54395336037189,17
922,Fixing the annotation errors results in a relative error reduction of between 19.4 and 52 % across all architectures .,3,0.94842947,40.26221157665422,20
922,We nevertheless argue that neural models must play a different role in ATIS dialogues because of the latter ’s lack of variety .,3,0.8900078,63.48547854232387,23
923,"We treat projective dependency trees as latent variables in our probabilistic model and induce them in such a way as to be beneficial for a downstream task , without relying on any direct tree supervision .",2,0.7764549,30.274188550383904,36
923,Our approach relies on Gumbel perturbations and differentiable dynamic programming .,2,0.72299284,56.84124589308951,11
923,"Unlike previous approaches to latent tree learning , we stochastically sample global structures and our parser is fully differentiable .",2,0.59364694,92.68987143690511,20
923,We illustrate its effectiveness on sentiment analysis and natural language inference tasks .,3,0.47441077,25.94719988944721,13
923,We also study its properties on a synthetic structure induction task .,2,0.4840718,76.57624235978648,12
923,Ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees .,0,0.6257458,56.5347158291653,17
924,"Although the proper use of idioms can enhance the elegance of writing , the active use of various expressions is a challenge because remembering idioms is difficult .",0,0.86174583,46.58191020055663,28
924,"In this study , we address the problem of idiom recommendation by leveraging a neural machine translation framework , in which we suppose that idioms are written with one pseudo target language .",1,0.7911937,51.26605744204634,33
924,Two types of real-life datasets are collected to support this study .,2,0.7597575,24.580897663925942,12
924,Experimental results show that the proposed approach achieves promising performance compared with other baseline methods .,3,0.9639932,9.777330484315575,16
925,We show that sampling latent variables multiple times at a gradient step helps in improving a variational autoencoder and propose a simple and effective method to better exploit these latent variables through hidden state averaging .,3,0.8214421,33.605834492160014,36
925,"Consistent gains in performance on two different datasets , Penn Treebank and Yahoo , indicate the generalizability of our method .",3,0.941243,42.822181867565774,21
926,"Recent work establishes dataset difficulty and removes annotation artifacts via partial-input baselines ( e.g. , hypothesis-only model for SNLI or question-only model for VQA ) .",0,0.81165504,80.56269504626225,28
926,A successful partial-input baseline indicates that the dataset is cheatable .,3,0.79303795,167.15328510825393,13
926,But the converse is not necessarily true : failures of partial-input baselines do not mean the dataset is free of artifacts .,0,0.52977586,51.601897987448986,24
926,We first design artificial datasets to illustrate how the trivial patterns that are only visible in the full input can evade any partial-input baseline .,2,0.81570756,136.19897383501458,27
926,"Next , we identify such artifacts in the SNLI dataset — a hypothesis-only model augmented with trivial patterns in the premise can solve 15 % of previously-thought “ hard ” examples .",3,0.5781292,141.35182335650623,34
926,Our work provides a caveat for the use and creation of partial-input baselines for datasets .,3,0.9710679,70.5894542556774,18
927,"While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks , its study in natural language tasks is still very limited .",0,0.9184245,23.831508386004792,31
927,"In this paper , we present a novel data augmentation method for neural machine translation .",1,0.9170626,8.06433857468666,16
927,"Different from previous augmentation methods that randomly drop , swap or replace words with other words in a sentence , we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words .",2,0.7790748,55.0301471217523,39
927,"More accurately , we replace the one-hot representation of a word by a distribution ( provided by a language model ) over the vocabulary , i.e. , replacing the embedding of this word by a weighted combination of multiple semantically similar words .",2,0.64775956,40.305931124591,43
927,"Since the weights of those words depend on the contextual information of the word to be replaced , the newly generated sentences capture much richer information than previous augmentation methods .",3,0.5983647,35.294348391305384,31
927,Experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines .,3,0.94294167,9.633284318270539,23
928,"Adversarial domain adaptation has been recently proposed as an effective technique for textual matching tasks , such as question deduplication .",0,0.91414297,30.163804388885392,21
928,Here we investigate the use of gradient reversal on adversarial domain adaptation to explicitly learn both shared and unshared ( domain specific ) representations between two textual domains .,1,0.8323788,74.93898375434622,29
928,"In doing so , gradient reversal learns features that explicitly compensate for domain mismatch , while still distilling domain specific knowledge that can improve target domain accuracy .",2,0.43216234,147.78669402538347,28
928,"We evaluate reversing gradients for adversarial adaptation on multiple domains , and demonstrate that it significantly outperforms other methods on question deduplication as well as on recognizing textual entailment ( RTE ) tasks , achieving up to 7 % absolute boost in base model accuracy on some datasets .",3,0.7372176,52.57415882354309,49
929,We report our ongoing work about a new deep architecture working in tandem with a statistical test procedure for jointly training texts and their label descriptions for multi-label and multi-class classification tasks .,3,0.45979166,62.7837997788307,33
929,A statistical hypothesis testing method is used to extract the most informative words for each given class .,2,0.81718034,45.57013150251956,18
929,These words are used as a class description for more label-aware text classification .,2,0.44082034,51.86410825137859,16
929,Intuition is to help the model to concentrate on more informative words rather than more frequent ones .,3,0.74088025,29.170877119543327,18
929,The model leverages the use of label descriptions in addition to the input text to enhance text classification performance .,2,0.6165987,27.88588671873003,20
929,"Our method is entirely data-driven , has no dependency on other sources of information than the training data , and is adaptable to different classification problems by providing appropriate training data without major hyper-parameter tuning .",3,0.7772465,38.83766213931832,36
929,"We trained and tested our system on several publicly available datasets , where we managed to improve the state-of-the-art on one set with a high margin and to obtain competitive results on all other ones .",2,0.5594021,16.21506335806287,42
930,"While very deep neural networks have shown effectiveness for computer vision and text classification applications , how to increase the network depth of the neural machine translation ( NMT ) models for better translation quality remains a challenging problem .",0,0.90138984,22.94890046307803,40
930,Directly stacking more blocks to the NMT model results in no improvement and even drop in performance .,3,0.92624766,73.87361785379757,18
930,"In this work , we propose an effective two-stage approach with three specially designed components to construct deeper NMT models , which result in significant improvements over the strong Transformer baselines on WMT14 English →German and English →French translation tasks .",1,0.51974857,27.237501495393026,44
931,Efficiently building an adversarial attacker for natural language processing ( NLP ) tasks is a real challenge .,0,0.9490128,32.940143698558934,18
931,"Firstly , as the sentence space is discrete , it is difficult to make small perturbations along the direction of gradients .",0,0.72815853,31.57846210864134,22
931,"Secondly , the fluency of the generated examples cannot be guaranteed .",0,0.67235494,28.58808865421067,12
931,"In this paper , we propose MHA , which addresses both problems by performing Metropolis-Hastings sampling , whose proposal is designed with the guidance of gradients .",1,0.7527953,89.92785177308423,27
931,Experiments on IMDB and SNLI show that our proposed MHAoutperforms the baseline model on attacking capability .,3,0.96315485,85.09984761362892,17
931,Adversarial training with MHA also leads to better robustness and performance .,3,0.82155883,33.84571620176736,12
932,"Building explainable systems is a critical problem in the field of Natural Language Processing ( NLP ) , since most machine learning models provide no explanations for the predictions .",0,0.9631089,26.782343698056504,30
932,Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs .,0,0.92055315,22.650032684156837,22
932,"However , the fine-grained information ( e.g .",0,0.83542794,21.765918877252766,8
932,"textual explanations for the labels ) is often ignored , and the systems do not explicitly generate the human-readable explanations .",0,0.8212478,68.17349902705794,21
932,"To solve this problem , we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time .",1,0.31809765,17.410106930995763,29
932,"More specifically , we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations .",2,0.52660346,104.91170453653798,22
932,"We construct two new datasets that contain summaries , rating scores , and fine-grained reasons .",2,0.8502474,92.45547705936868,17
932,"We conduct experiments on both datasets , comparing with several strong neural network baseline systems .",2,0.7574785,102.70327893441272,16
932,"Experimental results show that our method surpasses all baselines on both datasets , and is able to generate concise explanations at the same time .",3,0.9702115,13.654468616353348,25
933,"To combat adversarial spelling mistakes , we propose placing a word recognition model in front of the downstream classifier .",2,0.50419503,64.7653748606808,20
933,"Our word recognition models build upon the RNN semi-character architecture , introducing several new backoff strategies for handling rare and unseen words .",2,0.57189643,107.57926250006372,23
933,"Trained to recognize words corrupted by random adds , drops , swaps , and keyboard mistakes , our method achieves 32 % relative ( and 3.3 % absolute ) error reduction over the vanilla semi-character model .",3,0.8578472,122.43274690749233,37
933,"Notably , our pipeline confers robustness on the downstream classifier , outperforming both adversarial training and off-the-shelf spell checkers .",3,0.91313463,34.860027132011375,21
933,"Against a BERT model fine-tuned for sentiment analysis , a single adversarially-chosen character attack lowers accuracy from 90.3 % to 45.8 % .",3,0.9345859,35.77741258901794,25
933,Our defense restores accuracy to 75 % .,3,0.8966395,466.81639050197344,8
933,"Surprisingly , better word recognition does not always entail greater robustness .",0,0.504582,111.81252421992309,12
933,Our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity .,3,0.92072386,73.4295992672739,16
934,"In this paper , we investigate the aspect of structured output modeling for the state-of-the-art graph-based neural dependency parser ( Dozat and Manning , 2017 ) .",1,0.8712614,36.67310412254129,35
934,"With evaluations on 14 treebanks , we empirically show that global output-structured models can generally obtain better performance , especially on the metric of sentence-level Complete Match .",3,0.8962623,81.60960974217618,30
934,"However , probably because neural models already learn good global views of the inputs , the improvement brought by structured output modeling is modest .",3,0.5740763,166.92333538333096,25
935,"Automatically analyzing dialogue can help understand and guide behavior in domains such as counseling , where interactions are largely mediated by conversation .",0,0.80450064,84.22848718834233,23
935,"In this paper , we study modeling behavioral codes used to asses a psychotherapy treatment style called Motivational Interviewing ( MI ) , which is effective for addressing substance abuse and related problems .",1,0.9276339,66.20466513558696,34
935,"Specifically , we address the problem of providing real-time guidance to therapists with a dialogue observer that ( 1 ) categorizes therapist and client MI behavioral codes and , ( 2 ) forecasts codes for upcoming utterances to help guide the conversation and potentially alert the therapist .",1,0.62934893,103.69117667556925,48
935,"For both tasks , we define neural network models that build upon recent successes in dialogue modeling .",2,0.74762845,60.76408580461681,18
935,Our experiments demonstrate that our models can outperform several baselines for both tasks .,3,0.96121854,14.453651403055929,14
935,We also report the results of a careful analysis that reveals the impact of the various network design tradeoffs for modeling therapy dialogue .,3,0.73326296,58.608679341265315,24
936,"Developing Video-Grounded Dialogue Systems ( VGDS ) , where a dialogue is conducted based on visual and audio aspects of a given video , is significantly more challenging than traditional image or text-grounded dialogue systems because ( 1 ) feature space of videos span across multiple picture frames , making it difficult to obtain semantic information ;",0,0.9208002,47.36889112122472,59
936,"and ( 2 ) a dialogue agent must perceive and process information from different modalities ( audio , video , caption , etc. ) to obtain a comprehensive understanding .",0,0.56830317,52.05338322960513,30
936,"Most existing work is based on RNNs and sequence-to-sequence architectures , which are not very effective for capturing complex long-term dependencies ( like in videos ) .",0,0.82255507,27.732483559353156,29
936,"To overcome this , we propose Multimodal Transformer Networks ( MTN ) to encode videos and incorporate information from different modalities .",0,0.6021999,23.395965007778933,22
936,We also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities .,2,0.49447376,27.042256901041082,20
936,We develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference .,2,0.7382902,39.37157875905798,21
936,We get state of the art performance on Dialogue System Technology Challenge 7 ( DSTC7 ) .,3,0.62290853,48.92814170724091,17
936,"Our model also generalizes to another multimodal visual-grounded dialogue task , and obtains promising performance .",3,0.8465304,45.02992507889311,18
937,"Many real-world open-domain conversation applications have specific goals to achieve during open-ended chats , such as recommendation , psychotherapy , education , etc .",0,0.8514927,64.40152358319403,24
937,We study the problem of imposing conversational goals on open-domain chat agents .,1,0.6376891,46.90626304634105,13
937,"In particular , we want a conversational system to chat naturally with human and proactively guide the conversation to a designated target subject .",0,0.8544265,54.939927876039214,24
937,The problem is challenging as no public data is available for learning such a target-guided strategy .,0,0.9394178,38.429436371360325,19
937,We propose a structured approach that introduces coarse-grained keywords to control the intended content of system responses .,2,0.5130724,61.73599248278158,18
937,"We then attain smooth conversation transition through turn-level supervised learning , and drive the conversation towards the target with discourse-level constraints .",2,0.64246243,153.92282488955755,23
937,We further derive a keyword-augmented conversation dataset for the study .,2,0.6634594,42.64704809832661,13
937,"Quantitative and human evaluations show our system can produce meaningful and effective conversations , significantly improving over other approaches .",3,0.9568357,71.36196648706952,20
938,Developing intelligent persuasive conversational agents to change people ’s opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems .,0,0.8977088,51.22164688168887,28
938,"To do so , the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations .",1,0.47631887,150.09567247189594,24
938,We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity .,2,0.81375325,21.168713815182954,22
938,"We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset .",2,0.90320146,89.91481691171404,17
938,"Based on the annotation , we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus .",2,0.74111485,52.685283013706055,29
938,"Furthermore , to develop an understanding of personalized persuasion processes , we analyzed the relationships between individuals ’ demographic and psychological backgrounds including personality , morality , value systems , and their willingness for donation .",2,0.74180484,113.86092211545738,36
938,"Then , we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals ’ personal backgrounds .",2,0.784705,102.04319986180253,24
938,This work lays the ground for developing a personalized persuasive dialogue system .,3,0.90317535,48.29466045046049,13
939,Current neural network-based conversational models lack diversity and generate boring responses to open-ended utterances .,0,0.8966769,42.454356614979154,17
939,"Priors such as persona , emotion , or topic provide additional information to dialog models to aid response generation , but annotating a dataset with priors is expensive and such annotations are rarely available .",0,0.84276736,92.42810357791936,35
939,"While previous methods for improving the quality of open-domain response generation focused on either the underlying model or the training objective , we present a method of filtering dialog datasets by removing generic utterances from training data using a simple entropy-based approach that does not require human supervision .",2,0.6094678,29.83868887079284,51
939,"We conduct extensive experiments with different variations of our method , and compare dialog models across 17 evaluation metrics to show that training on datasets filtered this way results in better conversational quality as chatbots learn to output more diverse responses .",2,0.5412134,59.621913682472915,42
940,Word Sense Disambiguation ( WSD ) is a long-standing but open problem in Natural Language Processing ( NLP ) .,0,0.9653085,17.29332480497811,20
940,"WSD corpora are typically small in size , owing to an expensive annotation process .",0,0.9224721,73.46465660869221,15
940,Current supervised WSD methods treat senses as discrete labels and also resort to predicting the Most-Frequent-Sense ( MFS ) for words unseen during training .,0,0.8494918,94.67020290600048,27
940,This leads to poor performance on rare and unseen senses .,0,0.70584387,61.154783740692906,11
940,"To overcome this challenge , we propose Extended WSD Incorporating Sense Embeddings ( EWISE ) , a supervised model to perform WSD by predicting over a continuous sense embedding space as opposed to a discrete label space .",2,0.48670363,47.936368604892294,38
940,"This allows EWISE to generalize over both seen and unseen senses , thus achieving generalized zero-shot learning .",3,0.4972095,119.56990920672168,18
940,"To obtain target sense embeddings , EWISE utilizes sense definitions .",2,0.57826096,930.2199702760118,11
940,"EWISE learns a novel sentence encoder for sense definitions by using WordNet relations and also ConvE , a recently proposed knowledge graph embedding method .",2,0.5158839,84.43795771308044,25
940,We also compare EWISE against other sentence encoders pretrained on large corpora to generate definition embeddings .,3,0.48977023,59.19523592744211,17
940,EWISE achieves new state-of-the-art WSD performance .,3,0.82092696,21.18603215579253,10
941,Contextual embeddings represent a new generation of semantic representations learned from Neural Language Modelling ( NLM ) that addresses the issue of meaning conflation hampering traditional word embeddings .,0,0.77304715,39.16765229744047,29
941,"In this work , we show that contextual embeddings can be used to achieve unprecedented gains in Word Sense Disambiguation ( WSD ) tasks .",1,0.76671076,14.78326077574629,25
941,"Our approach focuses on creating sense-level embeddings with full-coverage of WordNet , and without recourse to explicit knowledge of sense distributions or task-specific modelling .",2,0.62303185,78.33808760572757,26
941,"As a result , a simple Nearest Neighbors ( k-NN ) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models .",3,0.8382126,67.30206000637962,32
941,"We also analyse the robustness of our approach when ignoring part-of-speech and lemma features , requiring disambiguation against the full sense inventory , and revealing shortcomings to be improved .",3,0.6837754,75.57313770962622,30
941,"Finally , we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective NLMs .",3,0.6328327,55.624981725543044,20
942,"We present an unsupervised method to generate Word2Sense word embeddings that are interpretable — each dimension of the embedding space corresponds to a fine-grained sense , and the non-negative value of the embedding along the j-th dimension represents the relevance of the j-th sense to the word .",2,0.72987145,16.098461633901955,49
942,"The underlying LDA-based generative model can be extended to refine the representation of a polysemous word in a short context , allowing us to use the embedings in contextual tasks .",3,0.62545776,60.486046414144475,33
942,"On computational NLP tasks , Word2 Sense embeddings compare well with other word embeddings generated by unsupervised methods .",3,0.6897561,57.14537770557899,19
942,"Across tasks such as word similarity , entailment , sense induction , and contextual interpretation , Word2 Sense is competitive with the state-of-the-art method for that task .",3,0.86607236,46.777462924691456,34
942,Word2 Sense embeddings are at least as sparse and fast to compute as prior art .,3,0.65565115,62.60043054272525,16
943,Semantic compositionality ( SC ) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents .,0,0.9302654,27.105443976094215,27
943,Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in models .,0,0.73155034,149.83473626284882,21
943,"In this paper , we verify the effectiveness of sememes , the minimum semantic units of human languages , in modeling SC by a confirmatory experiment .",1,0.89114535,104.92276082067977,27
943,"Furthermore , we make the first attempt to incorporate sememe knowledge into SC models , and employ the sememe-incorporated models in learning representations of multiword expressions , a typical task of SC .",3,0.4075363,61.34602754673234,34
943,"In experiments , we implement our models by incorporating knowledge from a famous sememe knowledge base HowNet and perform both intrinsic and extrinsic evaluations .",2,0.6516775,94.67927693721504,25
943,Experimental results show that our models achieve significant performance boost as compared to the baseline methods without considering sememe knowledge .,3,0.9750656,24.01274699329252,21
943,We further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying sememe knowledge in modeling SC .,3,0.6100174,66.73565605975382,20
943,All the code and data of this paper can be obtained on https://github.com/thunlp/Sememe-SC .,3,0.54274976,7.9703386774359855,14
944,The inability to quantify key aspects of creative language is a frequent obstacle to natural language understanding .,0,0.93919826,37.83125289472714,18
944,"To address this , we introduce novel tasks for evaluating the creativeness of language — namely , scoring and ranking text by humorousness and metaphor novelty .",2,0.420373,130.19885043916534,27
944,"To sidestep the difficulty of assigning discrete labels or numeric scores , we learn from pairwise comparisons between texts .",2,0.76320213,73.66779961229038,20
944,"We introduce a Bayesian approach for predicting humorousness and metaphor novelty using Gaussian process preference learning ( GPPL ) , which achieves a Spearman ’s ρ of 0.56 against gold using word embeddings and linguistic features .",2,0.75958437,90.0262315452563,37
944,"Our experiments show that given sparse , crowdsourced annotation data , ranking using GPPL outperforms best–worst scaling .",3,0.9703911,469.3405215288968,18
944,"We release a new dataset for evaluating humour containing 28,210 pairwise comparisons of 4,030 texts , and make our software freely available .",2,0.5020528,80.81112480519758,23
945,The purpose of the research is to answer the question whether linguistic information is retained in vector representations of sentences .,1,0.8961849,20.236984662023914,21
945,"We introduce a method of analysing the content of sentence embeddings based on universal probing tasks , along with the classification datasets for two contrasting languages .",2,0.60429233,56.72692783995007,27
945,"We perform a series of probing and downstream experiments with different types of sentence embeddings , followed by a thorough analysis of the experimental results .",2,0.797642,19.41818287007607,26
945,"Aside from dependency parser-based embeddings , linguistic information is retained best in the recently proposed LASER sentence embeddings .",3,0.60321134,39.40881547817618,20
946,Word embeddings typically represent different meanings of a word in a single conflated vector .,0,0.8731261,38.39980778548215,15
946,Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts .,0,0.94271195,23.16796553971142,32
946,"We present a large dataset based on manual Wikipedia annotations and word senses , where word senses from different words are related by semantic classes .",2,0.6704714,65.33268673513899,26
946,This is the basis for novel diagnostic tests for an embedding ’s content : we probe word embeddings for semantic classes and analyze the embedding space by classifying embeddings into semantic classes .,0,0.51468015,36.17800673701727,33
946,Our main findings are : ( i ) Information about a sense is generally represented well in a single-vector embedding – if the sense is frequent .,3,0.9858182,77.80191388015702,29
946,"( ii ) A classifier can accurately predict whether a word is single-sense or multi-sense , based only on its embedding .",3,0.4316697,30.21438219844673,23
946,"Although rare senses are not well represented in single-vector embeddings , this does not have negative impact on an NLP application whose performance depends on frequent senses .",3,0.7986828,47.79637485063949,28
947,We introduce a general method for the interpretation and comparison of neural models .,2,0.33409125,36.000018731331984,14
947,"The method is used to factor a complex neural model into its functional components , which are comprised of sets of co-firing neurons that cut across layers of the network architecture , and which we call neural pathways .",2,0.7639379,52.88325348759032,39
947,The function of these pathways can be understood by identifying correlated task level and linguistic heuristics in such a way that this knowledge acts as a lens for approximating what the network has learned to apply to its intended task .,3,0.4622668,45.534389581630464,41
947,"As a case study for investigating the utility of these pathways , we present an examination of pathways identified in models trained for two standard tasks , namely Named Entity Recognition and Recognizing Textual Entailment .",1,0.5477699,31.771806288715176,36
948,Lexical relation classification is the task of predicting whether a certain relation holds between a given pair of words .,0,0.90748453,18.02325441562004,20
948,"In this paper , we explore to which extent the current distributional landscape based on word embeddings provides a suitable basis for classification of collocations , i.e. , pairs of words between which idiosyncratic lexical relations hold .",1,0.9347859,40.173765299074304,38
948,"First , we introduce a novel dataset with collocations categorized according to lexical functions .",2,0.76124644,40.47785843571712,15
948,"Second , we conduct experiments on a subset of this benchmark , comparing it in particular to the well known DiffVec dataset .",2,0.8559196,58.22080935070845,23
948,"In these experiments , in addition to simple word vector arithmetic operations , we also investigate the role of unsupervised relation vectors as a complementary input .",2,0.7092872,47.71457933377718,27
948,"While these relation vectors indeed help , we also show that lexical function classification poses a greater challenge than the syntactic and semantic relations that are typically used for benchmarks in the literature .",3,0.95358163,43.32664629930907,34
949,In this paper we discuss the usefulness of applying a checking procedure to existing thesauri .,1,0.9103694,78.7468749745245,16
949,The procedure is based on the analysis of discrepancies of corpus-based and thesaurus-based word similarities .,2,0.72352326,43.01536341585478,18
949,"We applied the procedure to more than 30 thousand words of the Russian wordnet and found some serious errors in word sense description , including inaccurate relationships and missing senses of ambiguous words .",3,0.74715227,93.68847346108976,34
950,This paper proposes Confusionset-guided Pointer Networks for Chinese Spell Check ( CSC ) task .,1,0.8874442,124.09362720614655,17
950,"More concretely , our approach utilizes the off-the-shelf confusionset for guiding the character generation .",3,0.46038672,66.17493396201127,15
950,"To this end , our novel Seq2Seq model jointly learns to copy a correct character from an input sentence through a pointer network , or generate a character from the confusionset rather than the entire vocabulary .",2,0.59637386,49.74896077453594,37
950,"We conduct experiments on three human-annotated datasets , and results demonstrate that our proposed generative model outperforms all competitor models by a large margin of up to 20 % F1 score , achieving state-of-the-art performance on three datasets .",3,0.76866734,13.44453216084335,45
951,Low-resource language pairs with a paucity of parallel data pose challenges for machine translation in terms of both adequacy and fluency .,0,0.8739845,19.235949737869742,24
951,Data augmentation utilizing a large amount of monolingual data is regarded as an effective way to alleviate the problem .,0,0.918145,16.16620550120101,20
951,"In this paper , we propose a general framework of data augmentation for low-resource machine translation not only using target-side monolingual data , but also by pivoting through a related high-resource language .",1,0.8829175,21.75467116641517,36
951,"Specifically , we experiment with a two-step pivoting method to convert high-resource data to the low-resource language , making best use of available resources to better approximate the true distribution of the low-resource language .",2,0.8557903,25.13585663014598,36
951,"First , we inject low-resource words into high-resource sentences through an induced bilingual dictionary .",2,0.917803,33.426623191939285,16
951,"Second , we further edit the high-resource data injected with low-resource words using a modified unsupervised machine translation framework .",2,0.84521574,39.566679492793845,21
951,"Extensive experiments on four low-resource datasets show that under extreme low-resource settings , our data augmentation techniques improve translation quality by up to 1.5 to 8 BLEU points compared to supervised back-translation baselines .",3,0.8980007,12.221648857003903,36
952,"Multi-head self-attention is a key component of the Transformer , a state-of-the-art architecture for neural machine translation .",0,0.7001325,8.82252054918025,24
952,In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder .,1,0.7381552,26.218310337479803,30
952,We find that the most important and confident heads play consistent and often linguistically-interpretable roles .,3,0.98471653,95.68703750017163,18
952,"When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty , we observe that specialized heads are last to be pruned .",3,0.89888567,50.28828033712256,30
952,Our novel pruning method removes the vast majority of heads without seriously affecting performance .,3,0.8518537,51.7795980687455,15
952,"For example , on the English-Russian WMT dataset , pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU .",3,0.9191865,35.218141946980616,26
953,"Unseen words , also called out-of-vocabulary words ( OOVs ) , are difficult for machine translation .",0,0.9343095,34.18859336967548,20
953,"In neural machine translation , byte-pair encoding can be used to represent OOVs , but they are still often incorrectly translated .",0,0.89924675,61.51375374651278,22
953,We improve the translation of OOVs in NMT using easy-to-obtain monolingual data .,3,0.68319345,23.638665938049634,16
953,We look for OOVs in the text to be translated and translate them using simple-to-construct bilingual word embeddings ( BWEs ) .,2,0.62711215,26.284848287581326,26
953,"In our MT experiments we take the 5-best candidates , which is motivated by intrinsic mining experiments .",2,0.65852135,255.97205132492203,20
953,Using all five of the proposed target language words as queries we mine target-language sentences .,2,0.7283112,164.05106722836237,18
953,"We then back-translate , forcing the back-translation of each of the five proposed target-language OOV-translation-candidates to be the original source-language OOV .",2,0.815038,50.76052657687777,34
953,We show that by using this synthetic data to fine-tune our system the translation of OOVs can be dramatically improved .,3,0.95172423,29.636398259261696,22
953,In our experiments we use a system trained on Europarl and mine sentences containing medical terms from monolingual data .,2,0.7672888,51.80352862980764,20
954,Simultaneous translation is widely useful but remains one of the most difficult tasks in NLP .,0,0.9429267,21.691298970030772,16
954,"Previous work either uses fixed-latency policies , or train a complicated two-staged model using reinforcement learning .",0,0.76791817,83.91403492026609,20
954,"We propose a much simpler single model that adds a “ delay ” token to the target vocabulary , and design a restricted dynamic oracle to greatly simplify training .",2,0.57206,120.86686557121917,30
954,English simultaneous translation show that our work leads to flexible policies that achieve better BLEU scores and lower latencies compared to both fixed and RL-learned policies .,3,0.9795623,61.147902161808176,29
955,"To improve low-resource Neural Machine Translation ( NMT ) with multilingual corpus , training on the most related high-resource language only is generally more effective than us-ing all data available ( Neubig and Hu , 2018 ) .",0,0.80154884,71.38582412835869,39
955,"However , it remains a question whether a smart data selection strategy can further improve low-resource NMT with data from other auxiliary languages .",0,0.71976244,50.02948032616959,24
955,"In this paper , we seek to construct a sampling distribution over all multilingual data , so that it minimizes the training loss of the low-resource language .",1,0.7757664,28.14297248127728,28
955,"Based on this formulation , we propose and efficient algorithm , ( TCS ) , which first samples a target sentence , and then conditionally samples its source sentence .",2,0.6069333,103.45594305702956,30
955,"Experiments show TCS brings significant gains of up to 2 BLEU improvements on three of four languages we test , with minimal training overhead .",3,0.9261254,48.9712759464656,25
956,De-identification is the task of detecting protected health information ( PHI ) in medical text .,0,0.9593637,33.13852227630861,16
956,It is a critical step in sanitizing electronic health records ( EHR ) to be shared for research .,0,0.93218297,30.17855079563914,19
956,Automatic de-identification classifiers can significantly speed up the sanitization process .,0,0.5547989,27.040000413427272,11
956,"However , obtaining a large and diverse dataset to train such a classifier that works well across many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records .",0,0.9329877,38.45144133122055,36
956,We introduce a method to create privacy-preserving shareable representations of medical text ( i.e .,1,0.39073217,40.78893076817244,17
956,they contain no PHI ) that does not require expensive manual pseudonymization .,3,0.4169596,276.84939120409075,13
956,These representations can be shared between organizations to create unified datasets for training de-identification models .,3,0.43800548,53.87555938472931,16
956,"Our representation allows training a simple LSTM-CRF de-identification model to an F1 score of 97.4 % , which is comparable to a strong baseline that exposes private information in its representation .",3,0.88064337,39.70953250529932,34
956,"A robust , widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly .",3,0.9294028,60.57705361879669,24
957,Named entity recognition ( NER ) is one of the best studied tasks in natural language processing .,0,0.96758056,13.64122524328221,18
957,"However , most approaches are not capable of handling nested structures which are common in many applications .",0,0.87472415,30.769851646856512,18
957,"In this paper we introduce a novel neural network architecture that first merges tokens and / or entities into entities forming nested structures , and then labels each of them independently .",1,0.82115877,37.45382585774156,32
957,"Unlike previous work , our merge and label approach predicts real-valued instead of discrete segmentation structures , which allow it to combine word and nested entity embeddings while maintaining differentiability .",3,0.5672235,98.46641059215159,31
957,"We evaluate our approach using the ACE 2005 Corpus , where it achieves state-of-the-art F1 of 74.6 , further improved with contextual embeddings ( BERT ) to 82.4 , an overall improvement of close to 8 F1 points over previous approaches trained on the same data .",3,0.6635864,26.20722977664799,52
957,"Additionally we compare it against BiLSTM-CRFs , the dominant approach for flat NER structures , demonstrating that its ability to predict nested structures does not impact performance in simpler cases .",3,0.71672475,65.13945030718942,33
958,Entity resolution ( ER ) is the task of identifying different representations of the same real-world entities across databases .,0,0.9508751,56.91545088114689,20
958,It is a key step for knowledge base creation and text mining .,0,0.8504386,35.62479612956471,13
958,Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records .,0,0.9073987,86.89336049359903,24
958,"While these methods achieve state-of-the-art performance over benchmark data , they require large amounts of labeled data , which are typically unavailable in realistic ER applications .",0,0.69237155,18.951522650514402,33
958,"In this paper , we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of transfer learning and active learning .",1,0.8736762,25.838561544109215,29
958,We design an architecture that allows us to learn a transferable model from a high-resource setting to a low-resource one .,2,0.63246644,10.786444170979545,22
958,"To further adapt to the target dataset , we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model .",2,0.7172183,51.75539475429482,27
958,"Empirical evaluation demonstrates that our method achieves comparable , if not better , performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels .",3,0.9179479,14.733573530432373,36
959,Named entity recognition ( NER ) is the backbone of many NLP solutions .,0,0.96023387,21.69523492128761,14
959,"F1 score , the harmonic mean of precision and recall , is often used to select / evaluate the best models .",0,0.64135635,138.81620171029184,22
959,"However , when precision needs to be prioritized over recall , a state-of-the-art model might not be the best choice .",0,0.5181659,18.352380306541463,27
959,There is little in literature that directly addresses training-time modifications to achieve higher precision information extraction .,0,0.7994319,72.25919672638565,18
959,"In this paper , we propose a neural semi-Markov structured support vector machine model that controls the precision-recall trade-off by assigning weights to different types of errors in the loss-augmented inference during training .",1,0.79763126,39.272746147412114,38
959,"The semi-Markov property provides more accurate phrase-level predictions , thereby improving performance .",3,0.6479193,117.59087480112613,13
959,We empirically demonstrate the advantage of our model when high precision is required by comparing against strong baselines based on CRF .,3,0.8159023,34.930120023715084,22
959,"In our experiments with the CoNLL 2003 dataset , our model achieves a better precision-recall trade-off at various precision levels .",3,0.92577493,27.889364108082905,25
960,This paper studies automatic keyphrase extraction on social media .,1,0.8628126,80.66167516337627,10
960,"Previous works have achieved promising results on it , but they neglect human reading behavior during keyphrase annotating .",0,0.84494174,98.9162534310909,19
960,The human attention is a crucial element of human reading behavior .,0,0.960727,38.731959524962285,12
960,It reveals the relevance of words to the main topics of the target text .,0,0.5090505,49.46411718614326,15
960,"Thus , this paper aims to integrate human attention into keyphrase extraction models .",1,0.9079432,65.86607414454359,14
960,"First , human attention is represented by the reading duration estimated from eye-tracking corpus .",2,0.6726358,110.4484433163293,15
960,"Then , we merge human attention with neural network models by an attention mechanism .",2,0.8766502,51.524300793271394,15
960,"In addition , we also integrate human attention into unsupervised models .",2,0.6609099,51.5448811896288,12
960,"To the best of our knowledge , we are the first to utilize human attention on keyphrase extraction tasks .",3,0.88208646,17.681750861993667,20
960,The experimental results show that our models have significant improvements on two Twitter datasets .,3,0.9717538,23.389606914557376,15
961,In this paper we frame the task of supervised relation classification as an instance of meta-learning .,1,0.69046104,19.154871830338948,17
961,We propose a model-agnostic meta-learning protocol for training relation classifiers to achieve enhanced predictive performance in limited supervision settings .,1,0.36512792,33.450811487299994,21
961,"During training , we aim to not only learn good parameters for classifying relations with sufficient supervision , but also learn model parameters that can be fine-tuned to enhance predictive performance for relations with limited supervision .",2,0.5408631,36.62946482194922,37
961,"In experiments conducted on two relation classification datasets , we demonstrate that the proposed meta-learning approach improves the predictive performance of two state-of-the-art supervised relation classification models .",3,0.8073159,11.591859420951714,34
962,"We introduce VAMPIRE , a lightweight pretraining framework for effective text classification when data and computing resources are limited .",1,0.37808535,44.40995726277668,20
962,"We pretrain a unigram document model as a variational autoencoder on in-domain , unlabeled data and use its internal states as features in a downstream classifier .",2,0.84569,31.576534771090678,27
962,"Empirically , we show the relative strength of VAMPIRE against computationally expensive contextual embeddings and other popular semi-supervised baselines under low resource settings .",3,0.89766294,32.19724573722003,24
962,We also find that fine-tuning to in-domain data is crucial to achieving decent performance from contextual embeddings when working with limited supervision .,3,0.9791332,27.34885255565006,24
962,We accompany this paper with code to pretrain and use VAMPIRE embeddings in downstream tasks .,3,0.5461745,38.77045846891306,16
963,"Pivot Based Language Modeling ( PBLM ) ( Ziser and Reichart , 2018a ) , combining LSTMs with pivot-based methods , has yielded significant progress in unsupervised domain adaptation .",0,0.8592847,57.046522046947146,30
963,"However , this approach is still challenged by the large pivot detection problem that should be solved , and by the inherent instability of LSTMs .",0,0.84661895,53.315141384575014,26
963,"In this paper we propose a Task Refinement Learning ( TRL ) approach , in order to solve these problems .",1,0.8343235,41.39009224447211,21
963,"Our algorithms iteratively train the PBLM model , gradually increasing the information exposed about each pivot .",2,0.64965725,184.29416135343462,17
963,TRL-PBLM achieves stateof-the-art accuracy in six domain adaptation setups for sentiment classification .,3,0.64966416,37.2140664798351,18
963,"Moreover , it is much more stable than plain PBLM across model configurations , making the model much better fitted for practical use .",3,0.9422166,88.34552364378364,24
964,"String similarity models are vital for record linkage , entity resolution , and search .",0,0.93293977,391.4124441689187,15
964,"In this work , we present STANCE –a learned model for computing the similarity of two strings .",1,0.65023243,96.94172561625379,18
964,"Our approach encodes the characters of each string , aligns the encodings using Sinkhorn Iteration ( alignment is posed as an instance of optimal transport ) and scores the alignment with a convolutional neural network .",2,0.7667285,64.75012068394415,36
964,We evaluate STANCE ’s ability to detect whether two strings can refer to the same entity –a task we term alias detection .,2,0.6593999,106.4948409677098,23
964,We construct five new alias detection datasets ( and make them publicly available ) .,2,0.7502107,161.05656490951839,15
964,"We show that STANCE ( or one of its variants ) outperforms both state-of-the-art and classic , parameter-free similarity models on four of the five datasets .",3,0.94414186,36.71247996204486,35
964,We also demonstrate STANCE ’s ability to improve downstream tasks by applying it to an instance of cross-document coreference and show that it leads to a 2.8 point improvement in Bˆ3 F1 over the previous state-of-the-art approach .,3,0.8923717,20.15610813519051,44
965,We present a new architecture for storing and accessing entity mentions during online text processing .,1,0.46573022,45.592692339601534,16
965,"While reading the text , entity references are identified , and may be stored by either updating or overwriting a cell in a fixed-length memory .",0,0.41621315,57.87153450867881,28
965,The update operation implies coreference with the other mentions that are stored in the same cell ;,3,0.5626681,218.55615837954142,17
965,the overwrite operation causes these mentions to be forgotten .,3,0.49296704,156.32692712148406,10
965,"By encoding the memory operations as differentiable gates , it is possible to train the model end-to-end , using both a supervised anaphora resolution objective as well as a supplementary language modeling objective .",3,0.4128005,46.86529406377071,34
965,Evaluation on a dataset of pronoun-name anaphora demonstrates strong performance with purely incremental text processing .,3,0.8247373,119.0143915164912,16
966,Spectral models for learning weighted non-deterministic automata have nice theoretical and algorithmic properties .,0,0.79816407,55.165661785404396,14
966,"Despite this , it has been challenging to obtain competitive results in language modeling tasks , for two main reasons .",0,0.92447543,31.02450878985912,21
966,"First , in order to capture long-range dependencies of the data , the method must use statistics from long substrings , which results in very large matrices that are difficult to decompose .",0,0.6034187,40.87169520349784,34
966,"The second is that the loss function behind spectral learning , based on moment matching , differs from the probabilistic metrics used to evaluate language models .",0,0.48972586,112.34310400642997,27
966,"In this work we employ a technique for scaling up spectral learning , and use interpolated predictions that are optimized to maximize perplexity .",2,0.57142186,74.77047473472092,24
966,"Our experiments in character-based language modeling show that our method matches the performance of state-of-the-art ngram models , while being very fast to train .",3,0.9366256,14.128726265877866,33
967,It can be challenging to train multi-task neural networks that outperform or even match their single-task counterparts .,0,0.81893075,13.156814288408235,20
967,"To help address this , we propose using knowledge distillation where single-task models teach a multi-task model .",1,0.438012,46.43367055860924,20
967,"We enhance this training with teacher annealing , a novel method that gradually transitions the model from distillation to supervised learning , helping the multi-task model surpass its single-task teachers .",2,0.6262341,69.88319589056394,33
967,We evaluate our approach by multi-task fine-tuning BERT on the GLUE benchmark .,3,0.46150118,12.167118360922808,13
967,Our method consistently improves over standard single-task and multi-task training .,3,0.8953123,17.7240802820076,13
968,Neural natural language generation ( NNLG ) from structured meaning representations has become increasingly popular in recent years .,0,0.96191114,21.910349905583274,19
968,"While we have seen progress with generating syntactically correct utterances that preserve semantics , various shortcomings of NNLG systems are clear : new tasks require new training data which is not available or straightforward to acquire , and model outputs are simple and may be dull and repetitive .",0,0.76167375,70.43196827742001,49
968,"This paper addresses these two critical challenges in NNLG by : ( 1 ) scalably ( and at no cost ) creating training datasets of parallel meaning representations and reference texts with rich style markup by using data from freely available and naturally descriptive user reviews , and ( 2 ) systematically exploring how the style markup enables joint control of semantic and stylistic aspects of neural model output .",1,0.8315182,91.64683394157115,70
968,"We present YelpNLG , a corpus of 300,000 rich , parallel meaning representations and highly stylistically varied reference texts spanning different restaurant attributes , and describe a novel methodology that can be scalably reused to generate NLG datasets for other domains .",2,0.3523077,120.22010651362524,42
968,"The experiments show that the models control important aspects , including lexical choice of adjectives , output length , and sentiment , allowing the models to successfully hit multiple style targets without sacrificing semantics .",3,0.95378405,90.80689146339195,35
969,"In this paper , we explore a new approach for automated chess commentary generation , which aims to generate chess commentary texts in different categories ( e.g. , description , comparison , planning , etc. ) .",1,0.90309757,50.00319801353947,37
969,"We introduce a neural chess engine into text generation models to help with encoding boards , predicting moves , and analyzing situations .",2,0.5352185,294.8669952686809,23
969,"By jointly training the neural chess engine and the generation models for different categories , the models become more effective .",3,0.73707473,77.04369660954113,21
969,We conduct experiments on 5 categories in a benchmark Chess Commentary dataset and achieve inspiring results in both automatic and human evaluations .,2,0.6545512,59.718767794754356,23
970,Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge .,0,0.9202781,37.7553654190519,18
970,"However , traditional language models are only capable of remembering facts seen at training time , and often have difficulty recalling them .",0,0.8997237,56.624961801184654,23
970,"To address this , we introduce the knowledge graph language model ( KGLM ) , a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context .",1,0.36481297,25.800955096008927,37
970,"These mechanisms enable the model to render information it has never seen before , as well as generate out-of-vocabulary tokens .",3,0.4998356,28.507277531548315,24
970,"We also introduce the Linked WikiText-2 dataset , a corpus of annotated text aligned to the Wikidata knowledge graph whose contents ( roughly ) match the popular WikiText-2 benchmark .",2,0.7455633,41.72535192419498,34
970,"In experiments , we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model .",3,0.92383385,29.334006171925726,19
970,"We additionally compare different language model ’s ability to complete sentences requiring factual knowledge , showing that the KGLM outperforms even very large language models in generating facts .",3,0.76319194,74.0274675038629,29
971,Prior work on controllable text generation usually assumes that the controlled attribute can take on one of a small set of values known a priori .,0,0.8620906,39.44494946503375,26
971,"In this work , we propose a novel task , where the syntax of a generated sentence is controlled rather by a sentential exemplar .",1,0.7861159,47.99846783388304,25
971,"To evaluate quantitatively with standard metrics , we create a novel dataset with human annotations .",2,0.8090419,70.3619121240873,16
971,We also develop a variational model with a neural module specifically designed for capturing syntactic knowledge and several multitask training objectives to promote disentangled representation learning .,2,0.7069786,45.4918312915957,27
971,"Empirically , the proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics .",3,0.82728434,43.63062098967578,19
972,"The comprehensive descriptions for factual attribute-value tables , which should be accurate , informative and loyal , can be very helpful for end users to understand the structured data in this form .",3,0.7580321,119.74410391292783,34
972,"However previous neural generators might suffer from key attributes missing , less informative and groundless information problems , which impede the generation of high-quality comprehensive descriptions for tables .",0,0.82974637,185.96921869452416,29
972,"To relieve these problems , we first propose force attention ( FA ) method to encourage the generator to pay more attention to the uncovered attributes to avoid potential key attributes missing .",2,0.6744625,119.00741141162713,33
972,"Furthermore , we propose reinforcement learning for information richness to generate more informative as well as more loyal descriptions for tables .",3,0.50377434,72.97512971793878,22
972,"In our experiments , we utilize the widely used WIKIBIO dataset as a benchmark .",2,0.7964869,45.63751022053075,15
972,"Besides , we create WB-filter based on WIKIBIO to test our model in the simulated user-oriented scenarios , in which the generated descriptions should accord with particular user interests .",2,0.771299,93.0083776524608,32
972,Experimental results show that our model outperforms the state-of-the-art baselines on both automatic and human evaluation .,3,0.97016644,3.0442038560173335,23
973,Disentangling the content and style in the latent space is prevalent in unpaired text style transfer .,0,0.77108276,54.008797386541325,17
973,"However , two major issues exist in most of the current neural models .",0,0.8880305,43.77958672030093,14
973,1 ) It is difficult to completely strip the style information from the semantics for a sentence .,0,0.50947237,64.83616541359247,18
973,"2 ) The recurrent neural network ( RNN ) based encoder and decoder , mediated by the latent representation , cannot well deal with the issue of the long-term dependency , resulting in poor preservation of non-stylistic semantic content .",0,0.6524759,43.47948754208922,40
973,"In this paper , we propose the Style Transformer , which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation .",1,0.75841,46.813298847900924,40
974,Variational auto-encoders ( VAEs ) are widely used in natural language generation due to the regularization of the latent space .,0,0.91751677,17.759595213683962,21
974,"However , generating sentences from the continuous latent space does not explicitly model the syntactic information .",0,0.7355093,44.37262852341779,17
974,"In this paper , we propose to generate sentences from disentangled syntactic and semantic spaces .",1,0.8938561,22.525582875664984,16
974,"Our proposed method explicitly models syntactic information in the VAE ’s latent space by using the linearized tree sequence , leading to better performance of language generation .",3,0.5808297,77.74346806640318,28
974,"Additionally , the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications , such as the unsupervised paraphrase generation and syntax transfer generation .",3,0.54339254,39.89121141994526,32
974,"Experimental results show that our proposed model achieves similar or better performance in various tasks , compared with state-of-the-art related work .",3,0.97208035,11.385280194304942,28
975,Automatic story ending generation is an interesting and challenging task in natural language generation .,0,0.9427504,19.255088325169368,15
975,"Previous studies are mainly limited to generate coherent , reasonable and diversified story endings , and few works focus on controlling the sentiment of story endings .",0,0.8762804,113.142619163534,27
975,This paper focuses on generating a story ending which meets the given fine-grained sentiment intensity .,1,0.86176074,66.76512977593542,17
975,There are two major challenges to this task .,0,0.8404155,12.760516106731876,9
975,First is the lack of story corpus which has fine-grained sentiment labels .,0,0.83550215,51.77944992633905,14
975,Second is the difficulty of explicitly controlling sentiment intensity when generating endings .,0,0.76242447,139.28098907245044,13
975,"Therefore , we propose a generic and novel framework which consists of a sentiment analyzer and a sentimental generator , respectively addressing the two challenges .",1,0.40365613,37.122065836621324,26
975,The sentiment analyzer adopts a series of methods to acquire sentiment intensities of the story dataset .,2,0.7207196,59.216917840121106,17
975,The sentimental generator introduces the sentiment intensity into decoder via a Gaussian Kernel Layer to control the sentiment of the output .,2,0.6698468,81.81510306862849,22
975,"To the best of our knowledge , this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels .",3,0.9241223,22.713855460789375,28
975,Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better .,3,0.97220653,33.728187493692495,29
976,"Neural architectures based on self-attention , such as Transformers , recently attracted interest from the research community , and obtained significant improvements over the state of the art in several tasks .",0,0.92266744,25.065724381130956,32
976,We explore how Transformers can be adapted to the task of Neural Question Generation without constraining the model to focus on a specific answer passage .,3,0.54261655,24.561788005616236,26
976,"We study the effect of several strategies to deal with out-of-vocabulary words such as copy mechanisms , placeholders , and contextual word embeddings .",1,0.43792424,27.899087141380075,27
976,"We report improvements obtained over the state-of-the-art on the SQuAD dataset according to automated metrics ( BLEU , ROUGE ) , as well as qualitative human assessments of the system outputs .",3,0.6175072,23.70487319144394,36
977,Paraphrasing is an important task demonstrating the ability to abstract semantic content from its surface form .,0,0.9386951,32.56876998257312,17
977,Recent literature on automatic paraphrasing is dominated by methods leveraging machine translation as an intermediate step .,0,0.94616115,27.213186106928102,17
977,"This contrasts with humans , who can paraphrase without necessarily being bilingual .",0,0.8243994,111.30981803397856,13
977,This work proposes to learn paraphrasing models only from a monolingual corpus .,1,0.4383213,27.245886444752372,13
977,"To that end , we propose a residual variant of vector-quantized variational auto-encoder .",2,0.560402,24.371564717771054,14
977,"Our experiments consider paraphrase identification , and paraphrasing for training set augmentation , comparing to supervised and unsupervised translation-based approaches .",2,0.57542676,67.85508476894648,23
977,Monolingual paraphrasing is shown to outperform unsupervised translation in all contexts .,3,0.83407694,18.70348266718451,12
977,The comparison with supervised MT is more mixed : monolingual paraphrasing is interesting for identification and augmentation but supervised MT is superior for generation .,3,0.6942316,67.20431426439025,25
978,"Information need of humans is essentially multimodal in nature , enabling maximum exploitation of situated context .",0,0.84892195,145.1553663420857,17
978,We introduce a dataset for sequential procedural ( how-to ) text generation from images in cooking domain .,2,0.5862615,186.25089270004435,20
978,"The dataset consists of 16,441 cooking recipes with 160,479 photos associated with different steps .",2,0.7765386,101.53158285292734,15
978,We setup a baseline motivated by the best performing model in terms of human evaluation for the Visual Story Telling ( ViST ) task .,2,0.77903366,62.7830214047904,25
978,"In addition , we introduce two models to incorporate high level structure learnt by a Finite State Machine ( FSM ) in neural sequential generation process by : ( 1 ) Scaffolding Structure in Decoder ( SSiD ) ( 2 ) Scaffolding Structure in Loss ( SSiL ) .",2,0.8345405,51.812113232989695,49
978,"Our best performing model ( SSiL ) achieves a METEOR score of 0.31 , which is an improvement of 0.6 over the baseline model .",3,0.9281401,24.92388327373108,25
978,"We also conducted human evaluation of the generated grounded recipes , which reveal that 61 % found that our proposed ( SSiL ) model is better than the baseline model in terms of overall recipes .",3,0.92263025,93.16302095050362,36
978,We also discuss analysis of the output highlighting key important NLP issues for prospective directions .,3,0.6804234,182.0553740844526,16
979,Paraphrase generation can be regarded as monolingual translation .,0,0.8396977,24.488113292055928,9
979,"Unlike bilingual machine translation , paraphrase generation rewrites only a limited portion of an input sentence .",0,0.78119427,40.56739976932365,17
979,"Hence , previous methods based on machine translation often perform conservatively to fail to make necessary rewrites .",0,0.8995901,95.3817325420698,18
979,"To solve this problem , we propose a neural model for paraphrase generation that first identifies words in the source sentence that should be paraphrased .",0,0.3852852,13.455852078374287,26
979,"Then , these words are paraphrased by the negative lexically constrained decoding that avoids outputting these words as they are .",2,0.6798063,52.02658346460348,21
979,Experiments on text simplification and formality transfer show that our model improves the quality of paraphrasing by making necessary rewrites to an input sentence .,3,0.9372051,25.665470109416574,25
980,"Large-scale pretrained language models define state of the art in natural language processing , achieving outstanding performance on a variety of tasks .",0,0.8314908,12.815664856594454,23
980,"We study how these architectures can be applied and adapted for natural language generation , comparing a number of architectural and training schemes .",1,0.5915855,48.01759411054145,24
980,"We focus in particular on open-domain dialog as a typical high entropy generation task , presenting and comparing different architectures for adapting pretrained models with state of the art results .",2,0.3119758,54.424737333525265,31
981,Sequence-to-sequence ( seq2seq ) models have achieved tremendous success in text generation tasks .,0,0.9452984,10.713458239457092,14
981,"However , there is no guarantee that they can always generate sentences without grammatical errors .",0,0.89595795,30.212343628590737,16
981,"In this paper , we present a preliminary empirical study on whether and how much automatic grammatical error correction can help improve seq2seq text generation .",1,0.9140951,22.507851039282354,26
981,"We conduct experiments across various seq2seq text generation tasks including machine translation , formality style transfer , sentence compression and simplification .",2,0.8709222,63.12487444993597,22
981,Experiments show the state-of-the-art grammatical error correction system can improve the grammaticality of generated text and can bring task-oriented improvements in the tasks where target sentences are in a formal style .,3,0.9650472,19.937570659846582,38
982,"Despite the advancement of question answering ( QA ) systems and rapid improvements on held-out test sets , their generalizability is a topic of concern .",0,0.9624589,38.69723521658006,28
982,We explore the robustness of QA models to question paraphrasing by creating two test sets consisting of paraphrased SQuAD questions .,2,0.6543345,20.619756036979048,21
982,"Paraphrased questions from the first test set are very similar to the original questions designed to test QA models ’ over-sensitivity , while questions from the second test set are paraphrased using context words near an incorrect answer candidate in an attempt to confuse QA models .",2,0.4897402,35.70239323188639,47
982,We show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art QA models .,3,0.96873873,16.31029405912521,25
982,"Using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions , we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing .",2,0.68553925,19.60487192351104,45
983,"The conventional paradigm in neural question answering ( QA ) for narrative content is limited to a two-stage process : first , relevant text passages are retrieved and , subsequently , a neural network for machine comprehension extracts the likeliest answer .",0,0.9416143,109.3854217283196,43
983,"However , both stages are largely isolated in the status quo and , hence , information from the two phases is never properly fused .",0,0.8284553,118.92475952013251,25
983,"In contrast , this work proposes RankQA : RankQA extends the conventional two-stage process in neural QA with a third stage that performs an additional answer re-ranking .",3,0.34416193,53.656473282795986,29
983,"The re-ranking leverages different features that are directly extracted from the QA pipeline , i.e. , a combination of retrieval and comprehension features .",2,0.6257833,47.760093820688695,24
983,"While our intentionally simple design allows for an efficient , data-sparse estimation , it nevertheless outperforms more complex QA systems by a significant margin : in fact , RankQA achieves state-of-the-art performance on 3 out of 4 benchmark datasets .",3,0.86938787,29.889020830509683,46
983,"Furthermore , its performance is especially superior in settings where the size of the corpus is dynamic .",3,0.69412905,44.695097290742765,18
983,Here the answer re-ranking provides an effective remedy against the underlying noise-information trade-off due to a variable corpus size .,3,0.82099384,73.99249441189605,24
983,"As a consequence , RankQA represents a novel , powerful , and thus challenging baseline for future research in content-based QA .",3,0.8548088,74.26565957051567,24
984,Recent work on open domain question answering ( QA ) assumes strong supervision of the supporting evidence and / or assumes a blackbox information retrieval ( IR ) system to retrieve evidence candidates .,0,0.93976843,58.62802171520744,34
984,"We argue that both are suboptimal , since gold evidence is not always available , and QA is fundamentally different from IR .",3,0.79487073,79.49785480324245,23
984,We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system .,3,0.8267224,33.892147344382415,29
984,"In this setting , evidence retrieval from all of Wikipedia is treated as a latent variable .",2,0.46276373,98.91464976795426,17
984,"Since this is impractical to learn from scratch , we pre-train the retriever with an Inverse Cloze Task .",2,0.72262883,46.55620692110156,19
984,We evaluate on open versions of five QA datasets .,2,0.72589123,104.24896383393836,10
984,"On datasets where the questioner already knows the answer , a traditional IR system such as BM25 is sufficient .",0,0.49293384,104.0945802659983,20
984,"On datasets where a user is genuinely seeking an answer , we show that learned retrieval is crucial , outperforming BM25 by up to 19 points in exact match .",3,0.8644474,93.11434541604285,30
985,Multi-hop Reading Comprehension ( RC ) requires reasoning and aggregation across several paragraphs .,0,0.8936047,74.32120733158611,14
985,We propose a system for multi-hop RC that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models .,1,0.39318487,15.184065466768693,28
985,"Since annotations for such decomposition are expensive , we recast subquestion generation as a span prediction problem and show that our method , trained using only 400 labeled examples , generates sub-questions that are as effective as human-authored sub-questions .",3,0.73202556,47.20918676979918,40
985,We also introduce a new global rescoring approach that considers each decomposition ( i.e .,2,0.5697575,60.47211733489019,15
985,"the sub-questions and their answers ) to select the best final answer , greatly improving overall performance .",3,0.6554492,78.98801040418243,18
985,"Our experiments on HotpotQA show that this approach achieves the state-of-the-art results , while providing explainable evidence for its decision making in the form of sub-questions .",3,0.96972007,14.727571450459658,33
986,Winograd Schema Challenge ( WSC ) is a pronoun resolution task which seems to require reasoning with commonsense knowledge .,0,0.9183468,47.70441023583479,20
986,The needed knowledge is not present in the given text .,0,0.626936,32.0506690394269,11
986,Automatic extraction of the needed knowledge is a bottleneck in solving the challenge .,0,0.89318895,60.561024348414904,14
986,The existing state-of-the-art approach uses the knowledge embedded in their pre-trained language model .,0,0.6270309,11.60701724271417,19
986,"However , the language models only embed part of the knowledge , the ones related to frequently co-existing concepts .",0,0.56191885,127.89573511310851,20
986,This limits the performance of such models on the WSC problems .,0,0.60071516,66.25519455959439,12
986,"In this work , we build-up on the language model based methods and augment them with a commonsense knowledge hunting ( using automatic extraction from text ) module and an explicit reasoning module .",2,0.63133967,81.8932042978641,36
986,Our end-to-end system built in such a manner improves on the accuracy of two of the available language model based approaches by 5.53 % and 7.7 % respectively .,3,0.897245,21.054937952183582,31
986,"Overall our system achieves the state-of-the-art accuracy of 71.06 % on the WSC dataset , an improvement of 7.36 % over the previous best .",3,0.9567339,12.690107391396749,29
987,"Open book question answering is a type of natural language based QA ( NLQA ) where questions are expected to be answered with respect to a given set of open book facts , and common knowledge about a topic .",0,0.93669534,27.186273687164576,40
987,"Recently a challenge involving such QA , OpenBookQA , has been proposed .",0,0.91175455,105.76307776098022,13
987,"Unlike most other NLQA that focus on linguistic understanding , OpenBookQA requires deeper reasoning involving linguistic understanding as well as reasoning with common knowledge .",0,0.61991984,72.11521146703137,25
987,"In this paper we address QA with respect to the OpenBookQA dataset and combine state of the art language models with abductive information retrieval ( IR ) , information gain based re-ranking , passage selection and weighted scoring to achieve 72.0 % accuracy , an 11.6 % improvement over the current state of the art .",2,0.34380773,46.45024627916146,56
988,Relation detection is a core step in many natural language process applications including knowledge base question answering .,0,0.8973662,51.956919082040876,18
988,Previous efforts show that single-fact questions could be answered with high accuracy .,0,0.9175026,56.657183021947255,13
988,"However , one critical problem is that current approaches only get high accuracy for questions whose relations have been seen in the training data .",0,0.8740437,42.42876602704634,25
988,"But for unseen relations , the performance will drop rapidly .",3,0.7140399,126.96072572323965,11
988,The main reason for this problem is that the representations for unseen relations are missing .,0,0.90744877,22.359971787600948,16
988,"In this paper , we propose a simple mapping method , named representation adapter , to learn the representation mapping for both seen and unseen relations based on previously learned relation embedding .",1,0.8218682,61.00935668890175,33
988,We employ the adversarial objective and the reconstruction objective to improve the mapping performance .,2,0.78490573,47.58899412449841,15
988,We re-organize the popular SimpleQuestion dataset to reveal and evaluate the problem of detecting unseen relations .,2,0.7439682,65.97637493482584,17
988,Experiments show that our method can greatly improve the performance of unseen relations while the performance for those seen part is kept comparable to the state-of-the-art .,3,0.9585975,18.069774363991986,32
989,Text-based question answering ( TBQA ) has been studied extensively in recent years .,0,0.9653399,17.61274906082749,16
989,Most existing approaches focus on finding the answer to a question within a single paragraph .,0,0.87519187,18.11532595700285,16
989,"However , many difficult questions require multiple supporting evidence from scattered text among two or more documents .",0,0.90639395,108.70872587397221,18
989,"In this paper , we propose Dynamically Fused Graph Network ( DFGN ) , a novel method to answer those questions requiring multiple scattered evidence and reasoning over them .",1,0.8700564,56.416306323685916,30
989,"Inspired by human ’s step-by-step reasoning behavior , DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query , explores along the entity graph dynamically built from the text , and gradually finds relevant supporting entities from the given documents .",0,0.48285633,93.8237997398803,47
989,"We evaluate DFGN on HotpotQA , a public TBQA dataset requiring multi-hop reasoning .",2,0.7051918,65.19908347371074,14
989,DFGN achieves competitive results on the public board .,3,0.7787326,121.98159846597707,9
989,"Furthermore , our analysis shows DFGN produces interpretable reasoning chains .",3,0.9877368,403.28011900888663,11
990,Rule-based models are attractive for various tasks because they inherently lead to interpretable and explainable decisions and can easily incorporate prior knowledge .,0,0.89208215,28.274900326255377,24
990,"However , such systems are difficult to apply to problems involving natural language , due to its large linguistic variability .",0,0.90546775,42.54965996458783,21
990,"In contrast , neural models can cope very well with ambiguity by learning distributed representations of words and their composition from data , but lead to models that are difficult to interpret .",0,0.85852253,61.14279979611646,33
990,"In this paper , we describe a model combining neural networks with logic programming in a novel manner for solving multi-hop reasoning tasks over natural language .",1,0.8867676,24.51374568989849,27
990,"Specifically , we propose to use an Prolog prover which we extend to utilize a similarity function over pretrained sentence encoders .",2,0.66155565,56.916835007471555,22
990,We fine-tune the representations for the similarity function via backpropagation .,2,0.7502836,17.008979771819646,12
990,"This leads to a system that can apply rule-based reasoning to natural language , and induce domain-specific natural language rules from training data .",3,0.4395798,28.928972654947867,25
990,"We evaluate the proposed system on two different question answering tasks , showing that it outperforms two baselines – BiDAF ( Seo et al. , 2016a ) and FastQA ( Weissenborn et al. , 2017 ) on a subset of the WikiHop corpus and achieves competitive results on the MedHop data set ( Welbl et al. , 2017 ) .",3,0.59236467,27.724722228847515,60
991,Several deep learning models have been proposed for solving math word problems ( MWPs ) automatically .,0,0.90758723,45.3389418842009,17
991,"Although these models have the ability to capture features without manual efforts , their approaches to capturing features are not specifically designed for MWPs .",0,0.7835511,55.08436027354077,25
991,"To utilize the merits of deep learning models with simultaneous consideration of MWPs ’ specific features , we propose a group attention mechanism to extract global features , quantity-related features , quantity-pair features and question-related features in MWPs respectively .",2,0.72813,82.8316730116489,46
991,"The experimental results show that the proposed approach performs significantly better than previous state-of-the-art methods , and boost performance from 66.9 % to 69.5 % on Math23 K with training-test split , from 65.8 % to 66.9 % on Math23 K with 5-fold cross-validation and from 69.2 % to 76.1 % on MAWPS .",3,0.9255967,14.888586341162895,64
992,"We introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction , and by filtering the results to ensure roundtrip consistency .",2,0.47776258,42.44116981806928,31
992,"By pretraining on the resulting corpora we obtain significant improvements on SQuAD2 and NQ , establishing a new state-of-the-art on the latter .",3,0.8632395,15.719513515336404,29
992,"Our synthetic data generation models , for both question generation and answer extraction , can be fully reproduced by finetuning a publicly available BERT model on the extractive subsets of SQuAD2 and NQ .",3,0.8171876,41.653796983776886,34
992,"We also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation , obtaining exact match and F1 at less than 0.1 % and 0.4 % from human performance on SQuAD2 .",3,0.79645365,40.31781044281116,37
993,"Although current evaluation of question-answering systems treats predictions in isolation , we need to consider the relationship between predictions to measure true understanding .",0,0.8337433,48.391316335587845,26
993,A model should be penalized for answering “ no ” to “ Is the rose red ? ” if it answers “ red ” to “ What color is the rose ? ” .,3,0.7324008,30.286779236025406,34
993,"We propose a method to automatically extract such implications for instances from two QA datasets , VQA and SQuAD , which we then use to evaluate the consistency of models .",1,0.39258248,29.98682228609741,31
993,Human evaluation shows these generated implications are well formed and valid .,3,0.9353039,310.3938835252828,12
993,"Consistency evaluation provides crucial insights into gaps in existing models , while retraining with implication-augmented data improves consistency on both synthetic and human-generated implications .",3,0.72989553,60.84686454684212,27
994,"Conversational machine reading comprehension ( CMRC ) extends traditional single-turn machine reading comprehension ( MRC ) by multi-turn interactions , which requires machines to consider the history of conversation .",0,0.9414546,34.08839925214663,30
994,Most of models simply combine previous questions for conversation understanding and only employ recurrent neural networks ( RNN ) for reasoning .,0,0.87079096,132.60355809336463,22
994,"To comprehend context profoundly and efficiently from different perspectives , we propose a novel neural network model , Multi-perspective Convolutional Cube ( MCˆ2 ) .",2,0.4196555,76.62328734938735,25
994,We regard each conversation as a cube .,2,0.71702963,173.76177985027945,8
994,1D and 2D convolutions are integrated with RNN in our model .,2,0.6559832,34.18220342934994,12
994,"To avoid models previewing the next turn of conversation , we also extend causal convolution partially to 2D .",2,0.69092107,208.89269871650387,19
994,Experiments on the Conversational Question Answering ( CoQA ) dataset show that our model achieves state-of-the-art results .,3,0.8777371,5.55542123991405,23
995,"While neural machine translation ( NMT ) has achieved remarkable success , NMT systems are prone to make word omission errors .",0,0.95248026,31.081116450997836,22
995,"In this work , we propose a contrastive learning approach to reducing word omission errors in NMT .",1,0.8812226,34.46948184812902,18
995,"The basic idea is to enable the NMT model to assign a higher probability to a ground-truth translation and a lower probability to an erroneous translation , which is automatically constructed from the ground-truth translation by omitting words .",2,0.4704508,18.104842386155173,41
995,"We design different types of negative examples depending on the number of omitted words , word frequency , and part of speech .",2,0.8424867,67.65667827713575,23
995,"Experiments on Chinese-to-English , German-to-English , and Russian-to-English translation tasks show that our approach is effective in reducing word omission errors and achieves better translation performance than three baseline methods .",3,0.88910645,12.170567980796156,38
996,"In this work , we present novel approaches to exploit sentential context for neural machine translation ( NMT ) .",1,0.8956057,31.434080351973243,20
996,"Specifically , we show that a shallow sentential context extracted from the top encoder layer only , can improve translation performance via contextualizing the encoding representations of individual words .",3,0.8828176,76.65756656199855,30
996,"Next , we introduce a deep sentential context , which aggregates the sentential context representations from all of the internal layers of the encoder to form a more comprehensive context representation .",2,0.6936521,25.552844568945236,32
996,"Experimental results on the WMT14 English-German and English-French benchmarks show that our model consistently improves performance over the strong Transformer model , demonstrating the necessity and effectiveness of exploiting sentential context for NMT .",3,0.9538806,14.677983891764667,38
997,Multilingual individuals code switch between languages as a part of a complex communication process .,0,0.9116776,89.314482978508,15
997,"However , most computational studies have examined only one or a handful of contextual factors predictive of switching .",0,0.8892845,104.45362078187995,19
997,"Here , we examine Naija-English code switching in a rich contextual environment to understand the social and topical factors eliciting a switch .",1,0.87316453,107.07194746461292,24
997,We introduce a new corpus of 330 K articles and accompanying 389K comments labeled for code switching behavior .,2,0.78436667,189.99619243331148,19
997,"In modeling whether a comment will switch , we show that topic-driven variation , tribal affiliation , emotional valence , and audience design all play complementary roles in behavior .",3,0.8483304,205.7820322037062,30
998,Graphics Processing Units ( GPUs ) are commonly used to train and evaluate neural networks efficiently .,0,0.94596773,44.03309167906572,17
998,"While previous work in deep learning has focused on accelerating operations on dense matrices / tensors on GPUs , efforts have concentrated on operations involving sparse data structures .",0,0.9060119,71.23009372727957,29
998,"Operations using sparse structures are common in natural language models at the input and output layers , because these models operate on sequences over discrete alphabets .",0,0.7604031,57.845407660846035,27
998,"We present two new GPU algorithms : one at the input layer , for multiplying a matrix by a few-hot vector ( generalizing the more common operation of multiplication by a one-hot vector ) and one at the output layer , for a fused softmax and top-N selection ( commonly used in beam search ) .",2,0.7184533,54.23629140051767,58
998,"Our methods achieve speedups over state-of-the-art parallel GPU baselines of up to 7x and 50x , respectively .",3,0.8088384,16.36287187492415,23
998,We also illustrate how our methods scale on different GPU architectures .,3,0.6415625,53.3558458511049,12
999,"We present a fully automated workflow for phylogenetic reconstruction on large datasets , consisting of two novel methods , one for fast detection of cognates and one for fast Bayesian phylogenetic inference .",1,0.47142106,40.90795133628595,33
999,Our results show that the methods take less than a few minutes to process language families that have so far required large amounts of time and computational power .,3,0.98685,30.701452585068825,29
999,"Moreover , the cognates and the trees inferred from the method are quite close , both to gold standard cognate judgments and to expert language family trees .",3,0.8971443,167.1552777425673,28
999,"Given its speed and ease of application , our framework is specifically useful for the exploration of very large datasets in historical linguistics .",3,0.8137573,40.17155280225997,24
1000,Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets .,0,0.9653535,9.153279970723524,25
1000,"In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries , domains , or languages .",1,0.78466684,26.234667876962018,36
1000,"We revisit a popular graph-based ranking algorithm and modify how node ( aka sentence ) centrality is computed in two ways : ( a ) we employ BERT , a state-of-the-art neural representation learning model to better capture sentential meaning and ( b ) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document .",2,0.73630244,40.37109896137226,80
1000,Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin .,3,0.87768894,10.704532170896066,26
1001,We introduce a novel semantic parsing task based on Discourse Representation Theory ( DRT ; Kamp and Reyle 1993 ) .,2,0.5158105,64.8908484663601,21
1001,Our model operates over Discourse Representation Tree Structures which we formally define for sentences and documents .,2,0.75738853,73.57841915085353,17
1001,We present a general framework for parsing discourse structures of arbitrary length and granularity .,1,0.38252097,35.3866955125912,15
1001,We achieve this with a neural model equipped with a supervised hierarchical attention mechanism and a linguistically-motivated copy strategy .,2,0.6948151,32.22208085788454,22
1001,Experimental results on sentence-and document-level benchmarks show that our model outperforms competitive baselines by a wide margin .,3,0.9234911,10.428746783378928,21
1002,Automatic summarization is typically treated as a 1-to-1 mapping from document to summary .,0,0.9249204,18.681587159960916,18
1002,"Documents such as news articles , however , are structured and often cover multiple topics or aspects ;",0,0.93872416,171.58678086067584,18
1002,and readers may be interested in only some of them .,0,0.58979595,42.33654953955769,11
1002,"We tackle the task of aspect-based summarization , where , given a document and a target aspect , our models generate a summary centered around the aspect .",2,0.5024297,32.72599349074285,28
1002,"We induce latent document structure jointly with an abstractive summarization objective , and train our models in a scalable synthetic setup .",2,0.81400234,88.97340606171136,22
1002,"In addition to improvements in summarization over topic-agnostic baselines , we demonstrate the benefit of the learnt document structure : we show that our models ( a ) learn to accurately segment documents by aspect ;",3,0.7779651,88.07464556796873,36
1002,( b ) can leverage the structure to produce both abstractive and extractive aspect-based summaries ;,3,0.61295295,107.36049497443648,17
1002,and ( c ) that structure is particularly advantageous for summarizing long documents .,3,0.8447439,97.52736070706453,14
1002,All results transfer from synthetic training documents to natural news articles from CNN / Daily Mail and RCV1 .,3,0.8332301,166.67915673821665,19
1003,"Feature attribution methods , proposed recently , help users interpret the predictions of complex models .",0,0.9076243,181.61551528496037,16
1003,Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building .,2,0.56751335,81.14586524067096,21
1003,"To demonstrate the effectiveness our technique , we apply it to two tasks : ( 1 ) mitigating unintended bias in text classifiers by neutralizing identity terms ;",2,0.6045682,99.55862151402143,28
1003,( 2 ) improving classifier performance in scarce data setting by forcing model to focus on toxic terms .,3,0.5411344,242.58636792113526,19
1003,Our approach adds an L2 distance loss between feature attributions and task-specific prior values to the objective .,2,0.6159366,90.00674440966375,19
1003,Our experiments show that i ) a classifier trained with our technique reduces undesired model biases without a tradeoff on the original task ;,3,0.97235715,90.60006627951083,24
1003,ii ) incorporating prior helps model performance in scarce data settings .,3,0.43673265,852.1452197387133,12
1004,"Identifying the relationship between two articles , e.g. , whether two articles published from different sources describe the same breaking news , is critical to many document understanding tasks .",0,0.92616206,39.5497689648583,30
1004,"Existing approaches for modeling and matching sentence pairs do not perform well in matching longer documents , which embody more complex interactions between the enclosed entities than a sentence does .",0,0.8037818,76.89098426513563,31
1004,"To model article pairs , we propose the Concept Interaction Graph to represent an article as a graph of concepts .",2,0.6677382,47.97465934580445,21
1004,"We then match a pair of articles by comparing the sentences that enclose the same concept vertex through a series of encoding techniques , and aggregate the matching signals through a graph convolutional network .",2,0.8770159,58.95905746340274,35
1004,"To facilitate the evaluation of long article matching , we have created two datasets , each consisting of about 30 K pairs of breaking news articles covering diverse topics in the open domain .",2,0.87248707,63.23803242593315,34
1004,Extensive evaluations of the proposed methods on the two datasets demonstrate significant improvements over a wide range of state-of-the-art methods for natural language matching .,3,0.9199783,8.822524756086882,31
1005,Multi-Label Hierarchical Text Classification ( MLHTC ) is the task of categorizing documents into one or more topics organized in an hierarchical taxonomy .,0,0.93015414,24.98436017840723,24
1005,MLHTC can be formulated by combining multiple binary classification problems with an independent classifier for each category .,0,0.5396629,53.64336236725506,18
1005,"We propose a novel transfer learning based strategy , HTrans , where binary classifiers at lower levels in the hierarchy are initialized using parameters of the parent classifier and fine-tuned on the child category classification task .",2,0.71669865,58.59076818239895,37
1005,"In HTrans , we use a Gated Recurrent Unit ( GRU )-based deep learning architecture coupled with attention .",2,0.7986127,71.18289780371724,21
1005,"Compared to binary classifiers trained from scratch , our HTrans approach results in significant improvements of 1 % on micro-F1 and 3 % on macro-F1 on the RCV1 dataset .",3,0.94570875,44.633387590213445,34
1005,Our experiments also show that binary classifiers trained from scratch are significantly better than single multi-label models .,3,0.98081553,31.300573416332337,18
1006,The PAN series of shared tasks is well known for its continuous and high quality research in the field of digital text forensics .,0,0.93101436,67.91147203328184,24
1006,"Among others , PAN contributions include original corpora , tailored benchmarks , and standardized experimentation platforms .",0,0.70323396,541.3734543482302,17
1006,"In this paper we review , theoretically and practically , the authorship verification task and conclude that the underlying experiment design cannot guarantee pushing forward the state of the art — in fact , it allows for top benchmarking with a surprisingly straightforward approach .",1,0.6531342,83.3074352297517,45
1006,"In this regard , we present a “ Basic and Fairly Flawed ” ( BAFF ) authorship verifier that is on a par with the best approaches submitted so far , and that illustrates sources of bias that should be eliminated .",1,0.70568883,73.96360379192933,42
1006,We pinpoint these sources in the evaluation chain and present a refined authorship corpus as effective countermeasure .,3,0.5233306,128.87689310339903,18
1007,"In this paper , we attempt to answer the question of whether neural network models can learn numeracy , which is the ability to predict the magnitude of a numeral at some specific position in a text description .",1,0.93524534,18.090991623132435,39
1007,"A large benchmark dataset , called Numeracy-600K , is provided for the novel task .",2,0.61315054,91.28239811602896,18
1007,"We explore several neural network models including CNN , GRU , BiGRU , CRNN , CNN-capsule , GRU-capsule , and BiGRU-capsule in the experiments .",2,0.7362206,33.88516649619507,31
1007,"The results show that the BiGRU model gets the best micro-averaged F1 score of 80.16 % , and the GRU-capsule model gets the best macro-averaged F1 score of 64.71 % .",3,0.9813121,21.571669437043994,34
1007,"Besides discussing the challenges through comprehensive experiments , we also present an important application scenario , i.e. , detecting exaggerated information , for the task .",3,0.45381325,141.98797333086375,26
1008,We consider Large-Scale Multi-Label Text Classification ( LMTC ) in the legal domain .,2,0.5806535,58.8839288835955,14
1008,"We release a new dataset of 57 k legislative documents from EUR-LEX , annotated with ∼ 4.3 k EUROVOC labels , which is suitable for LMTC , few-and zero-shot learning .",2,0.66190445,171.19678469705866,35
1008,"Experimenting with several neural classifiers , we show that BIGRUs with label-wise attention perform better than other current state of the art methods .",3,0.8976683,55.516365793448735,26
1008,Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance .,3,0.82806987,47.31255816852927,10
1008,We also find that considering only particular zones of the documents is sufficient .,3,0.9803201,109.09205615064839,14
1008,"This allows us to bypass BERT ’s maximum text length limit and fine-tune BERT , obtaining the best results in all but zero-shot learning cases .",3,0.768293,37.91992985696238,27
1009,"To address the lack of comparative evaluation of Human-in-the-Loop Topic Modeling ( HLTM ) systems , we implement and evaluate three contrasting HLTM modeling approaches using simulation experiments .",1,0.53841484,47.694288789962684,31
1009,"These approaches extend previously proposed frameworks , including constraints and informed prior-based methods .",2,0.4750181,281.8478638465781,14
1009,"Users should have a sense of control in HLTM systems , so we propose a control metric to measure whether refinement operations ’ results match users ’ expectations .",1,0.39818656,147.38176432932173,29
1009,"Informed prior-based methods provide better control than constraints , but constraints yield higher quality topics .",3,0.6506894,318.8202475509262,16
1010,"While paragraph embedding models are remarkably effective for downstream classification tasks , what they learn and encode into a single vector remains opaque .",0,0.865159,69.68570403775072,24
1010,"In this paper , we investigate a state-of-the-art paragraph embedding method proposed by Zhang et al .",1,0.8614667,13.045139884841515,23
1010,( 2017 ) and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not .,0,0.36633617,42.01797190568536,22
1010,We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it .,3,0.53547543,56.7987626765329,28
1010,This result motivates us to replace the reconstruction-based objective of Zhang et al .,3,0.9235223,50.37992878471494,16
1010,( 2017 ) with our sentence content probe objective in a semi-supervised setting .,2,0.42864275,104.12962929691334,14
1010,"Despite its simplicity , our objective improves over paragraph reconstruction in terms of ( 1 ) downstream classification accuracies on benchmark datasets , ( 2 ) faster training , and ( 3 ) better generalization ability .",3,0.7831172,59.428648110128236,37
1011,We describe a multi-task learning approach to train a Neural Machine Translation ( NMT ) model with a Relevance-based Auxiliary Task ( RAT ) for search query translation .,1,0.46633825,16.627727517570577,31
1011,The translation process for Cross-lingual Information Retrieval ( CLIR ) task is usually treated as a black box and it is performed as an independent step .,0,0.96239895,23.511782948179125,27
1011,"However , an NMT model trained on sentence-level parallel data is not aware of the vocabulary distribution of the retrieval corpus .",0,0.7242612,32.37244370747096,24
1011,We address this problem and propose a multi-task learning architecture that achieves 16 % improvement over a strong baseline on Italian-English query-document dataset .,2,0.43005773,28.39856201223684,28
1011,We show using both quantitative and qualitative analysis that our model generates balanced and precise translations with the regularization effect it achieves from multi-task learning paradigm .,3,0.9084543,51.19699639894004,27
1012,We propose a novel neural topic model in the Wasserstein autoencoders ( WAE ) framework .,2,0.4934516,36.1120386831895,16
1012,"Unlike existing variational autoencoder based models , we directly enforce Dirichlet prior on the latent document-topic vectors .",2,0.71168506,62.105362647327375,18
1012,We exploit the structure of the latent space and apply a suitable kernel in minimizing the Maximum Mean Discrepancy ( MMD ) to perform distribution matching .,2,0.78834176,78.29622438847753,27
1012,We discover that MMD performs much better than the Generative Adversarial Network ( GAN ) in matching high dimensional Dirichlet distribution .,3,0.9477792,42.56714275184145,22
1012,We further discover that incorporating randomness in the encoder output during training leads to significantly more coherent topics .,3,0.97073156,53.688400564694476,19
1012,"To measure the diversity of the produced topics , we propose a simple topic uniqueness metric .",2,0.6444988,98.94234019104893,17
1012,"Together with the widely used coherence measure NPMI , we offer a more wholistic evaluation of topic quality .",3,0.76064634,90.14384476520088,19
1012,Experiments on several real datasets show that our model produces significantly better topics than existing topic models .,3,0.92773473,24.47132191384817,18
1013,Understanding narrated instructional videos is important for both research and real-world web applications .,0,0.8873459,37.71346832210493,14
1013,"Motivated by video dense captioning , we propose a model to generate procedure captions from narrated instructional videos which are a sequence of step-wise clips with description .",2,0.45089114,67.58545076586479,28
1013,Previous works on video dense captioning learn video segments and generate captions without considering transcripts .,0,0.7890238,122.00649578004274,16
1013,We argue that transcripts in narrated instructional videos can enhance video representation by providing fine-grained complimentary and semantic textual information .,3,0.90631276,87.33642109307951,22
1013,"In this paper , we introduce a framework to ( 1 ) extract procedures by a cross-modality module , which fuses video content with the entire transcript ;",1,0.75256985,109.01831793361207,28
1013,and ( 2 ) generate captions by encoding video frames as well as a snippet of transcripts within each extracted procedure .,2,0.7252294,111.29808871074756,22
1013,"Experiments show that our model can achieve state-of-the-art performance in procedure extraction and captioning , and the ablation studies demonstrate that both the video frames and the transcripts are important for the task .",3,0.9557576,18.710400254911928,40
1014,"In this work , we propose to model the interaction between visual and textual features for multi-modal neural machine translation ( MMT ) through a latent variable model .",1,0.75612944,21.509014944321144,29
1014,This latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language .,2,0.38775063,20.79454841525004,22
1014,It is used in a target-language decoder and also to predict image features .,0,0.51551765,48.591858278801,16
1014,"Importantly , our model formulation utilises visual and textual inputs during training but does not require that images be available at test time .",3,0.54082936,54.070228405785926,24
1014,"We show that our latent variable MMT formulation improves considerably over strong baselines , including a multi-task learning approach ( Elliott and Kadar , 2017 ) and a conditional variational auto-encoder approach ( Toyama et al. , 2016 ) .",3,0.80527246,37.539935447251665,40
1014,"Finally , we show improvements due to ( i ) predicting image features in addition to only conditioning on them , ( ii ) imposing a constraint on the KL term to promote models with non-negligible mutual information between inputs and latent variable , and ( iii ) by training on additional target-language image descriptions ( i.e .",3,0.886829,56.878254920840206,60
1014,synthetic data ) .,3,0.47168472,239.03846701417228,4
1015,We consider the task of identifying human actions visible in online videos .,0,0.43617797,45.311396467208475,13
1015,"We focus on the widely spread genre of lifestyle vlogs , which consist of videos of people performing actions while verbally describing them .",1,0.40919763,57.047909360790285,24
1015,Our goal is to identify if actions mentioned in the speech description of a video are visually present .,1,0.8534851,44.95276742073113,19
1015,"We construct a dataset with crowdsourced manual annotations of visible actions , and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video .",2,0.8164059,32.31739810521931,37
1016,"We introduce a new dataset for joint reasoning about natural language and images , with a focus on semantic diversity , compositionality , and visual reasoning challenges .",1,0.4046935,77.04703977613299,28
1016,"The data contains 107,292 examples of English sentences paired with web photographs .",2,0.75290436,123.50266342321711,13
1016,The task is to determine whether a natural language caption is true about a pair of photographs .,0,0.6389759,31.480235164336584,18
1016,We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language .,2,0.90987235,42.26222340628404,23
1016,"Qualitative analysis shows the data requires compositional joint reasoning , including about quantities , comparisons , and relations .",0,0.74142295,210.31955069529246,19
1016,Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge .,0,0.4931073,19.373821334275092,19
1017,We propose a segmental neural language model that combines the generalization power of neural networks with the ability to discover word-like units that are latent in unsegmented character sequences .,1,0.49591485,24.00407505575373,32
1017,"In contrast to previous segmentation models that treat word segmentation as an isolated task , our model unifies word discovery , learning how words fit together to form sentences , and , by conditioning the model on visual context , how words ’ meanings ground in representations of nonlinguistic modalities .",3,0.41730726,69.35906924184133,51
1017,"Experiments show that the unconditional model learns predictive distributions better than character LSTM models , discovers words competitively with nonparametric Bayesian word segmentation models , and that modeling language conditional on visual context improves performance on both .",3,0.9457536,96.34485903551936,38
1018,"The ability to engage in goal-oriented conversations has allowed humans to gain knowledge , reduce uncertainty , and perform tasks more efficiently .",0,0.9202471,39.82026690328381,23
1018,"Artificial agents , however , are still far behind humans in having goal-driven conversations .",0,0.9359133,92.6050501591402,16
1018,"In this work , we focus on the task of goal-oriented visual dialogue , aiming to automatically generate a series of questions about an image with a single objective .",1,0.83828294,28.96963954115359,30
1018,"This task is challenging since these questions must not only be consistent with a strategy to achieve a goal , but also consider the contextual information in the image .",0,0.87400097,36.16174268401714,30
1018,"We propose an end-to-end goal-oriented visual dialogue system , that combines reinforcement learning with regularized information gain .",1,0.46349373,37.428500898831935,19
1018,"Unlike previous approaches that have been proposed for the task , our work is motivated by the Rational Speech Act framework , which models the process of human inquiry to reach a goal .",2,0.5189364,50.650418853638556,34
1018,"dataset , obtaining significant results that outperform the current state-of-the-art models in the task of generating questions to find an undisclosed object in an image .",3,0.8334791,28.596009863790723,32
1019,A widespread approach to processing spoken language is to first automatically transcribe it into text .,0,0.94528794,48.86712319855699,16
1019,"An alternative is to use an end-to-end approach : recent works have proposed to learn semantic embeddings of spoken language from images with spoken captions , without an intermediate transcription step .",0,0.8132515,30.559550564704672,33
1019,We propose to use multitask learning to exploit existing transcribed speech within the end-to-end setting .,2,0.4670904,30.07057288611559,18
1019,"We describe a three-task architecture which combines the objectives of matching spoken captions with corresponding images , speech with text , and text with images .",1,0.47531602,58.47820147071007,28
1019,We show that the addition of the speech / text task leads to substantial performance improvements on image retrieval when compared to training the speech / image task in isolation .,3,0.9664307,38.0362371397118,31
1019,"We conjecture that this is due to a strong inductive bias transcribed speech provides to the model , and offer supporting evidence for this .",3,0.6753452,64.08162622694275,25
1020,"This paper presents a new model for visual dialog , Recurrent Dual Attention Network ( ReDAN ) , using multi-step reasoning to answer a series of questions about an image .",1,0.78443795,37.50585059966098,31
1020,"In each question-answering turn of a dialog , ReDAN infers the answer progressively through multiple reasoning steps .",0,0.3830126,73.09235132028938,20
1020,"In each step of the reasoning process , the semantic representation of the question is updated based on the image and the previous dialog history , and the recurrently-refined representation is used for further reasoning in the subsequent step .",2,0.4957646,21.59451683307703,42
1020,"On the VisDial v1.0 dataset , the proposed ReDAN model achieves a new state-of-the-art of 64.47 % NDCG score .",3,0.9085217,32.92159100483577,25
1020,"Visualization on the reasoning process further demonstrates that ReDAN can locate context-relevant visual and textual clues via iterative refinement , which can lead to the correct answer step-by-step .",3,0.948221,67.48224130159576,29
1021,"Recent advances in sequence modeling have highlighted the strengths of the transformer architecture , especially in achieving state-of-the-art machine translation results .",0,0.90594816,15.506940661034447,28
1021,"However , depending on the up-stream systems , e.g. , speech recognition , or word segmentation , the input to translation system can vary greatly .",0,0.83389014,67.93359302847367,26
1021,The goal of this work is to extend the attention mechanism of the transformer to naturally consume the lattice in addition to the traditional sequential input .,1,0.89678687,30.18703504299402,27
1021,We first propose a general lattice transformer for speech translation where the input is the output of the automatic speech recognition ( ASR ) which contains multiple paths and posterior scores .,2,0.600167,60.50462351080207,32
1021,"To leverage the extra information from the lattice structure , we develop a novel controllable lattice attention mechanism to obtain latent representations .",2,0.7296912,28.414491269846437,23
1021,"On the LDC Spanish-English speech translation corpus , our experiments show that lattice transformer generalizes significantly better and outperforms both a transformer baseline and a lattice LSTM .",3,0.9373875,40.07998011943456,30
1021,"Additionally , we validate our approach on the WMT 2017 Chinese-English translation task with lattice inputs from different BPE segmentations .",3,0.57239574,42.59808752962511,23
1021,"In this task , we also observe the improvements over strong baselines .",3,0.5301107,49.917209343828645,13
1022,"An image caption should fluently present the essential information in a given image , including informative , fine-grained entity mentions and the manner in which these entities interact .",0,0.6520374,51.81583161050651,31
1022,"However , current captioning models are usually trained to generate captions that only contain common object names , thus falling short on an important “ informativeness ” dimension .",0,0.91176945,47.44382705703576,29
1022,We present a mechanism for integrating image information together with fine-grained labels ( assumed to be generated by some upstream models ) into a caption that describes the image in a fluent and informative manner .,2,0.4310935,46.58049976081928,37
1022,"We introduce a multimodal , multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels .",2,0.6681423,41.347168254533976,22
1022,"We demonstrate that we can learn to control the appearance of these entity labels in the output , resulting in captions that are both fluent and informative .",3,0.8963163,29.024283151787714,28
1023,"In this work , we propose a goal-driven collaborative task that combines language , perception , and action .",1,0.83715576,64.249294819199,20
1023,"Specifically , we develop a Collaborative image-Drawing game between two agents , called CoDraw .",2,0.7403896,123.30612517046804,17
1023,Our game is grounded in a virtual world that contains movable clip art objects .,2,0.5726041,57.30864148721529,15
1023,The game involves two players : a Teller and a Drawer .,2,0.7442775,53.114325753811386,12
1023,"The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration , while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces .",0,0.53708094,95.10243071642792,35
1023,The two players communicate with each other using natural language .,2,0.68143505,26.885948879077635,11
1023,We collect the CoDraw dataset of ~10 K dialogs consisting of ~138 K messages exchanged between human players .,2,0.8727617,168.03458587310277,19
1023,"We define protocols and metrics to evaluate learned agents in this testbed , highlighting the need for a novel “ crosstalk ” evaluation condition which pairs agents trained independently on disjoint subsets of the training data .",3,0.52417713,58.43708617668967,37
1023,We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans .,2,0.6111747,66.49776580763334,25
1024,Image Captioning aims at generating a short description for an image .,0,0.93575674,33.404666387252114,12
1024,Existing research usually employs the architecture of CNN-RNN that views the generation as a sequential decision-making process and the entire dataset vocabulary is used as decoding space .,0,0.81251484,64.75728414131129,32
1024,They suffer from generating high frequent n-gram with irrelevant words .,0,0.76179093,164.62467944698656,11
1024,"To tackle this problem , we propose to construct an image-grounded vocabulary , based on which , captions are generated with limitation and guidance .",2,0.34830552,62.72191923657812,25
1024,"In specific , a novel hierarchical structure is proposed to construct the vocabulary incorporating both visual information and relations among words .",2,0.51214933,55.388387762063296,22
1024,"For generation , we propose a word-aware RNN cell incorporating vocabulary information into the decoding process directly .",2,0.66589063,73.72325173689923,19
1024,Reinforce algorithm is employed to train the generator using constraint vocabulary as action space .,2,0.7332424,130.8831278820738,15
1024,Experimental results on MS COCO and Flickr30 k show the effectiveness of our framework compared to some state-of-the-art models .,3,0.94838697,22.870764772883767,26
1025,"Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases , for example in the presence of ambiguous words where the textual context is not sufficient .",0,0.9151531,24.565741114652276,35
1025,"As a consequence , models tend to learn to ignore this information .",0,0.8918316,47.140004639420894,13
1025,We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder .,2,0.41594425,55.17434313075924,23
1025,This approach is trained jointly to generate a good first draft translation and to improve over this draft by ( i ) making better use of the target language textual context ( both left and right-side contexts ) and ( ii ) making use of visual context .,2,0.5964051,49.83383979757112,48
1025,This approach leads to the state of the art results .,3,0.7455304,15.10838332488775,11
1025,"Additionally , we show that it has the ability to recover from erroneous or missing words in the source language .",3,0.92447644,23.400511546164264,21
1026,We address the task of evaluating image description generation systems .,1,0.5322269,75.34163686990804,11
1026,We propose a novel image-aware metric for this task : VIFIDEL .,2,0.41993096,78.52598164178103,12
1026,"It estimates the faithfulness of a generated caption with respect to the content of the actual image , based on the semantic similarity between labels of objects depicted in images and words in the description .",0,0.39406466,28.846812792426462,36
1026,The metric is also able to take into account the relative importance of objects mentioned in human reference descriptions during evaluation .,3,0.66802704,34.434071540184384,22
1026,"Even if these human reference descriptions are not available , VIFIDEL can still reliably evaluate system descriptions .",3,0.590165,245.75741713157333,18
1026,The metric achieves high correlation with human judgments on two well-known datasets and is competitive with metrics that depend on and rely exclusively on human references .,3,0.8031931,22.03947027956551,28
1027,"Vision-and-Language Navigation ( VLN ) requires grounding instructions , such as “ turn right and stop at the door ” , to routes in a visual environment .",0,0.95008516,63.228625001711,30
1027,"The actual grounding can connect language to the environment through multiple modalities , e.g .",0,0.6428446,65.3728553634647,15
1027,"“ stop at the door ” might ground into visual objects , while “ turn right ” might rely only on the geometric structure of a route .",0,0.6780554,93.5431719084766,28
1027,We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models .,1,0.41363904,40.45160746904853,20
1027,"Surprisingly , we discover that visual features may actually hurt these models : models which only use route structure , ablating visual features , outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset .",3,0.94087464,135.93192347010182,40
1027,"To better use all the available modalities , we propose to decompose the grounding procedure into a set of expert models with access to different modalities ( including object detections ) and ensemble them at prediction time , improving the performance of state-of-the-art models on the VLN task .",2,0.52012306,33.244015925767364,55
1028,"Human language is often multimodal , which comprehends a mixture of natural language , facial gestures , and acoustic behaviors .",0,0.9462395,67.98339972071739,21
1028,"However , two major challenges in modeling such multimodal human language time-series data exist : 1 ) inherent data non-alignment due to variable sampling rates for the sequences from each modality ;",0,0.8150697,77.90946349709655,33
1028,and 2 ) long-range dependencies between elements across modalities .,0,0.34052894,88.60654469823506,11
1028,"In this paper , we introduce the Multimodal Transformer ( MulT ) to generically address the above issues in an end-to-end manner without explicitly aligning the data .",1,0.69582653,18.72915437900659,30
1028,"At the heart of our model is the directional pairwise crossmodal attention , which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another .",2,0.49000803,49.450097100808534,34
1028,Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin .,3,0.90021783,7.754646096286571,28
1028,"In addition , empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT .",3,0.97027403,62.81299577260152,24
1029,Chest X-Ray ( CXR ) images are commonly used for clinical screening and diagnosis .,0,0.9417679,29.87885361849834,15
1029,Automatically writing reports for these images can considerably lighten the workload of radiologists for summarizing descriptive findings and conclusive impressions .,0,0.6437274,192.33556501363208,21
1029,The complex structures between and within sections of the reports pose a great challenge to the automatic report generation .,0,0.9123374,56.78679291578453,20
1029,"Specifically , the section Impression is a diagnostic summarization over the section Findings ;",3,0.33546835,233.15935459144745,14
1029,and the appearance of normality dominates each section over that of abnormality .,3,0.6308768,137.55221645779264,13
1029,Existing studies rarely explore and consider this fundamental structure information .,0,0.9059004,236.3583764275162,11
1029,"In this work , we propose a novel framework which exploits the structure information between and within report sections for generating CXR imaging reports .",1,0.8688864,60.01519717900489,25
1029,"First , we propose a two-stage strategy that explicitly models the relationship between Findings and Impression .",2,0.61692125,29.550763779213955,18
1029,"Second , we design a novel co-operative multi-agent system that implicitly captures the imbalanced distribution between abnormality and normality .",2,0.8479611,33.69966054161657,20
1029,Experiments on two CXR report datasets show that our method achieves state-of-the-art performance in terms of various evaluation metrics .,3,0.8924853,13.741735956347695,26
1029,Our results expose that the proposed approach is able to generate high-quality medical reports through integrating the structure information .,3,0.9901974,41.97998131025269,20
1030,We introduce the first dataset for human edits of machine-generated visual stories and explore how these collected edits may be used for the visual story post-editing task .,1,0.3737256,36.02624098268145,30
1030,"The dataset , VIST-Edit , includes 14,905 human-edited versions of 2,981 machine-generated visual stories .",2,0.73320854,124.7016889269013,19
1030,"The stories were generated by two state-of-the-art visual storytelling models , each aligned to 5 human-edited versions .",2,0.9008795,34.47562958363106,24
1030,"We establish baselines for the task , showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models .",3,0.6113412,30.943338907909624,29
1030,"We also discuss the weak correlation between automatic evaluation scores and human ratings , motivating the need for new automatic metrics .",3,0.7521021,43.15132176539097,22
1031,"In this paper , we study abstractive summarization for open-domain videos .",1,0.9009097,25.005396306209914,12
1031,"Unlike the traditional text news summarization , the goal is less to “ compress ” text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities , in our case video and audio transcripts ( or text ) .",0,0.78351593,82.10266106901588,50
1031,"We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output , compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos .",3,0.5705889,33.04658157302947,42
1031,"We also propose a new evaluation metric ( Content F1 ) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries , which is covered by metrics like ROUGE and BLEU .",2,0.50663483,26.262385160484865,36
1032,"In this work , we propose a novel approach that predicts the relationships between various entities in an image in a weakly supervised manner by relying on image captions and object bounding box annotations as the sole source of supervision .",1,0.7379077,22.19854192158612,41
1032,"Our proposed approach uses a top-down attention mechanism to align entities in captions to objects in the image , and then leverage the syntactic structure of the captions to align the relations .",2,0.7261844,18.870640250716647,33
1032,"We use these alignments to train a relation classification network , thereby obtaining both grounded captions and dense relationships .",2,0.72966176,121.41649722461212,20
1032,We demonstrate the effectiveness of our model on the Visual Genome dataset by achieving a recall@50 of 15 % and recall@100 of 25 % on the relationships present in the image .,3,0.8724852,41.60050142849479,32
1032,We also show that the model successfully predicts relations that are not present in the corresponding captions .,3,0.9623571,20.35085723214059,18
1033,"Speech directed to children differs from adult-directed speech in linguistic aspects such as repetition , word choice , and sentence length , as well as in aspects of the speech signal itself , such as prosodic and phonemic variation .",0,0.82948744,39.285660333122166,40
1033,Human language acquisition research indicates that child-directed speech helps language learners .,0,0.9274905,105.23291064815008,14
1033,This study explores the effect of child-directed speech when learning to extract semantic information from speech directly .,1,0.94029987,40.96281091319409,20
1033,We compare the task performance of models trained on adult-directed speech ( ADS ) and child-directed speech ( CDS ) .,2,0.72167337,28.377411314953104,22
1033,"We find indications that CDS helps in the initial stages of learning , but eventually , models trained on ADS reach comparable task performance , and generalize better .",3,0.9818671,155.04804355528185,29
1033,"The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers , as we see the same pattern when looking at models trained on acoustically comparable synthetic speech .",3,0.98818994,41.09911096513549,38
1034,"Accurately diagnosing depression is difficult– requiring time-intensive interviews , assessments , and analysis .",0,0.92365235,152.81670210119694,16
1034,"Hence , automated methods that can assess linguistic patterns in these interviews could help psychiatric professionals make faster , more informed decisions about diagnosis .",3,0.61555207,115.83709699579222,25
1034,"We propose JLPC , a model that analyzes interview transcripts to identify depression while jointly categorizing interview prompts into latent categories .",1,0.5529259,130.4862419536759,22
1034,This latent categorization allows the model to define high-level conversational contexts that influence patterns of language in depressed individuals .,2,0.49312472,73.01289450626841,21
1034,"We show that the proposed model not only outperforms competitive baselines , but that its latent prompt categories provide psycholinguistic insights about depression .",3,0.94491196,41.021470177178735,24
1035,"As an essential task in task-oriented dialog systems , slot filling requires extensive training data in a certain domain .",0,0.7889889,43.86527140749948,22
1035,"However , such data are not always available .",0,0.9268616,30.40662322107288,9
1035,"Hence , cross-domain slot filling has naturally arisen to cope with this data scarcity problem .",0,0.87758446,90.20576288673308,16
1035,"In this paper , we propose a Coarse-to-fine approach ( Coach ) for cross-domain slot filling .",1,0.8723752,39.80374146824329,20
1035,Our model first learns the general pattern of slot entities by detecting whether the tokens are slot entities or not .,2,0.72366756,32.033923302444535,21
1035,It then predicts the specific types for the slot entities .,2,0.51374334,110.71757755488933,11
1035,"In addition , we propose a template regularization approach to improve the adaptation robustness by regularizing the representation of utterances based on utterance templates .",2,0.61006045,28.65420454998523,25
1035,Experimental results show that our model significantly outperforms state-of-the-art approaches in slot filling .,3,0.97228074,5.09932810321183,20
1035,"Furthermore , our model can also be applied to the cross-domain named entity recognition task , and it achieves better adaptation performance than other existing baselines .",3,0.9290606,20.35217217270819,27
1035,The code is available at https://github.com/zliucr/coach .,3,0.5618278,12.207798354216377,7
1036,Automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation .,0,0.9400371,24.132826462879002,17
1036,"However , existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust .",0,0.8769002,50.12249740768876,18
1036,"In this work , we propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained ( masked ) language models .",1,0.69914985,31.444304469745937,27
1036,Experimental results demonstrate that the proposed evaluator achieves a strong correlation ( > 0.6 ) with human judgement and generalizes robustly to diverse responses and corpora .,3,0.9623825,26.25457830469202,27
1036,We open-source the code and data in https://github.com/ZHAOTING/dialog-processing .,3,0.49499857,18.33030582442988,9
1037,Recent proposed approaches have made promising progress in dialogue state tracking ( DST ) .,0,0.92642426,46.195842058479066,15
1037,"However , in multi-domain scenarios , ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains .",0,0.7878815,71.18764993343311,27
1037,"To handle these phenomena , we propose a Dialogue State Tracking with Slot Connections ( DST-SC ) model to explicitly consider slot correlations across different domains .",2,0.5171265,66.05778064196036,29
1037,"Given a target slot , the slot connecting mechanism in DST-SC can infer its source slot and copy the source slot value directly , thus significantly reducing the difficulty of learning and reasoning .",3,0.7061789,71.31254087297448,36
1037,"Experimental results verify the benefits of explicit slot connection modeling , and our model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets .",3,0.9362035,9.257228810825824,30
1038,Knowledge-driven conversation approaches have achieved remarkable research attention recently .,0,0.94648933,65.88894269467627,10
1038,"However , generating an informative response with multiple relevant knowledge without losing fluency and coherence is still one of the main challenges .",0,0.8936678,40.72163211382593,23
1038,"To address this issue , this paper proposes a method that uses recurrent knowledge interaction among response decoding steps to incorporate appropriate knowledge .",1,0.8048731,92.54011720958931,24
1038,"Furthermore , we introduce a knowledge copy mechanism using a knowledge-aware pointer network to copy words from external knowledge according to knowledge attention distribution .",2,0.755595,49.79450442048537,27
1038,Our joint neural conversation model which integrates recurrent Knowledge-Interaction and knowledge Copy ( KIC ) performs well on generating informative responses .,3,0.6970648,196.87573566358807,22
1038,Experiments demonstrate that our model with fewer parameters yields significant improvements over competitive baselines on two datasets Wizard-of-Wikipedia ( average Bleu + 87 % ; abs. : 0.034 ) and DuConv( average Bleu + 20 % ;,3,0.9122075,87.20883004467946,37
1038,abs. : 0.047 ) ) with different knowledge formats ( textual & structured ) and different languages ( English & Chinese ) .,2,0.5355209,102.39755729896645,23
1039,Leveraging persona information of users in Neural Response Generators ( NRG ) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years .,0,0.94677305,28.697584456621698,37
1039,"Despite of the promising progress achieved by recent studies in this field , persona information tends to be incorporated into neural networks in the form of user embeddings , with the expectation that the persona can be involved via End-to-End learning .",0,0.8998059,34.791957818147,44
1039,"This paper proposes to adopt the personality-related characteristics of human conversations into variational response generators , by designing a specific conditional variational autoencoder based deep model with two new regularization terms employed to the loss function , so as to guide the optimization towards the direction of generating both persona-aware and relevant responses .",1,0.68251765,62.40021767388942,56
1039,"Besides , to reasonably evaluate the performances of various persona modeling approaches , this paper further presents three direct persona-oriented metrics from different perspectives .",2,0.3830111,95.9717629533309,25
1039,"The experimental results have shown that our proposed methodology can notably improve the performance of persona-aware response generation , and the metrics are reasonable to evaluate the results .",3,0.9760911,33.109736226579535,30
1040,Non-goal oriented dialog agents ( i.e .,0,0.4072709,52.26255930716048,8
1040,chatbots ) aim to produce varying and engaging conversations with a user ;,0,0.91564226,425.8420766155907,13
1040,"however , they typically exhibit either inconsistent personality across conversations or the average personality of all users .",0,0.8999084,110.56236575182419,18
1040,This paper addresses these issues by controlling an agent ’s persona upon generation via conditioning on prior conversations of a target actor .,1,0.84753895,104.80805229280271,23
1040,"In doing so , we are able to utilize more abstract patterns within a person ’s speech and better emulate them in generated responses .",3,0.544744,73.91279920353219,25
1040,"This work introduces the Generative Conversation Control model , an augmented and fine-tuned GPT-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor ’s persona .",1,0.45097196,61.602312207444854,36
1040,We introduce an accompanying data collection procedure to obtain 10.3 M conversations from 6 months worth of Reddit comments .,2,0.81809014,65.44867893616488,20
1040,We demonstrate that scaling model sizes from 117 M to 8.3B parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7 M held out Reddit conversations .,3,0.9248215,148.0087713343734,28
1040,"Increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism ( 31 % increased to 37 % preference ) , style matching ( 37 % to 42 % ) , grammar and content quality ( 29 % to 42 % ) , and conversation coherency ( 32 % to 40 % ) .",3,0.9596023,82.74482469032115,68
1040,We find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations .,3,0.9745233,111.49372117442057,15
1040,Through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control .,3,0.8190903,214.87021997216544,22
1041,Pre-training models have been proved effective for a wide range of natural language processing tasks .,0,0.8683447,5.905058095893565,16
1041,"Inspired by this , we propose a novel dialogue generation pre-training framework to support various kinds of conversations , including chit-chat , knowledge grounded dialogues , and conversational question answering .",2,0.41323158,37.25698987279758,31
1041,"In this framework , we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation .",2,0.63929445,37.717973378515616,23
1041,We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation .,2,0.65749854,35.783170802262006,21
1041,Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network .,2,0.449813,61.34778269576637,21
1041,Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework .,3,0.76786023,13.30456049283762,17
1042,Data-driven approaches using neural networks have achieved promising performances in natural language generation ( NLG ) .,0,0.94194436,18.06785733007065,17
1042,"However , neural generators are prone to make mistakes , e.g. , neglecting an input slot value and generating a redundant slot value .",0,0.84776497,48.95231834168335,24
1042,Prior works refer this to hallucination phenomenon .,0,0.8697835,408.96045700862464,8
1042,"In this paper , we study slot consistency for building reliable NLG systems with all slot values of input dialogue act ( DA ) properly generated in output sentences .",1,0.90242463,199.9680676309678,30
1042,We propose Iterative Rectification Network ( IRN ) for improving general NLG systems to produce both correct and fluent responses .,1,0.47501048,86.02348680298395,21
1042,It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training .,2,0.6516447,79.02037076659967,24
1042,"Comprehensive studies have been conducted on multiple benchmark datasets , showing that the proposed methods have significantly reduced the slot error rate ( ERR ) for all strong baselines .",3,0.7876764,27.506048284867553,30
1042,Human evaluations also have confirmed its effectiveness .,0,0.5142266,146.8673923098122,8
1043,"We introduce Span-ConveRT , a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task .",2,0.6673464,71.4252526539763,24
1043,"This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as ConveRT ( Henderson et al. , 2019 ) .",2,0.40567645,73.29149418336259,27
1043,"We show that leveraging such knowledge in Span-ConveRT is especially useful for few-shot learning scenarios : we report consistent gains over 1 ) a span extractor that trains representations from scratch in the target domain , and 2 ) a BERT-based span extractor .",3,0.9085049,56.555369316432426,46
1043,"In order to inspire more work on span extraction for the slot-filling task , we also release RESTAURANTS-8K , a new challenging data set of 8,198 utterances , compiled from actual conversations in the restaurant booking domain .",2,0.7407202,66.31853706837911,41
1044,Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition .,3,0.614124,26.124570558599814,24
1044,This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain .,1,0.75404274,50.23612839635261,32
1044,We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset .,3,0.95461506,19.597843238540666,33
1044,We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset .,3,0.95973575,57.84151861117338,27
1044,We improve the zero-shot learning state of the art on average across domains by 21 % .,3,0.8746425,33.98696724148027,17
1045,"This work proposes a standalone , complete Chinese discourse parser for practical applications .",1,0.73033303,386.9595404171274,14
1045,"We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder , but also by employing novel training strategies .",2,0.63951796,40.73767419998501,33
1045,"We revise the dynamic-oracle procedure for training the shift-reduce parser , and apply unsupervised data augmentation to enhance rhetorical relation recognition .",2,0.5467406,69.43031193465232,22
1045,Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance .,3,0.9764382,8.945289329659273,18
1046,Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues .,0,0.93771833,52.98189222949839,19
1046,"Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task , which have not fully exploited the annotated relation signal .",0,0.7656635,136.7797102631628,27
1046,"Therefore , we propose a novel TransS-driven joint learning architecture to address the issues .",1,0.6449532,63.27154266616374,17
1046,"Specifically , based on the multi-level encoder , we 1 ) translate discourse relations in low-dimensional embedding space ( called TransS ) , which could mine the latent geometric structure information of argument-relation instances ;",2,0.84608644,159.4687400148915,35
1046,2 ) further exploit the semantic features of arguments to assist discourse understanding ;,2,0.44366175,383.9403249905261,14
1046,"3 ) jointly learn 1 ) and 2 ) to mutually reinforce each other to obtain the better argument representations , so as to improve the performance of the task .",2,0.66744345,53.35741056011713,31
1046,Extensive experimental results on the Penn Discourse TreeBank ( PDTB ) show that our model achieves competitive results against several state-of-the-art systems .,3,0.7751344,12.486698093436894,29
1047,"Non-autoregressive ( NAR ) models generate all the tokens of a sequence in parallel , resulting in faster generation speed compared to their autoregressive ( AR ) counterparts but at the cost of lower accuracy .",0,0.7473126,22.820534151911215,36
1047,"Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation ( NMT ) , automatic speech recognition ( ASR ) , and text to speech ( TTS ) .",0,0.8944321,17.09407249586318,47
1047,"With the help of those techniques , NAR models can catch up with the accuracy of AR models in some tasks but not in some others .",3,0.69785446,29.79768295676151,27
1047,( 2 ) Why techniques like knowledge distillation and source-target alignment can help NAR models .,1,0.52993894,106.2251819834829,16
1047,"Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do , intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens .",3,0.5213862,27.12674608689346,43
1047,"To quantify such dependency , we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks .",2,0.4736584,87.81514953182081,23
1047,"We have several interesting findings : 1 ) Among the NMT , ASR and TTS tasks , ASR has the most target-token dependency while TTS has the least .",3,0.9814935,47.61082902923302,31
1047,2 ) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models .,3,0.70364195,47.97212016587774,22
1047,3 ) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models .,3,0.5746848,88.74278170377104,23
1048,Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations .,0,0.86185247,25.89355837691521,32
1048,We investigate potential solutions for combining existing language-generation annotations in English with translation capabilities in order to create solutions at web-scale in both domain and language coverage .,1,0.55544376,79.6527820199842,30
1048,"We describe an approach called Pivot-Language Generation Stabilization ( PLuGS ) , which leverages directly at training time both existing English annotations ( gold data ) as well as their machine-translated versions ( silver data ) ;",2,0.55444175,103.368909345848,39
1048,"at run-time , it generates first an English caption and then a corresponding target-language caption .",2,0.49720582,53.278519942750755,18
1048,"We show that PLuGS models outperform other candidate solutions in evaluations performed over 5 different target languages , under a large-domain testset using images from the Open Images dataset .",3,0.8922591,123.27285059155479,30
1048,"Furthermore , we find an interesting effect where the English captions generated by the PLuGS models are better than the captions generated by the original , monolingual English model .",3,0.9757823,40.50807631739827,30
1049,"We propose a novel text editing task , referred to as fact-based text editing , in which the goal is to revise a given document to better describe the facts in a knowledge base ( e.g. , several triples ) .",1,0.32747647,33.93802722717412,43
1049,The task is important in practice because reflecting the truth is a common requirement in text editing .,0,0.85665435,53.8578362911372,18
1049,"First , we propose a method for automatically generating a dataset for research on fact-based text editing , where each instance consists of a draft text , a revised text , and several facts represented in triples .",2,0.730811,43.07882093412687,40
1049,"We apply the method into two public table-to-text datasets , obtaining two new datasets consisting of 233 k and 37 k instances , respectively .",2,0.7478498,60.270893754969265,29
1049,"Next , we propose a new neural network architecture for fact-based text editing , called FactEditor , which edits a draft text by referring to given facts using a buffer , a stream , and a memory .",2,0.6091077,61.67049822615954,40
1049,A straightforward approach to address the problem would be to employ an encoder-decoder model .,0,0.4835253,10.884918592513667,15
1049,Our experimental results on the two datasets show that FactEditor outperforms the encoder-decoder approach in terms of fidelity and fluency .,3,0.9693462,15.790435692929814,21
1049,The results also show that FactEditor conducts inference faster than the encoder-decoder approach .,3,0.98699653,52.74984917371055,14
1050,"Neural-based end-to-end approaches to natural language generation ( NLG ) from structured data or knowledge are data-hungry , making their adoption for real-world applications difficult with limited data .",0,0.9587274,28.650706935763147,32
1050,"In this work , we propose the new task of few-shot natural language generation .",1,0.84010166,23.757333552175414,15
1050,"Motivated by how humans tend to summarize tabular data , we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains .",3,0.28088558,20.954384490343582,34
1050,"The design of the model architecture is based on two aspects : content selection from input data and language modeling to compose coherent sentences , which can be acquired from prior knowledge .",2,0.6149404,52.811900580581096,33
1050,"With just 200 training examples , across multiple domains , we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement .",3,0.8568476,26.159742106950574,34
1050,Our code and data can be found at https://github.com/czyssrs/Few-Shot-NLG .,3,0.8192529,13.880176800837852,10
1051,"Question answering ( QA ) is an important aspect of open-domain conversational agents , garnering specific research focus in the conversational QA ( ConvQA ) subtask .",0,0.94823027,34.429063973662096,27
1051,"One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus , thus ignoring the natural language generation ( NLG ) aspect of high-quality conversational agents .",0,0.88237965,72.71562195345602,34
1051,"In this work , we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness .",1,0.892977,58.40209824752422,27
1051,"From a technical perspective , we use data augmentation to generate training data for an end-to-end system .",2,0.6214175,11.252060866972169,20
1051,"Specifically , we develop Syntactic Transformations ( STs ) to produce question-specific candidate answer responses and rank them using a BERT-based classifier ( Devlin et al. , 2019 ) .",2,0.77928877,36.21918245441746,33
1051,"Human evaluation on SQuAD 2.0 data ( Rajpurkar et al. , 2018 ) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses .",3,0.8697783,45.49616993758608,29
1051,We further show our model ’s scalability by conducting tests on the CoQA dataset .,3,0.72652256,48.93919006744524,15
1051,The code and data are available at https://github.com/abaheti95/QADialogSystem .,3,0.57656264,25.013278983310254,9
1052,"One of the most crucial challenges in question answering ( QA ) is the scarcity of labeled data , since it is costly to obtain question-answer ( QA ) pairs for a target text domain with human annotation .",0,0.9602573,25.951870962214848,41
1052,An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts ( e.g .,0,0.6961762,27.716480673215205,29
1052,Wikipedia ) .,4,0.5256728,186.68541984320146,3
1052,"In this work , we propose a hierarchical conditional variational autoencoder ( HCVAE ) for generating QA pairs given unstructured texts as contexts , while maximizing the mutual information between generated QA pairs to ensure their consistency .",1,0.69494987,28.910507836530538,38
1052,"We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder ( Info-HCVAE ) on several benchmark datasets by evaluating the performance of the QA model ( BERT-base ) using only the generated QA pairs ( QA-based evaluation ) or by using both the generated and human-labeled pairs ( semi-supervised learning ) for training , against state-of-the-art baseline models .",2,0.68913686,28.395819992977597,70
1052,"The results show that our model obtains impressive performance gains over all baselines on both tasks , using only a fraction of data for training .",3,0.98249996,18.311776445093276,26
1053,Traditional Question Generation ( TQG ) aims to generate a question given an input passage and an answer .,0,0.9163521,23.245620147877297,19
1053,"When there is a sequence of answers , we can perform Sequential Question Generation ( SQG ) to produce a series of interconnected questions .",0,0.73212975,29.529550498183745,25
1053,"Since the frequently occurred information omission and coreference between questions , SQG is rather challenging .",0,0.7010468,408.5004980618307,16
1053,Prior works regarded SQG as a dialog generation task and recurrently produced each question .,0,0.76617795,222.63322453093798,15
1053,"However , they suffered from problems caused by error cascades and could only capture limited context dependencies .",0,0.59752876,124.98046248007495,18
1053,"To this end , we generate questions in a semi-autoregressive way .",2,0.8073281,23.00070777253146,12
1053,Our model divides questions into different groups and generates each group of them in parallel .,2,0.80973303,60.745863530418575,16
1053,"During this process , it builds two graphs focusing on information from passages , answers respectively and performs dual-graph interaction to get information for generation .",2,0.6607833,235.65906749556112,27
1053,"Besides , we design an answer-aware attention mechanism and the coarse-to-fine generation scenario .",2,0.7336708,44.12350130406465,16
1053,Experiments on our new dataset containing 81.9 K questions show that our model substantially outperforms prior works .,3,0.9379077,26.4657006821872,18
1054,"Paraphrasing natural language sentences is a multifaceted process : it might involve replacing individual words or short phrases , local rearrangement of content , or high-level restructuring like topicalization or passivization .",0,0.8743098,69.16444373905193,34
1054,Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner .,0,0.9430181,66.95271639119011,15
1054,"Our work , inspired by pre-ordering literature in machine translation , uses syntactic transformations to softly “ reorder ” the source sentence and guide our neural paraphrasing model .",2,0.5948057,82.66642371194693,29
1054,"First , given an input sentence , we derive a set of feasible syntactic rearrangements using an encoder-decoder model .",2,0.8923993,18.783997920631816,20
1054,"This model operates over a partially lexical , partially syntactic view of the sentence and can reorder big chunks .",0,0.38338506,88.28947115684231,20
1054,"Next , we use each proposed rearrangement to produce a sequence of position embeddings , which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order .",2,0.75467134,29.747781409746867,33
1054,"Our evaluation , both automatic and human , shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases .",3,0.94864845,28.772389990100937,33
1055,Conditional Text Generation has drawn much attention as a topic of Natural Language Generation ( NLG ) which provides the possibility for humans to control the properties of generated contents .,0,0.9629262,38.466139703214736,31
1055,Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion .,0,0.79853237,99.08686173251844,18
1055,"When a new condition added , these techniques require full retraining .",0,0.4711926,256.8751536132373,12
1055,"In this paper , we present a new framework named Pre-train and Plug-in Variational Auto-Encoder ( PPVAE ) towards flexible conditional text generation .",1,0.8723832,31.252282661122226,24
1055,PPVAE decouples the text generation module from the condition representation module to allow “ one-to-many ” conditional generation .,3,0.3585508,54.24701220883288,21
1055,"When a fresh condition emerges , only a lightweight network needs to be trained and works as a plug-in for PPVAE , which is efficient and desirable for real-world applications .",3,0.668584,67.11384663325111,31
1055,Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort .,3,0.93152696,53.466595386495904,21
1056,Masked language model and autoregressive language model are two types of language models .,0,0.69341135,16.752775367723977,14
1056,"While pretrained masked language models such as BERT overwhelm the line of natural language understanding ( NLU ) tasks , autoregressive language models such as GPT are especially capable in natural language generation ( NLG ) .",0,0.8735576,26.39966161547196,37
1056,"In this paper , we propose a probabilistic masking scheme for the masked language model , which we call probabilistically masked language model ( PMLM ) .",1,0.8257807,15.914311969113855,27
1056,We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM .,2,0.79638004,218.32013314620554,17
1056,We prove that u-PMLM is equivalent to an autoregressive permutated language model .,3,0.5214624,57.768998538540146,13
1056,"One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality , which could potentially enable new applications over traditional unidirectional generation .",3,0.8420417,39.22949256938919,31
1056,"Besides , the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks .",3,0.93629616,51.618054159731976,16
1057,Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways .,0,0.9365803,21.154097661945748,27
1057,It is therefore desirable to develop a deeper understanding of the fundamental properties of such models .,0,0.86866426,13.748122919951888,17
1057,The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area .,0,0.9267245,58.8382075075248,22
1057,"To this end , the extent and degree to which these artifacts surface in generated text is still unclear .",0,0.9284906,43.671115316220146,20
1057,"In the spirit of better understanding generative text models and their artifacts , we propose the new task of distinguishing which of several variants of a given model generated some piece of text .",1,0.4667066,37.70900875639542,34
1057,"Specifically , we conduct an extensive suite of diagnostic tests to observe whether modeling choices ( e.g. , sampling methods , top-k probabilities , model architectures , etc. ) leave detectable artifacts in the text they generate .",2,0.8697531,60.869035316168066,38
1057,"Our key finding , which is backed by a rigorous set of experiments , is that such artifacts are present and that different modeling choices can be inferred by looking at generated text alone .",3,0.93517596,54.19156879874458,35
1057,This suggests that neural text generators may actually be more sensitive to various modeling choices than previously thought .,3,0.9728002,35.23847612729061,19
1058,"While online reviews of products and services become an important information source , it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need .",0,0.9333165,64.61801687349788,29
1058,"We propose to explore question generation as a new way of review information exploitation , namely generating questions that can be answered by the corresponding review sentences .",1,0.55557007,65.19843060072024,28
1058,"One major challenge of this generation task is the lack of training data , i.e .",0,0.9065685,16.102419275986428,16
1058,explicit mapping relation between the user-posed questions and review sentences .,3,0.360916,155.34983430460613,11
1058,"To obtain proper training instances for the generation model , we propose an iterative learning framework with adaptive instance transfer and augmentation .",2,0.68639255,43.48430814700412,23
1058,"To generate to the point questions about the major aspects in reviews , related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation .",2,0.41653156,108.82168411218414,29
1058,"Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework , as well as the potentials of the proposed review-based question generation task .",3,0.7831721,22.465272289773445,33
1059,"Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code , e.g. , operator , string , etc .",0,0.7662478,84.01104324849905,31
1059,"However , introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information .",0,0.6763245,26.643515627846092,22
1059,"In order to address the issues above , we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node .",2,0.5672886,31.903854443332335,41
1059,"Specifically , our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code .",2,0.64585847,47.017859784647676,23
1059,We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework .,2,0.5023195,32.30472575333629,18
1059,Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies .,3,0.9018214,13.219880336549172,23
1060,"We propose UPSA , a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing .",1,0.61951,49.46198266941878,15
1060,"We model paraphrase generation as an optimization problem and propose a sophisticated objective function , involving semantic similarity , expression diversity , and language fluency of paraphrases .",2,0.70819855,62.689686544800054,28
1060,UPSA searches the sentence space towards this objective by performing a sequence of local editing .,2,0.4922071,219.51349003822364,16
1060,"We evaluate our approach on various datasets , namely , Quora , Wikianswers , MSCOCO , and Twitter .",2,0.7025621,78.55410720098233,19
1060,Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations .,3,0.97798866,8.352108395760867,28
1060,"Further , our approach outperforms most existing domain-adapted supervised models , showing the generalizability of UPSA .",3,0.95945203,49.76479783196169,17
1061,Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections .,0,0.9211369,54.82161700676668,17
1061,"Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately , we show that the tasks contain complementary information and are best addressed jointly .",3,0.5866399,28.40222522141896,31
1061,"We introduce Segment Pooling LSTM ( S-LSTM ) , which is capable of jointly segmenting a document and labeling segments .",2,0.7218509,37.293249160544924,23
1061,"In support of joint training , we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments .",2,0.6297547,40.50218544020088,27
1061,"We show that S-LSTM reduces segmentation error by 30 % on average , while also improving segment labeling .",3,0.9729934,53.13000540900861,19
1062,Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers .,0,0.93958646,21.852040833108422,21
1062,"Existing methods mainly generate pseudo-labels in a context-free manner ( e.g. , string matching ) , therefore , the ambiguous , context-dependent nature of human language has been long overlooked .",0,0.9263705,79.94185002596964,32
1062,"In this paper , we propose a novel framework ConWea , providing contextualized weak supervision for text classification .",1,0.84974563,51.47113700184795,19
1062,"Specifically , we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word , and thus create a contextualized corpus .",2,0.7972845,41.539865304660246,30
1062,This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner .,2,0.5618827,38.463526044937964,19
1062,"This process not only adds new contextualized , highly label-indicative keywords but also disambiguates initial seed words , making our weak supervision fully contextualized .",3,0.57061625,120.15826830144418,27
1062,"Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision , especially when the class labels are fine-grained .",3,0.79742795,26.225356084724428,28
1063,Text classification is fundamental in natural language processing ( NLP ) and Graph Neural Networks ( GNN ) are recently applied in this task .,0,0.9518334,22.588494879597818,25
1063,"However , the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words .",0,0.8740367,72.94729722412384,27
1063,"Therefore in this work , to overcome such problems , we propose TextING for inductive text classification via GNN .",1,0.7617851,102.05886893729735,20
1063,"We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure , which can also effectively produce embeddings for unseen words in the new document .",2,0.7607786,32.60059872965303,39
1063,"Finally , the word nodes are aggregated as the document embedding .",2,0.6637299,65.1516583888086,12
1063,Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods .,3,0.88244873,4.333142274550509,22
1064,"Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text , since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation ( LDA ) .",0,0.9398264,37.37372304063185,44
1064,"However , these models either typically assume improper prior ( e.g .",0,0.9164257,162.6616734161504,12
1064,Gaussian or Logistic Normal ) over latent topic space or could not infer topic distribution for a given document .,3,0.38695174,195.43745819144888,20
1064,"To address these limitations , we propose a neural topic modeling approach , called Bidirectional Adversarial Topic ( BAT ) model , which represents the first attempt of applying bidirectional adversarial training for neural topic modeling .",2,0.43726823,34.71115846685164,37
1064,The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution .,2,0.54288995,67.82341580798119,18
1064,It uses a generator to capture the semantic patterns from texts and an encoder for topic inference .,2,0.57522625,41.54976045481404,18
1064,"Furthermore , to incorporate word relatedness information , the Bidirectional Adversarial Topic model with Gaussian ( Gaussian-BAT ) is extended from BAT .",2,0.7138103,94.51242686365637,25
1064,"To verify the effectiveness of BAT and Gaussian-BAT , three benchmark corpora are used in our experiments .",2,0.7742653,54.14164202145574,20
1064,"The experimental results show that BAT and Gaussian-BAT obtain more coherent topics , outperforming several competitive baselines .",3,0.976904,95.77792426237019,20
1064,"Moreover , when performing text clustering based on the extracted topics , our models outperform all the baselines , with more significant improvements achieved by Gaussian-BAT where an increase of near 6 % is observed in accuracy .",3,0.9508509,55.83459746253268,40
1065,Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks .,0,0.84473103,5.133637886542048,22
1065,"However , the discrepancy between the semantic similarity of texts and labelling standards affects classifiers , i.e .",0,0.74194944,47.714795479068684,18
1065,leading to lower performance in cases where classifiers should assign different labels to semantically similar texts .,3,0.7194885,51.88900570926359,17
1065,"To address this problem , we propose a simple multitask learning model that uses negative supervision .",2,0.41585198,30.92316811309993,17
1065,"Specifically , our model encourages texts with different labels to have distinct representations .",3,0.5497748,70.7985543060559,14
1065,"Comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single-and multi-label classifications , sentence and document classifications , and classifications in three different languages .",3,0.94251025,18.629281041638894,37
1066,Neural machine translation ( NMT ) encodes the source sentence in a universal way to generate the target sentence word-by-word .,0,0.8324435,19.188655377771607,23
1066,"However , NMT does not consider the importance of word in the sentence meaning , for example , some words ( i.e. , content words ) express more important meaning than others ( i.e. , function words ) .",0,0.48107705,31.125684370844805,39
1066,"To address this limitation , we first utilize word frequency information to distinguish between content and function words in a sentence , and then design a content word-aware NMT to improve translation performance .",2,0.65647846,38.119513939176514,36
1066,"Empirical results on the WMT14 English-to-German , WMT14 English-to-French , and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT .",3,0.9465835,4.8886908749834275,40
1067,"Recently many efforts have been devoted to interpreting the black-box NMT models , but little progress has been made on metrics to evaluate explanation methods .",0,0.89930284,30.020149919807466,28
1067,"Word Alignment Error Rate can be used as such a metric that matches human understanding , however , it can not measure explanation methods on those target words that are not aligned to any source word .",0,0.7396133,84.8438162912111,37
1067,This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint .,1,0.7153557,77.5220994044392,16
1067,"To this end , it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT model .",1,0.5307008,46.50561904232664,23
1067,"As the exact computation for this metric is intractable , we employ an efficient approach as its approximation .",2,0.58979654,59.30064269016407,19
1067,"On six standard translation tasks , we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments .",3,0.6559814,54.626743318266215,32
1068,The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks .,0,0.9184113,12.465786885244249,19
1068,"However , few works have adopted this technique in the sequence-to-sequence models .",0,0.89123124,30.664056578006733,15
1068,"In this work , we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation ~ ( NAT ) .",1,0.7558157,36.39061817282669,27
1068,"Specifically , we first empirically study the functionalities of the encoder and the decoder in NAT models , and find that the encoder takes a more important role than the decoder regarding the translation quality .",3,0.72131723,15.47789725964804,36
1068,"Therefore , we propose to train the encoder more rigorously by masking the encoder input while training .",3,0.34337562,30.825899114541663,18
1068,"As for the decoder , we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words .",2,0.5922567,32.50259409075257,33
1068,The two types of masks are applied to the model jointly at the training stage .,2,0.657539,25.03146870310363,16
1068,"We conduct experiments on five benchmark machine translation tasks , and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German / German-English tasks with 5 + times speed up compared with an autoregressive model .",3,0.64920974,26.966734828716696,39
1069,"The Transformer translation model ( Vaswani et al. , 2017 ) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation ( NMT ) .",0,0.65916085,37.36532129882735,38
1069,"Though intuitively the attentional network can connect distant words via shorter network paths than RNNs , empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies ( Tang et al. , 2018 ) .",0,0.6522781,77.62271057234221,37
1069,"Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation ( SMT ) approach through the use of larger translation blocks ( “ phrases ” ) and its reordering ability , modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships .",0,0.6808545,66.78901111705952,52
1069,"In this paper , we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations .",1,0.8679577,38.23172055811171,25
1069,"In addition , we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships .",2,0.68062997,27.286984651486954,23
1069,"In our experiments , we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline , which shows the effectiveness of our approach .",3,0.9102837,15.049549493997686,36
1069,"Our approach helps Transformer Base models perform at the level of Transformer Big models , and even significantly better for long sentences , but with substantially fewer parameters and training steps .",3,0.89718634,81.98987031911133,32
1069,The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations .,3,0.96753377,58.806007753823714,25
1070,The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder / decoder structure .,2,0.5839622,54.14343631508436,24
1070,"Previous research shows that even with residual connection and layer normalization , deep Transformers still have difficulty in training , and particularly Transformer models with more than 12 encoder / decoder layers fail to converge .",0,0.8523649,101.94977066139772,36
1070,"In this paper , we first empirically demonstrate that a simple modification made in the official implementation , which changes the computation order of residual connection and layer normalization , can significantly ease the optimization of deep Transformers .",1,0.5387027,89.28688991689027,39
1070,"We then compare the subtle differences in computation order in considerable detail , and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence .",3,0.49653098,75.81106128839824,36
1070,"In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization , deep Transformers with the original computation order can converge , and obtain significant BLEU improvements with up to 24 layers .",3,0.97720075,128.55742533481785,37
1070,"In contrast to previous research which focuses on deep encoders , our approach additionally enables Transformers to also benefit from deep decoders .",3,0.793878,35.602897834049585,23
1071,Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set .,0,0.5391772,55.40106660886741,23
1071,"However , they are often unable to extrapolate patterns beyond the seen data , even when the abstractions required for such patterns are simple .",0,0.91335696,57.88914298714693,25
1071,"In this paper , we first review the notion of extrapolation , why it is important and how one could hope to tackle it .",1,0.88742834,33.63763414475997,25
1071,We then focus on a specific type of extrapolation which is especially useful for natural language processing : generalization to sequences that are longer than the training ones .,1,0.3658582,38.91232004609129,29
1071,We hypothesize that models with a separate content-and location-based attention are more likely to extrapolate than those with common attention mechanisms .,1,0.46828985,37.98961713489459,26
1071,We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task .,3,0.95207226,72.34273154723587,21
1071,This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues .,3,0.57394254,147.86655831913964,21
1072,Recent evidence reveals that Neural Machine Translation ( NMT ) models with deeper neural networks can be more effective but are difficult to train .,0,0.9341408,22.94559594869557,25
1072,"In this paper , we present a MultiScale Collaborative ( MSC ) framework to ease the training of NMT models that are substantially deeper than those used previously .",1,0.83774495,32.51449132388082,29
1072,We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models .,2,0.7598105,62.877484634186295,24
1072,"Then , instead of forcing the whole encoder stack directly learns a desired representation , we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration .",2,0.76000404,81.30780356567247,36
1072,We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth .,3,0.9431571,74.84820373840368,25
1072,"On IWSLT translation tasks with three translation directions , our extremely deep models ( with 72-layer encoders ) surpass strong baselines by +2.2~+3.1 BLEU points .",3,0.8841387,28.130548660845818,28
1072,"In addition , our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models .",3,0.918736,12.71454108059442,33
1072,We have included the source code in supplementary materials .,3,0.51870316,28.321932734069,10
1073,"A neural machine translation ( NMT ) system is expensive to train , especially with high-resource settings .",0,0.9413973,27.90354411002838,19
1073,"As the NMT architectures become deeper and wider , this issue gets worse and worse .",0,0.7198698,55.35588506413421,16
1073,"In this paper , we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method .",1,0.92896235,21.225153982113795,24
1073,"We use the norm ( aka length or module ) of a word embedding as a measure of 1 ) the difficulty of the sentence , 2 ) the competence of the model , and 3 ) the weight of the sentence .",2,0.8285073,35.550368010827334,43
1073,The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties .,2,0.34367335,76.9058348128148,19
1073,It is easy to determine and contains learning-dependent features .,0,0.59841794,76.82285520037189,11
1073,"The norm-based model competence makes NMT learn the curriculum in a fully automated way , while the norm-based sentence weight further enhances the learning of the vector representation of the NMT .",3,0.81244797,57.00978411276479,34
1073,Experimental results for the WMT’14 English-German and WMT’17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score ( + 1.17 / +1.56 ) and training speedup ( 2.22x/3.33 x ) .,3,0.8897975,11.218199903223493,42
1074,Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently .,0,0.9567559,40.599484875155504,18
1074,"Most existing frameworks , however , have difficulties in balancing between the translation quality and latency , i.e. , the decoding policy is usually either too aggressive or too conservative .",0,0.8723997,59.15316519088258,31
1074,"We propose an opportunistic decoding technique with timely correction ability , which always ( over-) generates a certain mount of extra words at each step to keep the audience on track with the latest information .",2,0.49028718,153.99528391202642,38
1074,"At the same time , it also corrects , in a timely fashion , the mistakes in the former overgenerated words when observing more source context to ensure high translation quality .",3,0.72300255,111.22556420542658,32
1074,"Experiments show our technique achieves substantial reduction in latency and up to + 3.1 increase in BLEU , with revision rate under 8 % in Chinese-to-English and English-to-Chinese translation .",3,0.93862784,35.303546997227016,36
1075,We develop a formal hierarchy of the expressive capacity of RNN architectures .,2,0.4324289,62.5619356107301,13
1075,"The hierarchy is based on two formal properties : space complexity , which measures the RNN ’s memory , and rational recurrence , defined as whether the recurrent update can be described by a weighted finite-state machine .",2,0.41304925,62.0766435692914,39
1075,We place several RNN variants within this hierarchy .,2,0.5077115,142.49320568276778,9
1075,"For example , we prove the LSTM is not rational , which formally separates it from the related QRNN ( Bradbury et al. , 2016 ) .",3,0.6800922,102.0411562467102,27
1075,We also show how these models ’ expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions .,3,0.8536026,83.5285941586367,23
1075,"Our results build on the theory of “ saturated ” RNNs ( Merrill , 2019 ) .",3,0.9544062,121.60225342226438,17
1075,"While formally extending these findings to unsaturated RNNs is left to future work , we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy .",3,0.95242697,49.34826729113636,29
1075,We provide empirical results to support this conjecture .,3,0.62680584,30.885310589675047,9
1075,Experimental findings from training unsaturated networks on formal languages support this conjecture .,0,0.5988381,202.41834887741652,13
1076,"We present that , the rank-frequency relation in textual data follows f ∝ r-𝛼( r + 𝛾 )-𝛽 , where f is the token frequency and r is the rank by frequency , with ( 𝛼 , 𝛽 , 𝛾 ) as parameters .",3,0.6163954,28.129381693944953,46
1076,"The formulation is derived based on the empirical observation that d2 ( x+y ) / dx2 is a typical impulse function , where ( x, y ) =( log r , log f ) .",2,0.6273439,73.71351472588361,36
1076,The formulation is the power law when 𝛽=0 and the Zipf – Mandelbrot law when 𝛼=0 .,3,0.52257097,23.384493807568546,18
1076,We illustrate that 𝛼 is related to the analytic features of syntax and 𝛽+𝛾 to those of morphology in natural languages from an investigation of multilingual corpora .,3,0.7369923,28.346596753188024,28
1077,"Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue : negative examples significantly outnumber positive examples , and the huge number of easy-negative examples overwhelms the training .",0,0.93447614,40.41135243595269,39
1077,"The most commonly used cross entropy ( CE ) criteria is actually an accuracy-oriented objective , and thus creates a discrepancy between training and test : at training time , each training instance contributes equally to the objective function , while at test time F1 score concerns more about positive examples .",0,0.78402966,94.13008742947855,53
1077,"In this paper , we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks .",1,0.8322359,48.77129599491172,22
1077,"Dice loss is based on the Sørensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives , and is more immune to the data-imbalance issue .",0,0.57591,81.11514852500349,34
1077,"To further alleviate the dominating influence from easy-negative examples in training , we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples .",2,0.5231134,63.05820744547885,31
1077,Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training .,3,0.90073496,31.828927541709984,23
1077,"With the proposed training objective , we observe significant performance boost on a wide range of data imbalanced NLP tasks .",3,0.8933625,25.029529188906906,21
1077,"Notably , we are able to achieve SOTA results on CTB5 , CTB6 and UD1.4 for the part of speech tagging task ;",3,0.97019625,88.68808416580005,23
1077,"SOTA results on CoNLL03 , OntoNotes 5.0 , MSRA and OntoNotes 4.0 for the named entity recognition task ;",3,0.8978839,42.85088046644583,19
1077,along with competitive results on the tasks of machine reading comprehension and paraphrase identification .,3,0.83899134,22.024956547737652,15
1078,This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance .,1,0.49064633,51.50194812521377,23
1078,"Our approach originates in the observable structure of a corpus , which we use to define and isolate grammaticality ( syntactic information ) and meaning / pragmatics information .",2,0.6815742,79.84523804792138,29
1078,"We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process , without any prior hypothesis on the form of the model .",3,0.51672447,51.46768877658424,40
1079,We examine a methodology using neural language models ( LMs ) for analyzing the word order of language .,1,0.45767474,59.890640269003654,19
1079,"This LM-based method has the potential to overcome the difficulties existing methods face , such as the propagation of preprocessor errors in count-based methods .",3,0.8415351,41.24530376375871,28
1079,"In this study , we explore whether the LM-based method is valid for analyzing the word order .",1,0.9452076,45.69016026277005,20
1079,"As a case study , this study focuses on Japanese due to its complex and flexible word order .",1,0.5867825,54.565642715335926,19
1079,"To validate the LM-based method , we test ( i ) parallels between LMs and human word order preference , and ( ii ) consistency of the results obtained using the LM-based method with previous linguistic studies .",2,0.46762475,53.971828160603685,42
1079,"Through our experiments , we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool .",3,0.9747195,116.63845000838052,21
1079,"Finally , using the LM-based method , we demonstrate the relationship between the canonical word order and topicalization , which had yet to be analyzed by large-scale experiments .",3,0.8082342,55.22519588112876,31
1080,This paper solves the fake news detection problem under a more realistic scenario on social media .,1,0.7619146,36.87832772415008,17
1080,"Given the source short-text tweet and the corresponding sequence of retweet users without text comments , we aim at predicting whether the source tweet is fake or not , and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern .",2,0.6984402,72.96907524700646,47
1080,"We develop a novel neural network-based model , Graph-aware Co-Attention Networks ( GCAN ) , to achieve the goal .",2,0.46503142,40.79148848099625,24
1080,Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16 % in accuracy on average .,3,0.8710307,24.132671113095753,29
1080,"In addition , the case studies also show that GCAN can produce reasonable explanations .",3,0.95773417,112.115397431994,15
1081,"Identifying controversial posts on social media is a fundamental task for mining public sentiment , assessing the influence of events , and alleviating the polarized views .",0,0.9236554,59.56116143548927,27
1081,"However , existing methods fail to 1 ) effectively incorporate the semantic information from content-related posts ;",0,0.83752227,142.58591424797376,17
1081,2 ) preserve the structural information for reply relationship modeling ;,2,0.52920127,1771.2059009403079,11
1081,3 ) properly handle posts from topics dissimilar to those in the training set .,3,0.48578537,97.78062788906595,15
1081,"To overcome the first two limitations , we propose Topic-Post-Comment Graph Convolutional Network ( TPC-GCN ) , which integrates the information from the graph structure and content of topics , posts , and comments for post-level controversy detection .",2,0.6661983,46.21613420439175,41
1081,"As to the third limitation , we extend our model to Disentangled TPC-GCN ( DTPC-GCN ) , to disentangle topic-related and topic-unrelated features and then fuse dynamically .",2,0.53419966,41.41069223360503,32
1081,Extensive experiments on two real-world datasets demonstrate that our models outperform existing methods .,3,0.8379408,6.808211411093693,14
1081,Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability .,3,0.9759477,29.787042583780565,21
1082,"Discovering the stances of media outlets and influential people on current , debatable topics is important for social statisticians and policy makers .",0,0.803825,82.33907287238216,23
1082,"Many supervised solutions exist for determining viewpoints , but manually annotating training data is costly .",0,0.8803172,124.26973091805586,16
1082,"In this paper , we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior ;",1,0.81680214,38.65733429468818,32
1082,"then , it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users , as well as their stance with respect to the target polarizing topic .",2,0.84382886,71.98740442429997,39
1082,"We evaluate the model by comparing its predictions to gold labels from the Media Bias / Fact Check website , achieving 82.6 % accuracy .",2,0.52005684,88.4174627309558,25
1083,The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science .,0,0.9511125,48.9467165139809,29
1083,"This is commonly approached by training word embeddings on each corpus , aligning the vector spaces , and looking for words whose cosine distance in the aligned space is large .",0,0.7521722,53.723638582865,31
1083,"However , these methods often require extensive filtering of the vocabulary to perform well , and-as we show in this work-result in unstable , and hence less reliable , results .",0,0.47687575,77.0680940371321,35
1083,"We propose an alternative approach that does not use vector space alignment , and instead considers the neighbors of each word .",2,0.5298926,42.3923647490217,22
1083,"The method is simple , interpretable and stable .",3,0.8407293,90.38935886209627,9
1083,"We demonstrate its effectiveness in 9 different setups , considering different corpus splitting criteria ( age , gender and profession of tweet authors , time of tweet ) and different languages ( English , French and Hebrew ) .",2,0.5368246,106.18142757081775,39
1084,Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging .,0,0.8573975,19.32241924763689,22
1084,Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process .,0,0.7312807,59.62313618259308,22
1084,"However , due to the lack of further consideration of content consistency , the common problem of response generation tasks , safe response , is intensified .",0,0.5922744,174.80682500506177,27
1084,"Besides , query emotions that can help model the relationship between query and response are simply ignored in previous models , which would further hurt the coherence .",3,0.4914254,80.3810421835909,28
1084,"To alleviate these problems , we propose a novel framework named Curriculum Dual Learning ( CDL ) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively .",2,0.42183656,54.95936978535577,38
1084,CDL utilizes two rewards focusing on emotion and content to improve the duality .,2,0.42560446,172.01104487088884,14
1084,"Additionally , it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions .",2,0.4848522,84.13050567456068,20
1084,"Experimental results show that CDL significantly outperforms the baselines in terms of coherence , diversity , and relation to emotion factors .",3,0.9704378,31.415821636482313,22
1085,Recent works in dialogue state tracking ( DST ) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches .,0,0.90348744,40.16751123958389,31
1085,"However , they are inefficient in that they predict the dialogue state at every turn from scratch .",0,0.8351267,57.36994113496406,18
1085,"Here , we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST .",1,0.4533097,69.41399212531415,24
1085,"This mechanism consists of two steps : ( 1 ) predicting state operation on each of the memory slots , and ( 2 ) overwriting the memory with new values , of which only a few are generated according to the predicted state operations .",2,0.52270895,32.893079659659875,45
1085,"Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks , thus reducing the burden of the decoder .",2,0.6254384,14.825256050405258,28
1085,This enhances the effectiveness of training and DST performance .,3,0.78167087,77.41489995004122,10
1085,Our SOM-DST ( Selectively Overwriting Memory for Dialogue State Tracking ) model achieves state-of-the-art joint goal accuracy with 51.72 % in MultiWOZ 2.0 and 53.01 % in MultiWOZ 2.1 in an open vocabulary-based DST setting .,3,0.88530517,28.81486333868753,46
1085,"In addition , we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance .",3,0.90667313,69.55420924563364,37
1086,The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal .,0,0.6013351,43.68445513194098,28
1086,"The traditional approach to build such a dialogue system is to take a pipelined modular architecture , where its modules are optimized individually .",0,0.8906998,50.3715934856646,24
1086,"However , such an optimization scheme does not necessarily yield the overall performance improvement of the whole system .",0,0.6653025,29.11322258358377,19
1086,"On the other hand , end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances , without taking into account the entire annotations available in the corpus .",0,0.87481785,30.900814925016324,34
1086,This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response .,3,0.5489165,31.077982045546804,31
1086,"In this paper , we present an end-to-end neural architecture for dialogue systems that addresses both challenges above .",1,0.89091945,17.57573002103749,21
1086,"In the human evaluation , our dialogue system achieved the success rate of 68.32 % , the language understanding score of 4.149 , and the response appropriateness score of 4.287 , which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge ( DSTC8 ) .",3,0.9478623,30.564607442503732,59
1087,Existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation .,0,0.7999207,24.912203416400885,16
1087,We focus on evaluating response generation systems via response selection .,1,0.5431083,107.360085427431,11
1087,"To evaluate systems properly via response selection , we propose a method to construct response selection test sets with well-chosen false candidates .",1,0.39168853,88.0821633990048,25
1087,"Specifically , we propose to construct test sets filtering out some types of false candidates : ( i ) those unrelated to the ground-truth response and ( ii ) those acceptable as appropriate responses .",2,0.74452204,72.15328815994127,37
1087,"Through experiments , we demonstrate that evaluating systems via response selection with the test set developed by our method correlates more strongly with human evaluation , compared with widely used automatic evaluation metrics such as BLEU .",3,0.8984844,36.78238612044728,37
1088,"Off-topic spoken response detection , the task aiming at predicting whether a response is off-topic for the corresponding prompt , is important for an automated speaking assessment system .",0,0.9362013,66.82765328734664,29
1088,"In many real-world educational applications , off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training .",0,0.84397036,39.15000689389235,34
1088,"In this paper , we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts .",1,0.90940654,41.031447276524695,25
1088,"We introduce a new model , Gated Convolutional Bidirectional Attention-based Model ( GCBiA ) , which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses , and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts .",2,0.7506876,68.89837804448845,56
1088,"Moreover , a new negative sampling method is proposed to augment training data .",2,0.5663184,41.70621617077798,14
1088,"Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall , for both seen and unseen prompts .",3,0.9831118,39.753049983700905,29
1089,Existing end-to-end dialog systems perform less effectively when data is scarce .,0,0.86260235,18.3433951303071,13
1089,"To obtain an acceptable success in real-life online services with only a handful of training examples , both fast adaptability and reliable performance are highly desirable for dialog systems .",0,0.76190275,49.55275137520521,30
1089,"In this paper , we propose the Meta-Dialog System ( MDS ) , which combines the advantages of both meta-learning approaches and human-machine collaboration .",1,0.8872909,34.84662361783194,25
1089,We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning .,2,0.66962385,43.894689995448935,22
1089,Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90 % per-turn accuracies with only 10 dialogs on the extended-bAbI dataset .,3,0.9614323,40.10831355599076,29
1090,Neural-based context-aware models for slot tagging have achieved state-of-the-art performance .,0,0.8853638,11.185579524662451,20
1090,"However , the presence of OOV ( out-of-vocab ) words significantly degrades the performance of neural-based models , especially in a few-shot scenario .",0,0.53140646,32.18220346582748,29
1090,"In this paper , we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge .",1,0.8998531,29.366650269023594,28
1090,"Besides , we use multi-level graph attention to explicitly model lexical relations .",2,0.74032176,37.633519028645246,13
1090,The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets .,3,0.96054167,31.968719826617203,26
1091,Many studies have applied reinforcement learning to train a dialog policy and show great promise these years .,0,0.9381136,93.2797058865618,18
1091,One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms .,0,0.74678856,25.951239854252357,23
1091,"However , modeling a realistic user simulator is challenging .",0,0.88300997,142.84437523295577,10
1091,"A rule-based simulator requires heavy domain expertise for complex tasks , and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator .",0,0.84660435,79.82414829393146,29
1091,"To avoid explicitly building a user simulator beforehand , we propose Multi-Agent Dialog Policy Learning , which regards both the system and the user as the dialog agents .",2,0.72012573,82.35042046747778,29
1091,Two agents interact with each other and are jointly learned simultaneously .,2,0.5205418,26.68023820174319,12
1091,The method uses the actor-critic framework to facilitate pretraining and improve scalability .,2,0.7203418,61.36211829261647,13
1091,We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog .,3,0.4965036,67.0044557520187,26
1091,"Results show that our method can successfully build a system policy and a user policy simultaneously , and two agents can achieve a high task success rate through conversational interaction .",3,0.9858613,55.171607044196165,31
1092,Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set .,0,0.8844919,26.024805238557516,18
1092,"However , the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings .",0,0.90439713,27.13018055754815,23
1092,We propose a paraphrase augmented response generation ( PARG ) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance .,2,0.38615197,23.578286912453304,29
1092,We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels .,2,0.7656633,61.6894096880296,21
1092,"PARG is applicable to various dialog generation models , such as TSCP ( Lei et al. , 2018 ) and DAMD ( Zhang et al. , 2019 ) .",3,0.603048,42.89921105833887,29
1092,Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ .,3,0.9735431,28.784596291439584,24
1092,"PARG also outperforms other data augmentation methods significantly in dialog generation tasks , especially under low resource settings .",3,0.9657131,33.347809954322784,19
1093,Neural conversation models are known to generate appropriate but non-informative responses in general .,0,0.9104953,23.505875339720767,14
1093,"A scenario where informativeness can be significantly enhanced is Conversing by Reading ( CbR ) , where conversations take place with respect to a given external document .",0,0.85986465,65.76530217694298,28
1093,"In previous work , the external document is utilized by ( 1 ) creating a context-aware document memory that integrates information from the document and the conversational context , and then ( 2 ) generating responses referring to the memory .",0,0.86101097,41.940625121299924,43
1093,"In this paper , we propose to create the document memory with some anticipated responses in mind .",1,0.9126774,89.25632607892508,18
1093,This is achieved using a teacher-student framework .,2,0.6318749,18.826878726170875,9
1093,"The teacher is given the external document , the context , and the ground-truth response , and learns how to build a response-aware document memory from three sources of information .",2,0.68195397,49.05909719846622,34
1093,"The student learns to construct a response-anticipated document memory from the first two sources , and teacher ’s insight on memory creation .",2,0.5570821,182.4292173162139,23
1093,Empirical results show that our model outperforms the previous state-of-the-art for the CbR task .,3,0.9739082,7.222988703033714,20
1094,Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems .,0,0.76466566,124.37282057781232,16
1094,This is insufficient for training intermediate dialogue turns since supervision signals ( or rewards ) are only provided at the end of dialogues .,0,0.44783333,91.27138648991668,24
1094,"To address this issue , reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards .",0,0.85241884,60.089820677029145,27
1094,"This approach requires complete state-action annotations of human-to-human dialogues ( i.e. , expert demonstrations ) , which is labor intensive .",0,0.4142019,102.75358625508684,21
1094,"To overcome this limitation , we propose a novel reward learning approach for semi-supervised policy learning .",2,0.47027135,19.189771693637454,17
1094,"The proposed approach learns a dynamics model as the reward function which models dialogue progress ( i.e. , state-action sequences ) based on expert demonstrations , either with or without annotations .",2,0.6743956,127.6709575847605,32
1094,The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations .,2,0.52482927,154.76255694486733,17
1094,We further propose to learn action embeddings for a better generalization of the reward function .,3,0.6065796,36.79690267925936,16
1094,"The proposed approach outperforms competitive policy learning baselines on MultiWOZ , a benchmark multi-domain dataset .",3,0.71425414,32.980653598090235,16
1095,"In modular dialogue systems , natural language understanding ( NLU ) and natural language generation ( NLG ) are two critical components , where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations .",0,0.88230115,22.126152346155102,48
1095,"However , the dual property between understanding and generation has been rarely explored .",0,0.9557832,130.99657588011343,14
1095,The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework .,0,0.642772,35.296746698878344,26
1095,"However , the prior work still learned both components in a supervised manner ;",0,0.61758447,252.16019571536935,14
1095,"instead , this paper introduces a general learning framework to effectively exploit such duality , providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion .",1,0.54279345,48.16781259598514,37
1095,The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG .,3,0.9470048,17.471436218259996,20
1095,The source code is available at : https://github.com/MiuLab/DuaLUG .,3,0.50877315,14.386654586437496,9
1096,The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research .,0,0.95097655,39.69808795901981,15
1096,Standard language generation metrics have been shown to be ineffective for evaluating dialog models .,0,0.89960027,32.52553212975807,15
1096,"To this end , this paper presents USR , an UnSupervised and Reference-free evaluation metric for dialog .",1,0.8341911,92.70476735125398,20
1096,USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog .,0,0.8019708,40.05591625604956,18
1096,"USR is shown to strongly correlate with human judgment on both Topical-Chat ( turn-level : 0.42 , system-level : 1.0 ) and PersonaChat ( turn-level : 0.48 and system-level : 1.0 ) .",3,0.913108,18.950718393252078,35
1096,USR additionally produces interpretable measures for several desirable properties of dialog .,3,0.54779196,205.50168534130722,12
1097,"Definition generation , which aims to automatically generate dictionary definitions for words , has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts .",0,0.9463216,40.63538327887691,30
1097,"However , previous works hardly consider explicitly modeling the “ components ” of definitions , leading to under-specific generation results .",0,0.83997333,132.5188565496676,21
1097,"In this paper , we propose ESD , namely Explicit Semantic Decomposition for definition Generation , which explicitly decomposes the meaning of words into semantic components , and models them with discrete latent variables for definition generation .",1,0.8048831,43.76086519125403,38
1097,"Experimental results show that achieves top results on WordNet and Oxford benchmarks , outperforming strong previous baselines .",3,0.9411691,45.51963833531444,18
1098,Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss .,0,0.8996983,25.963994925979293,20
1098,"While straightforward to optimize , this approach forces the model to reproduce all variations in the dataset , including noisy and invalid references ( e.g. , misannotations and hallucinated facts ) .",3,0.45343655,82.61696839089757,32
1098,Even a small fraction of noisy data can degrade the performance of log loss .,3,0.62848294,35.64543309125598,15
1098,"As an alternative , prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references .",0,0.8629768,73.04308557800033,27
1098,"However , distinguishability has not been used in practice due to challenges in optimization and estimation .",0,0.90618473,62.40039620259961,17
1098,We propose loss truncation : a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability .,2,0.4361733,137.58383447498116,25
1098,"Empirically , we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task .",3,0.92827296,43.44389381804303,17
1098,"Furthermore , we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references .",3,0.96272886,47.345078856010595,26
1099,Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models .,0,0.8845503,44.3926278286263,21
1099,This work focuses on AMR-to-text generation – A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations ( AMR ) .,1,0.73601735,27.050801024351536,28
1099,1 ) The message propagation process in AMR graphs is only guided by the first-order adjacency information .,3,0.5919173,61.1482228961602,18
1099,2 ) The relationships between labeled edges are not fully considered .,0,0.4690823,135.87172144802622,12
1099,"In this work , we propose a novel graph encoding framework which can effectively explore the edge relations .",1,0.78745407,41.86764290027565,19
1099,We also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in AMR graphs .,2,0.7351532,68.70504526746508,21
1099,Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets .,3,0.977509,6.018095437945671,22
1099,The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling .,3,0.98042184,38.60269990880419,20
1100,Neural text generation has made tremendous progress in various tasks .,0,0.9582574,28.801044215922833,11
1100,One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating .,0,0.87949866,70.93781349751319,22
1100,"However , we may confront some special text paradigms such as Lyrics ( assume the music score is given ) , Sonnet , SongCi ( classical Chinese poetry of the Song dynasty ) , etc .",0,0.77474916,156.13606587525695,36
1100,The typical characteristics of these texts are in three folds : ( 1 ) They must comply fully with the rigid predefined formats .,0,0.81401414,136.1851413096478,24
1100,( 2 ) They must obey some rhyming schemes .,3,0.4172341,553.2182041562304,10
1100,"( 3 ) Although they are restricted to some formats , the sentence integrity must be guaranteed .",0,0.49689776,238.7574377037048,18
1100,"To the best of our knowledge , text generation based on the predefined rigid formats has not been well investigated .",0,0.86177623,29.143355744034285,21
1100,"Therefore , we propose a simple and elegant framework named SongNet to tackle this problem .",1,0.5677034,28.16840742876429,16
1100,The backbone of the framework is a Transformer-based auto-regressive language model .,2,0.5579322,10.299490530185471,15
1100,"Sets of symbols are tailor-designed to improve the modeling performance especially on format , rhyme , and sentence integrity .",3,0.44551244,127.3262440750387,20
1100,We improve the attention mechanism to impel the model to capture some future information on the format .,3,0.7543031,67.26096254979686,18
1100,A pre-training and fine-tuning framework is designed to further improve the generation quality .,2,0.52212864,11.600806252463727,14
1100,Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation .,3,0.9349873,15.780398340900577,27
1101,Question Generation ( QG ) is fundamentally a simple syntactic transformation ;,0,0.9255568,178.68519191714861,12
1101,"however , many aspects of semantics influence what questions are good to form .",0,0.89616185,250.83659999590785,14
1101,"We implement this observation by developing Syn-QG , a set of transparent syntactic rules leveraging universal dependencies , shallow semantic parsing , lexical resources , and custom rules which transform declarative sentences into question-answer pairs .",2,0.682298,96.26307318817176,40
1101,"We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content , which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems .",2,0.64341664,171.36429585509865,34
1101,"In order to improve syntactic fluency and eliminate grammatically incorrect questions , we employ back-translation over the output of these syntactic rules .",2,0.7133325,37.84862877515152,24
1101,A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions .,3,0.94008815,22.422971890802398,41
1102,"Clustering short text streams is a challenging task due to its unique properties : infinite length , sparse data representation and cluster evolution .",0,0.93851864,78.67578844759284,24
1102,Existing approaches often exploit short text streams in a batch way .,0,0.8455058,133.1193164181859,12
1102,"However , determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve .",0,0.81004775,72.28008000072322,23
1102,"In addition , traditional independent word representation in graphical model tends to cause “ term ambiguity ” problem in short text clustering .",0,0.7353001,282.7231944497641,23
1102,"Therefore , in this paper , we propose an Online Semantic-enhanced Dirichlet Model for short sext stream clustering , called OSDM , which integrates the word-occurance semantic information ( i.e. , context ) into a new graphical model and clusters each arriving short text automatically in an online way .",1,0.70854205,100.1682328462943,50
1102,Extensive results have demonstrated that OSDM has better performance compared to many state-of-the-art algorithms on both synthetic and real-world data sets .,3,0.79897016,13.257469236188554,28
1103,Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint .,0,0.9110683,17.605708398971164,22
1103,"For the tractability of training , existing generative-hashing methods mostly assume a factorized form for the posterior distribution , enforcing independence among the bits of hash codes .",0,0.78878826,116.22527775299953,30
1103,"From the perspectives of both model representation and code space size , independence is always not the best assumption .",0,0.57376516,139.22693828118219,20
1103,"In this paper , to introduce correlations among the bits of hash codes , we propose to employ the distribution of Boltzmann machine as the variational posterior .",1,0.5989182,75.1280028313779,28
1103,"To address the intractability issue of training , we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution .",2,0.71509004,22.595195456185216,40
1103,"Based on that , an asymptotically-exact lower bound is further derived for the evidence lower bound ( ELBO ) .",2,0.56233615,70.34346139781817,22
1103,"With these novel techniques , the entire model can be optimized efficiently .",3,0.6868509,59.36503550887048,13
1103,"Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code , our model can achieve significant performance gains .",3,0.9259009,47.61144200371409,25
1104,"We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine , which helps the creation of flexible dictionaries with precise granularity required in typical text analysis .",1,0.39514717,60.18938649579602,39
1104,This paper introduces the first formulation of interactive dictionary construction to address this issue .,1,0.8309617,57.6567734471218,15
1104,"To optimize the interaction , we propose a new algorithm that effectively captures an analyst ’s intention starting from only a small number of sample terms .",2,0.53084636,68.03158847002948,27
1104,"Along with the algorithm , we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task .",2,0.5225218,46.42401795544879,27
1104,"Experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods , and works even with a small number of interactions .",3,0.8713037,40.12639088047881,26
1105,"This paper presents a tree-structured neural topic model , which has a topic distribution over a tree with an infinite number of branches .",1,0.6887843,24.236280718661227,24
1105,Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks .,2,0.79353654,83.36342526614507,18
1105,"With the help of autoencoding variational Bayes , our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures , as compared to a prior tree-structured topic model ( Blei et al. , 2010 ) .",3,0.59426504,43.73628056802125,41
1105,This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks .,3,0.37162027,22.62571250038144,20
1106,We focus on the task of Frequently Asked Questions ( FAQ ) retrieval .,1,0.47223288,32.64537624446295,14
1106,A given user query can be matched against the questions and / or the answers in the FAQ .,0,0.35268822,65.1812406312346,19
1106,We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models .,2,0.6348345,45.10794647671159,17
1106,"The two models match user queries to FAQ answers and questions , respectively .",2,0.43811586,319.2156063451476,14
1106,We alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases .,2,0.6932137,41.14640752941437,16
1106,We show that our model is on par and even outperforms supervised models on existing datasets .,3,0.9467616,24.877261917846713,17
1107,Humor plays an important role in human languages and it is essential to model humor when building intelligence systems .,0,0.9140525,41.087736142413696,20
1107,"Among different forms of humor , puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity .",0,0.7117922,72.48074234710374,23
1107,"However , identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks .",0,0.8716986,92.30764206804064,18
1107,"In this paper , we propose Pronunciation-attentive Contextualized Pun Recognition ( PCPR ) to perceive human humor , detect if a sentence contains puns and locate them in the sentence .",1,0.84592146,44.71671317124216,33
1107,PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols .,0,0.49504688,30.450572446887854,24
1107,Extensive experiments are conducted on two benchmark datasets .,2,0.73077697,10.779836955484328,9
1107,Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks .,3,0.9827784,11.837926290967525,24
1107,In-depth analyses verify the effectiveness and robustness of PCPR .,3,0.82974553,32.702368295807254,12
1108,"Even though BERT has achieved successful performance improvements in various supervised learning tasks , BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations .",0,0.8242215,33.97884077936403,32
1108,"To resolve this limitation , we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder ( T-TA ) .",2,0.4255056,18.793517046760687,24
1108,"The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture , such as that of BERT .",3,0.5628764,78.04588661607312,24
1108,"In computation time experiments in a CPU environment , the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task .",3,0.8956956,46.05497961069306,37
1108,"Furthermore , the T-TA shows competitive or even better accuracies than those of BERT on the above tasks .",3,0.9659801,51.6842820592918,19
1108,Code is available at https://github.com/joongbo/tta .,3,0.48756486,16.661486544926397,6
1109,Personalized news recommendation is a critical technology to improve users ’ online news reading experience .,0,0.94255847,53.13370435444013,16
1109,The core of news recommendation is accurate matching between user ’s interests and candidate news .,0,0.8924086,66.44115834565928,16
1109,The same user usually has diverse interests that are reflected in different news she has browsed .,0,0.92158806,117.88623833283454,17
1109,"Meanwhile , important semantic features of news are implied in text segments of different granularities .",0,0.6531906,99.07892433645898,16
1109,"Existing studies generally represent each user as a single vector and then match the candidate news vector , which may lose fine-grained information for recommendation .",0,0.84510785,70.66539764469856,27
1109,"In this paper , we propose FIM , a Fine-grained Interest Matching method for neural news recommendation .",1,0.8894864,47.47037123497183,18
1109,"Instead of aggregating user ’s all historical browsed news into a unified vector , we hierarchically construct multi-level representations for each news via stacked dilated convolutions .",2,0.781791,80.98397912975244,27
1109,Then we perform fine-grained matching between segment pairs of each browsed news and the candidate news at each semantic level .,2,0.8543411,44.0167593428046,22
1109,High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction .,2,0.5139621,234.61017797867552,18
1109,Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation .,3,0.7169863,23.316457260953346,19
1110,Operational risk management is one of the biggest challenges nowadays faced by financial institutions .,0,0.9606302,27.347170324951147,15
1110,"There are several major challenges of building a text classification system for automatic operational risk prediction , including imbalanced labeled / unlabeled data and lacking interpretability .",0,0.87411743,58.36637981779545,27
1110,"To tackle these challenges , we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification ( SemiORC ) .",2,0.45508307,31.218539843572305,29
1110,We empirically evaluate the framework on a real-world dataset .,2,0.6260663,13.604341832981692,10
1110,The results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations .,3,0.98857224,22.287188630030084,18
1110,SemiORC also outperforms other baseline methods on operational risk classification .,3,0.9485384,104.00403367079026,11
1111,Identifying user geolocation in online social networks is an essential task in many location-based applications .,0,0.9358743,21.117422679892226,18
1111,"Existing methods rely on the similarity of text and network structure , however , they suffer from a lack of interpretability on the corresponding results , which is crucial for understanding model behavior .",0,0.80833787,46.62691119444252,34
1111,"In this work , we adopt influence functions to interpret the behavior of GNN-based models by identifying the importance of training users when predicting the locations of the testing users .",2,0.6508358,73.38829441431771,33
1111,This methodology helps with providing meaningful explanations on prediction results .,3,0.723657,341.66308208443064,11
1111,"Furthermore , it also initiates an attempt to uncover the so-called “ black-box ” GNN-based models by investigating the effect of individual nodes .",3,0.39115712,43.565405429746335,28
1112,Language modeling is the technique to estimate the probability of a sequence of words .,0,0.9065815,19.16020668582602,15
1112,"A bilingual language model is expected to model the sequential dependency for words across languages , which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages .",0,0.8854084,27.872446664323267,37
1112,We propose a bilingual attention language model ( BALM ) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency .,2,0.407051,38.34705501157786,33
1112,The attention mechanism learns the bilingual context from a parallel corpus .,2,0.4832898,64.570829711059,12
1112,BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5 % over the best-reported result .,3,0.86185396,40.97774630063769,26
1112,"We also apply BALM in bilingual lexicon induction , and language normalization tasks to validate the idea .",3,0.5118682,141.11631566481918,18
1113,Chinese Spelling Check ( CSC ) is a task to detect and correct spelling errors in Chinese natural language .,0,0.96699834,21.69224539506548,20
1113,Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters .,0,0.8957946,89.92802329701294,14
1113,"However , they take the similarity knowledge as either an external input resource or just heuristic rules .",0,0.7275148,208.0811806482503,18
1113,This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network ( SpellGCN ) .,1,0.7715512,53.608189784462766,25
1113,"The model builds a graph over the characters , and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers .",2,0.7073332,55.55916146185111,25
1113,"These classifiers are applied to the representations extracted by another network , such as BERT , enabling the whole network to be end-to-end trainable .",2,0.41934258,33.75973266968464,27
1113,Experiments are conducted on three human-annotated datasets .,2,0.8088249,12.454839597841724,8
1113,Our method achieves superior performance against previous models by a large margin .,3,0.8741169,19.399603696849102,13
1114,Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability .,0,0.94475234,50.74426375507959,22
1114,Without loss of generality we consider Chinese spelling error correction ( CSC ) in this paper .,2,0.63730586,54.726572450766305,17
1114,"A state-of-the-art method for the task selects a character from a list of candidates for correction ( including non-correction ) at each position of the sentence on the basis of BERT , the language representation model .",2,0.4788659,26.44400977899059,41
1114,"The accuracy of the method can be sub-optimal , however , because BERT does not have sufficient capability to detect whether there is an error at each position , apparently due to the way of pre-training it using mask language modeling .",3,0.8734734,38.17487519400514,42
1114,"In this work , we propose a novel neural architecture to address the aforementioned issue , which consists of a network for error detection and a network for error correction based on BERT , with the former being connected to the latter with what we call soft-masking technique .",1,0.77945125,25.717063285941066,49
1114,"Our method of using ‘ Soft-Masked BERT ’ is general , and it may be employed in other language detection-correction problems .",3,0.8924006,106.04875538796445,26
1114,"Experimental results on two datasets , including one large dataset which we create and plan to release , demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT .",3,0.868045,20.44167077379012,40
1115,Sentence representation ( SR ) is the most crucial and challenging task in Machine Reading Comprehension ( MRC ) .,0,0.9650522,27.35607167362175,20
1115,"MRC systems typically only utilize the information contained in the sentence itself , while human beings can leverage their semantic knowledge .",0,0.8545451,69.48847208958959,22
1115,"To bridge the gap , we proposed a novel Frame-based Sentence Representation ( FSR ) method , which employs frame semantic knowledge to facilitate sentence modelling .",2,0.42522624,44.60326123553417,28
1115,"Specifically , different from existing methods that only model lexical units ( LUs ) , Frame Representation Models , which utilize both LUs in frame and Frame-to-Frame ( F-to-F ) relations , are designed to model frames and sentences with attention schema .",0,0.71341294,59.26793551359743,47
1115,Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations .,3,0.9364588,85.38042438036597,19
1115,Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task .,3,0.98301226,9.40985133133697,24
1116,"In this paper , we introduce a novel methodology to efficiently construct a corpus for question answering over structured data .",1,0.89556557,20.95681764199586,21
1116,"For this , we introduce an intermediate representation that is based on the logical query plan in a database , called Operation Trees ( OT ) .",2,0.6534909,113.15837383697907,27
1116,This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate .,2,0.42543814,29.28713642671537,21
1116,"Furthermore , it allows for fine-grained alignment of the tokens to the operations .",3,0.48205492,32.20133752990306,15
1116,"Thus , we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens .",2,0.74989295,84.58629653643601,25
1116,"We compare our corpus OTTA ( Operation Trees and Token Assignment ) , a large semantic parsing corpus for evaluating natural language interfaces to databases , to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries .",3,0.8141939,108.44209613722674,52
1116,"Finally , we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance .",3,0.84194314,13.026314976745523,41
1117,"Open-domain question answering can be formulated as a phrase retrieval problem , in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models .",0,0.89652365,36.60084875648381,38
1117,"In this paper , we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation ( Sparc ) .",1,0.87405026,29.0112696848268,26
1117,"Unlike previous sparse vectors that are term-frequency-based ( e.g. , tf-idf ) or directly learned ( only few thousand dimensions ) , we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space .",2,0.73512787,93.65550974215637,40
1117,"By augmenting the previous phrase retrieval model ( Seo et al. , 2019 ) with Sparc , we show 4 % + improvement in CuratedTREC and SQuAD-Open .",3,0.87470245,123.41171086207306,30
1117,Our CuratedTREC score is even better than the best known retrieve & read model with at least 45 x faster inference speed .,3,0.96593875,185.01493897393198,23
1118,"Building general reading comprehension systems , capable of solving multiple datasets at the same time , is a recent aspirational goal in the research community .",0,0.9038022,49.51857259367302,26
1118,"Prior work has focused on model architecture or generalization to held out datasets , and largely passed over the particulars of the multi-task learning set up .",0,0.9134149,92.74018238147833,27
1118,"We show that a simple dynamic sampling strategy , selecting instances for training proportional to the multi-task model ’s current performance on a dataset relative to its single task performance , gives substantive gains over prior multi-task sampling strategies , mitigating the catastrophic forgetting that is common in multi-task learning .",3,0.906943,55.03334854598565,51
1118,We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level .,3,0.9416025,43.91041170533754,39
1118,"Our final model shows greatly increased performance over the best model on ORB , a recently-released multitask reading comprehension benchmark .",3,0.95398396,43.29547162018372,23
1119,Multilingual pre-trained models could leverage the training data from a rich source language ( such as English ) to improve performance on low resource languages .,3,0.4783516,22.900605670144262,26
1119,"However , the transfer quality for multilingual Machine Reading Comprehension ( MRC ) is significantly worse than sentence classification tasks mainly due to the requirement of MRC to detect the word level answer boundary .",0,0.71236366,42.64854279826497,35
1119,"In this paper , we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary supervision : ( 1 ) A mixed MRC task , which translates the question or passage to other languages and builds cross-lingual question-passage pairs ;",1,0.5242494,54.74095305031372,45
1119,( 2 ) A language-agnostic knowledge masking task by leveraging knowledge phrases mined from web .,2,0.6319426,152.4602077639166,18
1119,"Besides , extensive experiments on two cross-lingual MRC datasets show the effectiveness of our proposed approach .",3,0.8509392,13.630968027101535,17
1120,The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions .,0,0.905852,43.92306014871053,23
1120,Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them .,0,0.90893626,45.824105173375386,22
1120,"In this paper , we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker ( EMT ) to track whether conditions listed in the rule text have already been satisfied to make a decision .",1,0.865752,47.05539439885827,41
1120,"Moreover , our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy , utilizing sentence-level entailment scores to weight token-level distributions .",3,0.48664948,78.6551201006844,29
1120,"On the ShARC benchmark ( blind , held-out ) testset , EMT achieves new state-of-the-art results of 74.6 % micro-averaged decision accuracy and 49.5 BLEU4 .",3,0.88906354,48.05567475541723,34
1120,We also show that EMT is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows .,3,0.9431012,37.535451655793786,20
1120,Code and models are released at https://github.com/Yifan-Gao/explicit_memory_tracker .,3,0.48622063,13.776721998843108,8
1121,Large pre-trained language models ( LMs ) are known to encode substantial amounts of linguistic information .,0,0.93863267,14.196495090922694,17
1121,"However , high-level reasoning skills , such as numerical reasoning , are difficult to learn from a language-modeling objective only .",0,0.8307772,51.00197011209784,23
1121,"Consequently , existing models for numerical reasoning have used specialized architectures with limited flexibility .",0,0.91316575,104.45436789506729,15
1121,"In this work , we show that numerical reasoning is amenable to automatic data generation , and thus one can inject this skill into pre-trained LMs , by generating large amounts of data , and training in a multi-task setup .",1,0.5040565,51.26395516689339,41
1121,"We show that pre-training our model , GenBERT , on this data , dramatically improves performance on DROP ( 49.3 –> 72.3 F1 ) , reaching performance that matches state-of-the-art models of comparable size , while using a simple and general-purpose encoder-decoder architecture .",3,0.8578517,35.61018161962619,52
1121,"Moreover , GenBERT generalizes well to math word problem datasets , while maintaining high performance on standard RC tasks .",3,0.8754856,85.0126885025763,20
1121,"Our approach provides a general recipe for injecting skills into large pre-trained LMs , whenever the skill is amenable to automatic data augmentation .",3,0.8840798,66.79060351015526,24
1122,"Despite recent progress in conversational question answering , most prior work does not focus on follow-up questions .",0,0.9019353,20.756205427450784,20
1122,"Practical conversational question answering systems often receive follow-up questions in an ongoing conversation , and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation , for more effective answer finding subsequently .",0,0.9111984,28.173337309923674,49
1122,"In this paper , we introduce a new follow-up question identification task .",1,0.8984976,23.454294573094508,15
1122,"We propose a three-way attentive pooling network that determines the suitability of a follow-up question by capturing pair-wise interactions between the associated passage , the conversation history , and a candidate follow-up question .",2,0.44798505,38.79554446156087,40
1122,It enables the model to capture topic continuity and topic shift while scoring a particular candidate follow-up question .,3,0.66161567,89.49848406506806,21
1122,Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins .,3,0.9703706,31.92513683988766,19
1123,Previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity : questions with constraints and questions with multiple hops of relations .,0,0.91363215,76.44683525745941,28
1123,"In this paper , we handle both types of complexity at the same time .",1,0.53820926,32.585204867581325,15
1123,"Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space , we propose a modified staged query graph generation method with more flexible ways to generate query graphs .",2,0.46565557,45.14690559830429,38
1123,Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets .,3,0.9760716,10.116866460134442,19
1124,"We present ASDiv ( Academia Sinica Diverse MWP Dataset ) , a diverse ( in terms of both language patterns and problem types ) English math word problem ( MWP ) corpus for evaluating the capability of various MWP solvers .",2,0.55875677,85.12330537487001,41
1124,Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types .,0,0.8920437,115.17660391929861,19
1124,"We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school .",2,0.5016666,137.1118301313672,25
1124,Each MWP is annotated with its problem type and grade level ( for indicating the level of difficulty ) .,2,0.74492276,92.18544736601045,20
1124,"Furthermore , we propose a metric to measure the lexicon usage diversity of a given MWP corpus , and demonstrate that ASDiv is more diverse than existing corpora .",3,0.6345973,50.83156805717895,29
1124,Experiments show that our proposed corpus reflects the true capability of MWP solvers more faithfully .,3,0.97448236,76.4419872031981,16
1125,Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image .,0,0.91288817,20.019866036826144,22
1125,Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions .,0,0.7660777,46.46367063592214,29
1125,It usually leads to over-penalization and thus a bad correlation to human judgment .,0,0.72852725,37.28651897547421,14
1125,"Recently , the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance .",0,0.7830968,63.5849704411439,29
1125,"In this paper , we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation .",1,0.88430643,28.736172098723976,32
1125,The experimental results show that our metric achieves state-of-the-art human judgment correlation .,3,0.9792063,17.567158593134454,18
1126,Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar .,0,0.8800057,9.789560069225686,25
1126,"The structures of embedding spaces largely depend on the co-occurrence statistics of each word , which the choice of context window determines .",0,0.6309643,41.38998369477765,23
1126,"Despite this obvious connection between the context window and mapping-based cross-lingual embeddings , their relationship has been underexplored in prior work .",0,0.87124735,26.74097907207375,24
1126,"In this work , we provide a thorough evaluation , in various languages , domains , and tasks , of bilingual embeddings trained with different context windows .",1,0.6642125,87.12678103944017,28
1126,"The highlight of our findings is that increasing the size of both the source and target window sizes improves the performance of bilingual lexicon induction , especially the performance on frequent nouns .",3,0.98991287,25.283557313502953,33
1127,"A major obstacle in Word Sense Disambiguation ( WSD ) is that word senses are not uniformly distributed , causing existing models to generally perform poorly on senses that are either rare or unseen during training .",0,0.9564146,25.38114066857817,37
1127,"We propose a bi-encoder model that independently embeds ( 1 ) the target word with its surrounding context and ( 2 ) the dictionary definition , or gloss , of each sense .",2,0.6857985,56.20046000545553,33
1127,"The encoders are jointly optimized in the same representation space , so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding .",2,0.54523134,29.580115605110855,30
1127,Our system outperforms previous state-of-the-art models on English all-words WSD ;,3,0.8714902,28.649777953750363,18
1127,"these gains predominantly come from improved performance on rare senses , leading to a 31.1 % error reduction on less frequent senses over prior work .",3,0.900557,69.6762012680765,26
1127,This demonstrates that rare senses can be more effectively disambiguated by modeling their definitions .,3,0.97801876,59.356091027041224,15
1128,"In this paper , we demonstrate how code-switching patterns can be utilised to improve various downstream NLP applications .",1,0.86056465,17.434764075514945,19
1128,"In particular , we encode various switching features to improve humour , sarcasm and hate speech detection tasks .",2,0.5739555,182.15410468843868,19
1128,We believe that this simple linguistic observation can also be potentially helpful in improving other similar NLP applications .,3,0.96730363,40.82958138810755,19
1129,"Recently , many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification , which has been widely recognized .",0,0.93478227,190.22508824480565,25
1129,"However , in these methods , the discovery process of evidence is nontransparent and unexplained .",0,0.7906814,116.59796746104665,16
1129,"Simultaneously , the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims .",0,0.59519243,59.46924167977976,27
1129,"In this paper , we propose a Decision Tree-based Co-Attention model ( DTCA ) to discover evidence for explainable claim verification .",1,0.916025,40.55265261717081,22
1129,"Specifically , we first construct Decision Tree-based Evidence model ( DTE ) to select comments with high credibility as evidence in a transparent and interpretable way .",2,0.90750384,60.27097997323604,27
1129,"Then we design Co-attention Self-attention networks ( CaSa ) to make the selected evidence interact with claims , which is for 1 ) training DTE to determine the optimal decision thresholds and obtain more powerful evidence ;",2,0.8984947,232.3678832211607,37
1129,and 2 ) utilizing the evidence to find the false parts in the claim .,2,0.4360573,143.047907025263,15
1129,"Experiments on two public datasets , RumourEval and PHEME , demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance , boosting the F1-score by more than 3.11 % , 2.41 % , respectively .",3,0.922231,35.347620759275586,52
1130,"We focus on the study of conversational recommendation in the context of multi-type dialogs , where the bots can proactively and naturally lead a conversation from a non-recommendation dialog ( e.g. , QA ) to a recommendation dialog , taking into account user ’s interests and feedback .",2,0.5523082,36.95479594737646,48
1130,"To facilitate the study of this task , we create a human-to-human Chinese dialog dataset DuRecDial ( about 10 k dialogs , 156 k utterances ) , where there are multiple sequential dialogs for a pair of a recommendation seeker ( user ) and a recommender ( bot ) .",2,0.860171,85.66106843888953,50
1130,"In each dialog , the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior .",2,0.5280481,211.80815644121105,25
1130,"This dataset allows us to systematically investigate different parts of the overall problem , e.g. , how to naturally lead a dialog , how to interact with users for recommendation .",3,0.45261016,64.94464859171384,31
1130,Finally we establish baseline results on DuRecDial for future studies .,3,0.8844093,232.36001643781054,11
1131,User intent classification plays a vital role in dialogue systems .,0,0.8451946,40.13872448015179,11
1131,"Since user intent may frequently change over time in many realistic scenarios , unknown ( new ) intent detection has become an essential problem , where the study has just begun .",0,0.95296705,119.03896701313549,32
1131,This paper proposes a semantic-enhanced Gaussian mixture model ( SEG ) for unknown intent detection .,1,0.8515136,88.39558401858906,16
1131,"In particular , we model utterance embeddings with a Gaussian mixture distribution and inject dynamic class semantic information into Gaussian means , which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection .",2,0.710194,51.735421043674826,36
1131,"Coupled with a density-based outlier detection algorithm , SEG achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection .",3,0.69034463,44.8160159602462,30
1131,"On top of that , we propose to integrate SEG as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance .",3,0.37391797,51.04111541881982,27
1131,"A case study on a state-of-the-art method , ReCapsNet , shows that SEG can push the classification performance to a significantly higher level .",3,0.83069557,35.9457545648186,28
1132,The curse of knowledge can impede communication between experts and laymen .,0,0.8448317,81.38759377770869,12
1132,We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases .,1,0.4667542,42.98551954184842,24
1132,"Solving this task not only simplifies the professional language , but also improves the accuracy and expertise level of laymen descriptions using simple words .",3,0.4948742,80.75442303139877,25
1132,"This is a challenging task , unaddressed in previous work , as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures .",0,0.76351744,47.41791951004086,35
1132,We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification .,2,0.5460957,17.807047858159468,22
1132,The results demonstrate a significant gap between machine and human performance .,3,0.98066837,27.154868497176025,12
1132,"We also discuss the challenges of automatic evaluation , to provide insights into future research directions .",1,0.46978363,34.74652234714299,17
1132,The dataset is publicly available at https://srhthu.github.io/expertise-style-transfer/ .,3,0.48941898,19.588939495916556,8
1133,Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions .,0,0.8035912,60.41931403589538,16
1133,"Most existing methods ignore the faithfulness between a generated text description and the original table , leading to generated information that goes beyond the content of the table .",0,0.8561635,44.279375269368686,29
1133,"In this paper , for the first time , we propose a novel Transformer-based generation framework to achieve the goal .",1,0.8358879,16.195717983609086,23
1133,The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the Transformer model .,2,0.54361504,50.288675997470385,34
1133,"Furthermore , to evaluate faithfulness , we propose a new automatic metric specialized to the table-to-text generation problem .",2,0.6295167,46.577079344183836,23
1133,We also provide detailed analysis on each component of our model in our experiments .,3,0.6096861,23.13554219128967,15
1133,Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin .,3,0.92525697,6.048581374456836,23
1134,This paper proposes Dynamic Memory Induction Networks ( DMIN ) for few-short text classification .,1,0.8185384,78.01783141498902,15
1134,"The model develops a dynamic routing mechanism over static memory , enabling it to better adapt to unseen classes , a critical capability for few-short classification .",3,0.41427714,166.86547966612238,27
1134,The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning .,3,0.56451064,48.8939857424305,22
1134,The proposed model brings forward the state-of-the-art performance significantly by 2~4 % improvement on the miniRCV1 and ODIC datasets .,3,0.91014904,40.6772192022317,26
1134,Detailed analysis is further performed to show how the proposed network achieves the new performance .,3,0.6530422,32.31137330359605,16
1135,Keyphrase generation ( KG ) aims to summarize the main ideas of a document into a set of keyphrases .,0,0.93025696,22.37629069976605,20
1135,"A new setting is recently introduced into this problem , in which , given a document , the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce .",0,0.9026598,25.39561959133267,37
1135,Previous work in this setting employs a sequential decoding process to generate keyphrases .,0,0.80979663,32.58301410628413,14
1135,"However , such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document .",0,0.84387475,68.99036294593105,20
1135,"Moreover , previous work tends to generate duplicated keyphrases , which wastes time and computing resources .",0,0.8140166,50.36422017778337,17
1135,"To overcome these limitations , we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism .",2,0.5487136,43.759811430208295,28
1135,The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set .,2,0.39739102,77.03958214210718,16
1135,Both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases .,3,0.56339365,32.24868810779862,28
1135,Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases .,3,0.87771416,20.186410917815007,22
1136,Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy .,0,0.9316442,18.939579778526006,18
1136,Existing methods have difficulties in modeling the hierarchical label structure in a global view .,0,0.85735977,62.310928126244534,15
1136,"Furthermore , they cannot make full use of the mutual interactions between the text feature space and the label space .",0,0.69139695,42.31932289453501,21
1136,"In this paper , we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies .",1,0.54683465,31.863474441577537,24
1136,"Based on the hierarchy encoder , we propose a novel end-to-end hierarchy-aware global model ( HiAGM ) with two variants .",2,0.65298444,32.96747750662658,25
1136,A multi-label attention variant ( HiAGM-LA ) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features .,2,0.6064977,68.42488799625261,30
1136,A text feature propagation model ( HiAGM-TP ) is proposed as the deductive variant that directly feeds text features into hierarchy encoders .,2,0.5034542,104.2170550397566,25
1136,"Compared with previous works , both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets .",3,0.8980683,39.63442228953792,21
1137,"Sequence-to-sequence models have lead to significant progress in keyphrase generation , but it remains unknown whether they are reliable enough to be beneficial for document retrieval .",0,0.9400436,20.17085708087571,27
1137,"This study provides empirical evidence that such models can significantly improve retrieval performance , and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models .",3,0.6310932,21.799125201579113,35
1137,"Using this framework , we point out and discuss the difficulties encountered with supplementing documents with-not present in text-keyphrases , and generalizing models across domains .",3,0.5460637,94.74964146999675,28
1137,Our code is available at https://github.com/boudinfl/ir-using-kg .,3,0.60570693,22.1383363344619,7
1138,"There has been little work on modeling the morphological well-formedness ( MWF ) of derivatives , a problem judged to be complex and difficult in linguistics .",0,0.9555464,32.504694197840664,29
1138,We present a graph auto-encoder that learns embeddings capturing information about the compatibility of affixes and stems in derivation .,2,0.48190323,37.63927983386768,20
1138,The auto-encoder models MWF in English surprisingly well by combining syntactic and semantic information with associative information from the mental lexicon .,3,0.5648605,63.49649859226802,22
1139,"We introduce the first treebank for a romanized user-generated content variety of Algerian , a North-African Arabic dialect known for its frequent usage of code-switching .",1,0.42861578,38.513381215029035,27
1139,"Made of 1500 sentences , fully annotated in morpho-syntax and Universal Dependency syntax , with full translation at both the word and the sentence levels , this treebank is made freely available .",0,0.6349213,74.2623662693146,33
1139,It is supplemented with 50 k unlabeled sentences collected from Common Crawl and web-crawled data using intensive data-mining techniques .,2,0.62821805,60.24994637214534,20
1139,Preliminary experiments demonstrate its usefulness for POS tagging and dependency parsing .,3,0.7951488,49.46336242937131,12
1139,We believe that what we present in this paper is useful beyond the low-resource language community .,3,0.90033215,27.888878710299906,17
1139,"This is the first time that enough unlabeled and annotated data is provided for an emerging user-generated content dialectal language with rich morphology and code switching , making it an challenging test-bed for most recent NLP approaches .",3,0.93949586,39.24255158616884,41
1140,"This paper introduces the Webis Gmane Email Corpus 2019 , the largest publicly available and fully preprocessed email corpus to date .",1,0.56510013,77.36685244677308,22
1140,"We crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent components using a new neural segmentation model .",2,0.8866134,52.678173882308734,25
1140,"With 96 % accuracy on 15 classes of email segments , our model achieves state-of-the-art performance while being more efficient to train than previous ones .",3,0.912419,28.231297794364636,32
1140,"All data , code , and trained models are made freely available alongside the paper .",3,0.44040564,82.54880405758354,16
1141,The patterns in which the syntax of different languages converges and diverges are often used to inform work on cross-lingual transfer .,0,0.8643237,20.42618323382273,22
1141,"Nevertheless , little empirical work has been done on quantifying the prevalence of different syntactic divergences across language pairs .",0,0.92720234,19.220335454764808,20
1141,"We propose a framework for extracting divergence patterns for any language pair from a parallel corpus , building on Universal Dependencies .",1,0.42372674,65.64580634884538,22
1141,"We show that our framework provides a detailed picture of cross-language divergences , generalizes previous approaches , and lends itself to full automation .",3,0.88286704,69.72598890197135,24
1141,"We further present a novel dataset , a manually word-aligned subset of the Parallel UD corpus in five languages , and use it to perform a detailed corpus study .",2,0.5601838,70.42199436355148,30
1141,We demonstrate the usefulness of the resulting analysis by showing that it can help account for performance patterns of a cross-lingual parser .,3,0.84096175,26.513212767509575,23
1142,"Recently research has started focusing on avoiding undesired effects that come with content moderation , such as censorship and overblocking , when dealing with hatred online .",0,0.90776056,51.31415218373604,27
1142,The core idea is to directly intervene in the discussion with textual responses that are meant to counter the hate content and prevent it from further spreading .,0,0.4945778,31.031388594509785,28
1142,"Accordingly , automation strategies , such as natural language generation , are beginning to be investigated .",0,0.88916147,100.78609431673128,17
1142,"Still , they suffer from the lack of sufficient amount of quality data and tend to produce generic / repetitive responses .",0,0.9149008,50.14776635971477,22
1142,"Being aware of the aforementioned limitations , we present a study on how to collect responses to hate effectively , employing large scale unsupervised language models such as GPT-2 for the generation of silver data , and the best annotation strategies / neural architectures that can be used for data filtering before expert validation / post-editing .",1,0.56389105,72.20240132888478,59
1143,"In recent years , a series of Transformer-based models unlocked major improvements in general natural language understanding ( NLU ) tasks .",0,0.9541669,42.035456717328906,24
1143,"Such a fast pace of research would not be possible without general NLU benchmarks , which allow for a fair comparison of the proposed methods .",3,0.87917465,44.448196970363355,26
1143,"However , such benchmarks are available only for a handful of languages .",0,0.9049802,21.691516178682818,13
1143,"To alleviate this issue , we introduce a comprehensive multi-task benchmark for the Polish language understanding , accompanied by an online leaderboard .",1,0.41805768,47.10360416900252,23
1143,"It consists of a diverse set of tasks , adopted from existing datasets for named entity recognition , question-answering , textual entailment , and others .",0,0.4235729,42.92641585015971,28
1143,"We also introduce a new sentiment analysis task for the e-commerce domain , named Allegro Reviews ( AR ) .",2,0.4590492,68.37733359126891,20
1143,"To ensure a common evaluation scheme and promote models that generalize to different NLU tasks , the benchmark includes datasets from varying domains and applications .",2,0.44478717,65.93379186034284,26
1143,"Additionally , we release HerBERT , a Transformer-based model trained specifically for the Polish language , which has the best average performance and obtains the best results for three out of nine tasks .",2,0.47819138,30.077413255641957,36
1143,"Finally , we provide an extensive evaluation , including several standard baselines and recently proposed , multilingual Transformer-based models .",3,0.5858642,57.35257262860799,22
1144,Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis .,0,0.94664836,75.53718225754616,20
1144,"Yet , manually curated lexicons are only available for a handful of languages , leaving most languages of the world without such a precious resource for downstream applications .",0,0.9264278,41.00947152796847,29
1144,"Even worse , their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature .",0,0.87143797,46.40704221350068,24
1144,"In order to break this bottleneck , we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language .",1,0.56904787,87.59258115296439,24
1144,"Our approach requires nothing but a source language emotion lexicon , a bilingual word translation model , and a target language embedding model .",2,0.57199925,66.2625561232477,24
1144,"Fulfilling these requirements for 91 languages , we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each .",3,0.6887604,75.40536005488175,29
1144,"We evaluated the automatically generated lexicons against human judgment from 26 datasets , spanning 12 typologically diverse languages , and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables .",3,0.7986048,37.911803034537435,52
1144,Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901 .,4,0.40414208,75.49713981851134,12
1145,Reliably evaluating Machine Translation ( MT ) through automated metrics is a long-standing problem .,0,0.97223705,60.50993230059294,15
1145,One of the main challenges is the fact that multiple outputs can be equally valid .,0,0.89945763,14.210182675638347,16
1145,"Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings , and the use of multiple references .",0,0.7893573,109.76558996638877,25
1145,The latter has been shown to significantly improve the performance of evaluation metrics .,0,0.60860676,12.227743274324126,14
1145,"However , collecting multiple references is expensive and in practice a single reference is generally used .",0,0.8916575,58.66171839836355,17
1145,"In this paper , we propose an alternative approach : instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these : ( i ) as surrogates to reference translations ;",1,0.71657246,168.30931893050234,42
1145,( ii ) to obtain a quantification of translation variability to either complement existing metric scores or ( iii ) replace references altogether .,2,0.33876756,194.13600672694116,24
1145,We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15 % .,3,0.9405252,68.62162062736839,29
1146,We propose approaches to Quality Estimation ( QE ) for Machine Translation that explore both text and visual modalities for Multimodal QE .,1,0.6194978,40.897487474419336,23
1146,We compare various multimodality integration and fusion strategies .,2,0.5457184,137.68306513021602,9
1146,"For both sentence-level and document-level predictions , we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality .",3,0.95525193,20.31765284785594,36
1147,Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks .,0,0.9187123,68.44077944802764,21
1147,"They struggle to achieve human-like thinking , however , because they lack the skill of iterative reasoning upon knowledge .",0,0.884309,72.9181193002462,20
1147,"To expose this problem in a new light , we introduce a challenge on learning from small data , PuzzLing Machines , which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students .",1,0.3644447,72.49992652420535,36
1147,These puzzles are carefully designed to contain only the minimal amount of parallel text necessary to deduce the form of unseen expressions .,0,0.694012,53.56831453286675,23
1147,"Solving them does not require external information ( e.g. , knowledge bases , visual signals ) or linguistic expertise , but meta-linguistic awareness and deductive skills .",0,0.7100208,95.42294773239459,27
1147,Our challenge contains around 100 puzzles covering a wide range of linguistic phenomena from 81 languages .,2,0.47544748,130.9293193858294,17
1147,"We show that both simple statistical algorithms and state-of-the-art deep neural models perform inadequately on this challenge , as expected .",3,0.9482355,25.34615178036641,27
1147,"We hope that this benchmark , available at https://ukplab.github.io/PuzzLing-Machines/, inspires further efforts towards a new paradigm in NLP — one that is grounded in human-like reasoning and understanding .",3,0.9077377,27.697006768336994,29
1148,This paper presents a new challenging information extraction task in the domain of materials science .,1,0.8410385,27.6936325950916,16
1148,"We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications , such as involved materials and measurement conditions .",2,0.41894007,160.3351346628669,28
1148,"With this paper , we publish our annotation guidelines , as well as our SOFC-Exp corpus consisting of 45 open-access scholarly articles annotated by domain experts .",1,0.5642042,85.09741292228765,29
1148,A corpus and an inter-annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality .,3,0.88724804,50.229301841202194,27
1148,We also present strong neural-network based models for a variety of tasks that can be addressed on the basis of our new data set .,3,0.6258755,23.377804407196646,27
1148,"On all tasks , using BERT embeddings leads to large performance gains , but with increasing task complexity , adding a recurrent neural network on top seems beneficial .",3,0.933144,50.07543598479995,29
1148,"Our models will serve as competitive baselines in future work , and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions .",3,0.95993865,60.37317776097381,29
1149,"We introduce TECHQA , a domain-adaptation question answering dataset for the technical support domain .",2,0.5049397,81.33013848799428,15
1149,The TECHQA corpus highlights two real-world issues from the automated customer support domain .,0,0.48693445,138.34438929320908,14
1149,"First , it contains actual questions posed by users on a technical forum , rather than questions generated specifically for a competition or a task .",2,0.37379363,88.8736764535973,26
1149,"Second , it has a real-world size – 600 training , 310 dev , and 490 evaluation question / answer pairs – thus reflecting the cost of creating large labeled datasets with actual data .",3,0.4887285,130.33221228962327,35
1149,"Hence , TECHQA is meant to stimulate research in domain adaptation rather than as a resource to build QA systems from scratch .",3,0.51182437,40.95456898933683,23
1149,TECHQA was obtained by crawling the IBMDeveloper and DeveloperWorks forums for questions with accepted answers provided in an IBM Technote — a technical document that addresses a specific technical issue .,2,0.7950799,131.70181249314865,31
1149,"We also release a collection of the 801,998 Technotes available on the web as of April 4 , 2019 as a companion resource that can be used to learn representations of the IT domain language .",2,0.5041054,73.19070331192003,36
1150,We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection .,1,0.47801894,17.980034736311197,17
1150,"The former occurs when an utterance is sarcastic from the perspective of its author , while the latter occurs when the utterance is interpreted as sarcastic by the audience .",0,0.8175102,12.171162841685781,30
1150,We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the iSarcasm dataset of tweets labeled for sarcasm directly by their authors .,3,0.42045596,50.94844638170203,27
1150,"Examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets , which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far .",3,0.97671354,36.626111447793704,44
1150,"By providing the iSarcasm dataset , we aim to encourage future NLP research to develop methods for detecting sarcasm in text as intended by the authors of the text , not as labeled under assumptions that we demonstrate to be sub-optimal .",3,0.5025339,34.01298011756542,42
1151,We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph .,3,0.32531407,28.519949329911075,28
1151,"At each time step , our model performs multiple rounds of attention , reasoning , and composition that aim to answer two critical questions : ( 1 ) which part of the input sequence to abstract ;",2,0.6434253,70.82607354887881,37
1151,and ( 2 ) where in the output graph to construct the new concept .,2,0.6050632,210.48920639256215,15
1151,We show that the answers to these two questions are mutually causalities .,3,0.7764484,54.65655042927843,13
1151,"We design a model based on iterative inference that helps achieve better answers in both perspectives , leading to greatly improved parsing accuracy .",2,0.4695144,68.34581196589048,24
1151,Our experimental results significantly outperform all previously reported Smatch scores by large margins .,3,0.9779618,52.403526285235685,14
1151,"Remarkably , without the help of any large-scale pre-trained language model ( e.g. , BERT ) , our model already surpasses previous state-of-the-art using BERT .",3,0.9001829,14.334516554275083,31
1151,"With the help of BERT , we can push the state-of-the-art results to 80.2 % on LDC2017T10 ( AMR 2.0 ) and 75.4 % on LDC2014T12 ( AMR 1.0 ) .",3,0.912281,10.2584805519198,36
1152,"Multi-document summarization ( MDS ) aims to compress the content in large document collections into short summaries and has important applications in story clustering for newsfeeds , presentation of search results , and timeline generation .",0,0.9587498,45.68537831785139,36
1152,"However , there is a lack of datasets that realistically address such use cases at a scale large enough for training supervised models for this task .",0,0.93783987,33.73047937812088,27
1152,This work presents a new dataset for MDS that is large both in the total number of document clusters and in the size of individual clusters .,1,0.5948052,20.675496615368868,27
1152,"We build this dataset by leveraging the Wikipedia Current Events Portal ( WCEP ) , which provides concise and neutral human-written summaries of news events , with links to external source articles .",2,0.8442173,56.39680618593841,33
1152,We also automatically extend these source articles by looking for related articles in the Common Crawl archive .,2,0.7638383,91.84956649830899,18
1152,We provide a quantitative analysis of the dataset and empirical results for several state-of-the-art MDS techniques .,3,0.56749,13.458399570848087,22
1153,"Cross-lingual summarization aims at summarizing a document in one language ( e.g. , Chinese ) into another language ( e.g. , English ) .",0,0.91881186,10.629715428590666,24
1153,"In this paper , we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary .",1,0.91971314,22.377363045070656,23
1153,"We first attend to some words in the source text , then translate them into the target language , and summarize to get the final summary .",2,0.85147524,43.50409377389274,27
1153,"Specifically , we first employ the encoder-decoder attention distribution to attend to the source words .",2,0.83907306,23.336895820199953,16
1153,"Second , we present three strategies to acquire the translation probability , which helps obtain the translation candidates for each source word .",2,0.77390087,55.3911874244071,23
1153,"Finally , each summary word is generated either from the neural distribution or from the translation candidates of source words .",2,0.7002931,89.79651768420217,21
1153,"Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines , achieving comparable performance with the state-of-the-art .",3,0.93932533,6.547786316679094,35
1154,Previous work on automatic news timeline summarization ( TLS ) leaves an unclear picture about how this task can generally be approached and how well it is currently solved .,0,0.9607873,90.29708364165533,30
1154,"This is mostly due to the focus on individual subtasks , such as date selection and date summarization , and to the previous lack of appropriate evaluation metrics for the full TLS task .",0,0.7275915,46.233150340289995,34
1154,"In this paper , we compare different TLS strategies using appropriate evaluation frameworks , and propose a simple and effective combination of methods that improves over the stateof-the-art on all tested benchmarks .",1,0.818096,33.99911599580137,37
1154,"For a more robust evaluation , we also present a new TLS dataset , which is larger and spans longer time periods than previous datasets .",2,0.53911424,34.791908047887716,26
1155,Most studies on abstractive summarization report ROUGE scores between system and reference summaries .,0,0.5125145,46.20930306256212,14
1155,"However , we have a concern about the truthfulness of generated summaries : whether all facts of a generated summary are mentioned in the source text .",0,0.863597,43.491037153650694,27
1155,This paper explores improving the truthfulness in headline generation on two popular datasets .,1,0.841099,86.31978533330744,14
1155,"Analyzing headlines generated by the state-of-the-art encoder-decoder model , we show that the model sometimes generates untruthful headlines .",3,0.8115545,12.989446503997767,24
1155,We conjecture that one of the reasons lies in untruthful supervision data used for training the model .,3,0.77344084,33.13548059274363,18
1155,"In order to quantify the truthfulness of article-headline pairs , we consider the textual entailment of whether an article entails its headline .",2,0.61075443,38.09676337642164,24
1155,"After confirming quite a few untruthful instances in the datasets , this study hypothesizes that removing untruthful instances from the supervision data may remedy the problem of the untruthful behaviors of the model .",1,0.41216677,25.454564647540224,34
1155,"Building a binary classifier that predicts an entailment relation between an article and its headline , we filter out untruthful instances from the supervision data .",2,0.7716364,34.370988419852885,26
1155,Experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines .,3,0.98169816,39.45871992263658,33
1156,"We study unsupervised multi-document summarization evaluation metrics , which require neither human-written reference summaries nor human annotations ( e.g .",2,0.59600693,31.7809809246954,20
1156,"preferences , ratings , etc. ) .",0,0.37528616,220.7796838149422,7
1156,"We propose SUPERT , which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary , i.e .",2,0.48232156,51.10799216627344,24
1156,"selected salient sentences from the source documents , using contextualized embeddings and soft token alignment techniques .",2,0.8686804,69.27237304011119,17
1156,"Compared to the state-of-the-art unsupervised evaluation metrics , SUPERT correlates better with human ratings by 18-39 % .",3,0.9317855,26.052779077883457,22
1156,"Furthermore , we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer , yielding favorable performance compared to the state-of-the-art unsupervised summarizers .",3,0.4712716,30.206415989701625,33
1156,All source code is available at https://github.com/yg211/acl20-ref-free-eval .,3,0.48663747,24.09992615716822,8
1157,"Copy module has been widely equipped in the recent abstractive summarization models , which facilitates the decoder to extract words from the source into the summary .",0,0.76155716,51.72476497291765,27
1157,"Generally , the encoder-decoder attention is served as the copy distribution , while how to guarantee that important words in the source are copied remains a challenge .",0,0.8594303,44.239487688576496,28
1157,"In this work , we propose a Transformer-based model to enhance the copy mechanism .",1,0.8105637,20.492996405380527,17
1157,"Specifically , we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the Transformer .",2,0.8093076,33.46689354653449,28
1157,We use the centrality of each source word to guide the copy process explicitly .,2,0.79727525,56.148093413518495,15
1157,Experimental results show that the self-attention graph provides useful guidance for the copy distribution .,3,0.97362727,28.766744869022787,15
1157,Our proposed models significantly outperform the baseline methods on the CNN / Daily Mail dataset and the Gigaword dataset .,3,0.913258,18.092656605952776,20
1158,Open Domain dialog system evaluation is one of the most important challenges in dialog research .,0,0.9116275,44.03719670849785,16
1158,"Existing automatic evaluation metrics , such as BLEU are mostly reference-based .",0,0.8452916,38.63270610000048,14
1158,They calculate the difference between the generated response and a limited number of available references .,0,0.36728802,41.04515507152721,16
1158,"Likert-score based self-reported user rating is widely adopted by social conversational systems , such as Amazon Alexa Prize chatbots .",0,0.8111586,72.37720096334023,21
1158,"However , self-reported user rating suffers from bias and variance among different users .",0,0.8397928,61.91045269763317,14
1158,"To alleviate this problem , we formulate dialog evaluation as a comparison task .",2,0.42149374,71.81779249838944,14
1158,We also propose an automatic evaluation model CMADE ( Comparison Model for Automatic Dialog Evaluation ) that automatically cleans self-reported user ratings as it trains on them .,2,0.58008873,114.78782042163293,28
1158,"Specifically , we first use a self-supervised method to learn better dialog feature representation , and then use KNN and Shapley to remove confusing samples .",2,0.8791297,53.815286158583575,26
1158,Our experiments show that CMADE achieves 89.2 % accuracy in the dialog comparison task .,3,0.97106576,68.84218896113309,15
1159,"Human conversations contain many types of information , e.g. , knowledge , common sense , and language habits .",0,0.9327607,70.45655648062986,19
1159,"In this paper , we propose a conversational word embedding method named PR-Embedding , which utilizes the conversation pairs < post , reply > to learn word embedding .",1,0.8138889,48.27083163827062,31
1159,"Different from previous works , PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply .",2,0.40640038,70.509826750688,25
1159,"To catch the information among the pair , we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window , then train the embedding on word-level and sentence-level .",2,0.8733845,40.82660273034583,37
1159,We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems .,2,0.5775669,24.4238816309341,18
1159,The experiment results show that PR-Embedding can improve the quality of the selected response .,3,0.9874241,34.454068018159425,17
1160,"In this paper , we explore the slot tagging with only a few labeled support sentences ( a.k.a .",1,0.55926996,52.24341123456904,19
1160,few-shot ) .,0,0.40106735,130.18786209685837,3
1160,Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels .,0,0.85973907,74.90486584732359,26
1160,"But it is hard to apply previously learned label dependencies to an unseen domain , due to the discrepancy of label sets .",0,0.9020492,69.9687547963303,23
1160,"To tackle this , we introduce a collapsed dependency transfer mechanism into the conditional random field ( CRF ) to transfer abstract label dependency patterns as transition scores .",2,0.7564483,187.3697995521165,29
1160,"In the few-shot setting , the emission score of CRF can be calculated as a word ’s similarity to the representation of each label .",3,0.38675904,53.29065229642001,25
1160,"To calculate such similarity , we propose a Label-enhanced Task-Adaptive Projection Network ( L-TapNet ) based on the state-of-the-art few-shot classification model – TapNet , by leveraging label name semantics in representing labels .",2,0.7774006,51.74208220267473,42
1160,Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting .,3,0.96417326,12.882616927505213,23
1161,"Deep reinforcement learning is a promising approach to training a dialog manager , but current methods struggle with the large state and action spaces of multi-domain dialog systems .",0,0.9210334,35.45849832323063,29
1161,"Building upon Deep Q-learning from Demonstrations ( DQfD ) , an algorithm that scores highly in difficult Atari games , we leverage dialog data to guide the agent to successfully respond to a user ’s requests .",2,0.7345235,117.72249261003495,37
1161,"We make progressively fewer assumptions about the data needed , using labeled , reduced-labeled , and even unlabeled data to train expert demonstrators .",2,0.45251402,117.6245226334001,26
1161,"We introduce Reinforced Fine-tune Learning , an extension to DQfD , enabling us to overcome the domain gap between the datasets and the environment .",2,0.46426478,99.0351383511987,25
1161,"Experiments in a challenging multi-domain dialog system framework validate our approaches , and get high success rates even when trained on out-of-domain data .",3,0.85557705,33.60892736502504,25
1162,Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques .,0,0.93442416,21.19225607884743,25
1162,"Given a context , current systems are able to yield a relevant and fluent response , but sometimes make logical mistakes because of weak reasoning capabilities .",0,0.9023011,85.51308743070368,27
1162,"To facilitate the conversation reasoning research , we introduce MuTual , a novel dataset for Multi-Turn dialogue Reasoning , consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams .",2,0.5762471,67.05502006034685,34
1162,"Compared to previous benchmarks for non-task oriented dialogue systems , MuTual is much more challenging since it requires a model that be able to handle various reasoning problems .",0,0.6238227,50.688460459107084,29
1162,"Empirical results show that state-of-the-art methods only reach 71 % , which is far behind human performance of 94 % , indicating that there is ample room for improving reasoning ability .",3,0.92355156,17.899474359883445,38
1163,"Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems , the majority of current work simply focus on mimicking human-like responses , leaving understudied the aspects of modeling understanding between interlocutors .",0,0.8364385,35.331881639233494,37
1163,"The research in cognitive science , instead , suggests that understanding is an essential signal for a high-quality chit-chat conversation .",0,0.6911245,53.21023651092779,21
1163,"Motivated by this , we propose Pˆ2 Bot , a transmitter-receiver based framework with the aim of explicitly modeling understanding .",2,0.38344336,69.1412295912195,23
1163,"Specifically , Pˆ2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation .",3,0.6016894,228.16443737066353,17
1163,"Experiments on a large public dataset , Persona-Chat , demonstrate the effectiveness of our approach , with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations .",3,0.86064905,17.885085641948212,40
1164,"Most previous studies on bridging anaphora resolution ( Poesio et al. , 2004 ; Hou et al. , 2013 b ; Hou , 2018a ) use the pairwise model to tackle the problem and assume that the gold mention information is given .",0,0.64992714,54.590600571392095,43
1164,"In this paper , we cast bridging anaphora resolution as question answering based on context .",1,0.84219354,65.60891123872463,16
1164,This allows us to find the antecedent for a given anaphor without knowing any gold mention information ( except the anaphor itself ) .,2,0.36576158,67.58819013799311,24
1164,"We present a question answering framework ( BARQA ) for this task , which leverages the power of transfer learning .",1,0.4437656,41.88370705781287,21
1164,"Furthermore , we propose a novel method to generate a large amount of “ quasi-bridging ” training data .",3,0.49250868,23.000384230839185,20
1164,"We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora ( ISNotes ( Markert et al. , 2012 ) and BASHI ( Ro ̈siger , 2018 ) ) .",3,0.8670282,34.299163429031154,56
1165,"Recent dialogue coherence models use the coherence features designed for monologue texts , e.g .",0,0.85524976,73.99898665292065,15
1165,"nominal entities , to represent utterances and then explicitly augment them with dialogue-relevant features , e.g. , dialogue act labels .",2,0.54026127,109.42642641244011,22
1165,"It indicates two drawbacks , ( a ) semantics of utterances are limited to entity mentions , and ( b ) the performance of coherence models strongly relies on the quality of the input dialogue act labels .",3,0.5625634,63.616814072637794,38
1165,We address these issues by introducing a novel approach to dialogue coherence assessment .,1,0.51638466,23.452454897123502,14
1165,We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment .,2,0.82668185,29.33029970691098,23
1165,Our approach alleviates the need for explicit dialogue act labels during evaluation .,3,0.81390816,56.61821198204606,13
1165,"The results of our experiments show that our model substantially ( more than 20 accuracy points ) outperforms its strong competitors on the DailyDialogue corpus , and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence .",3,0.9678572,58.42073172582942,43
1165,We release our source code .,2,0.48707867,38.59102230594764,6
1166,We propose a graph-based method to tackle the dependency tree linearization task .,2,0.46880892,26.434377885897202,14
1166,"We formulate the task as a Traveling Salesman Problem ( TSP ) , and use a biaffine attention model to calculate the edge costs .",2,0.867846,45.57912841678329,25
1166,We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree .,2,0.62347734,72.24186750204748,20
1166,"We then design a transition system as post-processing , inspired by non-projective transition-based parsing , to obtain non-projective sentences .",2,0.8263681,64.64550726565682,22
1166,Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding .,3,0.8732711,12.317959637318765,23
1167,"This paper proposes the problem of Deep Question Generation ( DQG ) , which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage .",1,0.71138304,16.919009379881345,32
1167,"In order to capture the global structure of the document and facilitate reasoning , we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN ( Att-GGNN ) .",2,0.70890886,24.69731083297049,49
1167,"Afterward , we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding .",2,0.7528056,45.37384885256143,21
1167,"On the HotpotQA deep-question centric dataset , our model greatly improves performance over questions requiring reasoning over multiple facts , leading to state-of-the-art performance .",3,0.8769354,37.27385316378335,31
1167,The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation .,3,0.54784447,13.899208337585456,8
1168,Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction .,0,0.92114687,16.553249895718547,14
1168,"However , few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities .",0,0.8726741,57.789138470528464,25
1168,"In this work , we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework ( CasRel ) derived from a principled problem formulation .",1,0.7808841,90.09846515627044,34
1168,"Instead of treating relations as discrete labels as in previous works , our new framework models relations as functions that map subjects to objects in a sentence , which naturally handles the overlapping problem .",2,0.5544434,55.93738076066482,35
1168,"Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder , showing the power of the new tagging framework .",3,0.9379306,28.29150892949364,37
1168,"It enjoys further performance boost when employing a pre-trained BERT encoder , outperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score on two public datasets NYT and WebNLG , respectively .",3,0.8752526,39.88000926371771,36
1168,In-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios .,3,0.9292757,30.162135981187443,23
1168,The source code and data are released online .,3,0.50096893,15.498191992954562,9
1169,Information Extraction ( IE ) from scientific texts can be used to guide readers to the central information in scientific documents .,0,0.91993695,41.96657165038892,22
1169,"But narrow IE systems extract only a fraction of the information captured , and Open IE systems do not perform well on the long and complex sentences encountered in scientific texts .",0,0.83922446,60.63288590625133,32
1169,"In this work we combine the output of both types of systems to achieve Semi-Open Relation Extraction , a new task that we explore in the Biology domain .",1,0.4938044,41.31351711420893,29
1169,"First , we present the Focused Open Biological Information Extraction ( FOBIE ) dataset and use FOBIE to train a state-of-the-art narrow scientific IE system to extract trade-off relations and arguments that are central to biology texts .",2,0.6630617,36.443807404194736,46
1169,We then run both the narrow IE system and a state-of-the-art Open IE system on a corpus of 10 K open-access scientific biological texts .,2,0.8581054,56.07331133290673,30
1169,We show that a significant amount ( 65 % ) of erroneous and uninformative Open IE extractions can be filtered using narrow IE extractions .,3,0.97381413,82.41696689306168,25
1169,"Furthermore , we show that the retained extractions are significantly more often informative to a reader .",3,0.97696394,49.85785771768138,17
1170,Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs — as these systems often process user-generated text or follow an error-prone upstream component .,0,0.83929515,78.59790724507322,32
1170,"Our data augmentation method trains a neural model using a mixture of clean and noisy samples , whereas our stability training algorithm encourages the model to create a noise-invariant latent representation .",2,0.6858507,43.56470951956654,32
1170,We employ a vanilla noise model at training time .,2,0.7951702,106.60097469624102,10
1170,"For evaluation , we use both the original data and its variants perturbed with real OCR errors and misspellings .",2,0.6560333,74.4434309062295,20
1170,"Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models , preserving accuracy on the original input .",3,0.8969304,62.406734302740844,29
1170,We make our code and data publicly available for the research community .,3,0.507922,8.261976283011277,13
1171,Named Entity Recognition ( NER ) performance often degrades rapidly when applied to target domains that differ from the texts observed during training .,0,0.9464297,38.046177564397425,24
1171,"When in-domain labelled data is available , transfer learning techniques can be used to adapt existing NER models to the target domain .",0,0.5847544,25.57741432566054,24
1171,This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision .,1,0.7532111,33.14848673259631,22
1171,The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain .,2,0.56599474,32.56643280147829,19
1171,These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions .,2,0.6654678,55.54885676792157,23
1171,A sequence labelling model can finally be trained on the basis of this unified annotation .,3,0.46051073,71.76302076747007,16
1171,We evaluate the approach on two English datasets ( CoNLL 2003 and news articles from Reuters and Bloomberg ) and demonstrate an improvement of about 7 percentage points in entity-level F1 scores compared to an out-of-domain neural NER model .,3,0.6667288,19.79732971515793,43
1172,"Despite the recent progress , little is known about the features captured by state-of-the-art neural relation extraction ( RE ) models .",0,0.950362,19.150136563951182,28
1172,"Common methods encode the source sentence , conditioned on the entity mentions , before classifying the relation .",0,0.6974719,119.32328731820827,18
1172,"However , the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder .",0,0.8094279,23.927206643272402,27
1172,"We introduce 14 probing tasks targeting linguistic properties relevant to RE , and we use them to study representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets , TACRED and SemEval 2010 Task 8 .",2,0.7711354,86.6774680486248,43
1172,We find that the bias induced by the architecture and the inclusion of linguistic features are clearly expressed in the probing task performance .,3,0.9791181,39.27943215004437,24
1172,"For example , adding contextualized word representations greatly increases performance on probing tasks with a focus on named entity and part-of-speech information , and yields better results in RE .",3,0.87836474,52.37449833173255,31
1172,"In contrast , entity masking improves RE , but considerably lowers performance on entity type related probing tasks .",3,0.9299457,241.92803560034898,19
1173,Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities .,0,0.8299467,30.133384838566457,24
1173,"However , effective aggregation of relevant information in the document remains a challenging research question .",0,0.9249244,48.3822718680277,16
1173,"Existing approaches construct static document-level graphs based on syntactic trees , co-references or heuristics from the unstructured text to model the dependencies .",0,0.78285015,37.61181176857567,24
1173,"Unlike previous methods that may not be able to capture rich non-local interactions for inference , we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph .",2,0.6535086,43.511987722189865,38
1173,"We further develop a refinement strategy , which enables the model to incrementally aggregate relevant information for multi-hop reasoning .",3,0.614394,44.984663516700955,20
1173,"Specifically , our model achieves an F1 score of 59.05 on a large-scale document-level dataset ( DocRED ) , significantly improving over the previous results , and also yields new state-of-the-art results on the CDR and GDA dataset .",3,0.93876547,21.3305867487919,47
1173,"Furthermore , extensive analyses show that the model is able to discover more accurate inter-sentence relations .",3,0.95603657,25.12437095164043,17
1174,"TACRED is one of the largest , most widely used crowdsourced datasets in Relation Extraction ( RE ) .",0,0.89217126,30.028410651321195,19
1174,"But , even with recent advances in unsupervised pre-training and knowledge enhanced neural RE , models still show a high error rate .",0,0.8150565,45.56569889358464,23
1174,"To answer these questions , we first validate the most challenging 5 K examples in the development and test sets using trained annotators .",2,0.60391915,68.39807342276282,24
1174,"We find that label errors account for 8 % absolute F1 test error , and that more than 50 % of the examples need to be relabeled .",3,0.9690944,45.7342016362442,28
1174,On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1 .,3,0.928441,26.593233788451318,21
1174,"After validation , we analyze misclassifications on the challenging instances , categorize them into linguistically motivated error groups , and verify the resulting error hypotheses on three state-of-the-art RE models .",2,0.69239914,64.11506375928768,37
1174,We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked .,3,0.96425116,52.5423805433191,32
1175,"In this paper , we propose a new task of machine translation ( MT ) , which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary .",1,0.8765779,38.15915086940065,34
1175,"Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary , we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora , while is independent on parallel sentences .",1,0.6161165,56.1783020419391,49
1175,We propose anchored training ( AT ) to tackle the task .,2,0.45975325,279.63623806218294,12
1175,AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language .,0,0.3919158,39.6492798175394,20
1175,"Experiments on various language pairs show that our approaches are significantly better than various baselines , including dictionary-based word-by-word translation , dictionary-supervised cross-lingual word embedding transformation , and unsupervised MT .",3,0.9166912,19.715671778737743,39
1175,"On distant language pairs that are hard for unsupervised MT to perform well , AT performs remarkably better , achieving performances comparable to supervised SMT trained on more than 4 M parallel sentences .",3,0.9138424,69.45713374575676,34
1176,"This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations , in a comparable way a human translator employs fuzzy matches .",1,0.8580521,76.62679496540653,29
1176,"In particular , we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches , we also extend the similarity to include semantically related translations retrieved using sentence distributed representations .",3,0.647753,110.18716122150538,42
1176,We show that translations based on fuzzy matching provide the model with “ copy ” information while translations based on embedding similarities tend to extend the translation “ context ” .,3,0.9509895,63.00032210601576,31
1176,"Results indicate that the effect from both similar sentences are adding up to further boost accuracy , combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs .",3,0.9883487,178.87478554587724,32
1176,Tests on multiple data sets and domains show consistent accuracy improvements .,3,0.93297845,49.98257774023527,12
1176,"To foster research around these techniques , we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation .",3,0.39532703,68.15604464975138,22
1177,We explore the suitability of self-attention models for character-level neural machine translation .,1,0.64364374,15.749888956969256,15
1177,"We test the standard transformer model , as well as a novel variant in which the encoder block combines information from nearby characters using convolutions .",2,0.77335817,79.26056732784319,26
1177,"We perform extensive experiments on WMT and UN datasets , testing both bilingual and multilingual translation to English using up to three input languages ( French , Spanish , and Chinese ) .",2,0.782518,49.09871787734362,33
1177,Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments .,3,0.9133378,40.02931858262661,25
1178,Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation ( MT ) models .,0,0.95001286,34.17632802057485,28
1178,"Although unnecessary for training neural MT models , word alignment still plays an important role in interactive applications of neural machine translation , such as annotation transfer and lexicon injection .",0,0.87448525,79.65730193556152,31
1178,"While statistical MT methods have been replaced by neural approaches with superior performance , the twenty-year-old GIZA ++ toolkit remains a key component of state-of-the-art word alignment systems .",0,0.9054779,38.01438822245637,38
1178,Prior work on neural word alignment has only been able to outperform GIZA ++ by using its output during training .,0,0.82975423,97.5226638485449,21
1178,We present the first end-to-end neural word alignment method that consistently outperforms GIZA ++ on three data sets .,1,0.40009153,33.74638210628327,21
1178,Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality .,2,0.5033459,29.451350968329404,33
1179,"Most neural machine translation models only rely on pairs of parallel sentences , assuming syntactic information is automatically learned by an attention mechanism .",0,0.8228991,40.33205870722366,24
1179,"In this work , we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel , parameter-free , dependency-aware self-attention mechanism that improves its translation quality , especially for long sentences and in low-resource scenarios .",1,0.7247733,25.717903304979068,46
1179,"We show the efficacy of each approach on WMT English-German and English-Turkish , and WAT English-Japanese translation tasks .",3,0.82333905,25.233501612203124,24
1180,"Massively multilingual models for neural machine translation ( NMT ) are theoretically attractive , but often underperform bilingual models and deliver poor zero-shot translations .",0,0.9550261,39.54454542239371,25
1180,"In this paper , we explore ways to improve them .",1,0.92472726,40.30335581187026,11
1180,"We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics , and overcome this bottleneck via language-specific components and deepening NMT architectures .",3,0.8551617,77.76842083576204,31
1180,We identify the off-target translation issue ( i.e .,3,0.7419684,62.1611807644792,9
1180,"translating into a wrong target language ) as the major source of the inferior zero-shot performance , and propose random online backtranslation to enforce the translation of unseen training language pairs .",2,0.5265937,116.61108938481696,32
1180,"Experiments on OPUS-100 ( a novel multilingual dataset with 100 languages ) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings , and improves zero-shot performance by ~10 BLEU , approaching conventional pivot-based methods .",3,0.87261903,24.821186390938916,53
1181,The performance of neural machine translation systems is commonly evaluated in terms of BLEU .,0,0.8784792,11.147468639740213,15
1181,"However , due to its reliance on target language properties and generation , the BLEU metric does not allow an assessment of which translation directions are more difficult to model .",0,0.51800215,40.29747549076458,31
1181,"In this paper , we propose cross-mutual information ( XMI ) : an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models .",1,0.8638719,31.864355687702584,34
1181,XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task .,3,0.65664804,34.2905208394754,32
1181,We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems .,1,0.6094179,25.422286757472015,19
1181,Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty .,3,0.67219996,19.857545117687415,11
1182,Multilingual neural machine translation ( NMT ) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages .,0,0.963702,28.800357553704654,24
1182,"However , the traditional multilingual model fails to capture the diversity and specificity of different languages , resulting in inferior performance compared with individual models that are sufficiently trained .",0,0.8367728,29.720119641171557,30
1182,"In this paper , we incorporate a language-aware interlingua into the Encoder-Decoder architecture .",1,0.6763753,20.56975087136964,18
1182,"The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages , while still allowing for language-specific specialization of a particular language-pair .",3,0.6217221,30.546205592153846,35
1182,Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models .,3,0.95100695,11.621876230686556,28
1183,Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity .,0,0.86048096,12.277793625933596,23
1183,"In this paper , we concern ourselves with reference-free machine translation ( MT ) evaluation where we directly compare source texts to ( sometimes low-quality ) system translations , which represents a natural adversarial setup for multilingual encoders .",1,0.673311,62.50870840400207,41
1183,Reference-free evaluation holds the promise of web-scale comparison of MT systems .,0,0.51377934,66.65843685451307,13
1183,We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER .,2,0.6500582,14.35371589138861,26
1183,"We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations , namely , ( a ) a semantic mismatch between representations of mutual translations and , more prominently , ( b ) the inability to punish “ translationese ” , i.e. , low-quality literal translations .",3,0.93286556,80.75688749841717,57
1183,We propose two partial remedies : ( 1 ) post-hoc re-alignment of the vector spaces and ( 2 ) coupling of semantic-similarity based metrics with target-side language modeling .,3,0.585347,47.193262856683376,31
1183,"In segment-level MT evaluation , our best metric surpasses reference-based BLEU by 5.7 correlation points .",3,0.9356342,52.329426986049285,19
1184,"We present a novel method to extract parallel sentences from two monolingual corpora , using neural machine translation .",1,0.4606158,17.795554670696294,19
1184,"Our method relies on translating sentences in one corpus , but constraining the decoding by a prefix tree built on the other corpus .",2,0.6742554,84.39420300321918,24
1184,We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search .,3,0.7283215,59.060551447720506,28
1184,"When benchmarked on the BUCC shared task , our method achieves results comparable to other submissions .",3,0.9224281,66.83447293966267,17
1185,"Position encoding ( PE ) , an essential part of self-attention networks ( SANs ) , is used to preserve the word order information for natural language processing tasks , generating fixed position indices for input sequences .",0,0.90110433,70.26123005054353,38
1185,"However , in cross-lingual scenarios , machine translation , the PEs of source and target sentences are modeled independently .",0,0.8333034,79.84101203718032,20
1185,"Due to word order divergences in different languages , modeling the cross-lingual positional relationships might help SANs tackle this problem .",0,0.6320057,58.81313057112499,21
1185,"In this paper , we augment SANs with cross-lingual position representations to model the bilingually aware latent structure for the input sentence .",1,0.4699844,50.84235528092708,23
1185,"Specifically , we utilize bracketing transduction grammar ( BTG )-based reordering information to encourage SANs to learn bilingual diagonal alignments .",2,0.82759804,171.21017303062692,23
1185,"Experimental results on WMT’14 English ⇒German , WAT’17 Japanese ⇒English , and WMT’17 Chinese ⇔English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines .",3,0.9215654,9.257425244395119,34
1185,Extensive analyses confirm that the performance gains come from the cross-lingual information .,3,0.92588264,21.757866419203783,13
1186,The main goal of machine translation has been to convey the correct content .,0,0.9565401,23.335287894178364,14
1186,Stylistic considerations have been at best secondary .,0,0.65406996,133.15670917227376,8
1186,"We show that as a consequence , the output of three commercial machine translation systems ( Bing , DeepL , Google ) make demographically diverse samples from five languages “ sound ” older and more male than the original .",3,0.91120887,104.90300040859697,40
1186,Our findings suggest that translation models reflect demographic bias in the training data .,3,0.9912168,61.38582325806963,14
1186,This opens up interesting new research avenues in machine translation to take stylistic considerations into account .,3,0.79729444,31.44003152345247,17
1187,"Current advances in machine translation ( MT ) increase the need for translators to switch from traditional translation to post-editing ( PE ) of machine-translated text , a process that saves time and reduces errors .",0,0.9642598,27.91250011260699,38
1187,"This affects the design of translation interfaces , as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals .",0,0.75707865,127.04217759064052,25
1187,"Since this paradigm shift offers potential for modalities other than mouse and keyboard , we present MMPE , the first prototype to combine traditional input modes with pen , touch , and speech modalities for PE of MT .",3,0.35245338,80.15273652324846,39
1187,"The results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks , while they are of limited use for longer insertions .",3,0.9746466,33.515809405439725,32
1187,"On the other hand , speech and multi-modal combinations of select & speech are considered suitable for replacements and insertions but offer less potential for deletion and reordering .",0,0.59941274,86.85934985122041,29
1187,"Overall , participants were enthusiastic about the new modalities and saw them as good extensions to mouse & keyboard , but not as a complete substitute .",3,0.9645274,38.74646027617959,27
1188,"We use the multilingual OSCAR corpus , extracted from Common Crawl via language classification , filtering and cleaning , to train monolingual contextualized word embeddings ( ELMo ) for five mid-resource languages .",2,0.8772554,75.01159403717058,33
1188,We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks .,2,0.59417754,16.38403402151064,27
1188,"We show that , despite the noise in the Common-Crawl-based OSCAR data , embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia .",3,0.95816183,24.181874869384473,30
1188,They actually equal or improve the current state of the art in tagging and parsing for all five languages .,3,0.870604,35.05415193483241,20
1188,"In particular , they also improve over multilingual Wikipedia-based contextual embeddings ( multilingual BERT ) , which almost always constitutes the previous state of the art , thereby showing that the benefit of a larger , more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures .",3,0.90556675,48.11537030530072,50
1189,"We present a new challenging stance detection dataset , called Will-They-Won’t-They ( WT--WT ) , which contains 51,284 tweets in English , making it by far the largest available dataset of the type .",2,0.36423418,47.25639361031309,42
1189,All the annotations are carried out by experts ;,2,0.69379497,118.99668668244458,9
1189,"therefore , the dataset constitutes a high-quality and reliable benchmark for future research in stance detection .",3,0.9615213,42.591212353684625,17
1189,Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain .,3,0.9553131,12.075893297828872,33
1190,"While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks , it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge .",0,0.90725124,22.170784015609335,37
1190,"Furthermore , existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations .",0,0.8895775,49.16447962332227,21
1190,"We present a systematic evaluation of the syntactic knowledge of neural language models , testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites .",2,0.46207172,37.37668147357874,35
1190,"We find substantial differences in syntactic generalization performance by model architecture , with sequential models underperforming other architectures .",3,0.97739387,52.265313120403526,19
1190,"Factorially manipulating model architecture and training dataset size ( 1 M-40 M words ) , we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments .",3,0.89633304,70.3263568658241,40
1190,Our results also reveal a dissociation between perplexity and syntactic generalization performance .,3,0.989165,36.99136084984841,13
1191,"Kirov and Cotterell ( 2018 ) argue that the answer is yes : modern Encoder-Decoder ( ED ) architectures learn human-like behavior when inflecting English verbs , such as extending the regular past tense form /-( e ) d / to novel words .",0,0.78919584,101.92459200974855,45
1191,"However , their work does not address the criticism raised by Marcus et al .",3,0.5449022,37.6895852289165,15
1191,"( 1995 ) : that neural models may learn to extend not the regular , but the most frequent class — and thus fail on tasks like German number inflection , where infrequent suffixes like /-s / can still be productively generalized .",0,0.49244943,192.368584435558,43
1191,"To investigate this question , we first collect a new dataset from German speakers ( production and ratings of plural forms for novel nouns ) that is designed to avoid sources of information unavailable to the ED model .",2,0.6526432,87.40708000614634,39
1191,"The speaker data show high variability , and two suffixes evince ‘ regular ’ behavior , appearing more often with phonologically atypical inputs .",3,0.93363136,220.35950670494276,24
1191,"Encoder-decoder models do generalize the most frequently produced plural class , but do not show human-like variability or ‘ regular ’ extension of these other plural markers .",3,0.82273287,209.29679949429698,29
1191,We conclude that modern neural models may still struggle with minority-class generalization .,3,0.9839987,71.86259944289488,14
1192,"With the advent of powerful neural language models over the last few years , research attention has increasingly focused on what aspects of language they represent that make them so successful .",0,0.9608071,20.686580927341748,32
1192,Several testing methodologies have been developed to probe models ’ syntactic representations .,0,0.80400115,47.58556772554565,13
1192,One popular method for determining a model ’s ability to induce syntactic structure trains a model on strings generated according to a template then tests the model ’s ability to distinguish such strings from superficially similar ones with different syntax .,0,0.81515,30.794474123189357,41
1192,We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models : an n-gram model and an LSTM model trained on scrambled inputs .,3,0.4272612,36.49469892377227,35
1193,"Suspense is a crucial ingredient of narrative fiction , engaging readers and making stories compelling .",0,0.9080015,107.59814174309386,16
1193,"While there is a vast theoretical literature on suspense , it is computationally not well understood .",0,0.9435705,63.925824998462666,17
1193,"We compare two ways for modelling suspense : surprise , a backward-looking measure of how unexpected the current state is given the story so far ;",2,0.8042141,167.75142212799307,28
1193,"and uncertainty reduction , a forward-looking measure of how unexpected the continuation of the story is .",0,0.3837775,103.86818808622499,19
1193,Both can be computed either directly over story representations or over their probability distributions .,0,0.6057712,77.66336191847736,15
1193,We propose a hierarchical language model that encodes stories and computes surprise and uncertainty reduction .,2,0.42570192,78.65238223340792,16
1193,"Evaluating against short stories annotated with human suspense judgements , we find that uncertainty reduction over representations is the best predictor , resulting in near human accuracy .",3,0.9473838,127.01982611608696,28
1193,We also show that uncertainty reduction can be used to predict suspenseful events in movie synopses .,3,0.94463575,37.71993383056771,17
1194,"Predicting reading time has been a subject of much previous work , focusing on how different words affect human processing , measured by reading time .",0,0.9493154,56.95663624793061,26
1194,"However , previous work has dealt with a limited number of participants as well as word level only predictions ( i.e .",0,0.88655406,39.3041071659416,22
1194,predicting the time to read a single word ) .,3,0.3590902,79.25897997962363,10
1194,"We seek to extend these works by examining whether or not document level predictions are effective , given additional information such as subject matter , font characteristics , and readability metrics .",1,0.62122357,94.06785290052623,32
1194,"We perform a novel experiment to examine how different features of text contribute to the time it takes to read , distributing and collecting data from over a thousand participants .",2,0.62150437,39.39811512200243,31
1194,We then employ a large number of machine learning methods to predict a user ’s reading time .,2,0.73525804,32.46968431748064,18
1194,"We find that despite extensive research showing that word level reading time can be most effectively predicted by neural networks , larger scale text can be easily and most accurately predicted by one factor , the number of words .",3,0.9766912,57.28342431768476,40
1195,"Natural language understanding ( NLU ) and natural language generation ( NLG ) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives : NLU tackles the transformation from natural language to formal representations , whereas NLG does the reverse .",0,0.9450061,30.51763403975031,47
1195,A key to success in either task is parallel training data which is expensive to obtain at a large scale .,0,0.77623874,33.68963482102121,21
1195,"In this work , we propose a generative model which couples NLU and NLG through a shared latent variable .",1,0.77536345,31.53515552832384,20
1195,"This approach allows us to explore both spaces of natural language and formal representations , and facilitates information sharing through the latent space to eventually benefit NLU and NLG .",3,0.5372477,81.02851581616507,30
1195,Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations .,3,0.7328314,13.226737420863207,22
1195,We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance .,3,0.92293125,10.531965225987525,22
1196,Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text .,0,0.9441864,20.25392222591802,18
1196,"The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest , but humans and machines rely on different cues to make their decisions .",0,0.96062636,29.588410448091086,35
1196,"Here , we perform careful benchmarking and analysis of three popular sampling-based decoding strategies — top-_k_ , nucleus sampling , and untruncated random sampling — and show that improvements in decoding methods have primarily optimized for fooling humans .",1,0.505659,104.06316531563877,41
1196,This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems .,0,0.8631254,37.368074153554176,17
1196,"We also show that though both human and automatic detector performance improve with longer excerpt length , even multi-sentence excerpts can fool expert human raters over 30 % of the time .",3,0.974595,87.01451497594297,32
1196,Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems .,3,0.9898608,27.827467671658717,21
1197,Many multi-domain neural machine translation ( NMT ) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains .,0,0.8732949,36.04490182828149,23
1197,"However , this design lacks adaptation to individual domains .",0,0.82680863,135.48141298896846,10
1197,"To overcome this limitation , we propose a novel multi-domain NMT model using individual modules for each domain , on which we apply word-level , adaptive and layer-wise domain mixing .",2,0.6125614,48.21724254561061,33
1197,We first observe that words in a sentence are often related to multiple domains .,3,0.5399677,27.1820090547046,15
1197,"Hence , we assume each word has a domain proportion , which indicates its domain preference .",2,0.7128895,122.08191677303735,17
1197,Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions .,2,0.71104294,91.23444413950808,18
1197,"We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains , and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions .",3,0.878561,82.76214760267355,34
1197,"Through this , we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well .",3,0.7174589,32.92239162470356,19
1197,Our experiments show that our proposed model outperforms existing ones in several NMT tasks .,3,0.96836877,10.989211351433767,15
1198,"To address the challenge of policy learning in open-domain multi-turn conversation , we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy , aimed at fostering a more coherent and controllable dialog .",1,0.45209548,39.621846890449866,42
1198,"To this end , we first construct a conversational graph ( CG ) from dialog corpora , in which there are vertices to represent “ what to say ” and “ how to say ” , and edges to represent natural transition between a message ( the last utterance in a dialog context ) and its response .",2,0.8886474,33.44838708549731,58
1198,"We then present a novel CG grounded policy learning framework that conducts dialog flow planning by graph traversal , which learns to identify a what-vertex and a how-vertex from the CG at each turn to guide response generation .",2,0.60133165,51.905313648767766,40
1198,"In this way , we effectively leverage the CG to facilitate policy learning as follows : ( 1 ) it enables more effective long-term reward design , ( 2 ) it provides high-quality candidate actions , and ( 3 ) it gives us more control over the policy .",3,0.7035161,61.70170671907156,49
1198,Results on two benchmark corpora demonstrate the effectiveness of this framework .,3,0.9176646,10.520809671256648,12
1199,Abstract Meaning Representations ( AMRs ) are broad-coverage sentence-level semantic graphs .,0,0.84992915,32.02424807354885,14
1199,Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only .,0,0.8857197,13.874284202887761,24
1199,"In this paper , we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring .",1,0.84386927,26.49687107262193,23
1199,"Despite the simplicity of the approach , our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset , including the recent use of transformer architectures .",3,0.95462644,45.10761308619597,31
1199,"In addition to the standard evaluation metrics , we provide human evaluation experiments that further substantiate the strength of our approach .",3,0.5964884,34.26051355075678,22
1200,We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies .,2,0.39487627,68.96043301658969,24
1200,"We propose an approach that learns to correlate changes across two distinct language representations , to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications .",2,0.49326572,70.0110059950465,35
1200,"We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects , with each example consisting of a concurrent update to a method and its corresponding comment .",2,0.83316964,55.77967253704239,36
1200,We compare our approach against multiple baselines using both automatic metrics and human evaluation .,2,0.56476724,15.806368547416326,15
1200,Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits .,3,0.9837037,33.9103091318993,19
1201,This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning .,1,0.8402674,32.37643425868829,22
1201,We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task .,3,0.7524119,64.49445470378103,23
1201,We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content .,2,0.81655955,52.705284160609956,28
1201,"For politeness as well as five other transfer tasks , our model outperforms the state-of-the-art methods on automatic metrics for content preservation , with a comparable or better performance on style transfer accuracy .",3,0.9191643,26.83264403747896,40
1201,"Additionally , our model surpasses existing methods on human evaluations for grammaticality , meaning preservation and transfer accuracy across all the six style transfer tasks .",3,0.90542793,62.049121253457024,26
1201,The data and code is located at https://github.com/tag-and-generate .,3,0.58099675,15.516034600575223,9
1202,Subword segmentation is widely used to address the open vocabulary problem in machine translation .,0,0.9168171,28.884471667863178,15
1202,"The dominant approach to subword segmentation is Byte Pair Encoding ( BPE ) , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .",0,0.86094844,49.14857581483988,30
1202,"While multiple segmentations are possible even with the same vocabulary , BPE splits words into unique sequences ;",0,0.44638875,288.21652618554526,18
1202,this may prevent a model from better learning the compositionality of words and being robust to segmentation errors .,3,0.6926177,32.823544816552754,19
1202,"So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another subword segmentation algorithm ( Kudo , 2018 ) .",0,0.8082612,140.43845954688672,29
1202,"In contrast , we show that BPE itself incorporates the ability to produce multiple segmentations of the same word .",3,0.9655078,46.13826401379535,20
1202,We introduce BPE-dropout-simple and effective subword regularization method based on and compatible with conventional BPE .,2,0.4871006,101.20941554770259,18
1202,"It stochastically corrupts the segmentation procedure of BPE , which leads to producing multiple segmentations within the same fixed BPE framework .",0,0.49251845,62.019333967650574,22
1202,Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization .,3,0.85413516,25.49447425663923,34
1203,Non-autoregressive ( NAR ) neural machine translation is usually done via knowledge distillation from an autoregressive ( AR ) model .,0,0.9218067,20.620060839473847,21
1203,"Under this framework , we leverage large monolingual corpora to improve the NAR model ’s performance , with the goal of transferring the AR model ’s generalization ability while preventing overfitting .",2,0.6995172,29.111181961953893,32
1203,"On top of a strong NAR baseline , our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the NAR model to approach the teacher AR model ’s performance , yields comparable or better results than the best non-iterative NAR methods in the literature and helps reduce overfitting in the training process .",3,0.9602519,28.541362775631267,68
1204,Sequence-to-sequence ( seq2seq ) network is a well-established model for text summarization task .,0,0.94137585,12.048075685115807,16
1204,It can learn to produce readable content ;,0,0.74322605,440.71670259213994,8
1204,"however , it falls short in effectively identifying key regions of the source .",0,0.7377113,109.96193685763359,14
1204,"In this paper , we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer .",1,0.80687404,42.61898390628621,23
1204,"Our experiments on two publicly available clinical data sets ( 107,372 reports of MIMIC-CXR , and 3,366 reports of OpenI ) show that our model statistically significantly boosts state-of-the-art results in terms of ROUGE metrics ( with improvements : 2.9 % RG-1 , 2.5 % RG-2 , 1.9 % RG-L ) , in the healthcare domain where any range of improvement impacts patients ’ welfare .",3,0.531465,41.17007621834678,79
1205,It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation .,0,0.90100783,36.08329346150064,34
1205,In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document .,3,0.440316,19.909976765127524,33
1205,We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce .,2,0.6691314,22.52949832315177,23
1205,Our human annotators found substantial amounts of hallucinated content in all model generated summaries .,3,0.9718807,67.74096477624182,15
1205,"However , our analysis does show that pretrained models are better summarizers not only in terms of raw metrics , i.e. , ROUGE , but also in generating faithful and factual summaries as evaluated by humans .",3,0.9845664,38.763729713813845,37
1205,"Furthermore , we show that textual entailment measures better correlate with faithfulness than standard metrics , potentially leading the way to automatic evaluation metrics as well as training and decoding criteria .",3,0.97233623,46.460757265249676,32
1206,"Most general-purpose extractive summarization models are trained on news articles , which are short and present all important information upfront .",0,0.81751466,40.788259759806536,23
1206,"As a result , such models are biased on position and often perform a smart selection of sentences from the beginning of the document .",0,0.8054703,61.780607188907915,25
1206,"When summarizing long narratives , which have complex structure and present information piecemeal , simple position heuristics are not sufficient .",0,0.7329028,139.20204479030178,21
1206,"In this paper , we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models .",1,0.886857,27.38276699318281,23
1206,"We formalize narrative structure in terms of key narrative events ( turning points ) and treat it as latent in order to summarize screenplays ( i.e. , extract an optimal sequence of scenes ) .",2,0.8465598,116.86313856710649,35
1206,"Experimental results on the CSI corpus of TV screenplays , which we augment with scene-level summarization labels , show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries .",3,0.88096005,69.62704679606439,47
1207,The supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization .,0,0.9209447,20.780849551764916,31
1207,"Unfortunately , in most domains ( other than news ) such training data is not available and cannot be easily sourced .",0,0.8418753,53.98221354413656,22
1207,"In this paper we enable the use of supervised learning for the setting where there are only documents available ( e.g. , product or business reviews ) without ground truth summaries .",1,0.40220943,53.94715319633838,32
1207,"We create a synthetic dataset from a corpus of user reviews by sampling a review , pretending it is a summary , and generating noisy versions thereof which we treat as pseudo-review input .",2,0.8883575,62.814523321265966,34
1207,We introduce several linguistically motivated noise generation functions and a summarization model which learns to denoise the input and generate the original review .,2,0.6870332,80.08114460400483,24
1207,"At test time , the model accepts genuine reviews and generates a summary containing salient opinions , treating those that do not reach consensus as noise .",2,0.5461736,122.91275565870697,27
1207,Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines .,3,0.9236016,17.011194086149295,19
1208,"In recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia , and those from healthy controls .",0,0.94785535,18.318977176364033,33
1208,The difference between perplexity estimates from two neural language models ( LMs )-one trained on transcripts of speech produced by healthy participants and one trained on those with dementia-as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance .,0,0.81209415,34.99521387178356,56
1208,"However , little is known about why this approach is effective , and on account of the lack of case / control matching in the most widely-used evaluation set of transcripts ( DementiaBank ) , it is unclear if these approaches are truly diagnostic , or are sensitive to other variables .",0,0.8616299,53.79220892328012,54
1208,"In this paper , we interrogate neural LMs trained on participants with and without dementia by using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency .",1,0.85030705,108.22059352568475,31
1208,"We find that perplexity of neural LMs is strongly and differentially associated with lexical frequency , and that using a mixture model resulting from interpolating control and dementia LMs improves upon the current state-of-the-art for models trained on transcript text exclusively .",3,0.97248286,46.16087500323313,48
1209,"Recently , there has been much interest in the question of whether deep natural language understanding ( NLU ) models exhibit systematicity , generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear .",0,0.9658201,33.895646389117914,43
1209,There is accumulating evidence that neural models do not learn systematically .,0,0.9406503,26.93793378071,12
1209,"We examine the notion of systematicity from a linguistic perspective , defining a set of probing tasks and a set of metrics to measure systematic behaviour .",2,0.4889208,34.87524840441789,27
1209,"We also identify ways in which network architectures can generalize non-systematically , and discuss why such forms of generalization may be unsatisfying .",3,0.45157948,29.2637610412876,23
1209,"As a case study , we perform a series of experiments in the setting of natural language inference ( NLI ) .",2,0.73292744,17.53019776108495,22
1209,"We provide evidence that current state-of-the-art NLU systems do not generalize systematically , despite overall high performance .",3,0.9495194,20.08798413126241,23
1210,"We investigate the use of NLP as a measure of the cognitive processes involved in storytelling , contrasting imagination and recollection of events .",1,0.8320924,51.914459720716955,24
1210,"To facilitate this , we collect and release Hippocorpus , a dataset of 7,000 stories about imagined and recalled events .",2,0.56327575,76.33853818619899,21
1210,We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events .,2,0.6694657,79.74117578158082,20
1210,"Additionally , we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory ( Tulving , 1972 ) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events .",2,0.89394814,186.3460339439224,42
1210,"Our analyses show that imagined stories have a substantially more linear narrative flow , compared to recalled stories in which adjacent sentences are more disconnected .",3,0.982001,140.6743116953933,26
1210,"In addition , while recalled stories rely more on autobiographical events based on episodic memory , imagined stories express more commonsense knowledge based on semantic memory .",0,0.5436735,121.14896597800185,27
1210,"Finally , our measures reveal the effect of narrativization of memories in stories ( e.g. , stories about frequently recalled memories flow more linearly ; Bartlett , 1932 ) .",3,0.91671133,135.8406264376465,30
1210,Our findings highlight the potential of using NLP tools to study the traces of human cognition in language .,3,0.9894084,20.203243534596087,19
1211,A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions ( i.e .,0,0.84099317,66.43168621858946,21
1211,is a grammatical sentence more probable than an ungrammatical sentence ) .,3,0.5762611,51.717637484241855,12
1211,"Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations , where stark grammaticality differences are absent .",2,0.5670713,274.95138894680434,26
1211,We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish .,3,0.5333264,37.46658855581511,27
1211,"Thus , English models may appear to acquire human-like syntactic preferences , while models trained on Spanish fail to acquire comparable human-like preferences .",3,0.8483242,42.684950334937625,24
1211,We conclude by relating these results to broader concerns about the relationship between comprehension ( i.e .,3,0.97738314,47.5194933295731,17
1211,"typical language model use cases ) and production ( which generates the training data for language models ) , suggesting that necessary linguistic biases are not present in the training signal at all .",3,0.74386376,87.54330948703294,34
1212,Recent work has found evidence that natural languages are shaped by pressures for efficient communication — e.g .,0,0.88622934,44.51818343572156,18
1212,"the more contextually predictable a word is , the fewer speech sounds or syllables it has ( Piantadosi et al .",0,0.7931249,79.3623372943906,21
1212,2011 ) .,4,0.87148476,133.23158972364206,3
1212,Research on the degree to which speech and language are shaped by pressures for effective communication — robustness in the face of noise and uncertainty — has been more equivocal .,0,0.9129884,31.862540043429348,31
1212,We develop a measure of contextual confusability during word recognition based on psychoacoustic data .,2,0.44386044,60.44974524066718,15
1212,"Applying this measure to naturalistic speech corpora , we find evidence suggesting that speakers alter their productions to make contextually more confusable words easier to understand .",3,0.9058296,71.48522027138937,27
1213,"We take up the scientific question of what determines the preferred order of adjectives in English , in phrases such as big blue box where multiple adjectives modify a following noun .",1,0.30921757,92.38386472145388,32
1213,"We implement and test four quantitative theories , all of which are theoretically motivated in terms of efficiency in human language production and comprehension .",2,0.7022519,85.96730885744753,25
1213,"The four theories we test are subjectivity ( Scontras et al. , 2017 ) , information locality ( Futrell , 2019 ) , integration cost ( Dyer , 2017 ) , and information gain , which we introduce .",2,0.72079396,84.05724465189466,39
1213,We evaluate theories based on their ability to predict orders of unseen adjectives in hand-parsed and automatically-parsed dependency treebanks .,2,0.7070022,49.43994710598662,24
1213,"We find that subjectivity , information locality , and information gain are all strong predictors , with some evidence for a two-factor account , where subjectivity and information gain reflect a factor involving semantics , and information locality reflects collocational preferences .",3,0.96319723,60.62059954571554,43
1214,This paper discusses the importance of uncovering uncertainty in end-to-end dialog tasks and presents our experimental results on uncertainty classification on the processed Ubuntu Dialog Corpus .,1,0.8550269,43.090202474113134,29
1214,"We show that instead of retraining models for this specific purpose , we can capture the original retrieval model ’s underlying confidence concerning the best prediction using trivial additional computation .",3,0.87074125,112.05548390355314,31
1215,"Being engaging , knowledgeable , and empathetic are all desirable general qualities in a conversational agent .",0,0.64805955,43.266578419892205,17
1215,Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them .,0,0.926994,61.07636215549666,27
1215,"But rather than being specialized in one single quality , a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow .",0,0.560982,26.520167046152782,29
1215,"In this work , we investigate several ways to combine models trained towards isolated capabilities , ranging from simple model aggregation schemes that require minimal additional training , to various forms of multi-task training that encompass several skills at all training stages .",1,0.7526354,67.86230050173239,43
1215,"We further propose a new dataset , BlendedSkillTalk , to analyze how these capabilities would mesh together in a natural conversation , and compare the performance of different architectures and training schemes .",2,0.36993533,86.59785971953943,33
1215,"Our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill , and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task .",3,0.97329944,49.54131645570074,58
1216,Human conversations naturally evolve around related concepts and hop to distant concepts .,0,0.9276778,197.63280891754047,13
1216,"This paper presents a new conversation generation model , ConceptFlow , which leverages commonsense knowledge graphs to explicitly model conversation flows .",1,0.8002578,35.134476410354075,22
1216,"By grounding conversations to the concept space , ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations .",0,0.5732775,156.34176178035588,24
1216,"The traverse is guided by graph attentions in the concept graph , moving towards more meaningful directions in the concept space , in order to generate more semantic and informative responses .",2,0.43108225,78.82772283503493,32
1216,"Experiments on Reddit conversations demonstrate ConceptFlow ’s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70 % fewer parameters , confirming the advantage of explicit modeling conversation structures .",3,0.94878024,97.90477692364783,37
1216,All source codes of this work are available at https://github.com/thunlp/ConceptFlow .,3,0.4565543,6.697619162037935,11
1217,"Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation , recent research results have revealed that the trained models have undesirable generation behaviors , such as malicious responses and generic ( boring ) responses .",0,0.8893979,50.6717978858889,42
1217,"In this work , we propose a framework named “ Negative Training ” to minimize such behaviors .",1,0.74051166,68.43144644390004,18
1217,"Given a trained model , the framework will first find generated samples that exhibit the undesirable behavior , and then use them to feed negative training signals for fine-tuning the model .",2,0.58187604,39.38986871812242,33
1217,"Our experiments show that negative training can significantly reduce the hit rate of malicious responses , or discourage frequent responses and improve response diversity .",3,0.98225486,71.07382189484308,25
1218,The Natural Language Understanding ( NLU ) component in task oriented dialog systems processes a user ’s request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker ( DST ) .,0,0.9120258,35.85710316763029,41
1218,This information is typically represented as a semantic frame that captures the intent and slot-labels provided by the user .,0,0.74463737,39.453197993880174,20
1218,"We first show that such a shallow representation is insufficient for complex dialog scenarios , because it does not capture the recursive nature inherent in many domains .",3,0.6369736,36.46537958838011,28
1218,"We propose a recursive , hierarchical frame-based representation and show how to learn it from data .",1,0.3577397,59.370442478961344,18
1218,"We formulate the frame generation task as a template-based tree decoding task , where the decoder recursively generates a template and then fills slot values into the template .",2,0.76130617,35.16303584579178,31
1218,We extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end .,2,0.47528088,48.15964809188802,22
1218,We achieve a small improvement on the widely used ATIS dataset and a much larger improvement on a more complex dataset we describe here .,3,0.91561127,22.484149891997333,25
1219,We study the task of semantic parse correction with natural language feedback .,1,0.46621343,93.10448905341066,13
1219,"Given a natural language utterance , most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form .",0,0.8812986,27.921652043240382,28
1219,"In this paper , we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance .",1,0.8261317,33.63586982690528,39
1219,"We focus on natural language to SQL systems and construct , SPLASH , a dataset of utterances , incorrect SQL interpretations and the corresponding natural language feedback .",2,0.7253489,166.10179168983402,28
1219,We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction .,3,0.8741543,37.17474649182299,36
1219,"While we estimated human correction accuracy is 81.5 % , our best model achieves only 25.1 % , which leaves a large gap for improvement in future research .",3,0.9649042,47.49013625959704,29
1219,SPLASH is publicly available at https://aka.ms/Splash_dataset .,3,0.44926608,20.40127873274566,7
1220,We address the problem of calibrating prediction confidence for output entities of interest in natural language processing ( NLP ) applications .,1,0.7399863,52.48359880961853,22
1220,"It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions , especially if the applications are to be deployed in a safety-critical domain such as healthcare .",0,0.82221633,24.436660870088303,39
1220,However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods .,0,0.84770507,65.82977720462107,22
1220,"In this study , we propose a general calibration scheme for output entities of interest in neural network based structured prediction models .",1,0.9072795,77.95799667361504,23
1220,Our proposed method can be used with any binary class calibration scheme and a neural network model .,3,0.8674188,54.74437258729861,18
1220,"Additionally , we show that our calibration method can also be used as an uncertainty-aware , entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements .",3,0.93528074,35.4966292842271,39
1220,"We show that our method outperforms current calibration techniques for Named Entity Recognition , Part-of-speech tagging and Question Answering systems .",3,0.94788265,15.789212202028732,21
1220,We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets .,3,0.940934,47.59592709678218,19
1220,Our method improves the calibration and model performance on out-of-domain test scenarios as well .,3,0.8574576,23.626833516415328,18
1221,Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies .,0,0.80948263,25.76177567760012,23
1221,Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state ;,0,0.534779,266.1431821980544,21
1221,"unfortunately , the number of such queries is often prohibitive , frequently rendering these approaches impractical .",0,0.90226,113.62174103206351,17
1221,"To combat this query complexity , we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance .",2,0.6689679,106.7344904172778,31
1221,"Our algorithm , LEAQI , learns a difference classifier that predicts when the expert is likely to disagree with the heuristic , and queries the expert only when necessary .",2,0.61543655,67.52089819582783,30
1221,"We apply LEAQI to three sequence labelling tasks , demonstrating significantly fewer queries to the expert and comparable ( or better ) accuracies over a passive approach .",3,0.562535,125.61209502447112,28
1222,Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text .,0,0.4461249,52.525596937668794,26
1222,"In this paper , we allow model developers to specify these types of inductive biases as natural language explanations .",1,0.6969374,46.51084169106162,20
1222,"We use BERT fine-tuned on MultiNLI to “ interpret ” these explanations with respect to the input sentence , producing explanation-guided representations of the input .",2,0.7862062,50.403849380188994,28
1222,"Across three relation extraction tasks , our method , ExpBERT , matches a BERT baseline but with 3– 20x less labeled data and improves on the baseline by 3–10 F1 points with the same amount of labeled data .",3,0.86177915,44.46076711676962,40
1223,"Recent Transformer-based architectures , e.g. , BERT , provide impressive results in many Natural Language Processing tasks .",0,0.8602537,28.32183144712572,20
1223,"However , most of the adopted benchmarks are made of ( sometimes hundreds of ) thousands of examples .",0,0.83714265,106.05623971415113,19
1223,"In many real scenarios , obtaining high-quality annotated data is expensive and time consuming ;",0,0.881759,38.64528082648951,15
1223,"in contrast , unlabeled examples characterizing the target task can be , in general , easily collected .",0,0.6458162,115.22131787088216,18
1223,"One promising method to enable semi-supervised learning has been proposed in image processing , based on Semi-Supervised Generative Adversarial Networks .",0,0.86941385,19.789920621503434,21
1223,"In this paper , we propose GAN-BERT that ex-tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting .",1,0.8249128,17.59969856488635,27
1223,"Experimental results show that the requirement for annotated examples can be drastically reduced ( up to only 50-100 annotated examples ) , still obtaining good performances in several sentence classification tasks .",3,0.9614037,35.2263884317986,34
1224,"Natural language processing covers a wide variety of tasks predicting syntax , semantics , and information content , and usually each type of output is generated with specially designed architectures .",0,0.934016,74.84245780379894,31
1224,"In this paper , we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans , thus a single task-independent model can be used across different tasks .",1,0.69145644,33.83702652244113,46
1224,"We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing ( syntax ) , semantic role labeling ( semantics ) , relation extraction ( information content ) , aspect based sentiment analysis ( sentiment ) , and many others , achieving performance comparable to state-of-the-art specialized models .",2,0.66676664,47.13177837053479,59
1224,"We further demonstrate benefits of multi-task learning , and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks .",3,0.9042813,28.292655640363378,31
1224,"Finally , we convert these datasets into a unified format to build a benchmark , which provides a holistic testbed for evaluating future models for generalized natural language analysis .",3,0.50478035,48.69913964968965,30
1225,Sequence labeling is a fundamental task for a range of natural language processing problems .,0,0.95255566,18.861725118817436,15
1225,"When used in practice , its performance is largely influenced by the annotation quality and quantity , and meanwhile , obtaining ground truth labels is often costly .",0,0.8538843,69.86680294453248,28
1225,"In many cases , ground truth labels do not exist , but noisy annotations or annotations from different domains are accessible .",0,0.85323745,103.64777410004731,22
1225,"In this paper , we propose a novel framework Consensus Network ( ConNet ) that can be trained on annotations from multiple sources ( e.g. , crowd annotation , cross-domain data ) .",1,0.7782105,38.316220113067615,33
1225,It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module .,2,0.45222768,64.7881083970909,19
1225,"Finally , it leads to a model reflecting the agreement ( consensus ) among multiple sources .",3,0.76142496,113.02884063438675,17
1225,We evaluate the proposed framework in two practical settings of multi-source learning : learning with crowd annotations and unsupervised cross-domain model adaptation .,2,0.51978767,38.98742821286737,23
1225,Extensive experimental results show that our model achieves significant improvements over existing methods in both settings .,3,0.94774944,12.456351147323321,17
1225,We also demonstrate that the method can apply to various tasks and cope with different encoders .,3,0.8981047,19.413780566729653,17
1226,"This paper presents MixText , a semi-supervised learning method for text classification , which uses our newly designed data augmentation method called TMix .",1,0.82376313,48.35195524401198,24
1226,TMix creates a large amount of augmented training samples by interpolating text in hidden space .,2,0.3819585,107.22660521148357,16
1226,"Moreover , we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data , hence making them as easy to use as labeled data .",2,0.59240186,26.912095602119173,28
1226,"By mixing labeled , unlabeled and augmented data , MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks .",3,0.87306094,30.025446834561734,37
1226,The improvement is especially prominent when supervision is extremely limited .,3,0.87837327,51.235167562704305,11
1226,We have publicly released our code at https://github.com/GT-SALT/MixText .,3,0.46789083,12.462039661743166,9
1227,Natural Language Processing ( NLP ) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters .,0,0.9590705,18.256618360632118,23
1227,"However , these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices .",0,0.86941195,34.5362368129876,23
1227,"In this paper , we propose MobileBERT for compressing and accelerating the popular BERT model .",1,0.87951154,53.53339537033037,16
1227,"Like the original BERT , MobileBERT is task-agnostic , that is , it can be generically applied to various downstream NLP tasks via simple fine-tuning .",3,0.46185315,26.608867273213313,28
1227,"Basically , MobileBERT is a thin version of BERT_LARGE , while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks .",0,0.55915743,62.0239771217548,26
1227,"To train MobileBERT , we first train a specially designed teacher model , an inverted-bottleneck incorporated BERT_LARGE model .",2,0.8723545,129.7895567793192,21
1227,"Then , we conduct knowledge transfer from this teacher to MobileBERT .",2,0.8542106,132.1530504307486,12
1227,Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks .,3,0.89005876,18.04546680058873,22
1227,"On the natural language inference tasks of GLUE , MobileBERT achieves a GLUE score of 77.7 ( 0.6 lower than BERT_BASE ) , and 62 ms latency on a Pixel 4 phone .",3,0.9055176,48.13725161705637,33
1227,"On the SQuAD v1.1 / v2.0 question answering task , MobileBERT achieves a dev F1 score of 90.0/79.2 ( 1.5 / 2.1 higher than BERT_BASE ) .",3,0.9320625,29.89287629810356,27
1228,"Language models that use additional latent structures ( e.g. , syntax trees , coreference chains , knowledge graph links ) provide several advantages over traditional language models .",0,0.77889246,63.24620475398282,28
1228,"However , likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space .",0,0.88391966,47.425132848331074,21
1228,Existing works avoid this issue by using importance sampling .,0,0.72117543,115.56235088560656,10
1228,"Although this approach has asymptotic guarantees , analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates .",0,0.8177683,55.15437807851606,30
1228,"In this paper , we carry out this analysis for three models : RNNG , EntityNLM , and KGLM .",1,0.5952167,88.73736543951664,20
1228,"In addition , we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates , as well as provide theoretical results which reinforce the validity of this technique .",3,0.71381193,51.76911803920121,40
1229,Transfer learning has fundamentally changed the landscape of natural language processing ( NLP ) .,0,0.9673989,19.2239101254928,15
1229,Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks .,0,0.75951576,5.195666967139788,23
1229,"However , due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models , aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data .",0,0.80267984,15.769724431115923,42
1229,"To address such an issue in a principled manner , we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance .",1,0.3664061,18.53443381027319,30
1229,The proposed framework contains two important ingredients : 1 .,2,0.43286416,123.7609920861429,10
1229,"Smoothness-inducing regularization , which effectively manages the complexity of the model ;",3,0.42756024,138.1810836489714,14
1229,2 .,4,0.8740741,90.9251343538622,2
1229,"Bregman proximal point optimization , which is an instance of trust-region methods and can prevent aggressive updating .",3,0.47143596,335.4009529682488,18
1229,"Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE , SNLI , SciTail and ANLI .",3,0.9607972,11.697257359829791,32
1229,"Moreover , it also outperforms the state-of-the-art T5 model , which is the largest pre-trained model containing 11 billion parameters , on GLUE .",3,0.934003,20.953100579632814,30
1230,Neural Network Language Models ( NNLMs ) generate probability distributions by applying a softmax function to a distance metric formed by taking the dot product of a prediction vector with all word vectors in a high-dimensional embedding space .,0,0.7542585,32.83887902766053,40
1230,The dot-product distance metric forms part of the inductive bias of NNLMs .,0,0.5330687,50.33402969911847,13
1230,"Although NNLMs optimize well with this inductive bias , we show that this results in a sub-optimal ordering of the embedding space that structurally impoverishes some words at the expense of others when assigning probability .",3,0.908815,56.09815628732947,36
1230,"We present numerical , theoretical and empirical analyses which show that words on the interior of the convex hull in the embedding space have their probability bounded by the probabilities of the words on the hull .",2,0.4786189,39.173816057745846,37
1231,"Extracting lexico-semantic relations as graph-structured taxonomies , also known as taxonomy construction , has been beneficial in a variety of NLP applications .",0,0.9342765,22.620394256751855,23
1231,Recently Graph Neural Network ( GNN ) has shown to be powerful in successfully tackling many tasks .,0,0.92965275,49.042432374183534,18
1231,"However , there has been no attempt to exploit GNN to create taxonomies .",0,0.94105947,51.08166695850552,14
1231,"In this paper , we propose Graph2Taxo , a GNN-based cross-domain transfer framework for the taxonomy construction task .",1,0.85467273,35.49738250419505,21
1231,Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain .,1,0.57780844,47.7610617164612,25
1231,We also propose a novel method of directed acyclic graph ( DAG ) generation for taxonomy construction .,2,0.38071722,27.03635819500825,18
1231,"Specifically , our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs , and a set of taxonomies for some known domains for training .",2,0.7875222,140.23763747404104,31
1231,The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain .,2,0.6962771,27.796083890034247,23
1231,Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art .,3,0.92029816,23.143723781217606,24
1232,Pretraining NLP models with variants of Masked Language Model ( MLM ) objectives has recently led to a significant improvements on many tasks .,0,0.95150846,27.77234911981177,24
1232,This paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task .,1,0.88570017,15.97539518911353,23
1232,"On several text classification tasks , we show that as the number of training examples grow into the millions , the accuracy gap between finetuning BERT-based model and training vanilla LSTM from scratch narrows to within 1 % .",3,0.8438475,22.958362540291414,41
1232,Our findings indicate that MLM-based models might reach a diminishing return point as the supervised data size increases significantly .,3,0.99167633,59.63522038091681,22
1233,Cross-lingual word embeddings ( CLWE ) are often evaluated on bilingual lexicon induction ( BLI ) .,0,0.8545373,22.291152987735103,17
1233,"Recent CLWE methods use linear projections , which underfit the training dictionary , to generalize on BLI .",0,0.75173324,270.00079717635197,18
1233,"However , underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary .",0,0.6993384,52.71900793055048,19
1233,"We address this limitation by retrofitting CLWE to the training dictionary , which pulls training translation pairs closer in the embedding space and overfits the training dictionary .",2,0.62657684,90.85696017733812,28
1233,"This simple post-processing step often improves accuracy on two downstream tasks , despite lowering BLI test accuracy .",3,0.7326856,116.49382289116011,18
1233,"We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE , which sometimes generalizes even better on downstream tasks .",2,0.68130136,126.28208098658013,25
1233,Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation .,3,0.98945343,151.8860993717067,23
1234,Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks .,0,0.75547504,5.342116203076655,22
1234,"However , the huge size of these models could be a deterrent to using them in practice .",0,0.6930509,26.89126979660712,18
1234,Some recent works use knowledge distillation to compress these huge models into shallow ones .,0,0.83952665,61.53781072511548,15
1234,In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition ( NER ) .,1,0.81880534,20.634121062754243,19
1234,"In particular , we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations , that is agnostic of teacher architecture , and show that it outperforms strategies employed in prior works .",3,0.5046382,72.3780637719536,38
1234,"Additionally , we investigate the role of several factors like the amount of unlabeled data , annotation resources , model architecture and inference latency to name a few .",2,0.45375443,36.47437031735691,29
1234,We show that our approach leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95 % of its F1-score for NER over 41 languages .,3,0.9122023,53.3124085207644,45
1235,Authorship attribution aims to identify the author of a text based on the stylometric analysis .,0,0.88285387,42.997829808565406,16
1235,"Authorship obfuscation , on the other hand , aims to protect against authorship attribution by modifying a text ’s style .",0,0.922359,75.30090814783932,21
1235,"In this paper , we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model .",1,0.92642826,18.638046314198686,25
1235,An obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated – a decision that is key to the adversary interested in authorship attribution .,0,0.8521131,33.77635783849481,39
1235,We show that the existing authorship obfuscation methods are not stealthy as their obfuscated texts can be identified with an average F1 score of 0.87 .,3,0.9688723,22.635321949427563,26
1235,"The reason for the lack of stealthiness is that these obfuscators degrade text smoothness , as ascertained by neural language models , in a detectable manner .",3,0.5132984,68.26090308463756,27
1235,Our results highlight the need to develop stealthy authorship obfuscation methods that can better protect the identity of an author seeking anonymity .,3,0.9887123,33.55857941652883,23
1236,Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications .,0,0.9131763,6.935991865852894,15
1236,"However , they are also notorious for being slow in inference , which makes them difficult to deploy in real-time applications .",0,0.88747966,28.9120311859493,22
1236,"We propose a simple but effective method , DeeBERT , to accelerate BERT inference .",1,0.32586193,55.27448770505866,15
1236,Our approach allows samples to exit earlier without passing through the entire model .,3,0.6689208,116.95757432238267,14
1236,Experiments show that DeeBERT is able to save up to ~40 % inference time with minimal degradation in model quality .,3,0.9501165,37.149486236563725,21
1236,Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy .,3,0.96703804,101.23847248486277,16
1236,Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks .,3,0.96265197,47.836903181806285,17
1236,Code is available at https://github.com/castorini/DeeBERT .,3,0.4757601,12.84653188963464,6
1237,"In hierarchical text classification , we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy .",0,0.5287503,23.905322054990975,29
1237,"Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure , but we prefer to look for efficient ways to strengthen a baseline model .",0,0.77271515,53.71916852780223,33
1237,We first define the task as a sequence-to-sequence problem .,2,0.69717425,7.329953908315272,10
1237,"Afterwards , we propose an auxiliary synthetic task of bottom-up-classification .",2,0.5329238,91.81746869723692,11
1237,"Then , from external dictionaries , we retrieve textual definitions for the classes of all the hierarchy ’s layers , and map them into the word vector space .",2,0.8206997,96.24035447339809,29
1237,We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search .,2,0.72171754,46.90738139213137,24
1237,"Whereas the modified search did not provide large gains , the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy .",3,0.9669807,69.5157804350521,28
1237,"With our efficient approaches , we outperform previous studies , using a drastically reduced number of parameters , in two well-known English datasets .",3,0.86217666,63.516484880434085,25
1238,We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts .,0,0.36835766,61.3843304531039,23
1238,"Motivated by recent advances in multi-task learning , we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main task ( grading ) and auxiliary prediction tasks : morpho-syntactic labeling , language modeling , and native language identification ( L1 ) .",2,0.44119623,35.12191358898063,60
1238,"We encode the transcriptions with both bi-directional recurrent neural networks and with bi-directional representations from transformers , compare against a feature-rich baseline , and analyse performance at different proficiency levels and with transcriptions of varying error rates .",2,0.8272217,56.29277606829844,40
1238,Our best performance comes from a transformer encoder with L1 prediction as an auxiliary task .,3,0.9310483,38.21978157288586,16
1238,We discuss areas for improvement and potential applications for text-only speech scoring .,3,0.55269635,65.09408612065795,14
1239,Representation learning is a critical ingredient for natural language processing systems .,0,0.9394351,19.163943796320993,12
1239,"Recent Transformer language models like BERT learn powerful textual representations , but these models are targeted towards token-and sentence-level training objectives and do not leverage information on inter-document relatedness , which limits their document-level representation power .",0,0.85289395,48.615057389007625,42
1239,"For applications on scientific documents , such as classification and recommendation , accurate embeddings of documents are a necessity .",0,0.86769617,68.99319216044137,20
1239,"We propose SPECTER , a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness : the citation graph .",2,0.32914147,55.99558488131561,36
1239,"Unlike existing pretrained language models , Specter can be easily applied to downstream applications without task-specific fine-tuning .",3,0.6699904,25.47487909180969,18
1239,"Additionally , to encourage further research on document-level models , we introduce SciDocs , a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction , to document classification and recommendation .",2,0.45175803,36.1634239412042,37
1239,We show that Specter outperforms a variety of competitive baselines on the benchmark .,3,0.93603194,24.55335683230692,14
1240,"We propose a method for program generation based on semantic scaffolds , lightweight structures representing the high-level semantic and syntactic composition of a program .",1,0.3953472,40.11061820087917,26
1240,"By first searching over plausible scaffolds then using these as constraints for a beam search over programs , we achieve better coverage of the search space when compared with existing techniques .",3,0.6501728,92.86368805152993,32
1240,"We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation , in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases .",2,0.8512221,53.46868600583896,41
1240,"By using semantic scaffolds during inference , we achieve a 10 % absolute improvement in top-100 accuracy over the previous state-of-the-art .",3,0.8473245,16.565078140235027,29
1240,"Additionally , we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems , demonstrating a substantial improvement in efficiency .",3,0.8822868,80.4854405544623,31
1241,"Open Information Extraction systems extract ( “ subject text ” , “ relation text ” , “ object text ” ) triples from raw text .",0,0.62668574,27.28003742266659,26
1241,"Some triples are textual versions of facts , i.e. , non-canonicalized mentions of entities and relations .",0,0.5854516,52.09462722057828,17
1241,"In this paper , we investigate whether it is possible to infer new facts directly from the open knowledge graph without any canonicalization or any supervision from curated knowledge .",1,0.92560863,46.66016204335198,30
1241,"For this purpose , we propose the open link prediction task , i.e. , predicting test facts by completing ( “ subject text ” , “ relation text ” , ? ) questions .",2,0.5047101,73.85308413313646,34
1241,An evaluation in such a setup raises the question if a correct prediction is actually a new fact that was induced by reasoning over the open knowledge graph or if it can be trivially explained .,3,0.5293304,54.17663501678484,36
1241,"For example , facts can appear in different paraphrased textual variants , which can lead to test leakage .",0,0.7313076,114.18153919716897,19
1241,"To this end , we propose an evaluation protocol and a methodology for creating the open link prediction benchmark OlpBench .",1,0.5225134,123.00967513737605,21
1241,We performed experiments with a prototypical knowledge graph embedding model for openlink prediction .,2,0.86907256,71.34165464004138,14
1241,"While the task is very challenging , our results suggests that it is possible to predict genuinely new facts , which can not be trivially explained .",3,0.98677593,55.515889294703875,27
1242,"In this paper , we observe that semi-structured tabulated text is ubiquitous ;",1,0.59169513,61.90189214028067,13
1242,"understanding them requires not only comprehending the meaning of text fragments , but also implicit relationships between them .",0,0.89701325,50.259980714816315,19
1242,We argue that such data can prove as a testing ground for understanding how we reason about information .,3,0.55623883,45.0410381797198,19
1242,"To study this , we introduce a new dataset called INFOTABS , comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes .",2,0.69192135,105.44030367949954,28
1242,"Our analysis shows that the semi-structured , multi-domain and heterogeneous nature of the premises admits complex , multi-faceted reasoning .",3,0.9825011,41.225621500712826,20
1242,"Experiments reveal that , while human annotators agree on the relationships between a table-hypothesis pair , several standard modeling strategies are unsuccessful at the task , suggesting that reasoning about tables can pose a difficult modeling challenge .",3,0.9063265,64.97150341725522,39
1243,Existing machine reading comprehension ( MRC ) models do not scale effectively to real-world applications like web-level information retrieval and question answering ( QA ) .,0,0.94678116,18.26393238990141,26
1243,We argue that this stems from the nature of MRC datasets : most of these are static environments wherein the supporting documents and all necessary information are fully observed .,3,0.65336597,90.79022245497907,30
1243,"In this paper , we propose a simple method that reframes existing MRC datasets as interactive , partially observable environments .",1,0.8859788,69.22720048946391,21
1243,"Specifically , we “ occlude ” the majority of a document ’s text and add context-sensitive commands that reveal “ glimpses ” of the hidden text to a model .",2,0.7838693,50.03574289585016,30
1243,"We repurpose SQuAD and NewsQA as an initial case study , and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making .",3,0.45780528,33.57697072654946,34
1243,We believe that this setting can contribute in scaling models to web-level QA scenarios .,3,0.9633472,71.44364645783365,15
1244,"Pretrained neural models such as BERT , when fine-tuned to perform natural language inference ( NLI ) , often show high accuracy on standard datasets , but display a surprising lack of sensitivity to word order on controlled challenge sets .",0,0.78586316,36.93980316121253,41
1244,"We hypothesize that this issue is not primarily caused by the pretrained model ’s limitations , but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage .",3,0.7203979,25.10796339138668,38
1244,"We explore several methods to augment standard training sets with syntactically informative examples , generated by applying syntactic transformations to sentences from the MNLI corpus .",2,0.72820807,52.35358669298569,26
1244,"The best-performing augmentation method , subject / object inversion , improved BERT ’s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73 , without affecting performance on the MNLI test set .",3,0.9347037,92.2727002064075,38
1244,"This improvement generalized beyond the particular construction used for data augmentation , suggesting that augmentation causes BERT to recruit abstract syntactic representations .",3,0.9819505,112.65418265833439,23
1245,Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech .,0,0.91425914,28.45862778578863,22
1245,"One example is Autoregressive Predictive Coding ( Chung et al. , 2019 ) , which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames .",0,0.7565688,58.06150812261231,34
1245,The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks .,0,0.8821616,40.9500093010045,25
1245,In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions .,1,0.8828747,30.228353343853605,29
1245,We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task .,1,0.35367966,44.852263347007934,20
1245,"Experimental results on phonetic classification , speech recognition , and speech translation not only support the hypothesis , but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content .",3,0.8987969,29.25609429602453,35
1246,"Recent Transformer-based contextual word representations , including BERT and XLNet , have shown state-of-the-art performance in multiple disciplines within NLP .",0,0.88075304,20.6430224623046,29
1246,Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream .,0,0.7529406,32.37171820554983,19
1246,"While fine-tuning these pre-trained models is straightforward for lexical applications ( applications with only language modality ) , it is not trivial for multimodal language ( a growing area in NLP focused on modeling face-to-face communication ) .",0,0.5515985,37.750945901675934,41
1246,"More specifically , this is due to the fact that pre-trained models do n’t have the necessary components to accept two extra modalities of vision and acoustic .",0,0.71872365,52.455714597591985,28
1246,"In this paper , we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate ( MAG ) .",1,0.8169853,90.5492324060109,20
1246,MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning .,3,0.7308675,37.150159384369296,13
1246,It does so by generating a shift to internal representation of BERT and XLNet ;,3,0.66409403,151.4604760918688,15
1246,a shift that is conditioned on the visual and acoustic modalities .,0,0.56259304,44.08000208894411,12
1246,"In our experiments , we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis .",2,0.77534723,20.946492431881172,22
1246,Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet .,3,0.91455656,29.60100527973414,26
1246,"On the CMU-MOSI dataset , MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community .",3,0.8283963,39.16366504800914,25
1247,"We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English , which embeds within a broader decision support system for emergency call-takers .",1,0.5751465,94.55326642021953,38
1247,We propose a novel multimodal approach to real-time sequence labeling in speech .,1,0.6010173,15.427140528282495,13
1247,"Our model treats speech and its own textual representation as two separate modalities or views , as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition .",2,0.64051086,88.73622298849557,34
1247,"Our results show significant gains of jointly learning from the two modalities when compared to text or audio only , under adverse noise and limited volume of training data .",3,0.98926044,65.46297392688872,30
1247,The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning .,3,0.96774936,59.887099171437455,19
1248,This paper presents an audio visual automatic speech recognition ( AV-ASR ) system using a Transformer-based architecture .,1,0.79727215,27.554836766994676,22
1248,"We particularly focus on the scene context provided by the visual information , to ground the ASR .",2,0.37439862,89.14730955050806,18
1248,We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer .,2,0.7784426,57.660127677509784,25
1248,"Additionally , we incorporate a multitask training criterion for multiresolution ASR , where we train the model to generate both character and subword level transcriptions .",2,0.79495925,67.61110852932376,26
1248,"Experimental results on the How2 dataset , indicate that multiresolution training can speed up convergence by around 50 % and relatively improves word error rate ( WER ) performance by upto 18 % over subword prediction models .",3,0.9457781,63.70247667226494,38
1248,"Further , incorporating visual information improves performance with relative gains upto 3.76 % over audio only models .",3,0.95180935,137.39220439011277,18
1248,"Our results are comparable to state-of-the-art Listen , Attend and Spell-based architectures .",3,0.98129547,46.9841860052178,18
1249,"End-to-end models for speech translation ( ST ) more tightly couple speech recognition ( ASR ) and machine translation ( MT ) than a traditional cascade of separate ASR and MT models , with simpler model architectures and the potential for reduced error propagation .",0,0.93149644,31.64771339183318,48
1249,"Their performance is often assumed to be superior , though in many conditions this is not yet the case .",0,0.8694081,28.43183261400107,20
1249,"We compare cascaded and end-to-end models across high , medium , and low-resource conditions , and show that cascades remain stronger baselines .",3,0.6918571,37.2459235994059,24
1249,"Further , we introduce two methods to incorporate phone features into ST models .",2,0.6375243,76.40612844259417,14
1249,"We show that these features improve both architectures , closing the gap between end-to-end models and cascades , and outperforming previous academic work – by up to 9 BLEU on our low-resource setting .",3,0.9179283,34.86872181306198,36
1250,"Effective dialogue involves grounding , the process of establishing mutual knowledge that is essential for communication between people .",0,0.8741688,48.16158861515497,19
1250,"Modern dialogue systems are not explicitly trained to build common ground , and therefore overlook this important aspect of communication .",0,0.89620656,77.72638022542314,21
1250,"Improvisational theater ( improv ) intrinsically contains a high proportion of dialogue focused on building common ground , and makes use of the yes-and principle , a strong grounding speech act , to establish coherence and an actionable objective reality .",0,0.93346226,192.69605837396807,43
1250,"We collect a corpus of more than 26,000 yes-and turns , transcribing them from improv dialogues and extracting them from larger , but more sparsely populated movie script dialogue corpora , via a bootstrapped classifier .",2,0.8497678,93.12917632389541,38
1250,"We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded , relevant conversation and confirm these findings with human evaluations .",3,0.78738487,58.07346965668882,25
1251,"To achieve the long-term goal of machines being able to engage humans in conversation , our models should captivate the interest of their speaking partners .",0,0.74216455,33.90071377547618,26
1251,"Communication grounded in images , whereby a dialogue is conducted based on a given photo , is a setup naturally appealing to humans ( Hu et al. , 2014 ) .",0,0.8940723,101.97899152503452,31
1251,In this work we study large-scale architectures and datasets for this goal .,1,0.678822,38.43592382135051,13
1251,"We test a set of neural architectures using state-of-the-art image and text representations , considering various ways to fuse the components .",2,0.81933194,28.86371604565994,28
1251,"To test such models , we collect a dataset of grounded human-human conversations , where speakers are asked to play roles given a provided emotional mood or style , as the use of such traits is also a key factor in engagingness ( Guo et al. , 2019 ) .",2,0.815993,71.69082022293146,50
1251,"Our dataset , Image-Chat , consists of 202 k dialogues over 202 k images using 215 possible style traits .",2,0.8704393,262.63719720981703,22
1251,Automatic metrics and human evaluations of engagingness show the efficacy of our approach ;,3,0.9229276,111.86019921241775,14
1251,"in particular , we obtain state-of-the-art performance on the existing IGC task , and our best performing model is almost on par with humans on the Image-Chat test set ( preferred 47.7 % of the time ) .",3,0.9359582,36.311958122793854,46
1252,"Evaluating the quality of a dialogue interaction between two agents is a difficult task , especially in open-domain chit-chat style dialogue .",0,0.7499796,23.46416642339873,23
1252,"There have been recent efforts to develop automatic dialogue evaluation metrics , but most of them do not generalize to unseen datasets and / or need a human-generated reference response during inference , making it infeasible for online evaluation .",0,0.9193372,34.751161818086764,40
1252,"Here , we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances , and leverages the temporal transitions that exist between them .",1,0.7138578,27.316061201824247,32
1252,"We show that our model achieves higher correlation with human annotations in an online setting , while not requiring true responses for comparison during inference .",3,0.94774866,72.24689702771201,26
1253,The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue .,0,0.91562337,46.81781933602422,22
1253,"We propose neural models that simulate the distributions of these response offsets , taking into account the response turn as well as the preceding turn .",2,0.61734676,79.37528062544345,26
1253,The models are designed to be integrated into the pipeline of an incremental spoken dialogue system ( SDS ) .,2,0.46530187,48.82941236900617,20
1253,We evaluate our models using offline experiments as well as human listening tests .,2,0.74772084,68.73158689884677,14
1253,We show that human listeners consider certain response timings to be more natural based on the dialogue context .,3,0.95310706,45.008544102783766,19
1253,The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions .,3,0.7799745,76.51966595352614,16
1254,"We introduce dodecaDialogue : a set of 12 tasks that measures if a conversational agent can communicate engagingly with personality and empathy , ask questions , answer questions by utilizing knowledge resources , discuss topics and situations , and perceive and converse about images .",2,0.39943337,116.63856124352763,45
1254,"By multi-tasking on such a broad large-scale set of data , we hope to both move towards and measure progress in producing a single unified agent that can perceive , reason and converse with humans in an open-domain setting .",3,0.5868399,50.55418014003671,40
1254,"We show that such multi-tasking improves over a BERT pre-trained baseline , largely due to multi-tasking with very large dialogue datasets in a similar domain , and that the multi-tasking in general provides gains to both text and image-based tasks using several metrics in both the fine-tune and task transfer settings .",3,0.89393,30.246131034047263,54
1254,"We obtain state-of-the-art results on many of the tasks , providing a strong baseline for this challenge .",3,0.9278315,14.811452733711123,23
1255,"In the last few years , a number of successful approaches have emerged that are able to adequately model various aspects of natural language .",0,0.95059884,16.41038727897617,25
1255,"In particular , language models based on neural networks have improved the state of the art with regard to predictive language modeling , while topic models are successful at capturing clear-cut , semantic dimensions .",0,0.82471555,58.13822097011049,35
1255,"In this paper , we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation .",1,0.87454647,39.73066956723398,27
1255,"The system is exclusively trained on standard , non-poetic text , and its output is constrained in order to confer a poetic character to the generated verse .",2,0.37433916,69.5219793326664,28
1255,"The framework is applied to the generation of poems in both English and French , and is equally evaluated for both languages .",3,0.46942523,38.480476713606414,23
1255,"Even though it only uses standard , non-poetic text as input , the system yields state of the art results for poetry generation .",3,0.838344,50.46038600523017,24
1256,"Generating sequential natural language descriptions from graph-structured data ( e.g. , knowledge graph ) is challenging , partly because of the structural differences between the input graph and the output text .",0,0.9329852,27.190966852234723,32
1256,"Hence , popular sequence-to-sequence models , which require serialized input , are not a natural fit for this task .",0,0.8889207,47.35923605706426,22
1256,"Graph neural networks , on the other hand , can better encode the input graph but broaden the structural gap between the encoder and decoder , making faithful generation difficult .",0,0.8365053,64.13845594580694,31
1256,"To narrow this gap , we propose DualEnc , a dual encoding model that can not only incorporate the graph structure , but can also cater to the linear structure of the output text .",2,0.47293574,47.19713361764502,35
1256,Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text .,3,0.8431928,16.209153363601523,20
1257,"We present a simple approach for text infilling , the task of predicting missing spans of text at any position in a document .",1,0.41987455,34.45203909888546,24
1257,"While infilling could enable rich functionality especially for writing assistance tools , more attention has been devoted to language modeling — a special case of infilling where text is predicted at the end of a document .",0,0.9018362,51.42648748078209,37
1257,"In this paper , we aim to extend the capabilities of language models ( LMs ) to the more general task of infilling .",1,0.93894404,24.572653263245165,24
1257,"To this end , we train ( or fine tune ) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked .",2,0.8896424,39.83059759701636,32
1257,"We show that this approach , which we call infilling by language modeling , can enable LMs to infill entire sentences effectively on three different domains : short stories , scientific abstracts , and lyrics .",3,0.85391235,73.80745843288315,36
1257,"Furthermore , we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories .",3,0.9662588,49.6257228899914,25
1258,"Missing sentence generation ( or sentence in-filling ) fosters a wide range of applications in natural language generation , such as document auto-completion and meeting note expansion .",0,0.90956914,65.65563602567104,29
1258,This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context .,0,0.59822845,41.00484706910053,20
1258,Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation .,0,0.8082236,67.75789282938628,21
1258,"In this paper , we propose a framework to decouple the challenge and address these three aspects respectively , leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2 .",1,0.7252789,19.090004384276085,35
1258,We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context .,3,0.82703185,39.98868291851027,26
1259,"Auto-regressive text generation models usually focus on local fluency , and may cause inconsistent semantic meaning in long text generation .",0,0.87632126,59.27932585301578,21
1259,"Further , automatically generating words with similar semantics is challenging , and hand-crafted linguistic rules are difficult to apply .",0,0.83666587,55.39438343835141,21
1259,We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues .,2,0.57080936,55.73893955580675,22
1259,"Specifically , we propose a novel guider network to focus on the generative process over a longer horizon , which can assist next-word prediction and provide intermediate rewards for generator optimization .",2,0.4052022,75.83138004724849,32
1259,Extensive experiments demonstrate that the proposed method leads to improved performance .,3,0.9120845,12.33319651909864,12
1260,Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output .,0,0.87807673,28.08775803111225,24
1260,We propose to extend this framework with a simple and effective post-generation ranking approach .,3,0.40871918,48.233362472418044,15
1260,"Our framework ( i ) retrieves several potentially relevant outputs for each input , ( ii ) edits each candidate independently , and ( iii ) re-ranks the edited candidates to select the final output .",2,0.65934473,46.4102177765508,36
1260,"We use a standard editing model with simple task-specific re-ranking approaches , and we show empirically that this approach outperforms existing , significantly more complex methodologies .",2,0.4821254,45.512443545647145,28
1260,Experiments on two machine translation ( MT ) datasets show new state-of-art results .,3,0.59243745,30.538588751010263,18
1260,"We also achieve near state-of-art performance on the Gigaword summarization dataset , where our analyses show that there is significant room for performance improvement with better candidate output selection in future work .",3,0.95610327,31.026676133615698,37
1261,Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation ( VLN ) .,0,0.95946956,44.938290308683165,22
1261,"In this paper , we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones .",1,0.9043004,38.68067783878693,24
1261,We show that existing state-of-the-art agents do not generalize well .,3,0.88105243,10.598826465905113,17
1261,"To this end , we propose BabyWalk , a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones ( BabySteps ) and completing them sequentially .",1,0.4416985,61.74347019147435,32
1261,A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps .,2,0.44287193,95.88233816663755,21
1261,The learning process is composed of two phases .,2,0.4102747,17.899359135938735,9
1261,"In the first phase , the agent uses imitation learning from demonstration to accomplish BabySteps .",2,0.609132,315.4512021193439,16
1261,"In the second phase , the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions .",2,0.73345524,81.67575225708765,24
1261,We create two new benchmark datasets ( of long navigation tasks ) and use them in conjunction with existing ones to examine BabyWalk ’s generalization ability .,2,0.832687,93.01090562829334,27
1261,"Empirical results show that BabyWalk achieves state-of-the-art results on several metrics , in particular , is able to follow long instructions better .",3,0.9732685,32.49786740887103,28
1261,The codes and the datasets are released on our project page : https://github.com/Sha-Lab/babywalk .,3,0.5282959,32.14897423778402,14
1262,"We introduce a new task , MultiMedia Event Extraction , which aims to extract events and their arguments from multimedia documents .",1,0.57487786,38.64444238406713,22
1262,We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments .,2,0.73353046,81.84503111426349,21
1262,"We propose a novel method , Weakly Aligned Structured Embedding ( WASE ) , that encodes structured representations of semantic information from textual and visual data into a common embedding space .",1,0.41424,24.678381325452456,32
1262,"The structures are aligned across modalities by employing a weakly supervised training strategy , which enables exploiting available resources without explicit cross-media annotation .",2,0.61074305,90.56702317584057,24
1262,"Compared to uni-modal state-of-the-art methods , our approach achieves 4.0 % and 9.8 % absolute F-score gains on text event argument role labeling and visual event extraction .",3,0.895541,27.49926819728276,33
1262,"Compared to state-of-the-art multimedia unstructured representations , we achieve 8.3 % and 5.0 % absolute F-score gains on multimedia event extraction and argument role labeling , respectively .",3,0.9327976,31.98517219820794,32
1262,"By utilizing images , we extract 21.4 % more event mentions than traditional text-only methods .",3,0.90393925,104.46019556133041,16
1263,"We apply a generative segmental model of task structure , guided by narration , to action segmentation in video .",2,0.809839,183.32767046488703,20
1263,We focus on unsupervised and weakly-supervised settings where no action labels are known during training .,2,0.6904813,27.878002696445794,18
1263,"Despite its simplicity , our model performs competitively with previous work on a dataset of naturalistic instructional videos .",3,0.91233444,36.754921209325936,19
1263,"Our model allows us to vary the sources of supervision used in training , and we find that both task structure and narrative language provide large benefits in segmentation quality .",3,0.9277622,60.39776786844868,31
1264,Task is a two-player game in which an Architect ( A ) instructs a Builder ( B ) to construct a target structure in a simulated Blocks World Environment .,2,0.5724924,39.93634668765086,31
1264,"We define the subtask of predicting correct action sequences ( block placements and removals ) in a given game context , and show that capturing B ’s past actions as well as B ’s perspective leads to a significant improvement in performance on this challenging language understanding problem .",3,0.53075796,53.370260719222344,49
1265,Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph .,0,0.9350817,24.195767727151978,36
1265,"Towards this goal , we propose a new approach called Memory-Augmented Recurrent Transformer ( MART ) , which uses a memory module to augment the transformer architecture .",1,0.53786033,26.320536830994012,29
1265,The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence ( w.r.t .,2,0.47271475,64.59558943274816,30
1265,"coreference and repetition aspects ) , thus encouraging coherent paragraph generation .",3,0.7608116,918.623591336146,12
1265,"Extensive experiments , human evaluations , and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods , while maintaining relevance to the input video events .",3,0.834778,81.76487041996245,41
1266,Visual features are a promising signal for learning bootstrap textual models .,0,0.75466645,68.57968469045491,12
1266,"However , blackbox learning models make it difficult to isolate the specific contribution of visual components .",0,0.8489285,63.059019300108865,17
1266,"In this analysis , we consider the case study of the Visually Grounded Neural Syntax Learner ( Shi et al. , 2019 ) , a recent approach for learning syntax from a visual training signal .",2,0.41884992,72.4393840224437,36
1266,"By constructing simplified versions of the model , we isolate the core factors that yield the model ’s strong performance .",2,0.65920997,64.32188290926423,21
1266,"Contrary to what the model might be capable of learning , we find significantly less expressive versions produce similar predictions and perform just as well , or even better .",3,0.97162205,74.4607201445002,30
1266,We also find that a simple lexical signal of noun concreteness plays the main role in the model ’s predictions as opposed to more complex syntactic reasoning .,3,0.9796668,36.25935898706545,28
1267,Variational Autoencoder ( VAE ) is widely used as a generative model to approximate a model ’s posterior on latent variables by combining the amortized variational inference and deep neural networks .,0,0.85836023,18.75539344380312,32
1267,"However , when paired with strong autoregressive decoders , VAE often converges to a degenerated local optimum known as “ posterior collapse ” .",0,0.57187647,65.04965304493336,24
1267,Previous approaches consider the Kullback –Leibler divergence ( KL ) individual for each datapoint .,0,0.7807478,198.22154488860446,16
1267,"We propose to let the KL follow a distribution across the whole dataset , and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL ’s distribution positive .",2,0.41902676,115.09963113203334,35
1267,"Then we propose Batch Normalized-VAE ( BN-VAE ) , a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior ’s parameters .",2,0.6599802,40.725525526200045,37
1267,"Without introducing any new model component or modifying the objective , our approach can avoid the posterior collapse effectively and efficiently .",3,0.8989855,135.58003238980277,22
1267,We further show that the proposed BN-VAE can be extended to conditional VAE ( CVAE ) .,3,0.8486529,43.29626645573112,19
1267,"Empirically , our approach surpasses strong autoregressive baselines on language modeling , text classification and dialogue generation , and rivals more complex approaches while keeping almost the same training time as VAE .",3,0.91445255,51.1934322789857,33
1268,"We study the settings for which deep contextual embeddings ( e.g. , BERT ) give large improvements in performance relative to classic pretrained embeddings ( e.g. , GloVe ) , and an even simpler baseline — random word embeddings — focusing on the impact of the training set size and the linguistic properties of the task .",2,0.60312897,33.38440340948303,57
1268,"Surprisingly , we find that both of these simpler baselines can match contextual embeddings on industry-scale data , and often perform within 5 to 10 % accuracy ( absolute ) on benchmark tasks .",3,0.9592576,96.52884366095974,35
1268,"Furthermore , we identify properties of data for which contextual embeddings give particularly large gains : language containing complex structure , ambiguous word usage , and words unseen in training .",3,0.7191639,140.63272902423358,31
1269,We study the potential for interaction in natural language classification .,1,0.6528275,44.15620915351328,11
1269,"We add a limited form of interaction for intent classification , where users provide an initial query using natural language , and the system asks for additional information using binary or multi-choice questions .",2,0.7694622,48.24319574356255,34
1269,"At each turn , our system decides between asking the most informative question or making the final classification pre-diction .",2,0.60522413,98.57394340940397,20
1269,"The simplicity of the model allows for bootstrapping of the system without interaction data , instead relying on simple crowd-sourcing tasks .",3,0.52448523,42.90772157256421,23
1269,"We evaluate our approach on two domains , showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction .",3,0.4989191,50.882457755531256,30
1270,Knowledge graph ( KG ) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications .,0,0.91191214,24.592551184328876,27
1270,"With a large KG , the embeddings consume a large amount of storage and memory .",0,0.47453514,27.090087104735908,16
1270,This is problematic and prohibits the deployment of these techniques in many real world settings .,0,0.8699693,40.4807344385851,16
1270,"Thus , we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes .",2,0.6406308,16.293604624034487,34
1270,The approach can be trained end-to-end with simple modifications to any existing KG embedding technique .,3,0.66398674,26.013533563909547,18
1270,We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance .,3,0.82153195,37.260525378455704,28
1270,The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference .,3,0.68130726,39.808401307165084,17
1271,This work revisits the task of training sequence tagging models with limited resources using transfer learning .,1,0.64340186,40.51788990998533,17
1271,We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings .,2,0.36856815,82.16426649030547,23
1271,"Specifically , our method demonstrates how by adding a decoding layer for sentence reconstruction , we can improve the performance of various baselines .",3,0.7088249,38.0572093968417,24
1271,We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it .,3,0.9403897,59.53648610098125,38
1272,Pretrained masked language models ( MLMs ) require finetuning for most NLP tasks .,0,0.8757933,24.357814827643534,14
1272,"Instead , we evaluate MLMs out of the box via their pseudo-log-likelihood scores ( PLLs ) , which are computed by masking tokens one by one .",2,0.8265551,42.73850414127058,27
1272,We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks .,3,0.9605015,16.466326104260602,20
1272,"By rescoring ASR and NMT hypotheses , RoBERTa reduces an end-to-end LibriSpeech model ’s WER by 30 % relative and adds up to + 1.7 BLEU on state-of-the-art baselines for low-resource translation pairs , with further gains from domain adaptation .",3,0.9003231,32.7879413581777,48
1272,"We attribute this success to PLL ’s unsupervised expression of linguistic acceptability without a left-to-right bias , greatly improving on scores from GPT-2 ( + 10 points on island effects , NPI licensing in BLiMP ) .",3,0.87465954,164.83527028755086,40
1272,"One can finetune MLMs to give scores without masking , enabling computation in a single inference pass .",3,0.51411545,198.4600662028925,18
1272,"In all , PLLs and their associated pseudo-perplexities ( PPPLs ) enable plug-and-play use of the growing number of pretrained MLMs ;",3,0.51865745,76.68948412765876,24
1272,"e.g. , we use a single cross-lingual model to rescore translations in multiple languages .",2,0.38781458,30.795487332374968,15
1272,We release our library for language model scoring at https://github.com/awslabs/mlm-scoring .,2,0.4675819,26.488672425729384,11
1273,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task , from TransE to the latest state-of-the-art RotatE .",0,0.59400076,31.349633895461572,30
1273,"However , complex relations such as N-to-1 , 1-to-N and N-to-N still remain challenging to predict .",0,0.8555904,16.230523014649215,25
1273,"In this work , we propose a novel distance-based approach for knowledge graph link prediction .",1,0.86749804,26.986550996804866,18
1273,"First , we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations .",2,0.8452292,89.37966710381203,21
1273,"The orthogonal transform embedding for relations keeps the capability for modeling symmetric / anti-symmetric , inverse and compositional relations while achieves better modeling capacity .",3,0.73354,82.48561234334754,25
1273,"Second , the graph context is integrated into distance scoring functions directly .",2,0.6209376,226.72849889991838,13
1273,"Specifically , graph context is explicitly modeled via two directed context representations .",2,0.66703486,165.22101609405019,13
1273,"Each node embedding in knowledge graph is augmented with two context representations , which are computed from the neighboring outgoing and incoming nodes / edges respectively .",2,0.6578877,75.60334197910537,27
1273,"The proposed approach improves prediction accuracy on the difficult N-to-1 , 1-to-N and N-to-N cases .",3,0.85308677,21.013128708471537,24
1273,"Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18 , especially on FB15k-237 which has many high in-degree nodes .",3,0.96862185,25.13512551244195,41
1274,Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability .,0,0.84965473,31.64806048226799,25
1274,"In many settings however , the quality of posterior probability itself ( e.g. , 65 % chance having diabetes ) , gives more reliable information than the final predicted class alone .",3,0.68956375,270.5144683837609,32
1274,"When these methods are shown to be poorly calibrated , most fixes to date have relied on posterior calibration , which rescales the predicted probabilities but often has little impact on final classifications .",0,0.6976914,84.43908508896888,34
1274,Here we propose an end-to-end training procedure called posterior calibrated ( PosCal ) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities .,1,0.4430031,46.49367901086339,32
1274,We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives .,3,0.96905184,51.15946353810605,25
1274,"Our PosCal achieves about 2.5 % of task performance gain and 16.1 % of calibration error reduction on GLUE ( Wang et al. , 2018 ) compared to the baseline .",3,0.9239192,48.20408155210534,31
1274,"We achieved the comparable task performance with 13.2 % calibration error reduction on xSLUE ( Kang and Hovy , 2019 ) , but not outperforming the two-stage calibration baseline .",3,0.9329213,99.49996186466502,31
1274,PosCal training can be easily extendable to any types of classification tasks as a form of regularization term .,3,0.51477,60.55865641947563,19
1274,"Also , PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process , making efficient use of large training sets .",3,0.6092688,76.5441894286812,29
1275,Text generation often requires high-precision output that obeys task-specific rules .,0,0.89671946,40.22490694942389,12
1275,This fine-grained control is difficult to enforce with off-the-shelf deep learning models .,0,0.67366344,10.73868338139008,16
1275,"In this work , we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach .",1,0.4102641,66.48226210705377,23
1275,"Under this formulation , task-specific knowledge can be encoded through a range of rich , posterior constraints that are effectively trained into the model .",3,0.3571422,77.31488993599206,26
1275,"This approach allows users to ground internal model decisions based on prior knowledge , without sacrificing the representational power of neural generative models .",2,0.4412347,43.57798566306756,24
1275,Experiments consider applications of this approach for text generation .,2,0.5703023,63.85660692153587,10
1275,"We find that this method improves over standard benchmarks , while also providing fine-grained control .",3,0.97257805,41.00573672363259,17
1276,We systematically measure out-of-distribution ( OOD ) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts .,2,0.7903552,44.44941567380658,24
1276,"We measure the generalization of previous models including bag-of-words models , ConvNets , and LSTMs , and we show that pretrained Transformers ’ performance declines are substantially smaller .",3,0.6115593,45.921367100417164,29
1276,"Pretrained transformers are also more effective at detecting anomalous or OOD examples , while many previous models are frequently worse than chance .",3,0.8137396,74.80553018376392,23
1276,"We examine which factors affect robustness , finding that larger models are not necessarily more robust , distillation can be harmful , and more diverse pretraining data can enhance robustness .",3,0.51613855,57.505897755659,31
1276,"Finally , we show where future work can improve OOD robustness .",3,0.78028905,69.16668642743372,12
1277,"Despite excellent performance on many tasks , NLP systems are easily fooled by small adversarial perturbations of inputs .",0,0.9069573,38.81152177287323,19
1277,"Existing procedures to defend against such perturbations are either ( i ) heuristic in nature and susceptible to stronger attacks or ( ii ) provide guaranteed robustness to worst-case attacks , but are incompatible with state-of-the-art models like BERT .",0,0.8172995,27.781786280564074,48
1277,"In this work , we introduce robust encodings ( RobEn ) : a simple framework that confers guaranteed robustness , without making compromises on model architecture .",1,0.54658407,112.57551370831101,27
1277,"The core component of RobEn is an encoding function , which maps sentences to a smaller , discrete space of encodings .",0,0.7649857,106.08436122690566,22
1277,"Systems using these encodings as a bottleneck confer guaranteed robustness with standard training , and the same encodings can be used across multiple tasks .",0,0.5281984,74.33676673080349,25
1277,"We identify two desiderata to construct robust encoding functions : perturbations of a sentence should map to a small set of encodings ( stability ) , and models using encodings should still perform well ( fidelity ) .",3,0.7963645,64.64430508701606,38
1277,We instantiate RobEn to defend against a large family of adversarial typos .,2,0.7357675,114.36026180632737,13
1277,"Across six tasks from GLUE , our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3 % against all adversarial typos in the family considered , while previous work using a typo-corrector achieves only 35.3 % accuracy against a simple greedy attack .",3,0.9102479,99.01568412503917,47
1278,"In natural language processing , a recently popular line of work explores how to best report the experimental results of neural networks .",0,0.9107553,47.33761811132377,23
1278,"One exemplar publication , titled “ Show Your Work : Improved Reporting of Experimental Results ” ( Dodge et al. , 2019 ) , advocates for reporting the expected validation effectiveness of the best-tuned model , with respect to the computational budget .",0,0.68264323,156.4757847545438,43
1278,"In the present work , we critically examine this paper .",1,0.878373,68.99072481573481,11
1278,"As far as statistical generalizability is concerned , we find unspoken pitfalls and caveats with this approach .",3,0.79614764,48.83583908708931,18
1278,We analytically show that their estimator is biased and uses error-prone assumptions .,3,0.66675436,32.87086986490967,13
1278,We find that the estimator favors negative errors and yields poor bootstrapped confidence intervals .,3,0.96990246,66.4617228944465,15
1278,We derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation .,2,0.4758853,89.53886485988194,16
1278,Our codebase is at https://github.com/castorini/meanmax .,3,0.49238542,19.19806832078495,6
1279,"BERT ( Bidirectional Encoder Representations from Transformers ) and related pre-trained Transformers have provided large gains across many language understanding tasks , achieving a new state-of-the-art ( SOTA ) .",0,0.605928,25.448805968804866,34
1279,BERT is pretrained on two auxiliary tasks : Masked Language Model and Next Sentence Prediction .,2,0.68200463,56.793535751259704,16
1279,In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding .,1,0.88104296,17.845758169774843,23
1279,"Span Selection PreTraining ( SSPT ) poses cloze-like training instances , but rather than draw the answer from the model ’s parameters , it is selected from a relevant passage .",2,0.53207636,146.17292302924568,32
1279,We find significant and consistent improvements over both BERT-BASE and BERT-LARGE on multiple Machine Reading Comprehension ( MRC ) datasets .,3,0.96826786,14.155278522494966,25
1279,"Specifically , our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions , a new benchmark MRC dataset , outperforming BERT-LARGE by 3 F1 points on short answer prediction .",3,0.91362584,33.62001917682143,37
1279,"We also show significant impact in HotpotQA , improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system .",3,0.97472703,47.47644929256537,30
1279,"Moreover , we show that our pre-training approach is particularly effective when training data is limited , improving the learning curve by a large amount .",3,0.9593456,19.029707129925015,26
1280,Sentence ordering is the task of arranging the sentences of a given text in the correct order .,0,0.88836503,19.33821789465277,18
1280,Recent work using deep neural networks for this task has framed it as a sequence prediction problem .,0,0.9352285,22.722987669194588,18
1280,"In this paper , we propose a new framing of this task as a constraint solving problem and introduce a new technique to solve it .",1,0.8966209,21.7732631162417,26
1280,"Additionally , we propose a human evaluation for this task .",3,0.4847175,33.26794517480854,11
1280,The results on both automatic and human metrics across four different datasets show that this new technique is better at capturing coherence in documents .,3,0.973435,32.103476880433085,25
1281,"Recently , NLP has seen a surge in the usage of large pre-trained models .",0,0.959083,14.39150205754991,15
1281,"Users download weights of models pre-trained on large datasets , then fine-tune the weights on a task of their choice .",2,0.6178525,31.43497220640892,23
1281,This raises the question of whether downloading untrusted pre-trained weights can pose a security threat .,0,0.78835,27.871662528876197,16
1281,"In this paper , we show that it is possible to construct “ weight poisoning ” attacks where pre-trained weights are injected with vulnerabilities that expose “ backdoors ” after fine-tuning , enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword .",1,0.5599842,36.59759398297147,47
1281,"We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery , such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure .",3,0.78137976,38.03096867961564,36
1281,"Our experiments on sentiment classification , toxicity detection , and spam detection show that this attack is widely applicable and poses a serious threat .",3,0.9338535,46.10172476119629,25
1281,"Finally , we outline practical defenses against such attacks .",1,0.55328894,69.32772314081004,10
1282,Transformers have gradually become a key component for many state-of-the-art natural language representation models .,0,0.9395576,12.871340410679297,21
1282,"A recent Transformer based model-BERTachieved state-of-the-art results on various natural language processing tasks , including GLUE , SQuAD v1.1 , and SQuAD v2.0 .",0,0.783194,16.924088704146808,30
1282,This model however is computationally prohibitive and has a huge number of parameters .,0,0.657099,17.782309366558223,14
1282,In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model .,1,0.7839237,59.06840924247834,18
1282,We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency .,3,0.66936487,68.62957237332239,22
1282,We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers .,3,0.9175351,88.86833695181527,29
1282,"In particular , our schuBERT gives 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters .",3,0.97039425,30.75052708280238,32
1283,We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model .,2,0.5384897,17.72537340868899,20
1283,"In particular , we view our non-autoregressive translation system as an inference network ( Tu and Gimpel , 2018 ) trained to minimize the autoregressive teacher energy .",2,0.5877967,83.44113448468843,28
1283,This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model .,0,0.51904136,26.439899405884695,26
1283,"Our approach , which we call ENGINE ( ENerGy-based Inference NEtworks ) , achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets , approaching the performance of autoregressive models .",3,0.5331978,35.71221756273331,45
1284,Over the last few years two promising research directions in low-resource neural machine translation ( NMT ) have emerged .,0,0.95102495,13.273453684170333,20
1284,The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT .,1,0.33023566,23.510633818855723,19
1284,"The second direction employs monolingual data with self-supervision to pre-train translation models , followed by fine-tuning on small amounts of supervised data .",2,0.6057629,25.047014114817856,23
1284,"In this work , we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT .",1,0.70500505,21.303223400507523,24
1284,We offer three major results : ( i ) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models .,3,0.9558087,40.27221524738469,24
1284,( ii ) Self-supervision improves zero-shot translation quality in multilingual models .,3,0.66260284,50.44454406914064,12
1284,"( iii ) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models , getting up to 33 BLEU on ro-en translation without any parallel data or back-translation .",3,0.91006196,62.940118895906295,37
1285,Back-translation is a widely used data augmentation technique which leverages target monolingual data .,0,0.90648687,14.431923371128809,16
1285,"However , its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation , or translationese .",0,0.9183329,50.351925726440115,31
1285,This is believed to be due to translationese inputs better matching the back-translated training data .,3,0.5490874,62.55674508318888,18
1285,"In this work , we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators .",1,0.58892214,26.48346274000193,36
1285,We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs .,3,0.8457922,18.370920112437908,23
1285,BLEU cannot capture human preferences because references are translationese when source sentences are natural text .,0,0.5591494,205.2148705094457,16
1285,We recommend complementing BLEU with a language model score to measure fluency .,3,0.9594674,41.59494753309888,13
1286,"Adaptive policies are better than fixed policies for simultaneous translation , since they can flexibly balance the tradeoff between translation quality and latency based on the current context information .",3,0.47820434,38.51343630890219,30
1286,"But previous methods on obtaining adaptive policies either rely on complicated training process , or underperform simple fixed policies .",0,0.87299097,153.6879898281557,20
1286,We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies .,2,0.72707516,62.165923467122454,20
1286,"English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency , and more surprisingly , it even surpasses the BLEU score of full-sentence translation in the greedy mode ( and very close to beam mode ) , but with much lower latency .",3,0.95917183,39.58672117544567,53
1287,Neural architectures are the current state of the art in Word Sense Disambiguation ( WSD ) .,0,0.92420787,15.992250534354625,17
1287,"However , they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases ( LKB ) .",0,0.88964146,39.310995329053526,22
1287,"We present Enhanced WSD Integrating Synset Embeddings and Relations ( EWISER ) , a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture , and to exploit pretrained synset embeddings , enabling the network to predict synsets that are not in the training set .",2,0.5562871,54.692163092878914,60
1287,"As a result , we set a new state of the art on almost all the evaluation settings considered , also breaking through , for the first time , the 80 % ceiling on the concatenation of all the standard all-words English WSD evaluation benchmarks .",3,0.85254055,54.02237113453646,47
1287,"On multilingual all-words WSD , we report state-of-the-art results by training on nothing but English .",3,0.38449183,33.27292665295482,23
1288,Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus .,0,0.9119554,42.95795979039987,20
1288,"We show that characters ’ written form , Glyphs , in ideographic languages could carry rich semantics .",3,0.8758294,298.8033633483145,18
1288,"We present a multi-modal model , Glyph2Vec , to tackle Chinese out-of-vocabulary word embedding problem .",2,0.47155797,21.394389393794043,19
1288,"Glyph2 Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding , without the need of accessing any corpus , which is useful for improving Chinese NLP systems , especially for low-resource scenarios .",3,0.3914353,45.804280010903334,44
1288,Experiments across different applications show the significant effectiveness of our model .,3,0.9243806,21.838712682028163,12
1289,We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object ( SVO ) structures .,1,0.57778794,41.94709524780616,27
1289,"Our model induces a joint function-specific word vector space , where vectors of e.g .",2,0.68215036,101.93052155702222,16
1289,plausible SVO compositions lie close together .,3,0.7660647,1092.1510224050844,7
1289,"The model retains information about word group membership even in the joint space , and can thereby effectively be applied to a number of tasks reasoning over the SVO structure .",3,0.7473089,108.54023290798357,31
1289,We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity .,3,0.8360371,20.515909470556423,31
1289,"The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work , while reducing the number of parameters by up to 95 % .",3,0.9853302,33.20101355699559,35
1290,"While automatic term extraction is a well-researched area , computational approaches to distinguish between degrees of technicality are still understudied .",0,0.9368059,31.496466179695876,23
1290,"We semi-automatically create a German gold standard of technicality across four domains , and illustrate the impact of a web-crawled general-language corpus on technicality prediction .",2,0.6617041,54.34097774188722,28
1290,"When defining a classification approach that combines general-language and domain-specific word embeddings , we go beyond previous work and align vector spaces to gain comparative embeddings .",2,0.38654166,42.72849907287629,29
1290,We suggest two novel models to exploit general-vs .,3,0.74039865,217.64170815811147,11
1290,"domain-specific comparisons : a simple neural network model with pre-computed comparative-embedding information as input , and a multi-channel model computing the comparison internally .",2,0.55213946,76.06525620446949,26
1290,"Both models outperform previous approaches , with the multi-channel model performing best .",3,0.8772452,43.348024088019066,13
1291,Metaphor is a linguistic device in which a concept is expressed by mentioning another .,0,0.88640994,45.329689771284464,15
1291,"Identifying metaphorical expressions , therefore , requires a non-compositional understanding of semantics .",0,0.8186003,64.10797134440462,13
1291,"Multiword Expressions ( MWEs ) , on the other hand , are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models .",0,0.9538522,45.826071772821656,30
1291,This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs .,3,0.7653504,29.05426884027471,38
1291,"To the best of our knowledge , this is the first “ MWE-aware ” metaphor identification system paving the way for further experiments on the complex interactions of these phenomena .",3,0.9691329,37.74690488140544,33
1291,The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets .,3,0.98292327,37.750423874726174,24
1292,Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language .,0,0.8934217,23.125532920381136,28
1292,"These embeddings have been widely used in various settings , such as cross-lingual transfer , where a natural language processing ( NLP ) model trained on one language is deployed to another language .",0,0.8700692,16.731396230838378,34
1292,"While the cross-lingual transfer techniques are powerful , they carry gender bias from the source to target languages .",0,0.8589056,37.98914615142619,19
1292,"In this paper , we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications .",1,0.9322342,18.150542965042643,21
1292,We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives .,2,0.6074985,23.960299598562862,26
1292,Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning .,3,0.9733489,23.606247963370475,40
1292,We further provide recommendations for using the multilingual word representations for downstream tasks .,3,0.7816296,28.38976140791069,14
1293,"As part of growing NLP capabilities , coupled with an awareness of the ethical dimensions of research , questions have been raised about whether particular datasets and tasks should be deemed off-limits for NLP research .",0,0.9155779,27.98122278596088,36
1293,"We examine this question with respect to a paper on automatic legal sentencing from EMNLP 2019 which was a source of some debate , in asking whether the paper should have been allowed to be published , who should have been charged with making such a decision , and on what basis .",1,0.78647465,46.40349070876313,53
1293,"We focus in particular on the role of data statements in ethically assessing research , but also discuss the topic of dual use , and examine the outcomes of similar debates in other scientific disciplines .",1,0.6047754,69.48439664278145,36
1294,"Most NLP datasets are not annotated with protected attributes such as gender , making it difficult to measure classification bias using standard measures of fairness ( e.g. , equal opportunity ) .",0,0.8439453,57.1750050891248,32
1294,"However , manually annotating a large dataset with a protected attribute is slow and expensive .",0,0.87331706,36.05161419091046,16
1294,"While it is possible to do so , the smaller this annotated sample is , the less certain we are that the estimate is close to the true bias .",3,0.70908654,33.04265810428226,30
1294,"In this work , we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval .",1,0.4644392,96.9002702835761,22
1294,We provide empirical evidence that a 95 % confidence interval derived this way consistently bounds the true bias .,3,0.85521936,82.5853010098562,19
1294,"In quantifying this uncertainty , our method , which we call Bernstein-bounded unfairness , helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim .",2,0.44168073,60.813826521052846,33
1294,Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases .,3,0.9914928,36.300592675937814,26
1294,"For example , consider a co-reference resolution system that is 5 % more accurate on gender-stereotypical sentences – to claim it is biased with 95 % confidence , we need a bias-specific dataset that is 3.8 times larger than WinoBias , the largest available .",3,0.67611045,45.71268243474806,45
1295,"Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds ( e.g. , African American Vernacular English , Colloquial Singapore English , etc. ) .",0,0.6446117,58.36874552272589,34
1295,"We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models , e.g. , BERT and Transformer , and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data .",3,0.5219117,35.60389947843234,50
1296,Advanced machine learning techniques have boosted the performance of natural language processing .,0,0.94545704,13.109152018305723,13
1296,"Nevertheless , recent studies , e.g. , ( CITATION ) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it .",0,0.8204698,66.90401573329571,28
1296,"However , their analysis is conducted only on models ’ top predictions .",0,0.7468167,204.23008905645267,13
1296,"In this paper , we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels .",1,0.87265706,45.11955215318196,32
1296,We further propose a bias mitigation approach based on posterior regularization .,3,0.45450535,51.82399815343379,12
1296,"With little performance loss , our method can almost remove the bias amplification in the distribution .",3,0.8995861,85.5987187180694,17
1296,Our study sheds the light on understanding the bias amplification .,3,0.96904147,150.7611091717524,11
1297,Recent developments in Neural Relation Extraction ( NRE ) have made significant strides towards Automated Knowledge Base Construction .,0,0.95845205,28.564631031345495,19
1297,"While much attention has been dedicated towards improvements in accuracy , there have been no attempts in the literature to evaluate social biases exhibited in NRE systems .",0,0.91034293,38.261037007698235,28
1297,"In this paper , we create WikiGenderBias , a distantly supervised dataset composed of over 45,000 sentences including a 10 % human annotated test set for the purpose of analyzing gender bias in relation extraction systems .",1,0.54572463,38.44927785311754,37
1297,"We find that when extracting spouse-of and hypernym ( i.e. , occupation ) relations , an NRE system performs differently when the gender of the target entity is different .",3,0.9752866,150.82460008164747,32
1297,"However , such disparity does not appear when extracting relations such as birthDate or birthPlace .",3,0.62162834,86.69482882277055,16
1297,"We also analyze how existing bias mitigation techniques , such as name anonymization , word embedding debiasing , and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases .",3,0.57161915,56.611327995415934,36
1297,"Unfortunately , due to NRE models rely heavily on surface level cues , we find that existing bias mitigation approaches have a negative effect on NRE .",3,0.88551354,66.34285972429305,27
1297,Our analysis lays groundwork for future quantifying and mitigating bias in NRE .,3,0.983174,72.08891004688279,13
1298,We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents .,1,0.5939791,62.04613300542404,19
1298,We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance .,2,0.63634866,151.62552095958483,20
1298,"Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables , and then modifies the result by generating a non-interpretable latent vector responsible for inking variations , jitter , noise from the archiving process , and other unforeseen phenomena associated with Early Modern printing .",2,0.5598255,97.90118227269825,59
1298,"Critically , by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template , we are able to control and isolate what the vector-valued latent variable captures .",3,0.5143678,92.40452744775271,39
1298,We show that our approach outperforms rigid interpretable clustering baselines ( c.f .,3,0.95294154,45.24005514098897,13
1298,Ocular ) and overly-flexible deep generative models ( VAE ) alike on the task of completely unsupervised discovery of typefaces in mixed-fonts documents .,0,0.31500497,103.81193927380887,27
1299,Pooling is an important technique for learning text representations in many neural NLP models .,0,0.84367234,18.212196429695464,15
1299,"In conventional pooling methods such as average , max and attentive pooling , text representations are weighted summations of the L1 or L∞ norm of input features .",0,0.58402175,128.8300742016394,28
1299,"However , their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks .",0,0.76615393,68.74663170773248,22
1299,"In addition , in many popular pooling methods such as max and attentive pooling some features may be over-emphasized , while other useful ones are not fully exploited .",0,0.71988225,60.28540890040145,29
1299,"In this paper , we propose an Attentive Pooling with Learnable Norms ( APLN ) approach for text representation .",1,0.8325651,53.49769540903824,20
1299,"Different from existing pooling methods that use a fixed pooling norm , we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks .",2,0.68240136,22.72496517282533,37
1299,"In addition , we propose two methods to ensure the numerical stability of the model training .",2,0.62989545,39.9013987937115,17
1299,"The first one is scale limiting , which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion .",0,0.48405078,71.96931675310083,22
1299,"The second one is re-formulation , which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation .",2,0.45606378,64.47957179864346,27
1299,Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling .,3,0.93182385,11.129813822301792,19
1300,Multi-task learning ( MTL ) and transfer learning ( TL ) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks .,0,0.8433674,15.315572992994301,32
1300,"However , finding beneficial auxiliary datasets for MTL or TL is a time-and resource-consuming trial-and-error approach .",0,0.8174256,60.38558673927305,25
1300,We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups .,3,0.49452484,112.92352236521168,24
1300,"Our methods can compute the similarity between any two sequence tagging datasets , they do not need to be annotated with the same tagset or multiple labels in parallel .",3,0.7493731,39.83944918279713,30
1300,"Additionally , our methods take tokens and their labels into account , which is more robust than only using either of them as an information source , as conducted in prior work .",2,0.47668877,53.303143248499424,33
1300,We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance .,3,0.9240377,53.200557737549936,31
1300,"We provide an efficient , open-source implementation .",3,0.6635344,82.3748879038774,8
1301,Self-attention networks ( SANs ) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words .,0,0.9245968,38.40240795815238,25
1301,"However , the underlying reasons for their strong performance have not been well explained .",0,0.92695594,28.576572050349377,15
1301,"In this paper , we bridge the gap by assessing the strengths of selective SANs ( SSANs ) , which are implemented with a flexible and universal Gumbel-Softmax .",1,0.89445007,69.85957394449866,30
1301,"Experimental results on several representative NLP tasks , including natural language inference , semantic role labelling , and machine translation , show that SSANs consistently outperform the standard SANs .",3,0.86818993,43.743768183946386,30
1301,"Through well-designed probing experiments , we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs : word order encoding and structure modeling .",3,0.79483837,48.797000608896035,37
1301,"Specifically , the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence .",3,0.87276477,62.14594728033137,23
1302,Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers .,0,0.55805033,27.9821034039438,11
1302,We generate randomly ordered transformers and train them with the language modeling objective .,2,0.88720554,50.63438446298866,14
1302,"We observe that some of these models are able to achieve better performance than the interleaved baseline , and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top .",3,0.97115135,21.585673462890785,39
1302,"We propose a new transformer pattern that adheres to this property , the sandwich transformer , and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks , at no cost in parameters , memory , or training time .",3,0.6314537,60.344569095953496,45
1302,"However , the sandwich reordering pattern does not guarantee performance gains across every task , as we demonstrate on machine translation models .",3,0.8055359,104.71568670168082,23
1302,"Instead , we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains .",3,0.94913244,36.731093423739,21
1303,Model ensemble techniques often increase task performance in neural networks ;,0,0.7853318,612.3319118341587,11
1303,"however , they require increased time , memory , and management effort .",0,0.775657,168.538606239632,13
1303,"In this study , we propose a novel method that replicates the effects of a model ensemble with a single model .",1,0.9158404,18.157091542639122,22
1303,Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors .,2,0.74466395,53.78338600425182,17
1303,Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1 / K-times fewer parameters .,3,0.88128406,42.8698974911014,29
1304,"Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training , especially for classes with low similarity .",0,0.92434263,33.27931323734967,26
1304,"In this situation , transferring from seen classes to unseen classes is extremely hard .",0,0.8893463,116.76549400699459,15
1304,"To tackle this problem , in this paper we propose a self-training based method to efficiently leverage unlabeled data .",1,0.6901791,18.172799572727577,20
1304,"Traditional self-training methods use fixed heuristics to select instances from unlabeled data , whose performance varies among different datasets .",0,0.78529257,40.1489176300031,20
1304,We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection .,2,0.4660066,63.0604626230373,18
1304,Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification .,3,0.9359358,9.067835666730849,23
1305,Multi-modal neural machine translation ( NMT ) aims to translate source sentences into a target language paired with images .,0,0.89136857,15.064673721013394,20
1305,"However , dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities , which have potential to refine multi-modal representation learning .",0,0.7770347,23.38681882232077,30
1305,"To deal with this issue , in this paper , we propose a novel graph-based multi-modal fusion encoder for NMT .",1,0.8101742,17.001195460607825,23
1305,"Specifically , we first represent the input sentence and image using a unified multi-modal graph , which captures various semantic relationships between multi-modal semantic units ( words and visual objects ) .",2,0.861818,39.13960076786658,32
1305,We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations .,2,0.813456,66.87202535619022,19
1305,"Finally , these representations provide an attention-based context vector for the decoder .",3,0.4729054,59.88550003471414,15
1305,We evaluate our proposed encoder on the Multi30 K datasets .,2,0.49977842,237.68036376306418,11
1305,Experimental results and in-depth analysis show the superiority of our multi-modal NMT model .,3,0.97200656,11.420511309765129,15
1306,Recently unsupervised Bilingual Lexicon Induction ( BLI ) without any parallel corpus has attracted much research interest .,0,0.96006846,28.6602444045938,18
1306,One of the crucial parts in methods for the BLI task is the matching procedure .,0,0.77993643,38.488844731384965,16
1306,Previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings .,0,0.8621465,131.20462121989462,18
1306,Thus We propose a relaxed matching procedure to find a more precise matching between two languages .,3,0.4223303,39.565821059839294,17
1306,We also find that aligning source and target language embedding space bidirectionally will bring significant improvement .,3,0.98460877,37.558493835095426,17
1306,We follow the previous iterative framework to conduct experiments .,2,0.7977107,52.100676270998015,10
1306,"Results on standard benchmark demonstrate the effectiveness of our proposed method , which substantially outperforms previous unsupervised methods .",3,0.9376499,17.847106979761794,19
1307,"This paper introduces Dynamic Programming Encoding ( DPE ) , a new segmentation algorithm for tokenizing sentences into subword units .",1,0.79419273,52.92439792317548,21
1307,We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference .,2,0.44467515,47.77698356977389,22
1307,"A mixed character-subword transformer is proposed , which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability .",2,0.38987872,175.67301558891447,29
1307,DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming .,2,0.47652012,78.07283505695118,24
1307,Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences .,3,0.8530415,28.884168659286026,28
1307,"DPE achieves an average improvement of 0.9 BLEU over BPE ( Sennrich et al. , 2016 ) and an average improvement of 0.55 BLEU over BPE dropout ( Provilkov et al. , 2019 ) on several WMT datasets including English < => ( German , Romanian , Estonian , Finnish , Hungarian ) .",3,0.6211115,28.079093911172823,54
1308,We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages .,1,0.36931694,22.380820508202184,23
1308,Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices .,2,0.7222039,49.98459171581213,20
1308,This viewpoint arises from the aim to align the second order information of the two language spaces .,0,0.7944888,84.66094715320608,18
1308,The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation .,3,0.4434613,64.35651986290924,21
1308,"Empirically , the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs .",3,0.89061534,23.002490075303633,28
1308,The performance improvement is more significant for distant language pairs .,3,0.95344335,39.432800840066605,11
1309,Non-autoregressive neural machine translation ( NAT ) predicts the entire target sequence simultaneously and significantly accelerates inference process .,0,0.9208984,55.65973909638131,19
1309,"However , NAT discards the dependency information in a sentence , and thus inevitably suffers from the multi-modality problem : the target tokens may be provided by different possible translations , often causing token repetitions or missing .",0,0.7972335,116.08005592226142,38
1309,"To alleviate this problem , we propose a novel semi-autoregressive model RecoverSAT in this work , which generates a translation as a sequence of segments .",2,0.37891045,42.69716434433182,26
1309,The segments are generated simultaneously while each segment is predicted token-by-token .,2,0.60420066,35.294584007286865,15
1309,"By dynamically determining segment length and deleting repetitive segments , RecoverSAT is capable of recovering from repetitive and missing token errors .",3,0.6256335,194.98283881508584,22
1309,Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model .,3,0.93317586,12.247808382985749,31
1310,"Confidence calibration , which aims to make model predictions equal to the true correctness measures , is important for neural machine translation ( NMT ) because it is able to offer useful indicators of translation errors in the generated output .",0,0.9113962,41.790523023671945,41
1310,"While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data , we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference .",0,0.5939406,16.45243043647075,46
1310,"By carefully designing experiments on three language pairs , our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze , understand and improve NMT models .",3,0.88242203,39.922609429628956,51
1310,"Based on these observations , we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance .",3,0.7236163,46.8609812692936,24
1311,"We propose a Semi-supervIsed GeNerative Active Learning ( SIGNAL ) model to address the imbalance , efficiency , and text camouflage problems of Chinese text spam detection task .",2,0.41495273,270.5661988608327,29
1311,A “ self-diversity ” criterion is proposed for measuring the “ worthiness ” of a candidate for annotation .,2,0.45658126,36.05045383400947,20
1311,A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation .,2,0.6979275,25.32662834843676,22
1311,"The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection , but also can improve the performance of a series of conventional active learning models for Chinese spam detection task .",3,0.96168745,61.02917123662814,37
1311,"To the best of our knowledge , this is the first work to integrate active learning and semi-supervised generative learning for text spam detection .",3,0.92333305,12.428860154857647,25
1312,"Legal Judgement Prediction ( LJP ) is the task of automatically predicting a law case ’s judgment results given a text describing the case ’s facts , which has great prospects in judicial assistance systems and handy services for the public .",0,0.9615389,73.31872381633829,42
1312,"In practice , confusing charges are often presented , because law cases applicable to similar law articles are easily misjudged .",0,0.8223433,336.1363239571055,21
1312,"To address this issue , existing work relies heavily on domain experts , which hinders its application in different law systems .",0,0.90793,87.11095372297434,22
1312,"In this paper , we present an end-to-end model , LADAN , to solve the task of LJP .",1,0.7726302,30.36228860984515,20
1312,"To distinguish confusing charges , we propose a novel graph neural network , GDL , to automatically learn subtle differences between confusing law articles , and also design a novel attention mechanism that fully exploits the learned differences to attentively extract effective discriminative features from fact descriptions .",2,0.70177543,80.94440722635419,48
1312,Experiments conducted on real-world datasets demonstrate the superiority of our LADAN .,3,0.8813677,26.264783401712172,12
1313,"Writing a good job posting is a critical step in the recruiting process , but the task is often more difficult than many people think .",0,0.8593096,26.017918825057986,26
1313,"It is challenging to specify the level of education , experience , relevant skills per the company information and job description .",0,0.7142941,128.75410650299537,22
1313,"To this end , we propose a novel task of Job Posting Generation ( JPG ) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions .",1,0.4340007,40.17924438707904,35
1313,"To deal with this task , we devise a data-driven global Skill-Aware Multi-Attention generation model , named SAMA .",2,0.50948113,62.81149821193378,21
1313,"Specifically , to model the complex mapping relationships between input and output , we design a hierarchical decoder that we first label the job description with multiple skills , then we generate a complete text guided by the skill labels .",2,0.87744313,62.70831250821031,41
1313,"At the same time , to exploit the prior knowledge about the skills , we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results .",2,0.74448025,36.16244104289644,35
1313,The proposed approach is evaluated on real-world job posting data .,2,0.5459333,27.66646271254588,11
1313,Experimental results clearly demonstrate the effectiveness of the proposed method .,3,0.96004003,6.942969723868511,11
1314,"The International Classification of Diseases ( ICD ) provides a standardized way for classifying diseases , which endows each disease with a unique code .",0,0.94683105,27.070259450750722,25
1314,ICD coding aims to assign proper ICD codes to a medical record .,0,0.8645559,111.71228109719895,13
1314,"Since manual coding is very laborious and prone to errors , many methods have been proposed for the automatic ICD coding task .",0,0.9439941,30.956813095256624,23
1314,"However , most of existing methods independently predict each code , ignoring two important characteristics : Code Hierarchy and Code Co-occurrence .",0,0.90682596,81.1047059436727,22
1314,"In this paper , we propose a Hyperbolic and Co-graph Representation method ( HyperCore ) to address the above problem .",1,0.84597355,46.748653439608645,21
1314,"Specifically , we propose a hyperbolic representation method to leverage the code hierarchy .",2,0.64915717,55.81101357116368,14
1314,"Moreover , we propose a graph convolutional network to utilize the code co-occurrence .",2,0.60739625,23.26561941721637,14
1314,Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods .,3,0.9307804,4.244911749370902,22
1315,"Although deep neural networks are effective at extracting high-level features , classification methods usually encode an input into a vector representation via simple feature aggregation operations ( e.g .",0,0.8802496,46.29473184343785,30
1315,pooling ) .,3,0.45530903,381.37541717532855,3
1315,Such operations limit the performance .,0,0.8024365,207.64981929188443,6
1315,"For instance , a multi-label document may contain several concepts .",0,0.7220169,65.40559437555285,11
1315,"In this case , one vector can not sufficiently capture its salient and discriminative content .",0,0.5950011,105.82522794969775,16
1315,"Thus , we propose Hyperbolic Capsule Networks ( HyperCaps ) for Multi-Label Classification ( MLC ) , which have two merits .",1,0.5877362,40.57782754882741,22
1315,"First , hyperbolic capsules are designed to capture fine-grained document information for each label , which has the ability to characterize complicated structures among labels and documents .",0,0.62536967,50.34383511800269,29
1315,"Second , Hyperbolic Dynamic Routing ( HDR ) is introduced to aggregate hyperbolic capsules in a label-aware manner , so that the label-level discriminative information can be preserved along the depth of neural networks .",2,0.6612971,39.205528050890486,39
1315,"To efficiently handle large-scale MLC datasets , we additionally present a new routing method to adaptively adjust the capsule number during routing .",2,0.66820395,74.79861051287824,23
1315,Extensive experiments are conducted on four benchmark datasets .,2,0.7373545,11.328994257877405,9
1315,"Compared with the state-of-the-art methods , HyperCaps significantly improves the performance of MLC especially on tail labels .",3,0.92086625,42.828767568349186,22
1316,Technical support problems are often long and complex .,0,0.8918507,49.12315447556959,9
1316,"They typically contain user descriptions of the problem , the setup , and steps for attempted resolution .",0,0.79045206,164.12805932397654,18
1316,"Often they also contain various non-natural language text elements like outputs of commands , snippets of code , error messages or stack traces .",0,0.8571189,74.33116638962129,24
1316,These elements contain potentially crucial information for problem resolution .,0,0.6949081,109.5117194460959,10
1316,"However , they cannot be correctly parsed by tools designed for natural language .",0,0.88403285,64.10567870568488,14
1316,"In this paper , we address the problem of segmentation for technical support questions .",1,0.91999185,32.02557662168167,15
1316,"We formulate the problem as a sequence labelling task , and study the performance of state of the art approaches .",2,0.5386264,18.950957859542715,21
1316,"We compare this against an intuitive contextual sentence-level classification baseline , and a state of the art supervised text-segmentation approach .",2,0.5986969,56.52231657658415,23
1316,"We also introduce a novel component of combining contextual embeddings from multiple language models pre-trained on different data sources , which achieves a marked improvement over using embeddings from a single pre-trained language model .",2,0.42864156,21.695576312081094,35
1316,"Finally , we also demonstrate the usefulness of such segmentation with improvements on the downstream task of answer retrieval .",3,0.9335778,39.789404734113965,20
1317,"The prosperity of Massive Open Online Courses ( MOOCs ) provides fodder for many NLP and AI research for education applications , e.g. , course concept extraction , prerequisite relation discovery , etc .",0,0.94047517,68.4016937550404,34
1317,"However , the publicly available datasets of MOOC are limited in size with few types of data , which hinders advanced models and novel attempts in related topics .",0,0.9114871,65.51528023660329,29
1317,"Therefore , we present MOOCCube , a large-scale data repository of over 700 MOOC courses , 100k concepts , 8 million student behaviors with an external resource .",2,0.5010268,108.57936750960953,28
1317,"Moreover , we conduct a prerequisite discovery task as an example application to show the potential of MOOCCube in facilitating relevant research .",2,0.47687396,76.29537874409496,23
1317,The data repository is now available at http://moocdata.cn/data/MOOCCube .,3,0.4972439,42.575496031848594,9
1318,The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability .,0,0.9505704,29.97206952256433,23
1318,"In this paper , we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks ( CNN ) towards building an accurate yet interpretable diagnosis system .",1,0.9132555,37.576165501257094,41
1318,"The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks , which is critical for AI-empowered healthcare .",3,0.6842859,24.982323057867404,33
1318,"The evaluation conducted on the real Electronic Medical Record ( EMR ) documents from hospitals and annotated by professional doctors proves that , the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable .",3,0.8880993,69.63531427688277,45
1319,News editorials argue about political issues in order to challenge or reinforce the stance of readers with different ideologies .,0,0.9540343,50.388721978075644,20
1319,Previous research has investigated such persuasive effects for argumentative content .,0,0.93153584,104.98516811869035,11
1319,"In contrast , this paper studies how important the style of news editorials is to achieve persuasion .",1,0.6035814,107.14646391587587,18
1319,"To this end , we first compare content-and style-oriented classifiers on editorials from the liberal NYTimes with ideology-specific effect annotations .",2,0.78455645,130.02166021557443,24
1319,"We find that conservative readers are resistant to NYTimes style , but on liberals , style even has more impact than content .",3,0.9842853,202.25809164938127,23
1319,"Focusing on liberals , we then cluster the leads , bodies , and endings of editorials , in order to learn about writing style patterns of effective argumentation .",2,0.55027825,172.15825396313758,29
1320,"In recent years , a new interesting task , called emotion-cause pair extraction ( ECPE ) , has emerged in the area of text emotion analysis .",0,0.9574648,59.13675132794558,29
1320,It aims at extracting the potential pairs of emotions and their corresponding causes in a document .,0,0.6174249,49.387650507449884,17
1320,"To solve this task , the existing research employed a two-step framework , which first extracts individual emotion set and cause set , and then pair the corresponding emotions and causes .",0,0.72568446,86.47068792468653,32
1320,"However , such a pipeline of two steps contains some inherent flaws : 1 ) the modeling does not aim at extracting the final emotion-cause pair directly ;",0,0.6826666,155.4420870194032,30
1320,2 ) the errors from the first step will affect the performance of the second step .,3,0.7330232,23.744620848818165,17
1320,"To address these shortcomings , in this paper we propose a new end-to-end approach , called ECPE-Two-Dimensional ( ECPE-2D ) , to represent the emotion-cause pairs by a 2D representation scheme .",1,0.7459781,25.50293673282667,40
1320,"A 2D transformer module and two variants , window-constrained and cross-road 2D transformers , are further proposed to model the interactions of different emotion-cause pairs .",2,0.53509647,67.3015465343698,29
1320,"The 2D representation , interaction , and prediction are integrated into a joint framework .",2,0.45595756,82.99729221912278,15
1320,"In addition to the advantages of joint modeling , the experimental results on the benchmark emotion cause corpus show that our approach improves the F1 score of the state-of-the-art from 61.28 % to 68.89 % .",3,0.9463257,20.20915464788405,42
1321,Emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document .,0,0.5525488,57.042577907588914,21
1321,"Previous work employs two-step approaches , in which the first step extracts emotion clauses and cause clauses separately , and the second step trains a classifier to filter out negative pairs .",0,0.7960601,51.08560086833798,33
1321,"However , such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well .",0,0.65802175,43.808947843143585,32
1321,"In this paper , we tackle emotion-cause pair extraction from a ranking perspective , i.e. , ranking clause pair candidates in a document , and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end extraction .",1,0.74882364,63.114972232673374,43
1321,"It models the interrelations between the clauses in a document to learn clause representations with graph attention , and enhances clause pair representations with kernel-based relative position embedding for effective ranking .",2,0.56419474,84.44528592541863,34
1321,"Experimental results show that our approach significantly outperforms the current two-step systems , especially in the condition of extracting multiple pairs in one document .",3,0.97446275,23.9352173695677,26
1322,We present a simple but effective method for aspect identification in sentiment analysis .,3,0.39603758,28.78986739100516,14
1322,"Our unsupervised method only requires word embeddings and a POS tagger , and is therefore straightforward to apply to new domains and languages .",3,0.54827225,30.533135793670688,24
1322,"We introduce Contrastive Attention ( CAt ) , a novel single-head attention mechanism based on an RBF kernel , which gives a considerable boost in performance and makes the model interpretable .",2,0.4293585,52.30128800431031,34
1322,Previous work relied on syntactic features and complex neural models .,0,0.86739665,66.11280045484592,11
1322,"We show that given the simplicity of current benchmark datasets for aspect extraction , such complex models are not needed .",3,0.88818735,79.68385687029543,21
1322,The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat .,3,0.61506695,14.47240645832389,15
1323,"Stance detection is an important task , which aims to classify the attitude of an opinionated text towards a given target .",0,0.92059094,38.8430423512959,22
1323,Remarkable success has been achieved when sufficient labeled training data is available .,0,0.78690195,29.500115212710558,13
1323,"However , annotating sufficient data is labor-intensive , which establishes significant barriers for generalizing the stance classifier to the data with new targets .",0,0.7617736,86.03443962526252,24
1323,"In this paper , we proposed a Semantic-Emotion Knowledge Transferring ( SEKT ) model for cross-target stance detection , which uses the external knowledge ( semantic and emotion lexicons ) as a bridge to enable knowledge transfer across different targets .",1,0.8690913,42.203822419169015,41
1323,"Specifically , a semantic-emotion heterogeneous graph is constructed from external semantic and emotion lexicons , which is then fed into a graph convolutional network to learn multi-hop semantic connections between words and emotion tags .",2,0.74119127,21.98683465732139,35
1323,"Then , the learned semantic-emotion graph representation , which serves as prior knowledge bridging the gap between the source and target domains , is fully integrated into the bidirectional long short-term memory ( BiLSTM ) stance classifier by adding a novel knowledge-aware memory unit to the BiLSTM cell .",2,0.7077581,35.54213042144816,52
1323,Extensive experiments on a large real-world dataset demonstrate the superiority of SEKT against the state-of-the-art baseline methods .,3,0.8445157,10.654273452634818,24
1324,"Cross-domain sentiment analysis has received significant attention in recent years , prompted by the need to combat the domain gap between different applications that make use of sentiment analysis .",0,0.9596531,21.796048607363236,30
1324,"In this paper , we take a novel perspective on this task by exploring the role of external commonsense knowledge .",1,0.89474756,19.143896202936702,21
1324,"We introduce a new framework , KinGDOM , which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts .",2,0.51906914,46.62795617801222,30
1324,These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner .,2,0.56878954,15.16811255294007,19
1324,"Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches , demonstrating the efficacy of our proposed framework .",3,0.81725234,28.01363042522432,32
1325,"The aspect-based sentiment analysis ( ABSA ) consists of two conceptual tasks , namely an aspect extraction and an aspect sentiment classification .",0,0.8485769,38.66224706887089,24
1325,"Rather than considering the tasks separately , we build an end-to-end ABSA solution .",2,0.6910961,39.55942584894288,15
1325,Previous works in ABSA tasks did not fully leverage the importance of syntactical information .,0,0.86057246,75.20154929256597,15
1325,"Hence , the aspect extraction model often failed to detect the boundaries of multi-word aspect terms .",3,0.75419706,104.91105420316596,17
1325,"On the other hand , the aspect sentiment classifier was unable to account for the syntactical correlation between aspect terms and the context words .",3,0.91640496,32.122701992672646,25
1325,This paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for syntactical learning .,1,0.78815913,22.96856230878544,18
1325,"We combine part-of-speech embeddings , dependency-based embeddings and contextualized embeddings ( e.g .",2,0.7968842,13.651226544217884,15
1325,"BERT , RoBERTa ) to enhance the performance of the aspect extractor .",2,0.3845255,55.377797866742924,13
1325,"We also propose the syntactic relative distance to de-emphasize the adverse effects of unrelated words , having weak syntactic connection with the aspect terms .",3,0.5643257,77.54742485132402,25
1325,This increases the accuracy of the aspect sentiment classifier .,3,0.7386986,87.75696461491214,10
1325,Our solutions outperform the state-of-the-art models on SemEval-2014 dataset in both two subtasks .,3,0.8820077,13.05154847813221,20
1326,The main barrier to progress in the task of Formality Style Transfer is the inadequacy of training data .,0,0.75432074,31.34327388145983,19
1326,"In this paper , we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems .",1,0.9402689,50.651843841788825,34
1326,"Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model , leading to the state-of-the-art results in the GYAFC benchmark dataset .",3,0.94159937,38.44992871720207,39
1327,Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews .,0,0.8915756,24.316716039834805,19
1327,Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words .,0,0.8510128,103.02430620182534,18
1327,"However , due to the complexity of language and the existence of multiple aspects in a single sentence , these models often confuse the connections .",0,0.91164875,43.2827252774775,26
1327,"In this paper , we address this problem by means of effective encoding of syntax information .",1,0.89779,43.84833223998296,17
1327,"Firstly , we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree .",2,0.7958918,48.05685487857375,25
1327,"Then , we propose a relational graph attention network ( R-GAT ) to encode the new tree structure for sentiment prediction .",2,0.71424353,45.54162041450881,24
1327,"Extensive experiments are conducted on the SemEval 2014 and Twitter datasets , and the experimental results confirm that the connections between aspects and opinion words can be better established with our approach , and the performance of the graph attention network ( GAT ) is significantly improved as a consequence .",3,0.8522632,41.94197506529829,51
1328,Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis ( ABSA ) .,0,0.782893,48.12125559509199,22
1328,The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems .,3,0.6624907,92.77454921440248,20
1328,"However , traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms .",0,0.87780195,103.03914326695748,18
1328,"Although some recent co-extraction methods have been proposed to extract both terms jointly , they fail to extract them as pairs .",0,0.84026027,37.41213850229669,22
1328,"To this end , this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms Extraction ( PAOTE ) .",1,0.80388916,32.61961602558224,27
1328,"Furthermore , this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works .",1,0.33499533,73.99507006978972,29
1328,"We propose a multi-task learning framework based on shared spans , where the terms are extracted under the supervision of span boundaries .",2,0.6969498,38.602515837225816,23
1328,"Meanwhile , the pair-wise relations are jointly identified using the span representations .",2,0.55966854,59.56428562466907,13
1328,Extensive experiments show that our model consistently outperforms state-of-the-art methods .,3,0.93764174,4.304248655787952,16
1329,Opinion role labeling ( ORL ) is a fine-grained opinion analysis task and aims to answer “ who expressed what kind of sentiment towards what ? ” .,0,0.9604346,44.654451946917895,29
1329,"Due to the scarcity of labeled data , ORL remains challenging for data-driven methods .",0,0.9285323,27.84919778390688,15
1329,"In this work , we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations .",1,0.82973677,61.3152620910112,21
1329,We also propose dependency graph convolutional networks ( DEPGCN ) to encode parser information at different processing levels .,2,0.5032216,72.06313365308237,19
1329,"In order to compensate for parser inaccuracy and reduce error propagation , we introduce multi-task learning ( MTL ) to train the parser and the ORL model simultaneously .",2,0.72041404,36.52778646916515,29
1329,We verify our methods on the benchmark MPQA corpus .,3,0.51611966,51.20336849195047,10
1329,"The experimental results show that syntactic information is highly valuable for ORL , and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline .",3,0.977774,39.3350805840451,32
1329,"In addition , we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations ( BERT ) .",3,0.956625,42.81014643637081,23
1329,Our best model achieves 4.34 higher F1 score than the current state-ofthe-art .,3,0.94344646,22.810905898011434,15
1330,State-of-the-art argument mining studies have advanced the techniques for predicting argument structures .,0,0.9611922,32.63523622631389,17
1330,"However , the technology for capturing non-tree-structured arguments is still in its infancy .",0,0.92933667,26.630299751322855,14
1330,"In this paper , we focus on non-tree argument mining with a neural model .",1,0.8263326,50.537067697715116,15
1330,We jointly predict proposition types and edges between propositions .,2,0.7831112,342.88110248974556,10
1330,Our proposed model incorporates ( i ) task-specific parameterization ( TSP ) that effectively encodes a sequence of propositions and ( ii ) a proposition-level biaffine attention ( PLBA ) that can predict a non-tree argument consisting of edges .,2,0.73719114,42.29906760010857,43
1330,Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines .,3,0.97690886,49.645568608568944,16
1331,"We propose a novel linearization of a constituent tree , together with a new locally normalized model .",2,0.31271648,108.57434548207969,18
1331,"For each split point in a sentence , our model computes the normalizer on all spans ending with that split point , and then predicts a tree span from them .",2,0.78772694,42.882696068212056,31
1331,"Compared with global models , our model is fast and parallelizable .",3,0.88765556,64.32053339661154,12
1331,"Different from previous local models , our linearization method is tied on the spans directly and considers more local features when performing span prediction , which is more interpretable and effective .",2,0.48418406,104.43260418731752,32
1331,Experiments on PTB ( 95.8 F1 ) and CTB ( 92.4 F1 ) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models .,3,0.96253645,15.518061955398393,31
1332,Unsupervised constituency parsing aims to learn a constituency parser from a training corpus without parse tree annotations .,0,0.82151043,34.617974293607084,18
1332,"While many methods have been proposed to tackle the problem , including statistical and neural methods , their experimental results are often not directly comparable due to discrepancies in datasets , data preprocessing , lexicalization , and evaluation metrics .",0,0.87170035,41.127811843780314,40
1332,"In this paper , we first examine experimental settings used in previous work and propose to standardize the settings for better comparability between methods .",1,0.7682749,34.81980714317762,25
1332,"We then empirically compare several existing methods , including decade-old and newly proposed ones , under the standardized settings on English and Japanese , two languages with different branching tendencies .",2,0.8358098,97.31733955725262,31
1332,We find that recent models do not show a clear advantage over decade-old models in our experiments .,3,0.9841834,24.703034921622454,18
1332,We hope our work can provide new insights into existing methods and facilitate future empirical evaluation of unsupervised constituency parsing .,3,0.94688153,24.188852019720965,21
1333,We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks .,1,0.42997587,47.62013801026106,19
1333,"Specifically , our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span .",2,0.7848062,69.95330910729007,28
1333,Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference .,3,0.71044195,67.72898201339777,25
1333,"The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models , which is higher than all the existing methods with similar time complexity .",3,0.9166242,21.732576602687303,35
1333,"Using pre-trained BERT , our model achieves 95.48 F1 , which is competitive with the state-of-the-art while being faster .",3,0.813885,11.128733878059917,26
1333,Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing .,3,0.90102273,41.44262420119384,25
1334,"In the deep learning ( DL ) era , parsing models are extremely simplified with little hurt on performance , thanks to the remarkable capability of multi-layer BiLSTMs in context representation .",0,0.9041888,100.72685553479997,32
1334,"As the most popular graph-based dependency parser due to its high efficiency and performance , the biaffine parser directly scores single dependencies under the arc-factorization assumption , and adopts a very simple local token-wise cross-entropy training loss .",0,0.4542309,45.95826743860431,41
1334,This paper for the first time presents a second-order TreeCRF extension to the biaffine parser .,1,0.497479,34.74390463334406,16
1334,"For a long time , the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF .",0,0.8472513,73.86410753079791,19
1334,"To address this issue , we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs , and to avoid the complex outside algorithm via efficient back-propagation .",1,0.5505191,113.71062945517237,38
1334,"Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era , such as structural learning ( global TreeCRF loss ) and high-order modeling are still useful , and can further boost parsing performance over the state-of-the-art biaffine parser , especially for partially annotated training data .",3,0.9427839,62.034299773268955,60
1334,We release our code at https://github.com/yzhangcs/crfpar .,3,0.51537186,17.983314417758535,7
1335,"Sequence-based neural networks show significant sensitivity to syntactic structure , but they still perform less well on syntactic tasks than tree-based networks .",3,0.5064038,24.334776335452098,27
1335,"Such tree-based networks can be provided with a constituency parse , a dependency parse , or both .",0,0.55829644,62.74938092073482,20
1335,We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task .,1,0.40572423,56.64111063323327,27
1335,"We find that a constituency-based network generalizes more robustly than a dependency-based one , and that combining the two types of structure does not yield further improvement .",3,0.98099536,22.664142369746582,32
1335,"Finally , we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data , suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking .",3,0.97597086,23.819079686947777,48
1336,Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages .,0,0.8781616,41.89063781381056,19
1336,"Compared with relying on multiple monolingual models , using a multilingual model has the benefit of a smaller model size , easier in online serving , and generalizability to low-resource languages .",3,0.71922547,41.03312014537578,32
1336,"However , current multilingual models still underperform individual monolingual models significantly due to model capacity limitations .",0,0.8928116,33.307222307038494,17
1336,"In this paper , we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models ( teachers ) to the unified multilingual model ( student ) .",1,0.91137457,16.98428498110317,39
1336,"We propose two novel KD methods based on structure-level information : ( 1 ) approximately minimizes the distance between the student ’s and the teachers ’ structure-level probability distributions , ( 2 ) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions .",2,0.7051658,37.708406395811274,56
1336,Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models .,3,0.92621726,22.29622903249072,31
1337,"Trending topics in social media content evolve over time , and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner .",0,0.9435487,28.553300875323654,29
1337,"Here we study dynamic online conversation recommendation , to help users engage in conversations that satisfy their evolving interests .",1,0.70033276,93.1571572211434,20
1337,"While most prior work assumes static user interests , our model is able to capture the temporal aspects of user interests , and further handle future conversations that are unseen during training time .",3,0.48351565,66.27990484443141,34
1337,"Concretely , we propose a neural architecture to exploit changes of user interactions and interests over time , to predict which discussions they are likely to enter .",2,0.4248774,66.67312324360557,28
1337,"We conduct experiments on large-scale collections of Reddit conversations , and results on three subreddits show that our model significantly outperforms state-of-the-art models that make a static assumption of user interests .",3,0.7676574,19.18769924081615,38
1337,"We further evaluate on handling “ cold start ” , and observe consistently better performance by our model when considering various degrees of sparsity of user ’s chatting history and conversation contexts .",3,0.82459563,114.79690680752678,33
1337,"Lastly , analyses on our model outputs indicate user interest change , explaining the advantage and efficacy of our approach .",3,0.95041156,195.20433815527937,21
1338,"In this paper , we study Multimodal Named Entity Recognition ( MNER ) for social media posts .",1,0.8888745,14.993975928957953,18
1338,"Existing approaches for MNER mainly suffer from two drawbacks : ( 1 ) despite generating word-aware visual representations , their word representations are insensitive to the visual context ;",0,0.81837636,94.10154507361821,31
1338,( 2 ) most of them ignore the bias brought by the visual context .,0,0.71509266,94.7801429795199,15
1338,"To tackle the first issue , we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations .",2,0.44077688,22.230410016741665,27
1338,"To alleviate the visual bias , we further propose to leverage purely text-based entity span detection as an auxiliary module , and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions .",2,0.5700208,46.28861745885355,40
1338,Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets .,3,0.9369348,4.999227137848922,22
1339,Previous works that integrated news articles to better process stock prices used a variety of neural networks to predict price movements .,0,0.86443466,78.53650414049328,22
1339,"The textual and price information were both encoded in the neural network , and it is therefore difficult to apply this approach in situations other than the original framework of the notoriously hard problem of price prediction .",3,0.54922616,53.673042387191906,38
1339,"In contrast , this paper presents a method to encode the influence of news articles through a vector representation of stocks called a stock embedding .",0,0.36236215,48.93092981202383,26
1339,The stock embedding is acquired with a deep learning framework using both news articles and price history .,2,0.79223186,87.94371152674466,18
1339,"Because the embedding takes the operational form of a vector , it is applicable to other financial problems besides price prediction .",3,0.6080497,61.5659282339732,22
1339,"As one example application , we show the results of portfolio optimization using Reuters & Bloomberg headlines , producing a capital gain 2.8 times larger than that obtained with a baseline method using only stock price data .",3,0.71315753,60.01121948165398,38
1339,This suggests that the proposed stock embedding can leverage textual financial semantics to solve financial prediction problems .,3,0.98111606,113.96081095047293,18
1340,"Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling , which is an understudied but an increasingly important research direction .",0,0.8648463,51.730364069757016,31
1340,"The present level of proliferation of fake , biased , and propagandistic content online has made it impossible to fact-check every single suspicious claim , either manually or automatically .",0,0.91204244,62.69434999067582,32
1340,"Thus , it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content .",0,0.93908274,42.12191539768785,26
1340,"This makes it possible to detect likely “ fake news ” the moment they are published , by simply checking the reliability of their source .",0,0.5009517,48.260716583356114,26
1340,"From a practical perspective , political bias and factuality of reporting have a linguistic aspect but also a social context .",0,0.6373183,91.91756504470382,21
1340,"Here , we study the impact of both , namely ( i ) what was written ( i.e. , what was published by the target medium , and how it describes itself in Twitter ) vs .",1,0.7062168,53.62479511949294,37
1340,"( ii ) who reads it ( i.e. , analyzing the target medium ’s audience on social media ) .",2,0.4893323,188.64972825177432,20
1340,We further study ( iii ) what was written about the target medium ( in Wikipedia ) .,3,0.4878091,275.09039753589536,18
1340,"The evaluation results show that what was written matters most , and we further show that putting all information sources together yields huge improvements over the current state-of-the-art .",3,0.9847038,27.870107613661425,34
1341,We explore the utilities of explicit negative examples in training neural language models .,2,0.43636975,72.10802489772803,14
1341,"Negative examples here are incorrect words in a sentence , such as barks in * The dogs barks .",0,0.5818954,160.71418097718802,19
1341,"Neural language models are commonly trained only on positive examples , a set of sentences in the training data , but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions , such as long-distance agreement .",0,0.91467065,31.462804778025944,46
1341,"In this paper , we first demonstrate that appropriately using negative examples about particular constructions ( e.g. , subject-verb agreement ) will boost the model ’s robustness on them in English , with a negligible loss of perplexity .",1,0.5549642,69.80962434892608,41
1341,The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word .,3,0.56151485,27.014566710875553,22
1341,We then provide a detailed analysis of the trained models .,3,0.44480148,23.70673831993614,11
1341,One of our findings is the difficulty of object-relative clauses for RNNs .,3,0.9822503,38.72110137078263,13
1341,We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause .,3,0.9807217,140.38282159561376,22
1341,"Augmentation of training sentences involving the constructions somewhat helps , but the accuracy still does not reach the level of subject-relative clauses .",3,0.91392714,113.125032633245,25
1341,"Although not directly cognitively appealing , our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions .",3,0.95198834,75.3573020999806,26
1342,"We conduct a thorough study to diagnose the behaviors of pre-trained language encoders ( ELMo , BERT , and RoBERTa ) when confronted with natural grammatical errors .",1,0.5380762,25.248173214423907,28
1342,"Specifically , we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data .",2,0.8972618,40.284027001761146,23
1342,We use this approach to facilitate debugging models on downstream applications .,2,0.44198665,57.631403091223135,12
1342,Results confirm that the performance of all tested models is affected but the degree of impact varies .,3,0.9900685,45.64450713075017,18
1342,"To interpret model behaviors , we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors .",2,0.8149158,86.78797496491657,26
1342,We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions .,3,0.97493875,51.08234897689192,24
1342,We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context .,2,0.4777134,55.41599438689301,23
1342,Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors .,3,0.98902357,20.959970667309868,17
1343,Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks .,0,0.78232384,13.178253141642418,17
1343,"The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words , owing to multi-head attention that is unique in the architecture .",0,0.62669957,32.55067479659313,29
1343,"However , little is known for how linguistic properties are processed , represented , and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model .",0,0.8818049,78.63250742436334,32
1343,"For the initial goal of examining the roles of attention heads in handling a set of linguistic features , we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families ( GPT , GPT2 , BERT , and ELECTRA ) .",2,0.87704843,35.43873001211407,49
1343,"Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads , resulting in additional performance improvements on the downstream tasks .",3,0.7468677,75.39734227586811,40
1344,Attention has been proven successful in many natural language processing ( NLP ) tasks .,0,0.96265584,19.68744150855011,15
1344,"Recently , many researchers started to investigate the interpretability of attention on NLP tasks .",0,0.9463946,42.91956954573016,15
1344,Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations .,0,0.86054593,78.83069234196564,19
1344,"In this work , we present a study on understanding the internal mechanism of attention by looking into the gradient update process , checking its behavior when approaching a local minimum during training .",1,0.9212486,98.43772682790545,34
1344,"We propose to analyze for each word token the following two quantities : its polarity score and its attention score , where the latter is a global assessment on the token ’s significance .",2,0.7710311,84.2275232753186,34
1344,"We discuss conditions under which the attention mechanism may become more ( or less ) interpretable , and show how the interplay between the two quantities can contribute towards model performance .",3,0.58988225,39.90264504693281,32
1345,Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems .,0,0.8719078,104.5530337582314,25
1345,"To this end , we introduce a novel embedding model , named R-MeN , that explores a relational memory network to encode potential dependencies in relationship triples .",2,0.5706201,65.02797501177939,30
1345,R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism .,2,0.547702,50.39882643496902,24
1345,Thus R-MeN encodes new information from interactions between the memory and each input vector to return a corresponding vector .,3,0.7162896,82.47542593128517,22
1345,"Consequently , R-MeN feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple .",3,0.43519697,76.53291203336931,26
1345,"Experimental results show that our proposed R-MeN obtains state-of-the-art results on SEARCH17 for the search personalization task , and on WN11 and FB13 for the triple classification task .",3,0.97670984,31.7091548087214,36
1346,It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data .,0,0.944222,8.679196581166277,25
1346,"In practice , we observe that fine-tuning a pre-trained model on a small dataset may lead to over-and / or under-estimate problem .",3,0.8107851,30.49565397483326,25
1346,"In this paper , we propose MC-Tailor , a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones .",1,0.83837503,24.255796381265903,36
1346,Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach .,3,0.9057312,15.504704050977303,21
1347,"Most Chinese pre-trained models take character as the basic unit and learn representation according to character ’s external contexts , ignoring the semantics expressed in the word , which is the smallest meaningful utterance in Chinese .",0,0.79254377,60.989315918211254,37
1347,"Hence , we propose a novel word-aligned attention to exploit explicit word information , which is complementary to various character-based Chinese pre-trained language models .",1,0.36158532,43.254026209192304,27
1347,"Specifically , we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion .",2,0.48815575,28.589083797526115,34
1347,"As a result , word and character information are explicitly integrated at the fine-tuning procedure .",3,0.5347254,44.99482140843485,16
1347,"Experimental results on five Chinese NLP benchmark tasks demonstrate that our method achieves significant improvements against BERT , ERNIE and BERT-wwm .",3,0.94309556,37.487264597871,24
1348,"Variational autoencoders ( VAEs ) combine latent variables with amortized variational inference , whose optimization usually converges into a trivial local optimum termed posterior collapse , especially in text modeling .",0,0.89000446,79.68146314414679,31
1348,"By tracking the optimization dynamics , we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold .",3,0.49331316,61.34532550117886,21
1348,We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them .,3,0.8335527,74.24515849021462,30
1348,"To this end , we propose Coupled-VAE , which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching .",2,0.5692096,26.239228045947325,39
1348,"We apply the proposed Coupled-VAE approach to various VAE models with different regularization , posterior family , decoder structure , and optimization strategy .",2,0.62166876,106.41433322812283,26
1348,"Experiments on benchmark datasets ( i.e. , PTB , Yelp , and Yahoo ) show consistently improved results in terms of probability estimation and richness of the latent space .",3,0.7760566,45.97419116051486,30
1348,"We also generalize our method to conditional language modeling and propose Coupled-CVAE , which largely improves the diversity of dialogue generation on the Switchboard dataset .",3,0.5706789,50.16068067424683,28
1349,State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution .,0,0.8834957,18.16357752478117,20
1349,"For security reasons , it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution .",0,0.8507112,61.41346134121867,34
1349,"In this work , we propose a certified robust method based on a new randomized smoothing technique , which constructs a stochastic ensemble by applying random word substitutions on the input sentences , and leverage the statistical properties of the ensemble to provably certify the robustness .",1,0.52782816,43.400216098349404,47
1349,"Our method is simple and structure-free in that it only requires the black-box queries of the model outputs , and hence can be applied to any pre-trained models ( such as BERT ) and any types of models ( world-level or subword-level ) .",2,0.46400598,22.00374158762862,51
1349,Our method significantly outperforms recent state-of-the-art methods for certified robustness on both IMDB and Amazon text classification tasks .,3,0.92202324,16.476976528458188,25
1349,"To the best of our knowledge , we are the first work to achieve certified robustness on large systems such as BERT with practically meaningful certified accuracy .",3,0.9182067,36.51210510192301,28
1350,Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages .,0,0.8554623,15.985941512982823,18
1350,"Recent methods are mostly based on unsupervised cross-lingual word embeddings , the key to which is to find initial solutions of word translations , followed by the learning and refinement of mappings between the embedding spaces of two languages .",0,0.84972954,26.891109512626425,40
1350,"However , previous methods find initial solutions just based on word-level information , which may be ( 1 ) limited and inaccurate , and ( 2 ) prone to contain some noise introduced by the insufficiently pre-trained embeddings of some words .",0,0.86608773,58.02087946574304,42
1350,"To deal with those issues , in this paper , we propose a novel graph-based paradigm to induce bilingual lexicons in a coarse-to-fine way .",1,0.73767793,22.44268075303053,29
1350,We first build a graph for each language with its vertices representing different words .,2,0.85576993,42.73823921110253,15
1350,Then we extract word cliques from the graphs and map the cliques of two languages .,2,0.8549833,40.70233560541888,16
1350,"Based on that , we induce the initial word translation solution with the central words of the aligned cliques .",2,0.6758219,160.24891802174835,20
1350,"This coarse-to-fine approach not only leverages clique-level information , which is richer and more accurate , but also effectively reduces the bad effect of the noise in the pre-trained embeddings .",3,0.5590293,27.698888824363195,32
1350,"Finally , we take the initial solution as the seed to learn cross-lingual embeddings , from which we induce bilingual lexicons .",2,0.7210316,31.792651918676633,22
1350,Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods .,3,0.9654559,11.633791539081956,17
1351,"Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy , which may significantly harm the credibility of these systems — fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance .",0,0.8693158,94.29217089800204,44
1351,"Instead of collecting and analyzing bad cases using limited handcrafted error features , here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning .",2,0.48093623,70.36738117802251,31
1351,"Our paradigm could expose pitfalls for a given performance metric , e.g. , BLEU , and could target any given neural machine translation architecture .",3,0.6976759,104.350222090179,25
1351,"We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures , RNN-search , and Transformer .",2,0.8025834,70.80831143957215,21
1351,The results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples .,3,0.9862635,46.75432696934919,17
1351,"We also present a qualitative and quantitative analysis for the preference pattern of the attack , demonstrating its capability of pitfall exposure .",3,0.5604374,77.77416889346054,23
1352,"The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions , and then performs iterative back-translation to jointly boost their translation performance .",0,0.80107015,37.361294834548694,30
1352,"The initialization stage is very important since bad initialization may wrongly squeeze the search space , and too much noise introduced in this stage may hurt the final performance .",0,0.44706672,80.81208815461716,30
1352,"In this paper , we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models .",1,0.89962846,25.579914681485516,20
1352,We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model .,2,0.9023104,40.855824378716676,35
1352,"The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models , followed by the iterative back-translation .",2,0.78880703,43.47279141657039,30
1352,Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores .,3,0.959597,15.15047859155889,24
1352,Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git .,3,0.5258499,40.99089863713254,7
1353,Most of the existing models for document-level machine translation adopt dual-encoder structures .,0,0.84335566,22.690299840340494,14
1353,The representation of the source sentences and the document-level contexts are modeled with two separate encoders .,2,0.66975164,27.431552632262093,18
1353,"Although these models can make use of the document-level contexts , they do not fully model the interaction between the contexts and the source sentences , and can not directly adapt to the recent pre-training models ( e.g. , BERT ) which encodes multiple sentences with a single encoder .",0,0.5045392,26.69949372802518,51
1353,"In this work , we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores .",1,0.76646054,15.647798086389475,29
1353,"Moreover , the pre-training models can further boost the performance of our proposed model .",3,0.9009565,18.659085562822764,15
1354,"In encoder-decoder neural models , multiple encoders are in general used to represent the contextual information in addition to the individual sentence .",0,0.7951315,27.976159770351575,24
1354,"In this paper , we investigate multi-encoder approaches in document-level neural machine translation ( NMT ) .",1,0.9095884,24.469682497364,18
1354,"Surprisingly , we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator .",3,0.9782106,27.784369641330713,23
1354,This makes us rethink the real benefits of multi-encoder in context-aware translation-some of the improvements come from robust training .,3,0.6755007,57.650065571705795,23
1354,We compare several methods that introduce noise and / or well-tuned dropout setup into the training of these encoders .,2,0.65054864,60.04153103277025,22
1354,"Experimental results show that noisy training plays an important role in multi-encoder-based NMT , especially when the training data is small .",3,0.9747805,16.82769734338501,24
1354,"Also , we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods .",2,0.49970812,29.235622285308274,26
1355,The choice of hyper-parameters affects the performance of neural models .,0,0.53144985,13.411834966987618,11
1355,"While much previous research ( Sutskever et al. , 2013 ; Duchi et al. , 2011 ; Kingma and Ba , 2015 ) focuses on accelerating convergence and reducing the effects of the learning rate , comparatively few papers concentrate on the effect of batch size .",0,0.6800445,60.74247461641532,47
1355,"In this paper , we analyze how increasing batch size affects gradient direction , and propose to evaluate the stability of gradients with their angle change .",1,0.92646474,68.38851797060364,27
1355,"Based on our observations , the angle change of gradient direction first tends to stabilize ( i.e .",3,0.9501529,119.41635145700845,18
1355,"gradually decrease ) while accumulating mini-batches , and then starts to fluctuate .",3,0.8258689,325.41690203637,13
1355,We propose to automatically and dynamically determine batch sizes by accumulating gradients of mini-batches and performing an optimization step at just the time when the direction of gradients starts to fluctuate .,2,0.4815974,54.00788314852736,32
1355,"To improve the efficiency of our approach for large models , we propose a sampling approach to select gradients of parameters sensitive to the batch size .",2,0.6244299,40.139202973886704,27
1355,Our approach dynamically determines proper and efficient batch sizes during training .,3,0.6577282,129.3425650872034,12
1355,"In our experiments on the WMT 14 English to German and English to French tasks , our approach improves the Transformer with a fixed 25 k batch size by + 0.73 and + 0.82 BLEU respectively .",3,0.88876677,20.25922023448222,37
1356,Unsupervised neural machine translation ( UNMT ) has recently achieved remarkable results for several language pairs .,0,0.9563173,25.91017043289897,17
1356,"However , it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time .",0,0.845948,20.483793415954228,25
1356,"That is , research on multilingual UNMT has been limited .",0,0.91463554,118.83887831179617,11
1356,"In this paper , we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder , making use of multilingual data to improve UNMT for all language pairs .",1,0.82162035,31.97793607637538,37
1356,"On the basis of the empirical findings , we propose two knowledge distillation methods to further enhance multilingual UNMT performance .",3,0.9166971,24.452355740704494,21
1356,"Our experiments on a dataset with English translated to and from twelve other languages ( including three language families and six language branches ) show remarkable results , surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs .",3,0.88172907,45.32242775321828,54
1357,This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation .,1,0.84022677,15.908693650695202,17
1357,Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads .,0,0.78251237,96.97746444973458,24
1357,"Leveraging the flexibility and speed of a recently proposed Levenshtein Transformer model ( Gu et al. , 2019 ) , our method injects terminology constraints at inference time without any impact on decoding speed .",3,0.52261335,44.03524388462676,35
1357,Our method does not require any modification to the training procedure and can be easily applied at runtime with custom dictionaries .,3,0.7413882,20.361816048436133,22
1357,Experiments on English-German WMT datasets show that our approach improves an unconstrained baseline and previous approaches .,3,0.9003448,20.48262135742641,19
1358,"The standard training algorithm in neural machine translation ( NMT ) suffers from exposure bias , and alternative algorithms have been proposed to mitigate this .",0,0.9522487,37.83912788926568,26
1358,"However , the practical impact of exposure bias is under debate .",0,0.91215986,87.30023886705729,12
1358,"In this paper , we link exposure bias to another well-known problem in NMT , namely the tendency to generate hallucinations under domain shift .",1,0.8722295,65.39751722953228,26
1358,"In experiments on three datasets with multiple test domains , we show that exposure bias is partially to blame for hallucinations , and that training with Minimum Risk Training , which avoids exposure bias , can mitigate this .",3,0.8836259,81.47247367082272,39
1358,"Our analysis explains why exposure bias is more problematic under domain shift , and also links exposure bias to the beam search problem , i.e .",3,0.9194585,104.74365254041797,26
1358,performance deterioration with increasing beam size .,3,0.79232293,155.9940026724674,7
1358,"Our results provide a new justification for methods that reduce exposure bias : even if they do not increase performance on in-domain test sets , they can increase model robustness to domain shift .",3,0.98922133,51.74908968061861,35
1359,We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references .,1,0.4897107,79.62612353489217,20
1359,The proposed method evaluates a translation hypothesis in a regression model .,2,0.71316105,93.00691411890949,12
1359,"The model takes the paired source , reference , and hypothesis sentence all together as an input .",2,0.7312559,176.7861947085665,18
1359,"A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors , and the model predicts a human evaluation score with those vectors .",2,0.6138517,40.69076014944562,28
1359,Our experiments show that our proposed method using Cross-lingual Language Model ( XLM ) trained with a translation language modeling ( TLM ) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences .,3,0.96221834,30.48786801657917,43
1359,"Additionally , using source sentences in our proposed method is confirmed to improve the evaluation performance .",3,0.9634065,61.064306225229,17
1360,This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions .,1,0.89084375,71.01348828902236,19
1360,"To facilitate the development of such agents , we introduce ChartDialogs , a new multi-turn dialog dataset , covering a popular plotting library , matplotlib .",2,0.575567,113.40717805157725,26
1360,"The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types .",2,0.6154984,52.20120313241078,18
1360,"Extensive experiments show the best-performing method achieving 61 % plotting accuracy , demonstrating that the dataset presents a non-trivial challenge for future research on this task .",3,0.9507859,34.20801503258253,27
1361,Code-switching is the use of more than one language in the same conversation or utterance .,0,0.91909546,25.85145230104079,16
1361,"Recently , multilingual contextual embedding models , trained on multiple monolingual corpora , have shown promising results on cross-lingual and multilingual tasks .",0,0.93823874,13.592423847990165,23
1361,"We present an evaluation benchmark , GLUECoS , for code-switched languages , that spans several NLP tasks in English-Hindi and English-Spanish .",2,0.47604907,45.980954657494024,25
1361,"Specifically , our evaluation benchmark includes Language Identification from text , POS tagging , Named Entity Recognition , Sentiment Analysis , Question Answering and a new task for code-switching , Natural Language Inference .",2,0.48699027,50.90914152972847,34
1361,We present results on all these tasks using cross-lingual word embedding models and multilingual models .,3,0.76045334,22.63554321444218,16
1361,"In addition , we fine-tune multilingual models on artificially generated code-switched data .",2,0.7648623,29.501929879414373,13
1361,"Although multilingual models perform significantly better than cross-lingual models , our results show that in most tasks , across both language pairs , multilingual models fine-tuned on code-switched data perform best , showing that multilingual models can be further optimized for code-switching tasks .",3,0.98188907,16.000850822009124,44
1362,"Recently , large-scale datasets have vastly facilitated the development in nearly all domains of Natural Language Processing .",0,0.9578924,27.083654923519838,18
1362,"However , there is currently no cross-task dataset in NLP , which hinders the development of multi-task learning .",0,0.9113949,18.301820608222574,21
1362,"We propose MATINF , the first jointly labeled large-scale dataset for classification , question answering and summarization .",1,0.36589757,95.45662456698645,18
1362,MATINF contains 1.07 million question-answer pairs with human-labeled categories and user-generated question descriptions .,2,0.4647147,39.40922889527141,16
1362,"Based on such rich information , MATINF is applicable for three major NLP tasks , including classification , question answering , and summarization .",3,0.49819124,67.12216776679301,24
1362,We benchmark existing methods and a novel multi-task baseline over MATINF to inspire further research .,2,0.39363635,116.06339638136419,16
1362,Our comprehensive comparison and experiments over MATINF and other datasets demonstrate the merits held by MATINF .,3,0.9744353,134.32463170373467,17
1363,News recommendation is an important technique for personalized news service .,0,0.94108933,86.31645139872053,11
1363,"Compared with product and movie recommendations which have been comprehensively studied , the research on news recommendation is much more limited , mainly due to the lack of a high-quality benchmark dataset .",0,0.8923682,37.01724610327138,33
1363,"In this paper , we present a large-scale dataset named MIND for news recommendation .",1,0.88194895,29.879202680554716,15
1363,"Constructed from the user click logs of Microsoft News , MIND contains 1 million users and more than 160k English news articles , each of which has rich textual content such as title , abstract and body .",0,0.54250187,74.1850687799466,38
1363,We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets .,3,0.5749533,37.353537097105416,34
1363,Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling .,3,0.986198,83.61183951640679,22
1363,Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation .,0,0.79389393,20.44517037132412,24
1363,The MIND dataset will be available at https://msnews.github.io .,3,0.44965744,13.231979576304719,9
1364,"The recent proliferation of ” fake news ” has triggered a number of responses , most notably the emergence of several manual fact-checking initiatives .",0,0.9250278,35.43654172350007,26
1364,"As a result and over time , a large number of fact-checked claims have been accumulated , which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization , as viral claims often come back after a while in social media , and politicians like to repeat their favorite statements , true or false , over and over again .",3,0.47413388,39.766672010071346,82
1364,"As manual fact-checking is very time-consuming ( and fully automatic fact-checking has credibility issues ) , it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked .",0,0.78273135,32.42435152946563,44
1364,"Interestingly , despite the importance of the task , it has been largely ignored by the research community so far .",0,0.9189012,18.63828627319542,21
1364,"Here , we aim to bridge this gap .",1,0.94144857,23.938145034668125,9
1364,"In particular , we formulate the task and we discuss how it relates to , but also differs from , previous work .",1,0.47520965,53.10612047720044,23
1364,"We further create a specialized dataset , which we release to the research community .",2,0.629531,52.11838029938797,15
1364,"Finally , we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches .",3,0.64320743,20.838689932306746,28
1365,Open-domain dialogue generation has gained increasing attention in Natural Language Processing .,0,0.96362036,11.740211714845863,12
1365,Its evaluation requires a holistic means .,0,0.8768692,443.75031499118427,7
1365,Human ratings are deemed as the gold standard .,0,0.5986814,87.50616529571576,9
1365,"As human evaluation is inefficient and costly , an automated substitute is highly desirable .",0,0.8838615,91.09294854356612,15
1365,"In this paper , we propose holistic evaluation metrics that capture different aspects of open-domain dialogues .",1,0.905397,27.427968841277373,17
1365,"Our metrics consist of ( 1 ) GPT-2 based context coherence between sentences in a dialogue , ( 2 ) GPT-2 based fluency in phrasing , ( 3 ) n-gram based diversity in responses to augmented queries , and ( 4 ) textual-entailment-inference based logical self-consistency .",2,0.77384496,34.718342591643705,55
1365,The empirical validity of our metrics is demonstrated by strong correlations with human judgments .,3,0.8494371,20.79747372693744,15
1365,We open source the code and relevant materials .,2,0.6067413,48.11813503990053,9
1366,The hypernymy detection task has been addressed under various frameworks .,0,0.91459614,51.08231243996181,11
1366,"Previously , the design of unsupervised hypernymy scores has been extensively studied .",0,0.9338601,21.737561729289006,13
1366,"In contrast , supervised classifiers , especially distributional models , leverage the global contexts of terms to make predictions , but are more likely to suffer from “ lexical memorization ” .",0,0.7700265,107.92530848006132,32
1366,"In this work , we revisit supervised distributional models for hypernymy detection .",1,0.8216175,45.23801661975161,13
1366,"Rather than taking embeddings of two terms as classification inputs , we introduce a representation learning framework named Bidirectional Residual Relation Embeddings ( BiRRE ) .",2,0.7559083,33.38529488113221,26
1366,"In this model , a term pair is represented by a BiRRE vector as features for hypernymy classification , which models the possibility of a term being mapped to another in the embedding space by hypernymy relations .",2,0.68236816,64.97072890119536,38
1366,"A Latent Projection Model with Negative Regularization ( LPMNR ) is proposed to simulate how hypernyms and hyponyms are generated by neural language models , and to generate BiRRE vectors based on bidirectional residuals of projections .",2,0.49305558,61.689233193465235,37
1366,Experiments verify BiRRE outperforms strong baselines over various evaluation frameworks .,3,0.8150533,89.4427236454829,11
1367,Biomedical named entities often play important roles in many biomedical text mining tools .,0,0.93467206,107.04928105199565,14
1367,"However , due to the incompleteness of provided synonyms and numerous variations in their surface forms , normalization of biomedical entities is very challenging .",0,0.93656266,53.58522690827993,25
1367,"In this paper , we focus on learning representations of biomedical entities solely based on the synonyms of entities .",1,0.8239924,29.555562120947123,20
1367,"To learn from the incomplete synonyms , we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates .",2,0.8227874,60.99138077318225,28
1367,Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves .,2,0.4878396,72.47376125805879,19
1367,"In this way , we avoid the explicit pre-selection of negative samples from more than 400 K candidates .",2,0.49051628,70.03664948636417,19
1367,"On four biomedical entity normalization datasets having three different entity types ( disease , chemical , adverse reaction ) , our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset .",3,0.8630737,74.42419380464024,43
1368,"Hypernymy detection , a.k.a , lexical entailment , is a fundamental sub-task of many natural language understanding tasks .",0,0.915442,34.84483742587719,19
1368,"Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages , e.g. , English , but few investigate the low-resource scenarios .",0,0.8547184,36.13487904233191,24
1368,This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages .,1,0.9011591,18.061926583038307,15
1368,We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue .,3,0.45430544,59.81532288736336,22
1368,"Experiments demonstrate the superiority of our method among the three settings , which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets .",3,0.9334205,27.072925105791914,28
1369,This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space .,1,0.85003287,15.887637881270555,24
1369,"To this end , we made several assumptions about the distribution , modeled the distribution accordingly , and validated each assumption by comparing the goodness of each model .",2,0.8839991,43.11216254783059,29
1369,"Specifically , we considered two types of word classes – the semantic class of direct objects of a verb and the semantic class in a thesaurus – and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class .",2,0.86732227,25.24631923535282,55
1369,"Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance , the geometry of the distribution and the existence of subgroups will have limited impact , and also the negative instances need to be considered for adequate modeling of the distribution .",3,0.9825315,60.72918147173103,53
1369,"We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class , while models based on the offset between positive and negative instances perform best in determining the degree of membership .",3,0.93405056,28.487709856288287,55
1370,"In the literature , existing studies always consider Aspect Sentiment Classification ( ASC ) as an independent sentence-level classification problem aspect by aspect , which largely ignore the document-level sentiment preference information , though obviously such information is crucial for alleviating the information deficiency problem in ASC .",0,0.8905368,58.48796188795224,51
1370,"In this paper , we explore two kinds of sentiment preference information inside a document , i.e. , contextual sentiment consistency w.r.t .",1,0.8293991,58.81716908696554,23
1370,the same aspect ( namely intra-aspect sentiment consistency ) and contextual sentiment tendency w.r.t .,3,0.47849458,126.93784381982314,15
1370,all the related aspects ( namely inter-aspect sentiment tendency ) .,2,0.42796108,365.4511375349753,11
1370,"On the basis , we propose a Cooperative Graph Attention Networks ( CoGAN ) approach for cooperatively learning the aspect-related sentence representation .",2,0.45387974,49.136086084836876,23
1370,"Specifically , two graph attention networks are leveraged to model above two kinds of document-level sentiment preference information respectively , followed by an interactive mechanism to integrate the two-fold preference .",2,0.8252117,88.45309572748523,33
1370,Detailed evaluation demonstrates the great advantage of the proposed approach to ASC over the state-of-the-art baselines .,3,0.9476928,15.530809279689334,23
1370,This justifies the importance of the document-level sentiment preference information to ASC and the effectiveness of our approach capturing such information .,3,0.9498171,61.7185970670127,23
1371,The current aspect extraction methods suffer from boundary errors .,0,0.86355937,84.16661831905893,10
1371,"In general , these errors lead to a relatively minor difference between the extracted aspects and the ground-truth .",3,0.88829154,68.83782317324368,20
1371,"However , they hurt the performance severely .",0,0.5623584,217.75402671553675,8
1371,"In this paper , we propose to utilize a pointer network for repositioning the boundaries .",1,0.868077,39.820238421619635,16
1371,"Recycling mechanism is used , which enables the training data to be collected without manual intervention .",2,0.52529496,29.575574176527645,17
1371,We conduct the experiments on the benchmark datasets SE14 of laptop and SE14-16 of restaurant .,2,0.86967605,112.80662911065026,18
1371,"Experimental results show that our method achieves substantial improvements over the baseline , and outperforms state-of-the-art methods .",3,0.9532676,5.481894893837437,23
1372,"Aspect-based sentiment analysis ( ABSA ) involves three subtasks , i.e. , aspect term extraction , opinion term extraction , and aspect-level sentiment classification .",0,0.7788348,49.31739240125729,26
1372,Most existing studies focused on one of these subtasks only .,0,0.84109634,46.74993521814627,11
1372,Several recent researches made successful attempts to solve the complete ABSA problem with a unified framework .,0,0.91996783,65.68992619332754,17
1372,"However , the interactive relations among three subtasks are still under-exploited .",0,0.8289543,35.99916901710207,12
1372,We argue that such relations encode collaborative signals between different subtasks .,3,0.667936,145.00241157591876,12
1372,"For example , when the opinion term is “ delicious ” , the aspect term must be “ food ” rather than “ place ” .",3,0.59284306,46.35300246256139,26
1372,"In order to fully exploit these relations , we propose a Relation-Aware Collaborative Learning ( RACL ) framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a stacked multi-layer network .",2,0.5739196,31.96763752991866,41
1372,Extensive experiments on three real-world datasets demonstrate that RACL significantly outperforms the state-of-the-art methods for the complete ABSA task .,3,0.90306103,10.752633276625193,26
1373,"We propose SentiBERT , a variant of BERT that effectively captures compositional sentiment semantics .",1,0.39764592,62.1991028602133,15
1373,The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition .,2,0.7215764,164.8762257568148,15
1373,Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification .,3,0.92614096,32.578935953658956,13
1373,"We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related tasks , such as emotion classification tasks .",3,0.9483102,31.00784083819221,34
1373,"Moreover , we conduct ablation studies and design visualization methods to understand SentiBERT .",2,0.6048845,61.4766305857128,14
1373,We show that SentiBERT is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics .,3,0.95729065,73.3766072602498,23
1374,Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text .,2,0.41567755,63.33073414268342,21
1374,"Most existing methods are pipelined framework , which identifies emotions and extracts causes separately , leading to a drawback of error propagation .",0,0.84751916,161.98888988480476,23
1374,"Towards this issue , we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction .",1,0.39084935,43.60049578472535,26
1374,"The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions , from which we can recognize emotions with the corresponding causes simultaneously , thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently .",3,0.45723355,115.50907719180853,43
1374,"Experimental results show that our approach achieves the best performance , outperforming the state-of-the-art methods by 6.71 % ( p< 0.01 ) in F1 measure .",3,0.9622007,12.189472646820564,32
1375,"Previous studies in multimodal sentiment analysis have used limited datasets , which only contain unified multimodal annotations .",0,0.8957106,45.05473200187682,18
1375,"However , the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities .",3,0.574445,54.71592645576617,26
1375,"In this paper , we introduce a Chinese single-and multi-modal sentiment analysis dataset , CH-SIMS , which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations .",1,0.6908148,52.650274163298896,37
1375,It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis .,3,0.59396327,33.933318326359014,19
1375,"Furthermore , we propose a multi-task learning framework based on late fusion as the baseline .",2,0.5217249,24.790911487460832,16
1375,Extensive experiments on the CH-SIMS show that our methods achieve state-of-the-art performance and learn more distinctive unimodal representations .,3,0.9317843,21.689447613134927,25
1375,The full dataset and codes are available for use at https://github.com/thuiar/MMSA .,3,0.57601863,14.61237527965943,12
1376,"End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe , understand , and learn cross-lingual semantics simultaneously .",0,0.939609,23.38296065093857,27
1376,"To obtain a powerful encoder , traditional methods pre-train it on ASR data to capture speech features .",0,0.8417156,86.474976204474,18
1376,"However , we argue that pre-training the encoder only through simple speech recognition is not enough , and high-level linguistic knowledge should be considered .",3,0.91800064,32.114822209510464,27
1376,"Inspired by this , we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages .",2,0.6258435,39.03223883156436,33
1376,The difficulty of these courses is gradually increasing .,0,0.78755987,74.54214355339045,9
1376,Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks .,3,0.95596313,25.717474094743245,20
1377,"In this work , we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition ( ASR ) system .",1,0.91738844,17.31410095463105,34
1377,"We use a state-of-the-art end-to-end ASR system , comprising convolutional and recurrent layers , that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents .",2,0.84791577,23.171860056771774,46
1377,"We examine the effects of accent on the internal representation using three main probing techniques : a) Gradient-based explanation methods , b) Information-theoretic measures , and c ) Outputs of accent and phone classifiers .",2,0.719011,80.10543437018457,39
1377,We find different accents exhibiting similar trends irrespective of the probing technique used .,3,0.98139256,191.36522390533614,14
1377,"We also find that most accent information is encoded within the first recurrent layer , which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents .",3,0.97896147,30.051830792013018,38
1378,Self-attentive neural syntactic parsers using contextualized word embeddings ( e.g .,2,0.47492528,18.045410869806208,11
1378,ELMo or BERT ) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts .,0,0.69389606,49.99891830947676,24
1378,"Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data , using additional unlabeled data to train a neural model might seem redundant .",3,0.55117273,13.189364582675069,28
1378,"However , we show that self-training — a semi-supervised technique for incorporating unlabeled data — sets a new state-of-the-art for the self-attentive parser on disfluency detection , demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations .",3,0.95148677,15.917537423431314,46
1378,We also show that ensembling self-trained parsers provides further gains for disfluency detection .,3,0.97518265,39.084806630067675,14
1379,Pre-trained language models have achieved huge improvement on many NLP tasks .,0,0.8822888,5.200729699481805,12
1379,"However , these methods are usually designed for written text , so they do not consider the properties of spoken language .",0,0.8814478,33.058882854574684,22
1379,"Therefore , this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems .",1,0.8928139,44.124553302854,20
1379,We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks .,1,0.42850134,28.090671222037756,20
1379,The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency .,3,0.75982213,39.153292631098886,17
1379,Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs .,3,0.9116115,24.14529798298663,24
1379,The code is available at https://github.com/MiuLab/Lattice-ELMo .,3,0.56077063,11.292303414030467,7
1380,An increasing number of people in the world today speak a mixed-language as a result of being multilingual .,0,0.94826305,21.193832557286804,21
1380,"However , building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data .",0,0.9350107,27.723155682042446,32
1380,"We therefore propose a new learning method , meta-transfer learning , to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets .",1,0.44539696,32.316057451172156,34
1380,"Our model learns to recognize individual languages , and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data .",2,0.6032589,55.978845997429744,29
1380,"Based on experimental results , our model outperforms existing baselines on speech recognition and language modeling tasks , and is faster to converge .",3,0.92522806,24.478907832532276,24
1381,Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means .,0,0.9379397,36.00874877524337,16
1381,"With the rapid growth of social media , multimodal sarcastic tweets are widely posted on various social platforms .",0,0.96592236,28.526260136444034,19
1381,"In multimodal context , sarcasm is no longer a pure linguistic phenomenon , and due to the nature of social media short text , the opposite is more often manifested via cross-modality expressions .",0,0.85614866,56.42983934632802,34
1381,Thus traditional text-based methods are insufficient to detect multimodal sarcasm .,0,0.8042489,22.62158077476589,13
1381,"To reason with multimodal sarcastic tweets , in this paper , we propose a novel method for modeling cross-modality contrast in the associated context .",1,0.83365047,44.05083739302898,25
1381,Our method models both cross-modality contrast and semantic association by constructing the Decomposition and Relation Network ( namely D&R Net ) .,2,0.69362533,100.05094557643636,22
1381,"The decomposition network represents the commonality and discrepancy between image and text , and the relation network models the semantic association in cross-modality context .",2,0.44513622,81.09333666179711,25
1381,Experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection .,3,0.93293995,10.127994533426193,17
1382,"In this work , we develop SimulSpeech , an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently .",1,0.7216962,24.041853776760433,31
1382,"SimulSpeech consists of a speech encoder , a speech segmenter and a text decoder , where 1 ) the segmenter builds upon the encoder and leverages a connectionist temporal classification ( CTC ) loss to split the input streaming speech in real time , 2 ) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation .",2,0.5905421,66.69194689611108,58
1382,SimulSpeech is more challenging than previous cascaded systems ( with simultaneous automatic speech recognition ( ASR ) and simultaneous neural machine translation ( NMT ) ) .,0,0.7955363,35.54375744830036,27
1382,1 ) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech ;,3,0.5158138,84.48411189908086,34
1382,2 ) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech .,3,0.46579546,47.27256423757193,29
1382,"Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation ( without simultaneous translation ) , and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay .",3,0.8966113,26.620054181356206,60
1383,"Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection , speaker diarization and Automatic speech recognition ( ASR ) .",0,0.9290012,125.26045652504858,28
1383,"We propose a novel framework for predicting utterance level labels directly from speech features , thus removing the dependency on first generating transcripts , and transcription free behavioral coding .",3,0.4167101,190.66088665042528,30
1383,Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features .,2,0.7599627,42.91184445982804,21
1383,This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec .,2,0.6504765,38.17080699581224,18
1383,Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels .,2,0.5683083,88.70927387354537,20
1383,We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes .,3,0.9531586,16.96272364010212,33
1384,Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context .,0,0.9344661,47.35403104377844,18
1384,"In this paper , we model users ’ tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user ’s historical tweet sequence and tweets posted by their neighbours .",2,0.47528842,71.75097659352548,44
1384,We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context .,2,0.846055,46.24510064521666,17
1384,Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines .,3,0.97711426,28.2551216445344,29
1385,"Trust is implicit in many online text conversations — striking up new friendships , or asking for tech support .",0,0.84134835,191.13167515999692,20
1385,But trust can be betrayed through deception .,0,0.8248856,81.46221815320055,8
1385,"We study the language and dynamics of deception in the negotiation-based game Diplomacy , where seven players compete for world domination by forging and breaking alliances with each other .",1,0.45380363,51.91817306503135,32
1385,"Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness .",2,0.7702956,48.63515987494259,28
1385,"Unlike existing datasets , this captures deception in long-lasting relationships , where the interlocutors strategically combine truth with lies to advance objectives .",3,0.44114456,147.3965935437656,23
1385,A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players .,3,0.5154166,60.11676058994418,22
1386,Generative feature matching network ( GFMN ) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks .,0,0.8845423,30.77268351342545,35
1386,"In this paper , we present new GFMN formulations that are effective for sequential data .",1,0.8922407,102.3011694464306,16
1386,"Our experimental results show the effectiveness of the proposed method , SeqGFMN , for three distinct generation tasks in English : unconditional text generation , class-conditional text generation , and unsupervised text style transfer .",3,0.9715522,56.447493653179386,35
1386,SeqGFMN is stable to train and outperforms various adversarial approaches for text generation and text style transfer .,3,0.8513946,57.82982545547527,18
1387,A number of researchers have recently questioned the necessity of increasingly complex neural network ( NN ) architectures .,0,0.9619169,30.607150374247077,19
1387,"In particular , several recent papers have shown that simpler , properly tuned models are at least competitive across several NLP tasks .",0,0.85104114,80.72497077069264,23
1387,"In this work , we show that this is also the case for text generation from structured and unstructured data .",1,0.7060448,15.334196089196526,21
1387,"We consider neural table-to-text generation and neural question generation ( NQG ) tasks for text generation from structured and unstructured data , respectively .",2,0.68245065,22.459927499651137,28
1387,"Table-to-text generation aims to generate a description based on a given table , and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models .",0,0.77817875,24.29251169896154,47
1387,Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks .,3,0.97746867,22.64563195800928,28
1387,Code is available at https://github.com/h-shahidi/2birds-gen .,3,0.52022517,22.00975968808684,6
1388,This paper presents the Bayesian Hierarchical Words Representation ( BHWR ) learning algorithm .,1,0.8045286,35.72091188064655,14
1388,BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors .,2,0.44112274,157.3145162070745,16
1388,"By propagating relevant information between related words , BHWR utilizes the taxonomy to improve the quality of such representations .",3,0.3736856,112.07087343917412,20
1388,Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors .,3,0.9152796,100.54637543159873,23
1388,"Finally , we further show that BHWR produces better representations for rare words .",3,0.9758488,63.15967997219081,14
1389,Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks .,0,0.9289346,12.350595739499914,16
1389,Most of the existing approaches rely on a randomly initialized classifier on top of such networks .,0,0.7790032,32.0958925976943,17
1389,"We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels , while it might have already learned an intrinsic textual representation of the task .",3,0.8964391,35.47006523326923,35
1389,"In this paper , we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase .",1,0.7429702,44.433437416029406,34
1389,"We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise , focusing on the COPA , Swag , HellaSwag and CommonsenseQA datasets .",2,0.7218725,65.65059578459385,30
1389,"By exploiting our scoring method without fine-tuning , we are able to produce strong baselines ( e.g .",3,0.807532,33.656911334534904,18
1389,80 % test accuracy on COPA ) that are comparable to supervised approaches .,3,0.886386,141.31799170136594,14
1389,"Moreover , when fine-tuning directly on the proposed scoring function , we show that our method provides a much more stable training phase across random restarts ( e.g x10 standard deviation reduction on COPA test accuracy ) and requires less annotated data than the standard classifier approach to reach equivalent performances .",3,0.9345648,71.81929931144815,52
1390,"In recent years , knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications , such as recommendation and question answering .",0,0.9508836,53.49305284708508,33
1390,"However , existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness , which makes them still far from satisfactory .",0,0.84291404,27.593999589116127,33
1390,"To mitigate this problem , we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity .",2,0.37488148,41.64018375419517,24
1390,Our framework focuses on the design of scoring functions and highlights two critical characteristics : 1 ) facilitating sufficient feature interactions ;,3,0.5906086,274.465673572553,22
1390,2 ) preserving both symmetry and antisymmetry properties of relations .,3,0.41463116,83.20370084562592,11
1390,"It is noteworthy that owing to the general and elegant design of scoring functions , our framework can incorporate many famous existing methods as special cases .",3,0.83939165,76.00768006996304,27
1390,"Moreover , extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework .",3,0.8255967,22.556796447364693,16
1390,Source codes and data can be found at https://github.com/Wentao-Xu/SEEK .,3,0.55239344,11.835129641234975,10
1391,"Machine translation ( MT ) has benefited from using synthetic training data originating from translating monolingual corpora , a technique known as backtranslation .",0,0.961617,65.55987505838756,24
1391,Combining backtranslated data from different sources has led to better results than when using such data in isolation .,3,0.59075814,34.21004589362236,19
1391,"In this work we analyse the impact that data translated with rule-based , phrase-based statistical and neural MT systems has on new MT systems .",1,0.9333067,88.09682291488197,27
1391,We use a real-world low-resource use-case ( Basque-to-Spanish in the clinical domain ) as well as a high-resource language pair ( German-to-English ) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora .,2,0.8878942,35.75599164455808,45
1391,"We exploit different data selection strategies in order to reduce the amount of data used , while at the same time maintaining high-quality MT systems .",2,0.7229415,31.673272441247178,26
1391,We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora .,2,0.6866554,49.47436650784509,28
1391,"Our experiments show that incorporating backtranslated data from different sources can be beneficial , and that availing of data selection can yield improved performance .",3,0.9809137,37.545332816718116,25
1392,Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs .,0,0.57847214,55.76850257096726,20
1392,This is relevant both for time-critical and on-device computations using neural networks .,0,0.63835573,29.42948633956338,13
1392,"The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations , using a mask computed based on the unpruned converged model .",3,0.53071547,148.52954880317944,29
1392,"On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks , we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85 % , and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity .",3,0.76912415,33.397722245544934,60
1392,"Furthermore , we confirm that the parameter ’s initial sign and not its specific value is the primary factor for successful training , and show that magnitude pruning cannot be used to find winning lottery tickets .",3,0.9758411,81.54285978268734,37
1393,"Neural models have achieved great success on machine reading comprehension ( MRC ) , many of which typically consist of two components : an evidence extractor and an answer predictor .",0,0.9492332,28.82573372665611,31
1393,"The former seeks the most relevant information from a reference text , while the latter is to locate or generate answers from the extracted evidence .",0,0.68115747,48.20241513235872,26
1393,"Despite the importance of evidence labels for training the evidence extractor , they are not cheaply accessible , particularly in many non-extractive MRC tasks such as YES / NO question answering and multi-choice MRC .",0,0.8653501,54.91913107834219,35
1393,"To address this problem , we present a Self-Training method ( STM ) , which supervises the evidence extractor with auto-generated evidence labels in an iterative process .",2,0.44271868,34.7308687051005,29
1393,"At each iteration , a base MRC model is trained with golden answers and noisy evidence labels .",2,0.7080152,156.85400912165568,18
1393,The trained model will predict pseudo evidence labels as extra supervision in the next iteration .,2,0.51205385,153.58264335036145,16
1393,We evaluate STM on seven datasets over three MRC tasks .,2,0.6843099,87.26827441796047,11
1393,"Experimental results demonstrate the improvement on existing MRC models , and we also analyze how and why such a self-training method works in MRC .",3,0.8830085,34.270030992015094,25
1394,"While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem ( MWP ) , most of these models do not capture the relationships and order information among the quantities well .",0,0.9077257,45.674509152518446,42
1394,This results in poor quantity representations and incorrect solution expressions .,0,0.53429675,399.1854660231323,11
1394,"In this paper , we propose Graph2Tree , a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions .",1,0.86018205,21.37472457444012,34
1394,"Included in our Graph2 Tree framework are two graphs , namely the Quantity Cell Graph and Quantity Comparison Graph , which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs .",2,0.6068074,82.26391982932168,43
1394,We conduct extensive experiments on two available datasets .,2,0.8386658,37.71192180007181,9
1394,Our experiment results show that Graph2 Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly .,3,0.9825272,16.54967071441977,22
1394,We also discuss case studies and empirically examine Graph2Tree ’s effectiveness in translating the MWP text into solution expressions .,3,0.62078977,100.22752543184987,21
1395,"In Ordinal Classification tasks , items have to be assigned to classes that have a relative ordering , such as “ positive ” , “ neutral ” , “ negative ” in sentiment analysis .",0,0.48629758,29.718652910825384,35
1395,"Remarkably , the most popular evaluation metrics for ordinal classification tasks either ignore relevant information ( for instance , precision / recall on each of the classes ignores their relative ordering ) or assume additional information ( for instance , Mean Average Error assumes absolute distances between classes ) .",0,0.7813604,93.65385739492929,50
1395,"In this paper we propose a new metric for Ordinal Classification , Closeness Evaluation Measure , that is rooted on Measurement Theory and Information Theory .",1,0.8881785,43.84408801703243,26
1395,Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously .,3,0.9544245,72.25523440997355,30
1395,"In addition , it generalizes some popular classification ( nominal scale ) and error minimization ( interval scale ) metrics , depending on the measurement scale in which it is instantiated .",3,0.49045008,127.58131539796476,32
1396,Distributed representations of words have been an indispensable component for natural language processing ( NLP ) tasks .,0,0.9716517,26.04172501314586,18
1396,"However , the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices ( e.g. , self-driving cars , mobile devices ) .",0,0.86031747,19.802631042346544,33
1396,"In this paper , we propose a novel method to adaptively compress word embeddings .",1,0.8938483,13.730876088203342,15
1396,"We fundamentally follow a code-book approach that represents words as discrete codes such as ( 8 , 5 , 2 , 4 ) .",2,0.73537874,51.30544213899652,24
1396,"However , unlike prior works that assign the same length of codes to all words , we adaptively assign different lengths of codes to each word by learning downstream tasks .",2,0.42195266,34.026776879205165,31
1396,The proposed method works in two steps .,2,0.5982025,28.31891453859403,8
1396,"First , each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks .",2,0.82319194,51.90419989351559,23
1396,"After selecting the code length , each word learns discrete codes through a neural network with a binary constraint .",2,0.6722579,130.4782779485544,20
1396,"To showcase the general applicability of the proposed method , we evaluate the performance on four different downstream tasks .",2,0.5655099,16.05194124365623,20
1396,Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy .,3,0.9815493,33.995881847284835,23
1396,"Moreover , we show that our model assigns word to each code-book by considering the significance of tasks .",3,0.9151967,98.81517917548508,19
1397,This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations .,1,0.8203272,20.178211546032184,19
1397,"We propose a novel method that exploits the BERT neural language model to obtain representations of word usages , clusters these representations into usage types , and measures change along time with three proposed metrics .",2,0.4726672,85.78974940078791,36
1397,We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements .,3,0.74072015,25.254976316753286,24
1397,Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena .,3,0.9647947,27.285273699688794,18
1397,We expect our work to inspire further research in this direction .,3,0.9250024,10.458658805947582,12
1398,Document clustering requires a deep understanding of the complex structure of long-text ;,0,0.9202688,68.19232355927497,13
1398,"in particular , the intra-sentential ( local ) and inter-sentential features ( global ) .",0,0.39754733,42.42996982505022,15
1398,Existing representation learning models do not fully capture these features .,0,0.86182004,31.989938715399713,11
1398,"To address this , we present a novel graph-based representation for document clustering that builds a graph autoencoder ( GAE ) on a Keyword Correlation Graph .",1,0.43554187,29.363380724845847,29
1398,The graph is constructed with topical keywords as nodes and multiple local and global features as edges .,2,0.6957686,42.969258175170005,18
1398,A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them .,2,0.7030433,55.08344096176862,22
1398,"Clustering is then performed on the learned representations , using vector dimensions as features for inducing document classes .",2,0.7286564,113.01283455151389,19
1398,"Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features , including term frequency-inverse document frequency and average embedding .",3,0.88416666,48.77091227297921,34
1399,"Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics , by representing the meaning of a word as a function ( a binary classifier ) , instead of a vector .",0,0.8064908,37.53874508712157,33
1399,"However , the large number of latent variables means that inference is computationally expensive , and training a model is therefore slow to converge .",0,0.8379393,29.80863272714483,25
1399,"In this paper , I introduce the Pixie Autoencoder , which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference .",1,0.664423,26.66039270397105,30
1399,"This allows the model to be trained more effectively , achieving better results on two tasks ( semantic similarity in context and semantic composition ) , and outperforming BERT , a large pre-trained language model .",3,0.7969893,37.996890947880495,36
1400,Pretraining deep language models has led to large performance gains in NLP .,0,0.92545366,18.794906122418666,13
1400,"Despite this success , Schick and Schütze ( 2020 ) recently showed that these models struggle to understand rare words .",0,0.88755244,62.87322728872773,21
1400,"For static word embeddings , this problem has been addressed by separately learning representations for rare words .",0,0.8430751,36.83808015095075,18
1400,"We introduce BERTRAM , a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models .",2,0.3708219,27.142440829547876,31
1400,This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture .,0,0.41885552,28.234124899590157,23
1400,Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks .,3,0.8795177,79.99668429724456,31
1401,Knowing the Most Frequent Sense ( MFS ) of a word has been proved to help Word Sense Disambiguation ( WSD ) models significantly .,0,0.9358072,21.554493225944544,25
1401,"However , the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary .",0,0.9295887,38.98759552911666,26
1401,"To address this issue , in this paper we present CluBERT , an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences .",1,0.78573984,31.907536188030775,31
1401,Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches .,3,0.9806818,47.668755901400154,22
1401,"When used to induce the MFS of a lemma , CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models .",3,0.8848225,17.701406745006384,40
1401,"Moreover , our distributions also prove to be effective in other languages , beating all their alternatives for computing the MFS on the multilingual WSD tasks .",3,0.9662939,131.84865864147352,27
1401,We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert .,2,0.56150335,22.668460219544702,12
1402,Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data .,0,0.91242784,15.971247910549543,15
1402,It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain .,0,0.8059799,71.03685682340694,19
1402,"In this paper , we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation .",1,0.9222518,15.827965671029874,21
1402,"Due to the pre-training task and corpus , BERT is task-agnostic , which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge .",3,0.6028794,51.207348400618294,33
1402,"To tackle these problems , we design a post-training procedure , which contains the target domain masked language model task and a novel domain-distinguish pre-training task .",2,0.74657,39.40653236642261,27
1402,The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way .,3,0.84769785,16.2538815855559,19
1402,"Based on this , we could then conduct the adversarial training to derive the enhanced domain-invariant features .",2,0.48928574,53.45861609097266,18
1402,Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin .,3,0.91501534,5.918915859507839,23
1402,The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method .,3,0.9852749,19.19078741449603,19
1403,Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem .,0,0.9403905,19.07862014797039,21
1403,"We propose to represent such summaries as a small set of talking points , termed key points , each scored according to its salience .",2,0.61884713,91.85832637325998,25
1403,"We show , by analyzing a large dataset of crowd-contributed arguments , that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments .",3,0.79241455,34.79309425844003,33
1403,"Furthermore , we found that a domain expert can often predict these key points in advance .",3,0.9849641,74.11725208578385,17
1403,"We study the task of argument-to-key point mapping , and introduce a novel large-scale dataset for this task .",1,0.56142646,37.549629784940905,22
1403,"We report empirical results for an extensive set of experiments with this dataset , showing promising performance .",3,0.8462861,57.90117944907252,18
1404,"Understanding emotion expressed in language has a wide range of applications , from building empathetic chatbots to detecting harmful online behavior .",0,0.9490391,28.056482042685328,22
1404,"Advancement in this area can be improved using large-scale datasets with a fine-grained typology , adaptable to multiple downstream tasks .",3,0.54343057,24.918648642376667,22
1404,"We introduce GoEmotions , the largest manually annotated dataset of 58 k English Reddit comments , labeled for 27 emotion categories or Neutral .",2,0.7754437,311.2593025814095,24
1404,We demonstrate the high quality of the annotations via Principal Preserved Component Analysis .,3,0.8502214,116.1548042119311,14
1404,We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies .,2,0.4735804,39.10370920687519,24
1404,"Our BERT-based model achieves an average F1-score of .46 across our proposed taxonomy , leaving much room for improvement .",3,0.93833166,21.361448180714582,24
1405,"In a context of offensive content mediation on social media now regulated by European laws , it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman .",0,0.8756147,34.04703199446796,54
1405,"We propose : ( 1 ) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies , ( 2 ) the first French dataset annotated for sexism detection , and ( 3 ) a set of deep learning experiments trained on top of a combination of several tweet ’s vectorial representations ( word embeddings , linguistic features , and various generalization strategies ) .",2,0.4119814,77.39862245486466,69
1405,Our results are encouraging and constitute a first step towards offensive content moderation .,3,0.99091023,39.20465875918285,14
1406,"Recently , sentiment analysis has seen remarkable advance with the help of pre-training approaches .",0,0.9509561,32.176157839597295,15
1406,"However , sentiment knowledge , such as sentiment words and aspect-sentiment pairs , is ignored in the process of pre-training , despite the fact that they are widely used in traditional sentiment analysis approaches .",0,0.8146873,34.718028048148014,35
1406,"In this paper , we introduce Sentiment Knowledge Enhanced Pre-training ( SKEP ) in order to learn a unified sentiment representation for multiple sentiment analysis tasks .",1,0.8158449,23.920812539604103,27
1406,"With the help of automatically-mined knowledge , SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives , so as to embed sentiment information at the word , polarity and aspect level into pre-trained sentiment representation .",2,0.6490886,112.43211797017365,40
1406,"In particular , the prediction of aspect-sentiment pairs is converted into multi-label classification , aiming to capture the dependency between words in a pair .",2,0.5887006,50.17754603478428,25
1406,"Experiments on three kinds of sentiment tasks show that SKEP significantly outperforms strong pre-training baseline , and achieves new state-of-the-art results on most of the test datasets .",3,0.92778224,13.295814842482834,34
1406,We release our code at https://github.com/baidu/Senta .,3,0.5041686,10.720372371530202,7
1407,Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces .,0,0.9400144,19.941758921979275,26
1407,"However , such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism .",0,0.92073435,41.71543484808173,21
1407,"In this study , we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis , and whether the patterns are consistent across different languages .",1,0.96623945,24.906478365352342,42
1407,"We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages , probing for two different syntactic annotation styles : Universal Dependencies ( UD ) , prioritizing deep syntactic relations , and Surface-Syntactic Universal Dependencies ( SUD ) , focusing on surface structure .",2,0.8370262,62.42750873893782,52
1407,We find that both models exhibit a preference for UD over SUD — with interesting variations across languages and layers — and that the strength of this preference is correlated with differences in tree shape .,3,0.977813,44.19926721019939,36
1408,Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences .,0,0.66841847,52.452550569534026,12
1408,"Top-down tree linearizations , which can be based on brackets or shift-reduce actions , have achieved the best accuracy to date .",0,0.57849365,74.70846365067219,24
1408,"In this paper , we show that these results can be improved by using an in-order linearization instead .",1,0.6608076,30.63147468263752,19
1408,"Based on this observation , we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al .",2,0.5358172,54.7653643483674,20
1408,"( 2015 ) ’s approach , achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers .",3,0.6288059,54.05695194857094,29
1408,"Finally , we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers , thus showing that sequence-to-sequence models can match them , not only in accuracy , but also in speed .",3,0.65687114,23.725256264077807,45
1409,"A key problem in processing graph-based meaning representations is graph parsing , i.e .",0,0.91838145,49.91573361806742,16
1409,computing all possible derivations of a given graph according to a ( competence ) grammar .,2,0.42432585,158.08656717317922,16
1409,"We demonstrate , for the first time , that exact graph parsing can be efficient for large graphs and with large Hyperedge Replacement Grammars ( HRGs ) .",3,0.6860155,65.70734431132105,28
1409,The advance is achieved by exploiting locality as terminal edge-adjacency in HRG rules .,3,0.44486305,205.1590034381565,14
1409,"In particular , we highlight the importance of 1 ) a terminal edge-first parsing strategy , 2 ) a categorization of a subclass of HRG , i.e .",3,0.78177446,95.25819375062002,28
1409,"what we call Weakly Regular Graph Grammar , and 3 ) distributing argument-structures to both lexical and phrasal rules .",2,0.48145163,131.19811480439853,21
1410,Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT .,0,0.932643,38.455557776114176,36
1410,"Most effort has been directed at designing the right transition mechanism , but less has been done to answer the question of what a probabilistic model for those transition parsers should look like .",0,0.9068327,24.29033987625525,34
1410,A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank .,3,0.8241822,179.42312741802922,27
1410,"We identify three biases as the causes of this problem : label bias , exposure bias and imbalanced probabilities bias .",3,0.60667515,85.76373602267009,21
1410,"While known techniques for tackling these biases improve results , they still do not make the parser state of the art .",0,0.67744106,66.93069139001335,22
1410,"Instead , we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation .",2,0.7420921,43.36620712034814,35
1410,"The new incremental parser gives better results than all previously published incremental CCG parsers , and outperforms even some widely used non-incremental CCG parsers .",3,0.96075284,33.84372310690533,25
1411,Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser .,0,0.84686434,34.52546008334384,24
1411,"However , all neural rerankers so far have been evaluated on English and Chinese only , both languages with a configurational word order and poor morphology .",0,0.759734,69.12557103856848,27
1411,"In the paper , we re-assess the potential of successful neural reranking models from the literature on English and on two morphologically rich ( er ) languages , German and Czech .",1,0.7881848,68.09851262379644,32
1411,"In addition , we introduce a new variation of a discriminative reranker based on graph convolutional networks ( GCNs ) .",2,0.7169434,20.820786794580563,21
1411,We show that the GCN not only outperforms previous models on English but is the only model that is able to improve results over the baselines on German and Czech .,3,0.9711454,22.48373712657781,31
1411,We explain the differences in reranking performance based on an analysis of a ) the gold tree ratio and b ) the variety in the k-best lists .,3,0.56257176,70.08395429961162,28
1412,"With the recent proliferation of the use of text classifications , researchers have found that there are certain unintended biases in text classification datasets .",0,0.9514776,33.85349604322817,25
1412,"For example , texts containing some demographic identity-terms ( e.g. , “ gay ” , “ black ” ) are more likely to be abusive in existing abusive language detection datasets .",3,0.73958105,35.27801057713308,33
1412,"As a result , models trained with these datasets may consider sentences like “ She makes me happy to be gay ” as abusive simply because of the word “ gay .",3,0.9169826,38.52070020796676,32
1412,"In this paper , we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution .",1,0.79207915,28.062616697366966,28
1412,"Based on this formalization , we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting , which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms .",2,0.612986,43.92763668327751,42
1412,Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models ’ generalization ability .,3,0.9599015,37.933574902415366,22
1413,"Given the fast development of analysis techniques for NLP and speech processing systems , few systematic studies have been conducted to compare the strengths and weaknesses of each method .",0,0.9274919,24.392515241786608,30
1413,As a step in this direction we study the case of representations of phonology in neural network models of spoken language .,1,0.38763636,25.701610606190194,22
1413,"We use two commonly applied analytical techniques , diagnostic classifiers and representational similarity analysis , to quantify to what extent neural activation patterns encode phonemes and phoneme sequences .",2,0.8651703,69.00319403472847,29
1413,We manipulate two factors that can affect the outcome of analysis .,2,0.76082665,57.77329593756748,12
1413,"First , we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models .",2,0.7212258,55.18057875804325,21
1413,"Second , we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal , and global activations pooled over the whole utterance .",2,0.86032593,37.182032715271816,34
1413,"We conclude that reporting analysis results with randomly initialized models is crucial , and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods .",3,0.98560774,48.14409228488457,39
1414,"To increase trust in artificial intelligence systems , a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions .",0,0.84392816,33.44124250228604,27
1414,"In this work , we show that such models are nonetheless prone to generating mutually inconsistent explanations , such as ” Because there is a dog in the image .",1,0.57144296,67.38860387377758,30
1414,” and ” Because there is no dog in the [ same ] image .,3,0.58933693,207.78927943331263,15
1414,"” , exposing flaws in either the decision-making process of the model or in the generation of the explanations .",0,0.5894894,36.52885767993453,21
1414,We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations .,2,0.42403767,51.1782021241385,21
1414,"Moreover , as part of the framework , we address the problem of adversarial attacks with full target sequences , a scenario that was not previously addressed in sequence-to-sequence attacks .",2,0.6584162,30.79495135524693,33
1414,"Finally , we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions .",2,0.47280848,13.413149257571643,29
1414,Our framework shows that this model is capable of generating a significant number of inconsistent explanations .,3,0.937183,31.744479911948602,17
1415,"By introducing a small set of additional parameters , a probe learns to solve specific linguistic tasks ( e.g. , dependency parsing ) in a supervised manner using feature representations ( e.g. , contextualized embeddings ) .",2,0.5619275,42.860117129015634,37
1415,The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge .,3,0.7524502,29.93277143014258,18
1415,"However , this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself .",0,0.83361894,34.217183409813764,27
1415,"Complementary to those works , we propose a parameter-free probing technique for analyzing pre-trained language models ( e.g. , BERT ) .",2,0.5392123,33.29174083790876,24
1415,"Our method does not require direct supervision from the probing tasks , nor do we introduce additional parameters to the probing process .",3,0.49882576,33.82651636441819,23
1415,Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines .,3,0.97184527,25.975015996227654,23
1415,We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema .,3,0.74170715,60.022380595367984,29
1416,"Language models keep track of complex information about the preceding context – including , e.g. , syntactic relations in a sentence .",0,0.7963636,65.50244180534345,22
1416,We investigate whether they also capture information beneficial for resolving pronominal anaphora in English .,1,0.8254851,78.70948461578938,15
1416,"We analyze two state of the art models with LSTM and Transformer architectures , via probe tasks and analysis on a coreference annotated corpus .",2,0.7837591,43.62662666986994,25
1416,The Transformer outperforms the LSTM in all analyses .,3,0.9467787,23.623707366027627,9
1416,"Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information , in the sense of capturing the fact that we use language to refer to entities in the world .",3,0.9885809,23.2832487720708,41
1416,"However , we find traces of the latter aspect , too .",3,0.95316684,183.6245179284053,12
1417,"In the Transformer model , “ self-attention ” combines information from attended embeddings into the representation of the focal embedding in the next layer .",0,0.4471875,35.52379762238597,25
1417,"Thus , across layers of the Transformer , information originating from different tokens gets increasingly mixed .",0,0.5153633,174.73990424296233,17
1417,This makes attention weights unreliable as explanations probes .,0,0.59986454,1138.777035663864,9
1417,"In this paper , we consider the problem of quantifying this flow of information through self-attention .",1,0.88810825,22.598654248329712,17
1417,"We propose two methods for approximating the attention to input tokens given attention weights , attention rollout and attention flow , as post hoc methods when we use attention weights as the relative relevance of the input tokens .",2,0.70961726,80.4385557511859,39
1417,"We show that these methods give complementary views on the flow of information , and compared to raw attention , both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients .",3,0.93653977,112.3560149655814,39
1418,"With the growing popularity of deep-learning based NLP models , comes a need for interpretable systems .",0,0.9485418,30.34180933716098,17
1418,In this opinion piece we reflect on the current state of interpretability evaluation research .,1,0.835657,29.184282250660242,15
1418,"We call for more clearly differentiating between different desired criteria an interpretation should satisfy , and focus on the faithfulness criteria .",3,0.6394393,111.65247651059778,22
1418,"We survey the literature with respect to faithfulness evaluation , and arrange the current approaches around three assumptions , providing an explicit form to how faithfulness is “ defined ” by the community .",1,0.416724,79.06597653931753,34
1418,We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted .,3,0.66911703,30.038836446744767,17
1418,"Finally , we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful .",3,0.965225,66.50559829447627,21
1418,"We call for discarding the binary notion of faithfulness in favor of a more graded one , which we believe will be of greater practical utility .",3,0.8351474,31.558336267364794,27
1419,Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model ’s predictions .,0,0.94665414,55.57071346792765,22
1419,Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model ’s prediction .,3,0.4778224,77.50095806887435,23
1419,They can be considered a plausible explanation if they provide a human-understandable justification for the model ’s predictions .,0,0.6573482,31.617712008602755,19
1419,"In this work , we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model ’s predictions .",1,0.8118244,40.59006764714224,30
1419,We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other ( high conicity ) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model ’s predictions .,3,0.9592353,42.87817728440587,52
1419,"Based on experiments on a wide variety of tasks and datasets , we observe attention distributions often attribute the model ’s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions .",3,0.73875505,37.843043445020555,39
1419,"To make attention mechanisms more faithful and plausible , we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse .",2,0.5479807,43.29442906580892,36
1419,We show that the resulting attention distributions offer more transparency as they ( i ) provide a more precise importance ranking of the hidden states ( ii ) are better indicative of words important for the model ’s predictions ( iii ) correlate better with gradient-based attribution methods .,3,0.95682234,72.88436537823826,51
1419,Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model ’s predictions .,3,0.8968866,41.741550573072075,21
1419,Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention .,3,0.63611853,14.41937321088917,10
1420,Multi-task Learning methods have achieved great progress in text classification .,0,0.9280675,14.31980785527872,11
1420,"However , existing methods assume that multi-task text classification problems are convex multiobjective optimization problems , which is unrealistic in real-world applications .",0,0.88150513,44.62203462974669,23
1420,"To address this issue , this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption .",1,0.7649057,97.22680007009676,22
1420,The extensive experiments back up our theoretical analysis and validate the superiority of our proposals .,3,0.95015866,42.16305006761385,16
1421,"This paper studies strategies to model word formation in NMT using rich linguistic information , namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology .",1,0.88723123,88.1385465759792,31
1421,Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation .,2,0.59945726,91.69557308375485,18
1421,"The best system variants employ source-side morphological analysis and model complex target-side words , improving over a standard system .",3,0.7152331,123.428601189872,21
1422,Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training .,0,0.8006242,53.39423898999983,22
1422,"However , when active learning is integrated with an end-user application , this can lead to frustration for participating users , as they spend time labeling instances that they would not otherwise be interested in reading .",0,0.568331,46.59476160672526,37
1422,"In this paper , we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system ( training efficiently ) and the user ( receiving useful instances ) .",1,0.88112676,65.09023736384219,36
1422,"We study our approach in an educational application , which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user , while the users should receive only exercises that match their skills .",2,0.4924552,57.17127015724977,46
1422,We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users .,3,0.8867634,96.60509702484488,35
1423,"This paper investigates how to effectively incorporate a pre-trained masked language model ( MLM ) , such as BERT , into an encoder-decoder ( EncDec ) model for grammatical error correction ( GEC ) .",1,0.9110322,20.111978718932694,35
1423,The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC .,0,0.55666786,50.01402408114297,34
1423,"For example , the distribution of the inputs to a GEC model can be considerably different ( erroneous , clumsy , etc. ) from that of the corpora used for pre-training MLMs ;",0,0.600869,78.51984104851262,33
1423,"however , this issue is not addressed in the previous methods .",0,0.53153193,36.082725672749184,12
1423,"Our experiments show that our proposed method , where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model , maximizes the benefit of the MLM .",3,0.9291679,19.713011428574585,46
1423,The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks .,3,0.8508764,7.4294435528031295,23
1423,Our code is publicly available at : https://github.com/kanekomasahiro/bert-gec .,3,0.55822355,28.570270548456612,9
1424,"With the explosion of news information , personalized news recommendation has become very important for users to quickly find their interested contents .",0,0.9661275,47.4724311226226,23
1424,Most existing methods usually learn the representations of users and news from news contents for recommendation .,0,0.8310272,96.21259443662368,17
1424,"However , they seldom consider high-order connectivity underlying the user-news interactions .",0,0.7837049,282.892300410949,12
1424,"Moreover , existing methods failed to disentangle a user ’s latent preference factors which cause her clicks on different news .",0,0.7901813,159.91717255654527,21
1424,"In this paper , we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement , named GNUD .",1,0.6882808,39.15972488648003,30
1424,Our model can encode high-order relationships into user and news representations by information propagation along the graph .,3,0.7104281,113.20041501818353,18
1424,"Furthermore , the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm , which can enhance expressiveness and interpretability .",3,0.4944582,76.12058919988776,24
1424,"A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference , improving the quality of the disentangled representations .",2,0.47422957,56.509246396693115,26
1424,Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods .,3,0.9342672,7.286975046730368,32
1425,"In this paper , we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case .",1,0.92095023,57.75442832796687,26
1425,We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story .,2,0.78322923,137.01588585297608,18
1425,"We propose to model the defendants with behavioral semantic information and statistical characteristics , then learning the importances of defendants within a learning-to-rank framework .",2,0.63391966,126.42011119724604,29
1425,Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants ’ impacts in a complex case .,3,0.91831326,52.5868203507539,22
1426,"The rise of online communication platforms has been accompanied by some undesirable effects , such as the proliferation of aggressive and abusive behaviour online .",0,0.93375766,26.344098590412177,25
1426,"Aiming to tackle this problem , the natural language processing ( NLP ) community has experimented with a range of techniques for abuse detection .",0,0.95286673,26.25070391103427,25
1426,"While achieving substantial success , these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users , disregarding the emotional state of the users and how this might affect their language .",0,0.90113676,37.276839241167664,42
1426,"The latter is , however , inextricably linked to abusive behaviour .",0,0.7729271,63.75821008706235,12
1426,"In this paper , we present the first joint model of emotion and abusive language detection , experimenting in a multi-task learning framework that allows one task to inform the other .",1,0.87530804,38.11056291591292,32
1426,Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets .,3,0.99015653,35.99801893079643,18
1427,The key to effortless end-user programming is natural language .,0,0.8679541,49.173752382436305,10
1427,"We examine how to teach intelligent systems new functions , expressed in natural language .",1,0.66552025,171.24160707426566,15
1427,"As a first step , we collected 3168 samples of teaching efforts in plain English .",2,0.8785561,121.79015216606774,16
1427,"Then we built fuSE , a novel system that translates English function descriptions into code .",2,0.79598176,143.6431194542178,16
1427,Our approach is three-tiered and each task is evaluated separately .,2,0.75530654,32.60233983595281,13
1427,We first classify whether an intent to teach new functionality is present in the utterance ( accuracy : 97.7 % using BERT ) .,2,0.7971243,96.02193220312826,24
1427,Then we analyze the linguistic structure and construct a semantic model ( accuracy : 97.6 % using a BiLSTM ) .,2,0.75073916,61.410005900427855,21
1427,"Finally , we synthesize the signature of the method , map the intermediate steps ( instructions in the method body ) to API calls and inject control structures ( F1 : 67.0 % with information retrieval and knowledge-based methods ) .",2,0.6837098,91.54498074546756,43
1427,In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6 % of the method signatures and 79.2 % of the API calls correctly .,3,0.9217349,39.14976420833014,27
1428,Moderation is crucial to promoting healthy online discussions .,0,0.6318409,45.167060043579966,9
1428,"Although several ‘ toxicity ’ detection datasets and models have been published , most of them ignore the context of the posts , implicitly assuming that comments may be judged independently .",0,0.8451275,61.796664587880265,32
1428,"We experiment with Wikipedia conversations , limiting the notion of context to the previous post in the thread and the discussion title .",2,0.8097848,152.4354922121195,23
1428,We find that context can both amplify or mitigate the perceived toxicity of posts .,3,0.9833304,46.96465394763242,15
1428,"Moreover , a small but significant subset of manually labeled posts ( 5 % in one of our experiments ) end up having the opposite toxicity labels if the annotators are not provided with context .",3,0.9459471,64.90242194386788,36
1428,"Surprisingly , we also find no evidence that context actually improves the performance of toxicity classifiers , having tried a range of classifiers and mechanisms to make them context aware .",3,0.97462976,64.10849102057978,31
1428,This points to the need for larger datasets of comments annotated in context .,3,0.7194163,28.532068944397235,14
1428,We make our code and data publicly available .,3,0.4864766,8.586361691169062,9
1429,Abstract Meaning Representations ( AMRs ) capture sentence-level semantics structural representations to broad-coverage natural sentences .,0,0.8389578,81.1135627131118,18
1429,We investigate parsing AMR with explicit dependency structures and interpretable latent structures .,1,0.42247075,131.39570323225624,13
1429,"We generate the latent soft structure without additional annotations , and fuse both dependency and latent structure via an extended graph neural networks .",2,0.85374784,160.75227283385752,24
1429,The fused structural information helps our experiments results to achieve the best reported results on both AMR 2.0 ( 77.5 % Smatch F1 on LDC2017T10 ) and AMR 1.0 ( ( 71.8 % Smatch F1 on LDC2014T12 ) .,3,0.8950438,23.62603363315286,39
1430,Answering natural language questions over tables is usually seen as a semantic parsing task .,0,0.9426828,24.492574252107975,15
1430,"To alleviate the collection cost of full logical forms , one popular approach focuses on weak supervision consisting of denotations instead of logical forms .",0,0.86869895,89.39330639924935,25
1430,"However , training semantic parsers from weak supervision poses difficulties , and in addition , the generated logical forms are only used as an intermediate step prior to retrieving the denotation .",0,0.6140839,90.77117590637745,32
1430,"In this paper , we present TaPas , an approach to question answering over tables without generating logical forms .",1,0.86042285,88.5686537512302,20
1430,"TaPas trains from weak supervision , and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection .",2,0.56391,287.6829585763686,25
1430,"TaPas extends BERT ’s architecture to encode tables as input , initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia , and is trained end-to-end .",2,0.49682102,101.80049096161086,32
1430,"We experiment with three different semantic parsing datasets , and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ , but with a simpler model architecture .",3,0.79682606,23.803167016747608,59
1430,"We additionally find that transfer learning , which is trivial in our setting , from WikiSQL to WikiTQ , yields 48.7 accuracy , 4.2 points above the state-of-the-art .",3,0.95607656,55.38339626236122,33
1431,"In argumentation , people state premises to reason towards a conclusion .",0,0.83386123,234.58400166004876,12
1431,"The conclusion conveys a stance towards some target , such as a concept or statement .",3,0.50816613,180.98684323137954,16
1431,"Often , the conclusion remains implicit , though , since it is self-evident in a discussion or left out for rhetorical reasons .",0,0.8403048,96.8469174516932,23
1431,"However , the conclusion is key to understanding an argument and , hence , to any application that processes argumentation .",0,0.74068016,126.62106944813239,21
1431,We thus study the question to what extent an argument ’s conclusion can be reconstructed from its premises .,1,0.73543656,37.50066454510764,19
1431,"In particular , we argue here that a decisive step is to infer a conclusion ’s target , and we hypothesize that this target is related to the premises ’ targets .",1,0.48687506,61.64168630917747,32
1431,"We develop two complementary target inference approaches : one ranks premise targets and selects the top-ranked target as the conclusion target , the other finds a new conclusion target in a learned embedding space using a triplet neural network .",2,0.82575727,50.88420469607152,40
1431,"Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best , outperforming several strong baselines .",3,0.94782764,31.607996697328165,22
1431,"According to human annotators , we infer a reasonably adequate conclusion target in 89 % of the cases .",3,0.9655768,123.34358451148371,19
1432,"Multimodal Machine Translation ( MMT ) aims to introduce information from other modality , generally static images , to improve the translation quality .",0,0.9418191,46.457268107337114,24
1432,"Previous works propose various incorporation methods , but most of them do not consider the relative importance of multiple modalities .",0,0.82280695,33.85862978078529,21
1432,Equally treating all modalities may encode too much useless information from less important modalities .,0,0.6515554,86.73386192130086,15
1432,"In this paper , we introduce the multimodal self-attention in Transformer to solve the issues above in MMT .",1,0.8842763,28.85502583502613,19
1432,"The proposed method learns the representation of images based on the text , which avoids encoding irrelevant information in images .",2,0.45518288,35.290057089726986,21
1432,Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics .,3,0.9438564,22.010982397850295,26
1433,"In this paper , we hypothesize that sarcasm is closely related to sentiment and emotion , and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario .",1,0.8723117,20.81325767356672,37
1433,"We , at first , manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes , both implicit and explicit .",2,0.88220114,113.6467203278573,25
1433,"For multi-tasking , we propose two attention mechanisms , viz .",2,0.6052621,63.0868090701228,11
1433,Inter-segment Inter-modal Attention ( Ie-Attention ) and Intra-segment Inter-modal Attention ( Ia-Attention ) .,2,0.352585,12.818002521614677,15
1433,The main motivation of Ie-Attention is to learn the relationship between the different segments of the sentence across the modalities .,0,0.59814024,36.45830333952965,21
1433,"In contrast , Ia-Attention focuses within the same segment of the sentence across the modalities .",3,0.6686816,115.55573854402998,16
1433,"Finally , representations from both the attentions are concatenated and shared across the five classes ( i.e. , sarcasm , implicit sentiment , explicit sentiment , implicit emotion , explicit emotion ) for multi-tasking .",2,0.668184,44.012656218488914,35
1433,Experimental results on the extended version of the MUStARD dataset show the efficacy of our proposed approach for sarcasm detection over the existing state-of-the-art systems .,3,0.9642202,14.407534692252542,32
1433,"The evaluation also shows that the proposed multi-task framework yields better performance for the primary task , i.e. , sarcasm detection , with the help of two secondary tasks , emotion and sentiment analysis .",3,0.9666078,38.748150844368354,35
1434,The task of Dialogue Act Classification ( DAC ) that purports to capture communicative intent has been studied extensively .,0,0.96255696,83.91587555363016,20
1434,But these studies limit themselves to text .,0,0.8538165,133.7081509491936,8
1434,"Non-verbal features ( change of tone , facial expressions etc. ) can provide cues to identify DAs , thus stressing the benefit of incorporating multi-modal inputs in the task .",3,0.62347525,65.74510986263101,30
1434,"Also , the emotional state of the speaker has a substantial effect on the choice of the dialogue act , since conversations are often influenced by emotions .",0,0.63808966,34.82580147850724,28
1434,"Hence , the effect of emotion too on automatic identification of DAs needs to be studied .",3,0.6236458,100.98356789015439,17
1434,"In this work , we address the role of both multi-modality and emotion recognition ( ER ) in DAC .",1,0.9262778,70.71130634947339,20
1434,DAC and ER help each other by way of multi-task learning .,3,0.4968078,41.96227946304434,12
1434,"One of the major contributions of this work is a new dataset-multimodal Emotion aware Dialogue Act dataset called EMOTyDA , collected from open-sourced dialogue datasets .",0,0.41635302,38.49905030289059,26
1434,"To demonstrate the utility of EMOTyDA , we build an attention based ( self , inter-modal , inter-task ) multi-modal , multi-task Deep Neural Network ( DNN ) for joint learning of DAs and emotions .",2,0.6649653,59.92740580160529,36
1434,We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants .,3,0.96221447,28.49993128382899,21
1435,Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts .,0,0.96211517,56.33719105490057,29
1435,"In this paper , we present the first computational study of parody .",1,0.87448907,44.51230368907046,13
1435,We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts .,2,0.74804205,42.44131148099335,19
1435,"We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training , across different genders and across countries .",2,0.8747462,67.81148313033572,36
1435,Our results show that political parody tweets can be predicted with an accuracy up to 90 % .,3,0.9900511,43.785912524588134,18
1435,"Finally , we identify the markers of parody through a linguistic analysis .",3,0.5410766,76.93642497580079,13
1435,"Beyond research in linguistics and political communication , accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances .",0,0.7508709,85.64285289022523,33
1436,"A central concern in Computational Social Sciences ( CSS ) is fairness : where the role of NLP is to scale up text analysis to large corpora , the quality of automatic analyses should be as independent as possible of textual properties .",0,0.95296764,65.07679953797685,43
1436,"We analyze the performance of a state-of-the-art neural model on the task of political claims detection ( i.e. , the identification of forward-looking statements made by political actors ) and identify a strong frequency bias : claims made by frequent actors are recognized better .",2,0.45850095,36.46206731568226,53
1436,"We propose two simple debiasing methods which mask proper names and pronouns during training of the model , thus removing personal information bias .",2,0.6030747,110.90177137404919,24
1436,We find that ( a ) these methods significantly decrease frequency bias while keeping the overall performance stable ;,3,0.9812315,134.82188081887088,19
1436,and ( b ) the resulting models improve when evaluated in an out-of-domain setting .,3,0.83284384,35.51164592087718,16
1437,Social biases are encoded in word embeddings .,0,0.40375218,22.50841450677089,8
1437,"This presents a unique opportunity to study society historically and at scale , and a unique danger when embeddings are used in downstream applications .",0,0.7014179,80.56238772455592,25
1437,"Here , we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods .",1,0.9122194,22.584036111761797,28
1437,"We find that biases found in word embeddings do , on average , closely mirror survey data across seventeen dimensions of social meaning .",3,0.9788592,90.84366069058017,24
1437,"However , we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning ( e.g .",3,0.9803602,39.86708029614035,24
1437,gender ) than others ( e.g .,3,0.7424431,62.62040351971436,7
1437,"race ) , and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases .",3,0.80305636,63.4428392884462,25
1438,"In an era where generating content and publishing it is so easy , we are bombarded with information and are exposed to all kinds of claims , some of which do not always rank high on the truth scale .",0,0.8983809,41.16820146021839,40
1438,"This paper suggests that the key to a longer-term , holistic , and systematic approach to navigating this information pollution is capturing the provenance of claims .",3,0.7286983,70.67163165882918,27
1438,"To do that , we develop a formal definition of provenance graph for a given natural language claim , aiming to understand where the claim may come from and how it has evolved .",2,0.49914104,39.90210277929722,34
1438,"To construct the graph , we model provenance inference , formulated mainly as an information extraction task and addressed via a textual entailment model .",2,0.858299,90.13430283673546,25
1438,"We evaluate our approach using two benchmark datasets , showing initial success in capturing the notion of provenance and its effectiveness on the application of claim verification .",2,0.52283156,44.9268169475071,28
1439,"Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules , a property known as compositionality .",0,0.8491175,83.9163957399416,27
1439,"In this paper , we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations , and whether it accomplishes this feat by strategies akin to human-language compositionality .",1,0.9260827,71.46470295714781,39
1439,"First , given sufficiently large input spaces , the emergent language will naturally develop the ability to refer to novel composite concepts .",3,0.5111578,121.97898105291452,23
1439,"Second , there is no correlation between the degree of compositionality of an emergent language and its ability to generalize .",0,0.66746885,20.073889236097536,21
1439,"The more compositional a language is , the more easily it will be picked up by new learners , even when the latter differ in architecture from the original agents .",3,0.5397774,45.49319792044281,31
1439,"We conclude that compositionality does not arise from simple generalization pressure , but if an emergent language does chance upon it , it will be more likely to survive and thrive .",3,0.9866832,73.25910444653904,32
1440,State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions .,0,0.9404386,12.753012850634649,29
1440,This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘ reasoning ’ behind model outputs .,0,0.9117928,81.79368801960142,23
1440,But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics ;,0,0.8982838,82.17167165032428,20
1440,this makes it difficult to track progress .,0,0.9069009,27.762954879954094,8
1440,We propose the Evaluating Rationales And Simple English Reasoning ( ERASER a benchmark to advance research on interpretable models in NLP .,1,0.53099656,80.71165340699207,22
1440,This benchmark comprises multiple datasets and tasks for which human annotations of “ rationales ” ( supporting evidence ) have been collected .,2,0.5433043,82.24709338459505,23
1440,"We propose several metrics that aim to capture how well the rationales provided by models align with human rationales , and also how faithful these rationales are ( i.e. , the degree to which provided rationales influenced the corresponding predictions ) .",2,0.49778286,35.85855652701321,42
1440,Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems .,3,0.8550749,33.831194312431734,16
1440,"The benchmark , code , and documentation are available at https://www.eraserbenchmark.com/ .",3,0.66809607,32.18702236709782,12
1441,In many settings it is important for one to be able to understand why a model made a particular prediction .,0,0.90637434,22.400908899083767,21
1441,In NLP this often entails extracting snippets of an input text ‘ responsible for ’ corresponding model output ;,0,0.8792563,161.1610437294588,19
1441,"when such a snippet comprises tokens that indeed informed the model ’s prediction , it is a faithful explanation .",3,0.50334257,134.53809088029658,20
1441,"In some settings , faithfulness may be critical to ensure transparency .",0,0.6089546,80.82072027786202,12
1441,Lei et al .,4,0.8933437,6.607560138168551,4
1441,( 2016 ) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules .,0,0.5294376,128.34843652349096,23
1441,"However , the discrete selection over input tokens performed by this method complicates training , leading to high variance and requiring careful hyperparameter tuning .",3,0.6351144,76.56514278736559,25
1441,We propose a simpler variant of this approach that provides faithful explanations by construction .,2,0.36225572,122.63865268087758,15
1441,"In our scheme , named FRESH , arbitrary feature importance scores ( e.g. , gradients from a trained model ) are used to induce binary labels over token inputs , which an extractor can be trained to predict .",2,0.7133481,104.2707887114468,39
1441,An independent classifier module is then trained exclusively on snippets provided by the extractor ;,2,0.6460431,135.41288699606153,15
1441,"these snippets thus constitute faithful explanations , even if the classifier is arbitrarily complex .",3,0.6976573,202.6265542755668,15
1441,"In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to ‘ end-to-end ’ approaches , while being more general and easier to train .",3,0.9463726,35.0867113895318,34
1441,Code is available at https://github.com/successar/FRESH .,3,0.5377669,14.866566006912608,6
1442,Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets .,0,0.9673006,14.382645421687899,16
1442,"In the clinical domain , however , creating such datasets is quite difficult due to the domain expertise required for annotation .",0,0.89989126,68.18315448503749,22
1442,"Recently , Pampari et al .",0,0.76338816,60.252503344177676,6
1442,"( EMNLP ’18 ) tackled this issue by using expert-annotated question templates and existing i2 b2 annotations to create emrQA , the first large-scale dataset for question answering ( QA ) based on clinical notes .",0,0.7637674,98.79086889175547,37
1442,"In this paper , we provide an in-depth analysis of this dataset and the clinical reading comprehension ( CliniRC ) task .",1,0.89404255,42.77963917603114,23
1442,"From our qualitative analysis , we find that ( i ) emrQA answers are often incomplete , and ( ii ) emrQA questions are often answerable without using domain knowledge .",3,0.97040915,24.711924021249054,31
1442,"From our quantitative experiments , surprising results include that ( iii ) using a small sampled subset ( 5 %-20 % ) , we can obtain roughly equal performance compared to the model trained on the entire dataset , ( iv ) this performance is close to human expert ’s performance , and ( v) BERT models do not beat the best performing base model .",3,0.9425111,54.5298264140869,67
1442,"Following our analysis of the emrQA , we further explore two desired aspects of CliniRC systems : the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts .",3,0.5982787,85.73450084129793,33
1442,We argue that both should be considered when creating future datasets .,3,0.9036382,49.724841193642646,12
1443,Transformer-based QA models use input-wide self-attention – i.e .,0,0.6071164,28.247510372939214,11
1443,"across both the question and the input passage – at all layers , causing them to be slow and memory-intensive .",3,0.56179184,73.67981421204668,23
1443,"It turns out that we can get by without input-wide self-attention at all layers , especially in the lower layers .",3,0.9339939,37.44828093202427,21
1443,"We introduce DeFormer , a decomposed transformer , which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers .",2,0.66682917,63.38429862912089,27
1443,"This allows for question-independent processing of the input text representations , which in turn enables pre-computing passage representations reducing runtime compute drastically .",3,0.49233237,152.35069018878417,25
1443,"Furthermore , because DeFormer is largely similar to the original model , we can initialize DeFormer with the pre-training weights of a standard transformer , and directly fine-tune on the target QA dataset .",3,0.6511247,46.64047556782219,34
1443,We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1 % drop in accuracy .,3,0.9354802,61.446795827518955,35
1443,We open source the code at https://github.com/StonyBrookNLP/deformer .,3,0.5048555,11.753335555065302,8
1444,Knowledge Graphs ( KG ) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges .,0,0.8510996,30.226025572144614,21
1444,Goal of the Question Answering over KG ( KGQA ) task is to answer natural language queries posed over the KG .,0,0.8539112,21.604182794085933,22
1444,Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer .,0,0.57951874,17.836710588886195,17
1444,"KGs are often incomplete with many missing links , posing additional challenges for KGQA , especially for multi-hop KGQA .",0,0.8296453,41.94002516013631,20
1444,"Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text , which is n’t always readily available .",0,0.9107697,61.48120379312797,23
1444,"In a separate line of research , KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction .",0,0.83843577,43.22745889550879,23
1444,"Such KG embedding methods , even though highly relevant , have not been explored for multi-hop KGQA so far .",0,0.6765246,47.22492464747079,20
1444,We fill this gap in this paper and propose EmbedKGQA .,1,0.6688427,46.445617326306,11
1444,EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs .,3,0.9139792,24.174715292426995,12
1444,"EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood , a sub-optimal constraint enforced by previous multi-hop KGQA methods .",3,0.6482068,37.991609821872764,23
1444,"Through extensive experiments on multiple benchmark datasets , we demonstrate EmbedKGQA ’s effectiveness over other state-of-the-art baselines .",3,0.70589995,20.494760295189156,23
1445,Question Answering ( QA ) is in increasing demand as the amount of information available online and the desire for quick access to this content grows .,0,0.9747497,24.489070811848485,27
1445,A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset .,0,0.90129817,11.605439972612578,20
1445,"This paradigm , however , relies on scarce , and costly to obtain , large-scale human-labeled data .",0,0.9302774,102.2583486937375,19
1445,We propose an unsupervised approach to training QA models with generated pseudo-training data .,2,0.44570783,19.467737010465207,14
1445,"We show that generating questions for QA training by applying a simple template on a related , retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships .",3,0.95295537,57.516263808513905,40
1445,"Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14 % , and 20 % when the answer is a named entity , achieving state-of-the-art performance on SQuAD for unsupervised QA .",3,0.8879173,14.435946272114824,54
1446,"Evidence retrieval is a critical stage of question answering ( QA ) , necessary not only to improve performance , but also to explain the decisions of the QA method .",0,0.94162345,35.63515984493631,31
1446,"We introduce a simple , fast , and unsupervised iterative evidence retrieval method , which relies on three ideas : ( a ) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings , ( b ) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications , which ( c ) stops when the terms in the given question and candidate answers are covered by the retrieved justifications .",3,0.32780364,61.07245973399178,82
1446,"Despite its simplicity , our approach outperforms all the previous methods ( including supervised methods ) on the evidence selection task on two datasets : MultiRC and QASC .",3,0.86981064,56.654940717557984,29
1446,"When these evidence sentences are fed into a RoBERTa answer classification component , we achieve state-of-the-art QA performance on these two datasets .",3,0.79534787,24.904031960628483,29
1447,A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions .,0,0.92581594,23.677941281515878,20
1447,"We present VoxClamantis v 1.0 , the first large-scale corpus for phonetic typology , with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages , along with acoustic-phonetic measures of vowels and sibilants .",2,0.53562325,62.846430620563275,38
1447,Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages .,0,0.6785997,37.345380295874506,20
1447,"However , it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages , many of which have few to no resources presently available .",0,0.91401523,25.06988411578941,28
1447,"We describe the methodology to create our corpus , discuss caveats with current methods and their impact on the utility of this data , and illustrate possible research directions through a series of case studies on the 48 highest-quality readings .",2,0.40608254,70.89804552508208,43
1447,Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io .,3,0.55783397,20.158833089275795,13
1448,Discourse representation structures ( DRSs ) are scoped semantic representations for texts of arbitrary length .,0,0.891597,108.0408012860257,16
1448,Evaluating the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance .,0,0.8063758,26.35828473174425,19
1448,DRSs are typically visualized as boxes which are not straightforward to process automatically .,0,0.8237285,65.74094048323217,14
1448,Counter transforms DRSs to clauses and measures clause overlap by searching for variable mappings between two DRSs .,2,0.4731636,110.71488508062093,18
1448,"However , this metric is computationally costly ( with respect to memory and CPU time ) and does not scale with longer texts .",0,0.59565115,31.2755761026083,24
1448,"We introduce Dscorer , an efficient new metric which converts box-style DRSs to graphs and then measures the overlap of n-grams .",2,0.5801689,106.9730978763736,22
1448,Experiments show that Dscorer computes accuracy scores that are correlated with Counter at a fraction of the time .,3,0.94271976,72.59491973187292,19
1449,"We report on methods to create the largest publicly available parallel corpora by crawling the web , using open source software .",1,0.40450606,54.365132904091624,22
1449,We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering .,2,0.49558696,102.36138291267535,18
1449,We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems .,1,0.45386353,49.82378922018532,20
1450,Correctly resolving textual mentions of people fundamentally entails making inferences about those people .,0,0.9323488,91.99754521303758,14
1450,"Such inferences raise the risk of systemic biases in coreference resolution systems , including biases that can harm binary and non-binary trans and cis stakeholders .",0,0.5746713,59.18894175525088,26
1450,"To better understand such biases , we foreground nuanced conceptualizations of gender from sociology and sociolinguistics , and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems .",1,0.4742143,65.10038739927388,35
1450,"Through these studies , conducted on English text , we confirm that without acknowledging and building systems that recognize the complexity of gender , we build systems that lead to many potential harms .",3,0.92207766,100.67336398071504,34
1451,"Motivated by human attention , computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data .",0,0.9495793,29.79384687024249,26
1451,"While attention mechanisms are claimed to achieve interpretability , little is known about the actual relationships between machine and human attention .",0,0.94931823,39.25216153217773,22
1451,"In this work , we conduct the first quantitative assessment of human versus computational attention mechanisms for the text classification task .",1,0.86086494,28.841854457386287,22
1451,"To achieve this , we design and conduct a large-scale crowd-sourcing study to collect human attention maps that encode the parts of a text that humans focus on when conducting text classification .",2,0.53874815,27.037789238243,34
1451,"Based on this new resource of human attention dataset for text classification , YELP-HAT , collected on the publicly available YELP dataset , we perform a quantitative comparative analysis of machine attention maps created by deep learning models and human attention maps .",2,0.8287113,59.836261823187044,45
1451,"Our analysis offers insights into the relationships between human versus machine attention maps along three dimensions : overlap in word selections , distribution over lexical categories , and context-dependency of sentiment polarity .",3,0.90934724,77.57630965205543,34
1451,Our findings open promising future research opportunities ranging from supervised attention to the design of human-centric attention-based explanations .,3,0.98794496,55.63920043770956,21
1452,The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually “ know ” about natural language .,0,0.95307565,13.921531631335968,29
1452,Probes are a natural way of assessing this .,0,0.89162344,76.13118869841587,9
1452,"When probing , a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network ’s learned representations .",2,0.6336093,96.49869967524543,28
1452,"If the probe does well , the researcher may conclude that the representations encode knowledge related to the task .",3,0.6318124,48.47524277194408,20
1452,A commonly held belief is that using simpler models as probes is better ;,0,0.8744736,130.918082114463,14
1452,"the logic is that simpler models will identify linguistic structure , but not learn the task itself .",0,0.6174097,176.48769408392772,18
1452,"We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom : one should always select the highest performing probe one can , even if it is more complex , since it will result in a tighter estimate , and thus reveal more of the linguistic information inherent in the representation .",2,0.36919743,73.14161541444643,58
1452,"The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT , comparing these estimates to several baselines .",1,0.54414225,42.21870700070128,27
1452,We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research — plus English — totalling eleven languages .,2,0.8139847,67.53654749504696,23
1452,Our implementation is available in https://github.com/rycolab/info-theoretic-probing .,3,0.661151,18.71466983002092,7
1453,"State-of-the-art unsupervised multilingual models ( e.g. , multilingual BERT ) have been shown to generalize in a zero-shot cross-lingual setting .",0,0.8962339,10.053568121806041,25
1453,This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions .,0,0.7272977,37.55729393213656,27
1453,We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level .,1,0.467993,22.94205670699905,22
1453,"More concretely , we first train a transformer-based masked language model on one language , and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective , freezing parameters of all other layers .",2,0.8503272,37.89916878847094,45
1453,This approach does not rely on a shared vocabulary or joint training .,0,0.3655449,41.24859816696984,13
1453,"However , we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset ( XQuAD ) .",3,0.8911678,15.338393702538465,28
1453,Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages .,3,0.9883513,29.18565302364748,29
1453,"We also release XQuAD as a more comprehensive cross-lingual benchmark , which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators .",2,0.6297589,44.11679014223254,32
1454,This paper investigates contextual word representation models from the lens of similarity analysis .,1,0.8854094,57.04535237717594,14
1454,"Given a collection of trained models , we measure the similarity of their internal representations and attention .",2,0.69988,50.1825110329877,18
1454,"Critically , these models come from vastly different architectures .",0,0.9147674,79.80778283016372,10
1454,"We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models , and facilitate the investigation of which design factors affect model similarity , without requiring any external linguistic annotation .",2,0.7628116,78.2320352902253,41
1454,"The analysis reveals that models within the same family are more similar to one another , as may be expected .",3,0.9705263,30.219864688718655,21
1454,"Surprisingly , different architectures have rather similar representations , but different individual neurons .",0,0.73284674,269.6747507876885,14
1454,We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks .,3,0.9813665,19.62077052993521,26
1455,The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding .,0,0.930323,15.241324676774752,22
1455,"However , existing self-supervision techniques operate at the word form level , which serves as a surrogate for the underlying semantic content .",0,0.87282604,44.69612029204257,23
1455,This paper proposes a method to employ weak-supervision directly at the word sense level .,1,0.80860496,68.83375306400781,17
1455,"Our model , named SenseBERT , is pre-trained to predict not only the masked words but also their WordNet supersenses .",2,0.748703,62.70669783970788,21
1455,"Accordingly , we attain a lexical-semantic level language model , without the use of human annotation .",2,0.5795472,61.895221630063716,17
1455,"SenseBERT achieves significantly improved lexical understanding , as we demonstrate by experimenting on SemEval Word Sense Disambiguation , and by attaining a state of the art result on the ‘ Word in Context ’ task .",3,0.8473415,37.26523398474771,36
1456,"In order to simplify a sentence , human editors perform multiple rewriting transformations : they split it into several shorter sentences , paraphrase words ( i.e .",0,0.6267789,121.85608407548206,27
1456,"replacing complex words or phrases by simpler synonyms ) , reorder components , and / or delete information deemed unnecessary .",0,0.4300796,268.200257913994,21
1456,"Despite these varied range of possible text alterations , current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation , such as lexical paraphrasing or splitting .",0,0.7902429,71.14635095409852,34
1456,This makes it impossible to understand the ability of simplification models in more realistic settings .,0,0.8304383,30.558690832972726,16
1456,"To alleviate this limitation , this paper introduces ASSET , a new dataset for assessing sentence simplification in English .",1,0.8105609,47.083550990147046,20
1456,ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations .,2,0.5186393,103.50874137769041,17
1456,"Through quantitative and qualitative experiments , we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task .",3,0.8985165,63.317568988234385,30
1456,"Furthermore , we motivate the need for developing better methods for automatic evaluation using ASSET , since we show that current popular metrics may not be suitable when multiple simplification transformations are performed .",3,0.8650204,59.179374766493396,34
1457,"Thanks to the wealth of high-quality annotated images available in popular repositories such as ImageNet , multimodal language-vision research is in full bloom .",0,0.8466515,23.857392006048062,25
1457,"However , events , feelings and many other kinds of concepts which can be visually grounded are not well represented in current datasets .",0,0.87631583,84.57899641751148,24
1457,"Nevertheless , we would expect a wide-coverage language understanding system to be able to classify images depicting recess and remorse , not just cats , dogs and bridges .",3,0.8289313,138.89724525158783,30
1457,"We fill this gap by presenting BabelPic , a hand-labeled dataset built by cleaning the image-synset association found within the BabelNet Lexical Knowledge Base ( LKB ) .",2,0.4431024,126.4760047842864,28
1457,"BabelPic explicitly targets non-concrete concepts , thus providing refreshing new data for the community .",3,0.55294186,269.7156458076462,15
1457,We also show that pre-trained language-vision systems can be used to further expand the resource by exploiting natural language knowledge available in the LKB .,3,0.94337887,29.71318342440746,25
1457,BabelPic is available for download at http://babelpic.org .,3,0.44422132,19.02940768792249,8
1458,"Predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task , which usually treats labels as anonymous classes to predict .",0,0.8990989,60.57881565321886,31
1458,They ignore information that may be conveyed by the emotion labels themselves .,0,0.5785227,66.14874877008573,13
1458,We propose that the semantics of emotion labels can guide a model ’s attention when representing the input story .,3,0.49475962,89.41922679496236,20
1458,"Further , we observe that the emotions evoked by an event are often related : an event that evokes joy is unlikely to also evoke sadness .",3,0.97493374,30.365966210873978,27
1458,"In this work , we explicitly model label classes via label embeddings , and add mechanisms that track label-label correlations both during training and inference .",2,0.551034,59.33560305438557,28
1458,We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data .,2,0.60095656,30.242330949525087,16
1458,"Our empirical evaluations show that modeling label semantics yields consistent benefits , and we advance the state-of-the-art on an emotion inference task .",3,0.9428362,33.592320535374434,29
1459,We propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game Minecraft .,2,0.40778083,87.85397487105944,20
1459,The dataset consists of 7 K human utterances and their corresponding parses .,2,0.728022,85.94841343459372,13
1459,"Given proper world state , the parses can be interpreted and executed in game .",0,0.37491378,162.25992796182487,15
1459,"We report the performance of baseline models , and analyze their successes and failures .",3,0.39517868,47.981236675455065,15
1460,Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address .,0,0.8919774,36.612770854107644,19
1460,"They tend to produce generations that ( i ) rely too much on copying from the context , ( ii ) contain repetitions within utterances , ( iii ) overuse frequent words , and ( iv ) at a deeper level , contain logical flaws .",0,0.7811581,60.035433145409684,46
1460,"In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss ( Welleck et al. , 2019 ) to these cases .",1,0.60750574,51.21334326407477,31
1460,We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues .,3,0.9179011,116.11172117486977,22
1460,"For the last important general issue , we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency , potentially paving the way to generative models with greater reasoning ability .",3,0.9570493,79.22523745546918,40
1460,We demonstrate the efficacy of our approach across several dialogue tasks .,3,0.8116804,26.450321580118878,12
1461,"Large pretrained language models like BERT , after fine-tuning to a downstream task , have achieved high performance on a variety of NLP problems .",0,0.81579095,16.273342993298506,25
1461,Yet explaining their decisions is difficult despite recent work probing their internal representations .,0,0.93404514,97.49736980745632,14
1461,"We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon , and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks , and measurement of cross-dataset consistency .",2,0.47783035,43.93339730156155,51
1461,We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue .,2,0.748357,69.18030902344407,30
1461,"We find that after fine-tuning BERT and RoBERTa on a negation scope task , the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models .",3,0.96992534,34.05569441005368,36
1461,"However , only the base models ( not the large models ) improve compared to a control task , indicating there is evidence for a shallow encoding of negation only in the base models .",3,0.9771599,72.32614101698381,35
1462,LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing ( NLP ) tasks .,0,0.9049186,8.35633699111292,25
1462,"Despite their performance , it is unclear whether , or how , LSTMs learn structural features of natural languages such as subject-verb number agreement in English .",0,0.8984283,42.84013411295489,29
1462,"Lacking this understanding , the generality of LSTM performance on this task and their suitability for related tasks remains uncertain .",0,0.8628596,53.99652728475937,21
1462,"Further , errors cannot be properly attributed to a lack of structural capability , training data omissions , or other exceptional faults .",3,0.57299453,128.5006119048325,23
1462,"We introduce * influence paths * , a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network .",2,0.7057019,202.37598077294254,27
1462,The approach refines the notion of influence ( the subject ’s grammatical number has influence on the grammatical number of the subsequent verb ) into a set of gate or neuron-level paths .,2,0.6153595,76.33788297008189,33
1462,"The set localizes and segments the concept ( e.g. , subject-verb agreement ) , its constituent elements ( e.g. , the subject ) , and related or interfering elements ( e.g. , attractors ) .",2,0.43344888,54.72294527377624,37
1462,"We exemplify the methodology on a widely-studied multi-layer LSTM language model , demonstrating its accounting for subject-verb number agreement .",3,0.45258018,59.43746181560098,24
1462,The results offer both a finer and a more complete view of an LSTM ’s handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation .,3,0.9866508,48.75494979059625,34
1463,Contextualized representations ( e.g .,0,0.5169792,28.772328251316125,5
1463,"ELMo , BERT ) have become the default pretrained representations for downstream NLP applications .",0,0.77855986,73.11825179793253,15
1463,"In some settings , this transition has rendered their static embedding predecessors ( e.g .",0,0.82273537,215.71078679917625,15
1463,"Word2Vec , GloVe ) obsolete .",3,0.46834543,277.2543040391209,6
1463,"As a side-effect , we observe that older interpretability methods for static embeddings — while more diverse and mature than those available for their dynamic counterparts — are underutilized in studying newer contextualized representations .",3,0.9622158,43.79915165324114,35
1463,"Consequently , we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights .",2,0.71939933,64.72036376961091,33
1463,Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation .,3,0.96269685,78.4418523900349,22
1463,"Complementary to analyzing representational quality , we consider social biases encoded in pretrained representations with respect to gender , race / ethnicity , and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data .",3,0.8858062,50.25950140057436,47
1463,"Concerningly , we find dramatic inconsistencies between social bias estimators for word embeddings .",3,0.9578231,100.66357147764394,14
1464,Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing .,0,0.9305079,31.01340075630426,14
1464,"In addition to yielding gains in predictive accuracy , attention weights are often claimed to confer interpretability , purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders .",0,0.72487116,108.79148819279713,38
1464,We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks .,3,0.47937813,61.7962815187705,24
1464,"Our method diminishes the total weight assigned to designated impermissible tokens , even when the models can be shown to nevertheless rely on these features to drive predictions .",3,0.8369411,137.60948845451,29
1464,"Across multiple models and tasks , our approach manipulates attention weights while paying surprisingly little cost in accuracy .",3,0.79854345,98.97301154381636,19
1464,"Through a human study , we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender .",3,0.66919565,54.520193580533835,34
1464,"Consequently , our results cast doubt on attention ’s reliability as a tool for auditing algorithms in the context of fairness and accountability .",3,0.98950255,49.47245565622482,24
1465,We propose a general framework to study language emergence through signaling games with neural agents .,1,0.44131443,119.03465316389341,16
1465,"Using a continuous latent space , we are able to ( i ) train using backpropagation , ( ii ) show that discrete messages nonetheless naturally emerge .",3,0.5060938,117.24491768594135,28
1465,We explore whether categorical perception effects follow and show that the messages are not compositional .,1,0.3810231,143.40023848178643,16
1466,"Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks , little is known about exactly what information these embeddings encode about the context words that they are understood to reflect .",0,0.92493314,12.326223712259907,44
1466,"To address this question , we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words .",1,0.49109876,29.519463337596008,29
1466,"We apply these tasks to examine the popular BERT , ELMo and GPT contextual encoders , and find that each of our tested information types is indeed encoded as contextual information across tokens , often with near-perfect recoverability — but the encoders vary in which features they distribute to which tokens , how nuanced their distributions are , and how robust the encoding of each feature is to distance .",3,0.7641589,53.25673945577092,70
1466,We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings .,3,0.96217686,31.307186012206103,25
1467,Videos convey rich information .,0,0.8869149,431.1211253268715,5
1467,"Dynamic spatio-temporal relationships between people / objects , and diverse multimodal events are present in a video clip .",0,0.78737986,38.99592506772296,19
1467,"Hence , it is important to develop automated models that can accurately extract such information from videos .",0,0.91090256,32.14702740889259,18
1467,Answering questions on videos is one of the tasks which can evaluate such AI abilities .,0,0.90337867,63.901961884113696,16
1467,"In this paper , we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions .",1,0.90503,29.457685267235693,27
1467,"Specifically , we first employ dense image captions to help identify objects and their detailed salient regions and actions , and hence give the model useful extra information ( in explicit textual format to allow easier matching ) for answering questions .",2,0.7720974,165.44522841992674,42
1467,"Moreover , our model is also comprised of dual-level attention ( word / object and frame level ) , multi-head self / cross-integration for different sources ( video and dense captions ) , and gates which pass more relevant information to the classifier .",3,0.56072557,127.72965771446336,45
1467,"Finally , we also cast the frame selection problem as a multi-label classification task and introduce two loss functions , In-andOut Frame Score Margin ( IOFSM ) and Balanced Binary Cross-Entropy ( BBCE ) , to better supervise the model with human importance annotations .",2,0.7495344,67.31040447562182,45
1467,"We evaluate our model on the challenging TVQA dataset , where each of our model components provides significant gains , and our overall model outperforms the state-of-the-art by a large margin ( 74.09 % versus 70.52 % ) .",3,0.8028638,24.6745453960399,45
1467,"We also present several word , object , and frame level visualization studies .",3,0.52806616,241.27586169935898,14
1468,"By describing the features and abstractions of our world , language is a crucial tool for human learning and a promising source of supervision for machine learning models .",0,0.9208978,43.39422535691497,29
1468,"We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training , but unavailable for novel tasks at test time .",2,0.7308877,40.327827927803895,31
1468,Existing models for this setting sample new descriptions at test time and use those to classify images .,0,0.5479186,120.14835651487132,18
1468,"Instead , we propose language-shaped learning ( LSL ) , an end-to-end model that regularizes visual representations to predict language .",2,0.52208817,45.66032214029694,25
1468,"LSL is conceptually simpler , more data efficient , and outperforms baselines in two challenging few-shot domains .",3,0.74324864,43.89282720986398,18
1469,"While much work on deep latent variable models of text uses continuous latent variables , discrete latent variables are interesting because they are more interpretable and typically more space efficient .",0,0.87310976,43.935115159404205,31
1469,We consider several approaches to learning discrete latent variable models for text in the case where exact marginalization over these variables is intractable .,2,0.51445174,60.670888195535845,24
1469,We compare the performance of the learned representations as features for low-resource document and sentence classification .,2,0.6623038,39.56872660038528,17
1469,"Our best models outperform the previous best reported results with continuous representations in these low-resource settings , while learning significantly more compressed representations .",3,0.9467989,52.21131004889066,24
1469,"Interestingly , we find that an amortized variant of Hard EM performs particularly well in the lowest-resource regimes .",3,0.9842216,65.62089439860513,20
1470,Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions .,0,0.9120729,62.984612677097374,18
1470,"Past work has shown that domain knowledge , framed as constraints over the output space , can help improve predictive accuracy .",0,0.92260194,79.12571842567387,22
1470,"However , designing good constraints often relies on domain expertise .",0,0.87672937,200.74028672109492,11
1470,"In this paper , we study the problem of learning such constraints .",1,0.8881899,34.47905733999105,13
1470,"We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures , and show a construction for converting a trained network into a system of linear constraints over the inference variables .",2,0.58348477,80.4236749639708,40
1470,"Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy , especially when the number of training examples is small .",3,0.9360813,15.8294412491111,27
1471,Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations .,0,0.93079424,30.47658154460589,29
1471,"We propose Conpono , an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences .",1,0.47604954,49.738002346921675,21
1471,"Given an anchor sentence , our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus .",2,0.7688751,69.43815872580325,37
1471,"On the discourse representation benchmark DiscoEval , our model improves over the previous state-of-the-art by up to 13 % and on average 4 % absolute across 7 tasks .",3,0.90270305,33.246655385560345,35
1471,"Our model is the same size as BERT-Base , but outperforms the much larger BERT-Large model and other more recent approaches that incorporate discourse .",3,0.751502,43.879507457507714,29
1471,"We also show that Conpono yields gains of 2 %-6 % absolute even for tasks that do not explicitly evaluate discourse : textual entailment ( RTE ) , common sense reasoning ( COPA ) and reading comprehension ( ReCoRD ) .",3,0.94623584,87.16542672429371,43
1472,Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools .,0,0.91950953,27.37153364551927,22
1472,"In the cooking domain , the web offers many , partially-overlapping , text and video recipes ( i.e .",0,0.83317995,106.38627643098566,21
1472,procedures ) that describe how to make the same dish ( i.e .,0,0.39604074,73.83491496677392,13
1472,high-level task ) .,0,0.35923627,92.77844227109277,6
1472,"Aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions , providing commonsense insight into how real-world procedures are structured .",0,0.7677057,104.14745620538558,34
1472,Learning to align these different instruction sets is challenging because : a ) different recipes vary in their order of instructions and use of ingredients ;,0,0.798474,108.34146500433178,26
1472,and b ) video instructions can be noisy and tend to contain far more information than text instructions .,3,0.49285206,77.05809897610628,19
1472,"To address these challenges , we use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish .",2,0.6128806,31.78744491336348,25
1472,We then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish .,2,0.7922117,92.00092309934685,23
1472,We release the Microsoft Research Multimodal Aligned Recipe Corpus containing ~ 150 K pairwise alignments between recipes across 4262 dishes with rich commonsense information .,2,0.7592821,142.89942154932768,25
1473,"We introduce a new large-scale NLI benchmark dataset , collected via an iterative , adversarial human-and-model-in-the-loop procedure .",2,0.7048288,32.266592338038265,26
1473,"We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks , while posing a more difficult challenge with its new test set .",3,0.9171283,17.045268368080215,39
1473,"Our analysis sheds light on the shortcomings of current state-of-the-art models , and shows that non-expert annotators are successful at finding their weaknesses .",3,0.9766733,15.636382500808747,30
1473,"The data collection method can be applied in a never-ending learning scenario , becoming a moving target for NLU , rather than a static benchmark that will quickly saturate .",3,0.7811288,62.15068882070768,30
1474,"Although measuring held-out accuracy has been the primary approach to evaluate generalization , it often overestimates the performance of NLP models , while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors .",0,0.85857207,44.3181576418518,40
1474,"Inspired by principles of behavioral testing in software engineering , we introduce CheckList , a task-agnostic methodology for testing NLP models .",2,0.42214844,45.29410403864958,24
1474,"CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation , as well as a software tool to generate a large and diverse number of test cases quickly .",0,0.34060887,95.74148615781907,35
1474,"We illustrate the utility of CheckList with tests for three tasks , identifying critical failures in both commercial and state-of-art models .",3,0.60796237,66.71128482782036,26
1474,"In a user study , a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model .",0,0.67301416,91.39267463103823,25
1474,"In another user study , NLP practitioners with CheckList created twice as many tests , and found almost three times as many bugs as users without it .",3,0.90159225,69.48754432458745,28
1475,"There is an increasing interest in studying natural language and computer code together , as large corpora of programming texts become readily available on the Internet .",0,0.95925134,34.94578015351657,27
1475,"For example , StackOverflow currently has over 15 million programming related questions written by 8.5 million users .",0,0.8376527,41.56757568328672,18
1475,"Meanwhile , there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences .",0,0.942994,54.81761758209821,28
1475,"In this paper , we introduce a new named entity recognition ( NER ) corpus for the computer programming domain , consisting of 15,372 sentences annotated with 20 fine-grained entity types .",1,0.7553602,26.56039220037818,34
1475,"We trained in-domain BERT representations ( BERTOverflow ) on 152 million sentences from StackOverflow , which lead to an absolute increase of + 10 F1 score over off-the-shelf BERT .",2,0.63416046,26.691251476808887,34
1475,We also present the SoftNER model which achieves an overall 79.10 F-1 score for code and named entity recognition on StackOverflow data .,3,0.7513065,52.83104288313086,24
1475,Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERT-based tagging model .,2,0.6497487,66.93225524527631,22
1475,Our code and data are available at : https://github.com/jeniyat/StackOverflowNER/ .,3,0.55057245,23.353359787805093,10
1476,"We present the first human-annotated dialogue-based relation extraction ( RE ) dataset DialogRE , aiming to support the prediction of relation ( s ) between two arguments that appear in a dialogue .",1,0.44138697,52.98511345362054,35
1476,We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences .,3,0.874773,121.08041437401658,18
1476,"We argue that speaker-related information plays a critical role in the proposed task , based on an analysis of similarities and differences between dialogue-based and traditional RE tasks .",3,0.7631933,31.116817608997934,31
1476,"Considering the timeliness of communication in a dialogue , we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE .",2,0.5491021,25.676242029148426,37
1476,Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings .,3,0.97900003,31.235187007193616,25
1476,DialogRE is available at https://dataset.org/dialogre/ .,3,0.5092243,14.577950050276055,6
1477,Commonly adopted metrics for extractive summarization focus on lexical overlap at the token level .,0,0.872157,62.909123940552185,15
1477,"In this paper , we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries .",1,0.8828424,48.68161052791164,22
1477,"Specifically , we treat each sentence in the reference summary as a facet , identify the sentences in the document that express the semantics of each facet as support sentences of the facet , and automatically evaluate extractive summarization methods by comparing the indices of extracted sentences and support sentences of all the facets in the reference summary .",2,0.9020062,30.860990730049835,59
1477,"To facilitate this new evaluation setup , we construct an extractive version of the CNN / Daily Mail dataset and perform a thorough quantitative investigation , through which we demonstrate that facet-aware evaluation manifests better correlation with human judgment than ROUGE , enables fine-grained evaluation as well as comparative analysis , and reveals valuable insights of state-of-the-art summarization methods .",3,0.5245271,37.515053140721626,70
1477,Data can be found at https://github.com/morningmoni/FAR .,3,0.70799696,17.652800371484936,7
1478,Automated generation of conversational dialogue using modern neural architectures has made notable advances .,0,0.954385,55.54443348828733,14
1478,"However , these models are known to have a drawback of often producing uninteresting , predictable responses ;",0,0.92202586,219.3324818213029,18
1478,this is known as the diversity problem .,0,0.90407616,32.246058684252944,8
1478,"We introduce a new strategy to address this problem , called Diversity-Informed Data Collection .",1,0.48387232,58.265523082952726,16
1478,"Unlike prior approaches , which modify model architectures to solve the problem , this method uses dynamically computed corpus-level statistics to determine which conversational participants to collect data from .",2,0.69122225,70.25804732193156,30
1478,"Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods , and better results on two downstream tasks : emotion classification and dialogue generation .",3,0.9328312,46.363789900589026,29
1478,This method is generalizable and can be used with other corpus-level metrics .,3,0.8000029,19.168316866715436,13
1479,"We introduce S2ORC , a large corpus of 81.1M English-language academic papers spanning many academic disciplines .",2,0.4902923,49.153626503721995,19
1479,"The corpus consists of rich metadata , paper abstracts , resolved bibliographic references , as well as structured full text for 8.1 M open access papers .",2,0.43650028,119.79887383701036,27
1479,"Full text is annotated with automatically-detected inline mentions of citations , figures , and tables , each linked to their corresponding paper objects .",2,0.589066,93.23630425839755,26
1479,"In S2ORC , we aggregate papers from hundreds of academic publishers and digital archives into a unified source , and create the largest publicly-available collection of machine-readable academic text to date .",2,0.6321332,51.522765272329515,36
1479,We hope this resource will facilitate research and development of tools and tasks for text mining over academic text .,3,0.7399406,45.670109945154486,20
1480,Automatic metrics are fundamental for the development and evaluation of machine translation systems .,0,0.9328356,12.997023792690275,14
1480,"Judging whether , and to what extent , automatic metrics concur with the gold standard of human evaluation is not a straightforward problem .",0,0.82452947,29.19930164020105,24
1480,"We show that current methods for judging metrics are highly sensitive to the translations used for assessment , particularly the presence of outliers , which often leads to falsely confident conclusions about a metric ’s efficacy .",3,0.9641516,52.44263451653388,37
1480,"Finally , we turn to pairwise system ranking , developing a method for thresholding performance improvement under an automatic metric against human judgements , which allows quantification of type I versus type II errors incurred , i.e. , insignificant human differences in system quality that are accepted , and significant human differences that are rejected .",2,0.6022907,116.91859778592031,56
1480,"Together , these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation .",3,0.9888911,65.0572218996112,20
1481,Generating a readable summary that describes the functionality of a program is known as source code summarization .,0,0.9320342,27.932704917727417,18
1481,"In this task , learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial .",0,0.7863433,71.85848754961427,24
1481,"To learn code representation for summarization , we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies .",2,0.7192645,24.064959199564527,29
1481,"In this work , we show that despite the approach is simple , it outperforms the state-of-the-art techniques by a significant margin .",3,0.4866383,12.442293991374674,29
1481,"We perform extensive analysis and ablation studies that reveal several important findings , e.g. , the absolute encoding of source code tokens ’ position hinders , while relative encoding significantly improves the summarization performance .",3,0.7327529,71.53708511403205,35
1481,We have made our code publicly available to facilitate future research .,3,0.7791101,9.12624403234931,12
1482,Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input .,0,0.89837664,39.39940201725932,18
1482,Existing automatic evaluation metrics for summarization are largely insensitive to such errors .,0,0.8803118,43.37442765043327,13
1482,"We propose QAGS ( pronounced “ kags ” ) , an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary .",1,0.39066884,83.02052669594347,26
1482,"QAGS is based on the intuition that if we ask questions about a summary and its source , we will receive similar answers if the summary is factually consistent with the source .",0,0.8435846,21.430702308197812,33
1482,"To evaluate QAGS , we collect human judgments of factual consistency on model-generated summaries for the CNN / DailyMail ( Hermann et al. , 2015 ) and XSUM ( Narayan et al. , 2018 ) summarization datasets .",2,0.732792,47.56350626486052,39
1482,QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics .,3,0.88680804,66.71087129364889,14
1482,The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why .,3,0.4686844,205.61813143803099,19
1482,We believe QAGS is a promising tool in automatically generating usable and factually consistent text .,3,0.97490793,54.821381738414956,16
1482,Code for QAGS will be available at https://github.com/W4ngatang/qags .,3,0.5060057,28.20666012041979,9
1483,Recently BERT has been adopted for document encoding in state-of-the-art text summarization models .,0,0.8538378,11.064315913628164,20
1483,"However , sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries .",0,0.86392224,46.64768185590841,19
1483,"Also , long-range dependencies throughout a document are not well captured by BERT , which is pre-trained on sentence pairs instead of documents .",3,0.47870946,28.88621402886954,25
1483,"To address these issues , we present a discourse-aware neural summarization model-DiscoBert .",1,0.4947988,64.49543881856924,16
1483,DiscoBert extracts sub-sentential discourse units ( instead of sentences ) as candidates for extractive selection on a finer granularity .,0,0.41929018,135.13552089518856,20
1483,"To capture the long-range dependencies among discourse units , structural discourse graphs are constructed based on RST trees and coreference mentions , encoded with Graph Convolutional Networks .",2,0.64872307,74.33889355318367,28
1483,Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models .,3,0.9395515,8.3985005866273,31
1484,"Automatic sentence summarization produces a shorter version of a sentence , while preserving its most important information .",0,0.69230056,33.15010693128701,18
1484,A good summary is characterized by language fluency and high information overlap with the source sentence .,0,0.53919506,47.59979683796496,17
1484,"We model these two aspects in an unsupervised objective function , consisting of language modeling and semantic similarity metrics .",2,0.77924365,57.95285975001487,20
1484,We search for a high-scoring summary by discrete optimization .,2,0.8143047,114.55166279502392,10
1484,Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores .,3,0.86827916,10.291800039658828,21
1484,"Additionally , we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length .",3,0.9640488,41.61217694388442,17
1484,"Since this is unwillingly exploited in recent work , we emphasize that future evaluation should explicitly group summarization systems by output length brackets .",3,0.8189746,150.7128078396367,24
1485,"We present a new summarization task , generating summaries of novel chapters using summary / chapter pairs from online study guides .",1,0.36991304,122.27125632821537,22
1485,"This is a harder task than the news summarization task , given the chapter length as well as the extreme paraphrasing and generalization found in the summaries .",0,0.6800679,43.95983251132833,28
1485,"We focus on extractive summarization , which requires the creation of a gold-standard set of extractive summaries .",1,0.3804547,16.680871246253055,18
1485,We present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also experiment with different alignment methods .,2,0.43821406,108.80466545008647,25
1485,Our experiments demonstrate significant improvement over prior alignment approaches for our task as shown through automatic metrics and a crowd-sourced pyramid analysis .,3,0.96028656,55.22440588401532,23
1486,"Neural abstractive summarization models are prone to generate content inconsistent with the source document , i.e .",0,0.8339183,24.219009520426752,17
1486,unfaithful .,4,0.53646255,831.6213489423673,2
1486,Existing automatic metrics do not capture such mistakes effectively .,0,0.8277049,80.09080616012812,10
1486,We tackle the problem of evaluating faithfulness of a generated summary given its source document .,0,0.47800493,40.81642242345007,16
1486,We first collected human annotations of faithfulness for outputs from numerous models on two datasets .,2,0.91924804,107.86578244526082,16
1486,We find that current models exhibit a trade-off between abstractiveness and faithfulness : outputs with less word overlap with the source document are more likely to be unfaithful .,3,0.98041254,26.521387394034715,30
1486,"Next , we propose an automatic question answering ( QA ) based metric for faithfulness , FEQA , which leverages recent advances in reading comprehension .",2,0.46805948,39.602920457036355,26
1486,"Given question-answer pairs generated from the summary , a QA model extracts answers from the document ;",2,0.63318217,74.94473709960819,19
1486,non-matched answers indicate unfaithful information in the summary .,3,0.6684743,82.28981338949949,9
1486,"Among metrics based on word overlap , embedding similarity , and learned language understanding models , our QA-based metric has significantly higher correlation with human faithfulness scores , especially on highly abstractive summaries .",3,0.9347432,64.00879039751236,36
1487,Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient .,0,0.94115007,55.57792144060983,16
1487,"We introduce a new evaluation metric which is based on fact-level content weighting , i.e .",2,0.6079156,35.36588793422645,17
1487,relating the facts of the document to the facts of the summary .,0,0.5270606,20.021971088123788,13
1487,"We fol-low the assumption that a good summary will reflect all relevant facts , i.e .",2,0.4606919,73.53163074449499,16
1487,the ones present in the ground truth ( human-generated refer-ence summary ) .,2,0.43992025,176.184740728597,14
1487,We confirm this hypothe-sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight-based metric of Hardy et al .,3,0.96189916,85.16881887764465,32
1487,( 2019 ) .,4,0.7555199,230.9923792802069,4
1488,"Current summarization systems only produce plain , factual headlines , far from the practical needs for the exposure and memorableness of the articles .",0,0.86750215,209.83081342161793,24
1488,"We propose a new task , Stylistic Headline Generation ( SHG ) , to enrich the headlines with three style options ( humor , romance and clickbait ) , thus attracting more readers .",1,0.4898925,95.2640534522252,34
1488,"With no style-specific article-headline pair ( only a standard headline summarization dataset and mono-style corpora ) , our method TitleStylist generates stylistic headlines by combining the summarization and reconstruction tasks into a multitasking framework .",2,0.5725587,113.64769576966832,37
1488,We also introduced a novel parameter sharing scheme to further disentangle the style from text .,3,0.4932366,39.88404091679385,16
1488,"Through both automatic and human evaluation , we demonstrate that TitleStylist can generate relevant , fluent headlines with three target styles : humor , romance , and clickbait .",3,0.86300224,127.15272071419092,29
1488,"The attraction score of our model generated headlines outperforms the state-of-the-art summarization model by 9.68 % , even outperforming human-written references .",3,0.93113685,34.36176244612634,28
1489,"Sequence-to-sequence models for abstractive summarization have been studied extensively , yet the generated summaries commonly suffer from fabricated content , and are often found to be near-extractive .",0,0.9359268,29.144078378009837,28
1489,"We argue that , to address these issues , the summarizer should acquire semantic interpretation over input , e.g. , via structured representation , to allow the generation of more informative summaries .",3,0.6763592,84.43437436825064,33
1489,"In this paper , we present ASGARD , a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD .",1,0.8455148,64.27616860559618,22
1489,"We propose the use of dual encoders — a sequential document encoder and a graph-structured encoder — to maintain the global context and local characteristics of entities , complementing each other .",2,0.5226812,34.28423445838853,32
1489,We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions .,2,0.73304313,45.04257382860409,22
1489,Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN / Daily Mail datasets .,3,0.98839265,24.77476304141418,30
1489,We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models .,3,0.94299835,17.298314415073847,19
1489,Human judges further rate our model outputs as more informative and containing fewer unfaithful errors .,3,0.91584986,108.19598140748872,16
1490,Neural abstractive summarization models are able to generate summaries which have high overlap with human references .,0,0.7184147,22.65776707449003,17
1490,"However , existing models are not optimized for factual correctness , a critical metric in real-world applications .",0,0.9121534,39.8961953935814,18
1490,"In this work , we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module .",1,0.58246243,40.01812534876474,34
1490,We further propose a training strategy which optimizes a neural summarization model with a factual correctness reward via reinforcement learning .,2,0.5110832,43.55239271056122,21
1490,"We apply the proposed method to the summarization of radiology reports , where factual correctness is a key requirement .",2,0.44657654,37.87689289207414,20
1490,"On two separate datasets collected from hospitals , we show via both automatic and human evaluation that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system , producing radiology summaries that approach the quality of human-authored ones .",3,0.7904391,36.89508120000502,48
1491,This paper describes the Critical Role Dungeons and Dragons Dataset ( CRD3 ) and related analyses .,1,0.90899307,69.5447575689118,17
1491,"Critical Role is an unscripted , live-streamed show where a fixed group of people play Dungeons and Dragons , an open-ended role-playing game .",0,0.7972158,35.4289471292319,25
1491,"The dataset is collected from 159 Critical Role episodes transcribed to text dialogues , consisting of 398,682 turns .",2,0.82761115,194.01255593149878,19
1491,It also includes corresponding abstractive summaries collected from the Fandom wiki .,2,0.44920295,75.81160353330283,12
1491,The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction .,3,0.42434835,92.09471991404543,19
1491,"For each dialogue , there are a large number of turns , multiple abstractive summaries with varying levels of detail , and semantic ties to the previous dialogues .",0,0.5191303,44.238243099220036,29
1491,"In addition , we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural ML approaches , and we provide an abstractive summarization benchmark and evaluation .",2,0.48186335,79.65642831843796,34
1492,This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint .,1,0.74982065,23.064341864821145,25
1492,It introduces a novel method that encourages the inclusion of key terms from the original document into the summary : key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary .,2,0.50616014,31.986171201378035,44
1492,A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries .,2,0.7138337,51.57958544071642,20
1492,"When tested on popular news summarization datasets , the method outperforms previous unsupervised methods by more than 2 R-1 points , and approaches results of competitive supervised methods .",3,0.8681846,34.210616840124004,31
1492,"Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work , and learns to compress and merge sentences without supervision .",3,0.7587592,107.8737036357233,28
1493,"Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents , such as product reviews .",0,0.9316556,36.266318803335565,23
1493,"While the majority of previous work has focused on the extractive setting , i.e. , selecting fragments from input reviews to produce a summary , we let the model generate novel sentences and hence produce abstractive summaries .",0,0.70486563,35.56601790724467,38
1493,Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs .,0,0.9527441,21.601973197344236,20
1493,"Since such training data is expensive to acquire , we instead consider the unsupervised setting , in other words , we do not use any summaries in training .",2,0.6714446,36.849754451545955,29
1493,"We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product , we should be able to control the “ amount of novelty ” going into the new review or , equivalently , vary the extent to which it deviates from the input .",2,0.7532618,33.83882559506661,62
1493,"At test time , when generating summaries , we force the novelty to be minimal , and produce a text reflecting consensus opinions .",2,0.5255866,158.8465758521585,24
1493,We capture this intuition by defining a hierarchical variational autoencoder model .,2,0.8170106,23.82870170535433,12
1493,"Both individual reviews and the products they correspond to are associated with stochastic latent codes , and the review generator ( “ decoder ” ) has direct access to the text of input reviews through the pointer-generator mechanism .",2,0.4746727,74.27631953594361,41
1493,"Experiments on Amazon and Yelp datasets , show that setting at test time the review ’s latent code to its mean , allows the model to produce fluent and coherent summaries reflecting common opinions .",3,0.8911076,109.94935343268106,35
1494,Human speakers have an extensive toolkit of ways to express themselves .,0,0.93940735,41.652029296327946,12
1494,"In this paper , we engage with an idea largely absent from discussions of meaning in natural language understanding — namely , that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed .",1,0.87539524,40.48886169875969,40
1494,"We first define this phenomenon more precisely , drawing on considerable prior work in theoretical cognitive semantics and psycholinguistics .",2,0.384349,58.413043651250774,20
1494,We then survey some dimensions of construed meaning and show how insights from construal could inform theoretical and practical work in NLP .,3,0.42899713,66.80101869712628,23
1495,The success of the large neural language models on many NLP tasks is exciting .,0,0.8119264,22.327795523325193,15
1495,"However , we find that these successes sometimes lead to hype in which these models are being described as “ understanding ” language or capturing “ meaning ” .",3,0.9349349,53.25875838002556,29
1495,"In this position paper , we argue that a system trained only on form has a priori no way to learn meaning .",1,0.6916523,40.05569660493512,23
1495,"In keeping with the ACL 2020 theme of “ Taking Stock of Where We’ve Been and Where We ’re Going ” , we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding .",3,0.7805284,36.109421404630645,48
1496,We extracted information from the ACL Anthology ( AA ) and Google Scholar ( GS ) to examine trends in citations of NLP papers .,2,0.9619486,73.92580551995614,25
1496,etc .,4,0.6067753,80.44964142568887,2
1496,"Notably , we show that only about 56 % of the papers in AA are cited ten or more times .",3,0.98086584,76.07359893228474,21
1496,"CL Journal has the most cited papers , but its citation dominance has lessened in recent years .",0,0.7931454,52.17804675404003,18
1496,"On average , long papers get almost three times as many citations as short papers ;",3,0.8036844,90.53101344441654,16
1496,"and papers on sentiment classification , anaphora resolution , and entity recognition have the highest median citations .",3,0.8949696,167.7279866625104,18
1496,"The analyses presented here , and the associated dataset of NLP papers mapped to citations , have a number of uses including : understanding how the field is growing and quantifying the impact of different types of papers .",3,0.8198304,57.20642088526383,39
1497,"This position paper describes and critiques the Pretraining-Agnostic Identically Distributed ( PAID ) evaluation paradigm , which has become a central tool for measuring progress in natural language understanding .",1,0.6450983,50.438086028392,32
1497,This paradigm consists of three stages : ( 1 ) pre-training of a word prediction model on a corpus of arbitrary size ;,0,0.48229778,28.34901634765206,23
1497,( 2 ) fine-tuning ( transfer learning ) on a training set representing a classification task ;,2,0.67453855,100.85912170572045,18
1497,( 3 ) evaluation on a test set drawn from the same distribution as that training set .,2,0.6125965,58.48043227961159,18
1497,"This paradigm favors simple , low-bias architectures , which , first , can be scaled to process vast amounts of data , and second , can capture the fine-grained statistical properties of a particular data set , regardless of whether those properties are likely to generalize to examples of the task outside the data set .",0,0.6978229,45.123306620871226,57
1497,"This contrasts with humans , who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm , and generalize to new tasks in a consistent way .",3,0.7184479,46.66358856121684,34
1497,We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans .,3,0.86402076,54.437597938497234,21
1498,"Legal Artificial Intelligence ( LegalAI ) focuses on applying the technology of artificial intelligence , especially natural language processing , to benefit tasks in the legal domain .",0,0.94046307,65.38887984131819,28
1498,"In recent years , LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals , as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork .",0,0.92036015,55.258385966764116,36
1498,"Legal professionals often think about how to solve tasks from rule-based and symbol-based methods , while NLP researchers concentrate more on data-driven and embedding methods .",0,0.8216578,40.02474739542108,29
1498,"In this paper , we introduce the history , the current state , and the future directions of research in LegalAI .",1,0.90238345,53.75343991321385,22
1498,We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI .,3,0.43041912,54.02640270523284,21
1498,We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions .,3,0.49961406,18.898794174432936,23
1498,You can find the implementation of our work from https://github.com/thunlp/CLAIM .,3,0.499716,13.068945306926555,11
1499,"While pretrained models such as BERT have shown large gains across natural language understanding tasks , their performance can be improved by further training the model on a data-rich intermediate task , before fine-tuning it on a target task .",0,0.6545447,17.55726434021476,42
1499,"However , it is still poorly understood when and why intermediate-task training is beneficial for a given target task .",0,0.9449653,27.481846975640405,22
1499,"To investigate this , we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations .",2,0.65580636,40.53925428195959,22
1499,We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer .,3,0.5055917,161.36951221801132,20
1499,We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best .,3,0.97619617,49.591162718861256,17
1499,We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution .,3,0.9731218,27.464554655446516,20
1499,"However , we fail to observe more granular correlations between probing and target task performance , highlighting the need for further work on broad-coverage probing benchmarks .",3,0.9817587,30.42474512938124,27
1499,"We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis , highlighting the need for further work on transfer learning methods in these settings .",3,0.98070073,34.535759239620134,31
1500,"An increasing number of natural language processing papers address the effect of bias on predictions , introducing mitigation techniques at different parts of the standard NLP pipeline ( data and models ) .",0,0.9103383,49.835241812259284,33
1500,"However , these works have been conducted individually , without a unifying framework to organize efforts within the field .",0,0.91282153,92.53552816046113,20
1500,"This situation leads to repetitive approaches , and focuses overly on bias symptoms / effects , rather than on their origins , which could limit the development of effective countermeasures .",0,0.71715224,145.2592959546027,31
1500,"In this paper , we propose a unifying predictive bias framework for NLP .",1,0.92229056,46.64492376229066,14
1500,We summarize the NLP literature and suggest general mathematical definitions of predictive bias .,1,0.60187036,93.16826308162001,14
1500,"We differentiate two consequences of bias : outcome disparities and error disparities , as well as four potential origins of biases : label bias , selection bias , model overamplification , and semantic bias .",2,0.5691367,96.822029517523,35
1500,"Our framework serves as an overview of predictive bias in NLP , integrating existing work into a single structure , and providing a conceptual baseline for improved frameworks .",3,0.7228379,70.38113957353326,29
1501,"Pre-trained visually grounded language models such as ViLBERT , LXMERT , and UNITER have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear .",0,0.8642744,37.81832089364236,34
1501,"In this work , we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions .",1,0.7310465,117.49021313994524,25
1501,"Specifically , some heads can map entities to image regions , performing the task known as entity grounding .",0,0.6570876,208.33126810788642,19
1501,"Some heads can even detect the syntactic relations between non-entity words and image regions , tracking , for example , associations between verbs and regions corresponding to their arguments .",0,0.65511346,114.76691351833679,30
1501,We denote this ability as syntactic grounding .,2,0.45200053,125.56113343120494,8
1501,"We verify grounding both quantitatively and qualitatively , using Flickr30 K Entities as a testbed .",2,0.6125159,159.82409297190333,16
1502,"Throughout a conversation , participants make choices that can orient the flow of the interaction .",0,0.475318,61.17595819961665,16
1502,"Such choices are particularly salient in the consequential domain of crisis counseling , where a difficulty for counselors is balancing between two key objectives : advancing the conversation towards a resolution , and empathetically addressing the crisis situation .",0,0.7871229,73.53906439435511,39
1502,"In this work , we develop an unsupervised methodology to quantify how counselors manage this balance .",1,0.8407818,76.34243319808525,17
1502,"Our main intuition is that if an utterance can only receive a narrow range of appropriate replies , then its likely aim is to advance the conversation forwards , towards a target within that range .",3,0.4040121,60.770547478161824,36
1502,"Likewise , an utterance that can only appropriately follow a narrow range of possible utterances is likely aimed backwards at addressing a specific situation within that range .",0,0.519713,61.584660799661684,28
1502,"By applying this intuition , we can map each utterance to a continuous orientation axis that captures the degree to which it is intended to direct the flow of the conversation forwards or backwards .",2,0.46749148,36.26992459984844,35
1502,"This unsupervised method allows us to characterize counselor behaviors in a large dataset of crisis counseling conversations , where we show that known counseling strategies intuitively align with this axis .",2,0.46610966,63.01937091238867,31
1502,"We also illustrate how our measure can be indicative of a conversation ’s progress , as well as its effectiveness .",3,0.7167033,56.286253721752175,21
1503,"Natural disasters ( e.g. , hurricanes ) affect millions of people each year , causing widespread destruction in their wake .",0,0.95432407,44.61095045821508,21
1503,"People have recently taken to social media websites ( e.g. , Twitter ) to share their sentiments and feelings with the larger community .",0,0.94304204,25.298167546324294,24
1503,"Consequently , these platforms have become instrumental in understanding and perceiving emotions at scale .",0,0.90584004,72.01871678994576,15
1503,"In this paper , we introduce HurricaneEmo , an emotion dataset of 15,000 English tweets spanning three hurricanes : Harvey , Irma , and Maria .",1,0.69811285,41.467224541019405,26
1503,We present a comprehensive study of fine-grained emotions and propose classification tasks to discriminate between coarse-grained emotion groups .,1,0.69149643,29.816458486203892,20
1503,"Our best BERT model , even after task-guided pre-training which leverages unlabeled Twitter data , achieves only 68 % accuracy ( averaged across all groups ) .",3,0.9499769,104.20334024249469,29
1503,HurricaneEmo serves not only as a challenging benchmark for models but also as a valuable resource for analyzing emotions in disaster-centric domains .,0,0.48813763,56.225120004337406,23
1504,Not all documents are equally important .,0,0.6779004,52.11331074197018,7
1504,"Language processing is increasingly finding use as a supplement for questionnaires to assess psychological attributes of consenting individuals , but most approaches neglect to consider whether all documents of an individual are equally informative .",0,0.8741036,66.57267198287467,35
1504,"In this paper , we present a novel model that uses message-level attention to learn the relative weight of users ’ social media posts for assessing their five factor personality traits .",1,0.8837944,39.398528426845864,34
1504,"We demonstrate that models with message-level attention outperform those with word-level attention , and ultimately yield state-of-the-art accuracies for all five traits by using both word and message attention in combination with past approaches ( an average increase in Pearson r of 2.5 % ) .",3,0.91114056,42.87923026160743,54
1504,"In addition , examination of the high-signal posts identified by our model provides insight into the relationship between language and personality , helping to inform future work .",3,0.9676897,52.820589318672816,28
1505,People vary in their ability to make accurate predictions about the future .,0,0.9430487,14.011775199119421,13
1505,Prior studies have shown that some individuals can predict the outcome of future events with consistently better accuracy .,0,0.9094668,30.687584703331755,19
1505,In this paper we explore connections between the language people use to describe their predictions and their forecasting skill .,1,0.9152328,39.69298678022763,20
1505,"Datasets from two different forecasting domains are explored : ( 1 ) geopolitical forecasts from Good Judgment Open , an online prediction forum and ( 2 ) a corpus of company earnings forecasts made by financial analysts .",2,0.7304052,99.2022146711337,38
1505,"We present a number of linguistic metrics which are computed over text associated with people ’s predictions about the future including : uncertainty , readability , and emotion .",2,0.51557606,99.1882611784257,29
1505,"By studying linguistic factors associated with predictions , we are able to shed some light on the approach taken by skilled forecasters .",3,0.659805,57.424077065780764,23
1505,"Furthermore , we demonstrate that it is possible to accurately predict forecasting skill using a model that is based solely on language .",3,0.9449163,24.980041918282318,23
1505,This could potentially be useful for identifying accurate predictions or potentially skilled forecasters earlier .,3,0.8744808,187.29199631717898,15
1506,Many applications of computational social science aim to infer causal conclusions from non-experimental data .,0,0.94700485,47.8498042096531,15
1506,"Such observational data often contains confounders , variables that influence both potential causes and potential effects .",0,0.89332485,99.21437236471155,17
1506,"Unmeasured or latent confounders can bias causal estimates , and this has motivated interest in measuring potential confounders from observed text .",0,0.95999694,65.96495594779229,22
1506,"For example , an individual ’s entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders .",0,0.6439016,48.72673467751919,28
1506,"Yet , methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent .",0,0.89190906,67.52089819582783,19
1506,This review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions .,1,0.5505819,43.129907304823064,22
1506,"Despite increased attention on adjusting for confounding using text , there are still many open problems , which we highlight in this paper .",0,0.5639636,65.89029369526293,24
1507,"Ideal point models analyze lawmakers ’ votes to quantify their political positions , or ideal points .",0,0.8174083,381.735658067916,17
1507,But votes are not the only way to express a political position .,0,0.87106067,19.65457914633043,13
1507,"Lawmakers also give speeches , release press statements , and post tweets .",0,0.7896184,188.49956286488728,13
1507,"In this paper , we introduce the text-based ideal point model ( TBIP ) , an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors .",1,0.83437586,46.26874576993761,34
1507,We demonstrate the TBIP with two types of politicized text data : U.S .,3,0.6281501,143.27119785067325,14
1507,Senate speeches and senator tweets .,2,0.34261137,551.601397678895,6
1507,"Though the model does not analyze their votes or political affiliations , the TBIP separates lawmakers by party , learns interpretable politicized topics , and infers ideal points close to the classical vote-based ideal points .",3,0.58687174,238.96769440825858,37
1507,"One benefit of analyzing texts , as opposed to votes , is that the TBIP can estimate ideal points of anyone who authors political texts , including non-voting actors .",3,0.48826042,177.51383298841588,30
1507,"To this end , we use it to study tweets from the 2020 Democratic presidential candidates .",2,0.784067,45.400652874664196,17
1507,"Using only the texts of their tweets , it identifies them along an interpretable progressive-to-moderate spectrum .",2,0.42284796,92.54885468222062,19
1508,"While national politics often receive the spotlight , the overwhelming majority of legislation proposed , discussed , and enacted is done at the state level .",0,0.8860809,73.57750694847657,26
1508,"Despite this fact , there is little awareness of the dynamics that lead to adopting these policies .",0,0.89530617,40.11311425197173,18
1508,"In this paper , we take the first step towards a better understanding of these processes and the underlying dynamics that shape them , using data-driven methods .",1,0.8935936,17.036541327118798,28
1508,"We build a new large-scale dataset , from multiple data sources , connecting state bills and legislator information , geographical information about their districts , and donations and donors ’ information .",2,0.8834597,160.17153071070587,32
1508,"We suggest a novel task , predicting the legislative body ’s vote breakdown for a given bill , according to different criteria of interest , such as gender , rural-urban and ideological splits .",3,0.49347302,101.03139486079452,34
1508,"Finally , we suggest a shared relational embedding model , representing the interactions between the text of the bill and the legislative context in which it is presented .",3,0.79177976,47.278774777833505,29
1508,Our experiments show that providing this context helps improve the prediction over strong text-based models .,3,0.97709846,37.011112819974095,18
1509,"Understanding human preferences , along with cultural and social nuances , lives at the heart of natural language understanding .",0,0.9166294,87.7260041991733,20
1509,"Concretely , we present a new task and corpus for learning alignments between machine and human preferences .",3,0.5075294,56.4750626361007,18
1509,Our newly introduced problem is concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations .,2,0.3731927,147.18168080506575,24
1509,"Our problem is framed as a natural language inference task with crowd-sourced preference votes by human players , obtained from a gamified voting platform .",2,0.7453616,79.32998819990001,25
1509,"We benchmark several state-of-the-art neural models , along with BERT and friends on this task .",2,0.6023648,25.115812532710297,21
1509,Our experimental results show that current state-of-the-art NLP models still leave much room for improvement .,3,0.9845732,6.964304087407042,21
1510,Understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event .,0,0.9487391,39.55570050203365,18
1510,"To enable computational modeling of news structures , we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources .",2,0.83765686,64.9743537158636,43
1510,"Next , we propose several document-level neural-network models to automatically construct news content structures .",2,0.4821047,46.71004937242297,18
1510,"Finally , we demonstrate that incorporating system predicted news structures yields new state-of-the-art performance for event coreference resolution .",3,0.9578672,35.07894922770625,25
1510,The news documents we annotated are openly available and the annotations are publicly released for future research .,3,0.6465256,57.44549373069633,18
1511,Pragmatic inferences often subtly depend on the presence or absence of linguistic features .,0,0.8910558,42.96009017417617,14
1511,"For example , the presence of a partitive construction ( of the ) increases the strength of a so-called scalar inference : listeners perceive the inference that Chris did not eat all of the cookies to be stronger after hearing “ Chris ate some of the cookies ” than after hearing the same utterance without a partitive , “ Chris ate some cookies ” .",0,0.49503362,88.43095519445544,65
1511,"In this work , we explore to what extent neural network sentence encoders can learn to predict the strength of scalar inferences .",1,0.9284088,29.518133187737845,23
1511,We first show that an LSTM-based sentence encoder trained on an English dataset of human inference strength ratings is able to predict ratings with high accuracy ( r = 0.78 ) .,3,0.86647475,34.18281466024267,34
1511,We then probe the model ’s behavior using manually constructed minimal sentence pairs and corpus data .,2,0.8651931,151.31415285800162,17
1511,"We first that the model inferred previously established associations between linguistic features and inference strength , suggesting that the model learns to use linguistic features to predict pragmatic inferences .",3,0.6671219,79.5552295920637,30
1512,Implicit relation classification on Penn Discourse TreeBank ( PDTB ) 2.0 is a common benchmark task for evaluating the understanding of discourse relations .,0,0.9036076,57.385372061347226,24
1512,"However , the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature .",0,0.68209046,42.33260303989593,21
1512,"In this work , we highlight these inconsistencies and propose an improved evaluation protocol .",1,0.79512554,58.703607561738885,15
1512,"Paired with this protocol , we report strong baseline results from pretrained sentence encoders , which set the new state-of-the-art for PDTB 2.0 .",3,0.8897845,33.42235179785987,28
1512,"Furthermore , this work is the first to explore fine-grained relation classification on PDTB 3.0 .",3,0.9654358,32.48057826062239,17
1512,"We expect our work to serve as a point of comparison for future work , and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability .",3,0.9342072,51.5164271262139,34
1513,"We propose PeTra , a memory-augmented neural network designed to track entities in its memory slots .",1,0.58256024,82.2962880529297,19
1513,PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture .,2,0.43357036,106.99482981017741,27
1513,"We empirically compare key modeling choices , finding that we can simplify several aspects of the design of the memory module while retaining strong performance .",3,0.8084872,81.86275117768052,26
1513,"To measure the people tracking capability of memory models , we ( a ) propose a new diagnostic evaluation based on counting the number of unique entities in text , and ( b ) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach .",2,0.5688649,65.59830655991811,58
1513,"PeTra is highly effective in both evaluations , demonstrating its ability to track people in its memory despite being trained with limited annotation .",3,0.94726205,117.45856401334672,24
1514,"Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions , respectively .",0,0.56691825,149.26930440256334,20
1514,"We propose to better explore their interaction by solving both tasks together , while the previous work treats them separately .",3,0.5382545,85.01848552107471,21
1514,"For zero pronoun resolution , we study this task in a more realistic setting , where no parsing trees or only automatic trees are available , while most previous work assumes gold trees .",2,0.74477494,111.52737928683212,34
1514,Experiments on two benchmarks show that joint modeling significantly outperforms our baseline that already beats the previous state of the arts .,3,0.8895188,29.776739676886084,22
1515,Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like “ gay ” or “ black ” are used in offensive or prejudiced ways .,0,0.8877934,25.88897804106903,29
1515,"Such biases manifest in false positives when these identifiers are present , due to models ’ inability to learn the contexts which constitute a hateful usage of identifiers .",0,0.65958685,104.11537989640608,29
1515,We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms .,2,0.7684588,43.510110056706445,16
1515,"Then , we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves .",2,0.5371966,33.235464881375336,30
1515,Our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance .,3,0.89364487,39.07200504745891,25
1516,Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models .,0,0.6693006,38.561066878081014,19
1516,"Some commonly adopted debiasing approaches , including the seminal Hard Debias algorithm , apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace .",0,0.8133288,57.28036513446495,31
1516,We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms .,3,0.9386587,37.48352884199446,23
1516,"We propose a simple but effective technique , Double Hard Debias , which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace .",2,0.4670278,87.70722408232973,30
1516,Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches .,3,0.9214772,17.950775613552384,32
1517,"We survey 146 papers analyzing “ bias ” in NLP systems , finding that their motivations are often vague , inconsistent , and lacking in normative reasoning , despite the fact that analyzing “ bias ” is an inherently normative process .",3,0.47291222,51.12149503482566,42
1517,We further find that these papers ’ proposed quantitative techniques for measuring or mitigating “ bias ” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP .,3,0.9691516,73.46174911994703,35
1517,"Based on these findings , we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “ bias ” in NLP systems .",3,0.93196464,44.749775403015875,29
1517,"These recommendations rest on a greater recognition of the relationships between language and social hierarchies , encouraging researchers and practitioners to articulate their conceptualizations of “ bias ”---i.e. , what kinds of system behaviors are harmful , in what ways , to whom , and why , as well as the normative reasoning underlying these statements — and to center work around the lived experiences of members of communities affected by NLP systems , while interrogating and reimagining the power relations between technologists and such communities .",3,0.2656619,45.30156673190701,88
1518,Warning : this paper contains content that may be offensive or upsetting .,4,0.34855792,35.86229278589208,13
1518,Language has the power to reinforce stereotypes and project social biases onto others .,0,0.86413527,32.731799054374605,14
1518,"At the core of the challenge is that it is rarely what is stated explicitly , but rather the implied meanings , that frame people ’s judgments about others .",0,0.74786675,58.116823166551185,30
1518,"For example , given a statement that “ we should n’t lower our standards to hire more women , ” most listeners will infer the implicature intended by the speaker-that “ women ( candidates ) are less qualified .",3,0.70715594,89.25330431804247,41
1518,"Most semantic formalisms , to date , do not capture such pragmatic implications in which people express social biases and power differentials in language .",0,0.88902,175.66983245573545,25
1518,"We introduce Social Bias Frames , a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others .",2,0.37687922,51.088085599062694,28
1518,"In addition , we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts , covering over 34 k implications about a thousand demographic groups .",2,0.76145256,96.7458817139646,36
1518,We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text .,2,0.66576374,61.91570769501453,16
1518,"We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias ( 80 % F1 ) , they are not effective at spelling out more detailed explanations in terms of Social Bias Frames .",3,0.9693484,35.980394593311125,52
1518,Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications .,3,0.97324276,83.33993586581144,17
1519,Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models .,0,0.8269086,84.91274200639396,20
1519,"In particular , representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained .",0,0.9112519,67.31894258022008,22
1519,"In this paper , we present evidence of such undesirable biases towards mentions of disability in two different English language models : toxicity prediction and sentiment analysis .",1,0.9025684,101.972864660155,28
1519,"Next , we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability .",3,0.7359569,65.51609248475953,27
1519,We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases ;,3,0.59236616,99.51457608375308,20
1519,"for instance , gun violence , homelessness , and drug addiction are over-represented in texts discussing mental illness .",0,0.65854734,94.9365098810193,19
1520,"As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare , legal systems , and social science , it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes .",0,0.92079526,24.176853720172335,39
1520,"Previous work has revealed the presence of social biases in widely used word embeddings involving gender , race , religion , and other social constructs .",0,0.9177558,29.043478457740502,26
1520,"While some methods were proposed to debias these word-level embeddings , there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT .",0,0.834975,23.424368844236994,39
1520,"In this paper , we investigate the presence of social biases in sentence-level representations and propose a new method , Sent-Debias , to reduce these biases .",1,0.9293838,27.019964619217912,29
1520,"We show that Sent-Debias is effective in removing biases , and at the same time , preserves performance on sentence-level downstream tasks such as sentiment analysis , linguistic acceptability , and natural language understanding .",3,0.95851886,46.9595034968805,37
1520,We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP .,3,0.9276558,35.321833502332154,24
1521,Knowledge Graph Completion ( KGC ) aims at automatically predicting missing links for large-scale knowledge graphs .,0,0.92246693,32.67748242043257,17
1521,"A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields , including data mining , machine learning , and natural language processing .",0,0.9148897,26.46377622815972,35
1521,"However , we notice that several recent papers report very high performance , which largely outperforms previous state-of-the-art methods .",3,0.7190819,22.751089779519052,25
1521,"In this paper , we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem .",1,0.7197733,27.751598648678733,30
1521,"The proposed protocol is robust to handle bias in the model , which can substantially affect the final results .",3,0.9265981,59.9814952585016,20
1521,We conduct extensive experiments and report performance of several existing methods using our protocol .,2,0.4665394,64.24555728087277,15
1521,The reproducible code has been made publicly available .,3,0.49916407,24.3422154603935,9
1522,A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy .,0,0.8840272,22.24973212927628,21
1522,"However , these studies are based primarily on monolingual evidence from English .",0,0.86752665,33.478417392688876,13
1522,"To investigate how these models ’ ability to learn syntax varies by language , we introduce CLAMS ( Cross-Linguistic Assessment of Models on Syntax ) , a syntactic evaluation suite for monolingual and multilingual models .",1,0.6298034,36.97604467563196,37
1522,"CLAMS includes subject-verb agreement challenge sets for English , French , German , Hebrew and Russian , generated from grammars we develop .",2,0.61964405,102.24693935446366,25
1522,We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT .,2,0.7798335,22.532914824996574,16
1522,"Across languages , monolingual LSTMs achieved high accuracy on dependencies without attractors , and generally poor accuracy on agreement across object relative clauses .",3,0.9066626,175.20345771496193,24
1522,"On other constructions , agreement accuracy was generally higher in languages with richer morphology .",3,0.977398,125.0400123957887,15
1522,Multilingual models generally underperformed monolingual models .,3,0.7709439,40.197257843145046,7
1522,"Multilingual BERT showed high syntactic accuracy on English , but noticeable deficiencies in other languages .",3,0.9383997,57.96032143111278,16
1523,Algorithmic approaches to interpreting machine learning models have proliferated in recent years .,0,0.9580131,15.042715730366467,13
1523,"We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability , simulatability , while avoiding important confounding experimental factors .",2,0.7072056,81.27097985389398,37
1523,A model is simulatable when a person can predict its behavior on new inputs .,0,0.7493067,115.04821651600676,15
1523,"Through two kinds of simulation tests involving text and tabular data , we evaluate five explanations methods : ( 1 ) LIME , ( 2 ) Anchor , ( 3 ) Decision Boundary , ( 4 ) a Prototype model , and ( 5 ) a Composite approach that combines explanations from each method .",2,0.86215997,75.43675624495958,55
1523,"Clear evidence of method effectiveness is found in very few cases : LIME improves simulatability in tabular classification , and our Prototype method is effective in counterfactual simulation tests .",3,0.96266735,116.15967836374834,30
1523,"We also collect subjective ratings of explanations , but we do not find that ratings are predictive of how helpful explanations are .",3,0.8440055,34.834786611188974,23
1523,Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains .,3,0.9875563,53.14470137037411,24
1523,"We show that ( 1 ) we need to be careful about the metrics we use to evaluate explanation methods , and ( 2 ) there is significant room for improvement in current methods .",3,0.9373354,26.95495236718589,35
1524,Modern deep learning models for NLP are notoriously opaque .,0,0.9235885,23.483726626559427,10
1524,"This has motivated the development of methods for interpreting such models , e.g. , via gradient-based saliency maps or the visualization of attention weights .",0,0.93579334,48.99368666824188,27
1524,Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text .,0,0.8053234,50.86189931688557,21
1524,"While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input , we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning .",3,0.7984823,35.015294113653724,40
1524,"In this work , we investigate the use of influence functions for NLP , providing an alternative approach to interpreting neural text classifiers .",1,0.9062225,42.5906843210686,24
1524,Influence functions explain the decisions of a model by identifying influential training examples .,0,0.58945113,108.56890952851711,14
1524,"Despite the promise of this approach , influence functions have not yet been extensively evaluated in the context of NLP , a gap addressed by this work .",0,0.49580234,35.68196149505722,28
1524,We conduct a comparison between influence functions and common word-saliency methods on representative tasks .,2,0.81372225,115.60225337522643,15
1524,"As suspected , we find that influence functions are particularly useful for natural language inference , a task in which ‘ saliency maps ’ may not have clear interpretation .",3,0.97477734,75.50304402338953,30
1524,"Furthermore , we develop a new quantitative measure based on influence functions that can reveal artifacts in training data .",3,0.46820453,61.657531197356825,20
1525,"Recent work has found evidence that Multilingual BERT ( mBERT ) , a transformer-based multilingual masked language model , is capable of zero-shot cross-lingual transfer , suggesting that some aspects of its representations are shared cross-lingually .",0,0.8736636,20.352521543896774,39
1525,"To better understand this overlap , we extend recent work on finding syntactic trees in neural networks ’ internal representations to the multilingual setting .",1,0.65403306,66.63997219978963,25
1525,"We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English , and that these subspaces are approximately shared across languages .",3,0.8908974,35.74632572294142,27
1525,"Motivated by these results , we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels , in the form of clusters which largely agree with the Universal Dependencies taxonomy .",3,0.43493822,51.31894823277469,36
1525,"This evidence suggests that even without explicit supervision , multilingual masked language models learn certain linguistic universals .",3,0.93839777,41.96974354874267,18
1526,Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness .,0,0.945628,33.24686147832787,20
1526,"In natural language processing , existing methods usually provide important features which are words or phrases selected from an input text as an explanation , but ignore the interactions between them .",0,0.8910798,47.320849836790075,32
1526,It poses challenges for humans to interpret an explanation and connect it to model prediction .,0,0.9357665,72.66020014133461,16
1526,"In this work , we build hierarchical explanations by detecting feature interactions .",1,0.4883142,182.69768883750763,13
1526,"Such explanations visualize how words and phrases are combined at different levels of the hierarchy , which can help users understand the decision-making of black-box models .",3,0.5907882,38.856991775340695,30
1526,"The proposed method is evaluated with three neural text classifiers ( LSTM , CNN , and BERT ) on two benchmark datasets , via both automatic and human evaluations .",2,0.6483745,25.770603367243393,30
1526,Experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans .,3,0.935853,13.51439581680996,22
1527,"Neural module networks ( NMNs ) are a popular approach for modeling compositionality : they achieve high accuracy when applied to problems in language and vision , while reflecting the compositional structure of the problem in the network architecture .",0,0.92645824,47.705377003135766,40
1527,"However , prior work implicitly assumed that the structure of the network modules , describing the abstract reasoning process , provides a faithful explanation of the model ’s reasoning ;",0,0.87605494,88.91678560743368,30
1527,"that is , that all modules perform their intended behaviour .",0,0.5684858,172.9661998873526,11
1527,"In this work , we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP , two datasets which require composing multiple reasoning steps .",1,0.8528108,78.87690325787567,31
1527,"We find that the intermediate outputs differ from the expected output , illustrating that the network structure does not provide a faithful explanation of model behaviour .",3,0.97576606,50.924558723896325,27
1527,"To remedy that , we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness , at a minimal cost to accuracy .",2,0.6175447,111.23187572584683,31
1528,Selecting input features of top relevance has become a popular method for building self-explaining models .,0,0.9376805,49.780592456452666,17
1528,"In this work , we extend this selective rationalization approach to text matching , where the goal is to jointly select and align text pieces , such as tokens or sentences , as a justification for the downstream prediction .",1,0.5729847,62.70248195684754,40
1528,Our approach employs optimal transport ( OT ) to find a minimal cost alignment between the inputs .,2,0.71058184,223.29600929729062,18
1528,"However , directly applying OT often produces dense and therefore uninterpretable alignments .",0,0.8646115,196.7682750739174,13
1528,"To overcome this limitation , we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity .",2,0.5044227,66.44385133735027,24
1528,Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations .,3,0.5332692,40.43590940855704,22
1528,"We evaluate our model on the StackExchange , MultiNews , e-SNLI , and MultiRC datasets .",2,0.55607474,83.63975263701522,16
1528,Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models .,3,0.90331066,200.6396140834435,21
1529,Complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer .,0,0.78824157,167.9040326464839,20
1529,"A large combinatorial space of possible decision paths that result in the same answer , compounded by the lack of intermediate supervision to help choose the right path , makes the learning particularly hard for this task .",0,0.78254575,65.397610781498,38
1529,"In this work , we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection .",1,0.89326215,129.11081200237504,21
1529,We find that these intermediate annotations can provide two-fold benefits .,3,0.974357,53.507759927968195,12
1529,"First , we observe that for any collection budget , spending a fraction of it on intermediate annotations results in improved model performance , for two complex compositional datasets : DROP and Quoref .",3,0.91616285,138.86605378017944,34
1529,"Second , these annotations encourage the model to learn the correct latent reasoning steps , helping combat some of the biases introduced during the data collection process .",3,0.6899265,62.42441296909019,28
1530,Answer retrieval is to find the most aligned answer from a large set of candidates given a question .,0,0.8804891,33.626215835375184,19
1530,Learning vector representations of questions / answers is the key factor .,0,0.76558936,106.66392243082966,12
1530,Question-answer alignment and question / answer semantics are two important signals for learning the representations .,0,0.5674189,46.407208178313425,18
1530,Existing methods learned semantic representations with dual encoders or dual variational auto-encoders .,0,0.53415036,26.389737575447278,13
1530,The semantic information was learned from language models or question-to-question ( answer-to-answer ) generative processes .,2,0.8065174,52.42407040492184,21
1530,"However , the alignment and semantics were too separate to capture the aligned semantics between question and answer .",3,0.9221419,106.55411853302734,19
1530,"In this work , we propose to cross variational auto-encoders by generating questions with aligned answers and generating answers with aligned questions .",1,0.6616202,28.501432999191497,23
1530,Experiments show that our method outperforms the state-of-the-art answer retrieval method on SQuAD .,3,0.9512821,6.902459873565126,20
1531,"Many natural language questions require qualitative , quantitative or logical comparisons between two entities or events .",0,0.9182793,80.96687397293555,17
1531,This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models .,1,0.9080218,33.9439830614966,24
1531,Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model .,2,0.7598769,31.682343087262566,25
1531,"Improving the global consistency of predictions , our approach achieves large improvements over previous methods in a variety of question answering ( QA ) tasks , including multiple-choice qualitative reasoning , cause-effect reasoning , and extractive machine reading comprehension .",3,0.8278377,36.82767390500677,41
1531,"In particular , our method significantly improves the performance of RoBERTa-based models by 1-5 % across datasets .",3,0.94559854,21.87439766255685,22
1531,We advance state of the art by around 5-8 % on WIQA and QuaRel and reduce consistency violations by 58 % on HotpotQA .,3,0.9000301,56.69407220735814,26
1531,We further demonstrate that our approach can learn effectively from limited data .,3,0.9437163,26.882679921861527,13
1532,Automatic question generation ( QG ) has shown promise as a source of synthetic training data for question answering ( QA ) .,0,0.9616659,17.43160105922014,23
1532,"Using top-p nucleus sampling to derive samples from a transformer-based question generator , we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search .",3,0.8254806,89.9057279292574,36
1532,"We also show that standard QG evaluation metrics such as BLEU , ROUGE and METEOR are inversely correlated with diversity , and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA .",3,0.85425776,28.45763039862353,42
1533,"We address the problem of extractive question answering using document-level distant super-vision , pairing questions and relevant documents with answer strings .",1,0.34343693,89.55675604395184,23
1533,We compare previously used probability space and distant supervision assumptions ( assumptions on the correspondence between the weak answer string labels and possible answer mention spans ) .,2,0.8319555,334.8656096771097,28
1533,"We show that these assumptions interact , and that different configurations provide complementary benefits .",3,0.80875903,91.08478284706952,15
1533,We demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation .,3,0.9242235,38.79217776104616,21
1533,Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries .,3,0.85823786,14.810661737917773,29
1534,"We introduce SCDE , a dataset to evaluate the performance of computational models through sentence prediction .",2,0.36102742,57.903526298511,17
1534,"SCDE is a human created sentence cloze dataset , collected from public school English examinations .",2,0.52432346,212.22983829665856,16
1534,Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers .,2,0.7328167,82.53766526775138,25
1534,"Experimental results demonstrate that this task requires the use of non-local , discourse-level context beyond the immediate sentence neighborhood .",3,0.93219155,54.978399121195295,22
1534,The blanks require joint solving and significantly impair each other ’s context .,0,0.7864935,228.82230194746128,13
1534,"Furthermore , through ablations , we show that the distractors are of high quality and make the task more challenging .",3,0.9459356,32.296154512516125,21
1534,"Our experiments show that there is a significant performance gap between advanced models ( 72 % ) and humans ( 87 % ) , encouraging future models to bridge this gap .",3,0.9832507,35.29861496716388,32
1535,"To avoid giving wrong answers , question answering ( QA ) models need to know when to abstain from answering .",0,0.95287204,32.85545804871363,21
1535,"Moreover , users often ask questions that diverge from the model ’s training data , making errors more likely and thus abstention more critical .",3,0.49659106,56.931818293437296,25
1535,"In this work , we propose the setting of selective question answering under domain shift , in which a QA model is tested on a mixture of in-domain and out-of-domain data , and must answer ( i.e. , not abstain on ) as many questions as possible while maintaining high accuracy .",1,0.58455706,24.53132065145488,56
1535,"Abstention policies based solely on the model ’s softmax probabilities fare poorly , since models are overconfident on out-of-domain inputs .",3,0.7724431,66.95801624108304,22
1535,"Instead , we train a calibrator to identify inputs on which the QA model errs , and abstain when it predicts an error is likely .",2,0.7087654,62.411733821763235,26
1535,"Crucially , the calibrator benefits from observing the model ’s behavior on out-of-domain data , even if from a different domain than the test data .",3,0.5609575,34.09790139898019,29
1535,We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets .,2,0.69281834,25.090567522664998,23
1535,Our method answers 56 % of questions while maintaining 80 % accuracy ;,3,0.94043607,274.6970279131343,13
1535,"in contrast , directly using the model ’s probabilities only answers 48 % at 80 % accuracy .",3,0.9492742,360.0844944746781,18
1536,Large transformer-based language models have been shown to be very effective in many classification tasks .,0,0.88112676,10.79388148584798,18
1536,"However , their computational complexity prevents their use in applications requiring the classification of a large set of candidates .",0,0.8290593,44.081420893124914,20
1536,"While previous works have investigated approaches to reduce model size , relatively little attention has been paid to techniques to improve batch throughput during inference .",0,0.88918257,36.751687783863574,26
1536,"In this paper , we introduce the Cascade Transformer , a simple yet effective technique to adapt transformer-based models into a cascade of rankers .",1,0.8621178,28.68337022448826,27
1536,"Each ranker is used to prune a subset of candidates in a batch , thus dramatically increasing throughput at inference time .",2,0.4492948,40.90771725978049,22
1536,"Partial encodings from the transformer model are shared among rerankers , providing further speed-up .",3,0.67882454,70.20972107238077,17
1536,"When compared to a state-of-the-art transformer model , our approach reduces computation by 37 % with almost no impact on accuracy , as measured on two English Question Answering datasets .",3,0.8936148,23.717988721080317,36
1537,We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue .,1,0.42286494,52.36543351545657,15
1537,"First , three language modeling tasks are used to pre-train the transformers , token-and utterance-level language modeling and utterance order prediction , that learn both token and utterance embeddings for better understanding in dialogue contexts .",2,0.83168644,48.549278158737614,40
1537,"Then , multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering ( QA ) .",2,0.8258406,28.145938372468564,25
1537,"Our approach is evaluated on the FriendsQA dataset and shows improvements of 3.8 % and 1.4 % over the two state-of-the-art transformer models , BERT and RoBERTa , respectively .",3,0.75561386,12.939440946328661,36
1538,"Empirical research in Natural Language Processing ( NLP ) has adopted a narrow set of principles for assessing hypotheses , relying mainly on p-value computation , which suffers from several known issues .",0,0.9577117,42.534607997403484,33
1538,"While alternative proposals have been well-debated and adopted in other fields , they remain rarely discussed or used within the NLP community .",0,0.8698517,31.08868330419167,25
1538,"We address this gap by contrasting various hypothesis assessment techniques , especially those not commonly used in the field ( such as evaluations based on Bayesian inference ) .",1,0.6922405,64.03360940675374,29
1538,"Since these statistical techniques differ in the hypotheses they can support , we argue that practitioners should first decide their target hypothesis before choosing an assessment method .",0,0.5585199,64.10564813772251,28
1538,"This is crucial because common fallacies , misconceptions , and misinterpretation surrounding hypothesis assessment methods often stem from a discrepancy between what one would like to claim versus what the method used actually assesses .",0,0.8027771,56.68031365239423,35
1538,Our survey reveals that these issues are omnipresent in the NLP research community .,3,0.94671845,26.339865588729378,14
1538,"As a step forward , we provide best practices and guidelines tailored to NLP research , as well as an easy-to-use package for Bayesian assessment of hypotheses , complementing existing tools .",3,0.70869684,36.32858422319917,36
1539,"We present STARC ( Structured Annotations for Reading Comprehension ) , a new annotation framework for assessing reading comprehension with multiple choice questions .",1,0.49694067,58.37172366408147,24
1539,Our framework introduces a principled structure for the answer choices and ties them to textual span annotations .,2,0.6132141,118.27271332298267,18
1539,"The framework is implemented in OneStopQA , a new high-quality dataset for evaluation and analysis of reading comprehension in English .",2,0.58753717,39.957871227612664,21
1539,We use this dataset to demonstrate that STARC can be leveraged for a key new application for the development of SAT-like reading comprehension materials : automatic annotation quality probing via span ablation experiments .,3,0.73491526,93.44816722845967,36
1539,"We further show that it enables in-depth analyses and comparisons between machine and human reading comprehension behavior , including error distributions and guessing ability .",3,0.9149713,63.8807577052993,27
1539,"Our experiments also reveal that the standard multiple choice dataset in NLP , RACE , is limited in its ability to measure reading comprehension .",3,0.98075575,59.521044374245335,25
1539,"47 % of its questions can be guessed by machines without accessing the passage , and 18 % are unanimously judged by humans as not having a unique correct answer .",3,0.79669845,98.64014686739824,31
1539,OneStopQA provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance .,3,0.76885825,85.83287699583452,22
1540,"In this paper , we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge ( WSC ) .",1,0.9178618,25.319582587354493,24
1540,"For each of the questions , we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories .",2,0.79998034,48.14381680296694,27
1540,"By doing so , we better understand the limitation of existing methods ( i.e. , what kind of knowledge cannot be effectively represented or inferred with existing methods ) and shed some light on the commonsense knowledge that we need to acquire in the future for better commonsense reasoning .",3,0.58986807,26.234874287237883,50
1540,"Moreover , to investigate whether current WSC models can understand the commonsense or they simply solve the WSC questions based on the statistical bias of the dataset , we leverage the collected reasons to develop a new task called WinoWhy , which requires models to distinguish plausible reasons from very similar but wrong reasons for all WSC questions .",2,0.652978,65.17213458196508,59
1540,"Experimental results prove that even though pre-trained language representation models have achieved promising progress on the original WSC dataset , they are still struggling at WinoWhy .",3,0.9583823,55.92745927657956,27
1540,"Further experiments show that even though supervised models can achieve better performance , the performance of these models can be sensitive to the dataset distribution .",3,0.96048445,25.63516163614546,26
1540,WinoWhy and all codes are available at : https://github.com/HKUST-KnowComp/WinoWhy .,3,0.4942501,45.22631577497487,10
1541,"In online debates , users express different levels of agreement / disagreement with one another ’s arguments and ideas .",0,0.87630004,43.05567678992587,20
1541,"Often levels of agreement / disagreement are implicit in the text , and must be predicted to analyze collective opinions .",0,0.73687774,105.24716243835536,21
1541,"Existing stance detection methods predict the polarity of a post ’s stance toward a topic or post , but do n’t consider the stance ’s degree of intensity .",0,0.8478038,64.25523856563676,29
1541,"We introduce a new research problem , stance polarity and intensity prediction in response relationships between posts .",1,0.6700433,239.16513503572054,18
1541,This problem is challenging because differences in stance intensity are often subtle and require nuanced language understanding .,0,0.8711884,43.84128663682737,18
1541,Cyber argumentation research has shown that incorporating both stance polarity and intensity data in online debates leads to better discussion analysis .,0,0.89741653,81.89097849391223,22
1541,"We explore five different learning models : Ridge-M regression , Ridge-S regression , SVR-RF-R , pkudblab-PIP , and T-PAN-PIP for predicting stance polarity and intensity in argumentation .",2,0.7611898,101.87779957418672,37
1541,These models are evaluated using a new dataset for stance polarity and intensity prediction collected using a cyber argumentation platform .,2,0.719131,88.42876252904034,21
1541,The SVR-RF-R model performs best for prediction of stance polarity with an accuracy of 70.43 % and intensity with RMSE of 0.596 .,3,0.95625126,62.22467403821481,26
1541,This work is the first to train models for predicting a post ’s stance polarity and intensity in one combined value in cyber argumentation with reasonably good accuracy .,3,0.9376601,139.58569886988195,29
1542,Recent neural network models have achieved impressive performance on sentiment classification in English as well as other languages .,0,0.9130017,11.104930112285615,19
1542,Their success heavily depends on the availability of a large amount of labeled data or parallel corpus .,0,0.87966704,21.042918751373925,18
1542,"In this paper , we investigate an extreme scenario of cross-lingual sentiment classification , in which the low-resource language does not have any labels or parallel corpus .",1,0.8588771,26.73129637078291,28
1542,We propose an unsupervised cross-lingual sentiment classification model named multi-view encoder-classifier ( MVEC ) that leverages an unsupervised machine translation ( UMT ) system and a language discriminator .,1,0.42761546,29.266112392946507,29
1542,"Unlike previous language model ( LM ) based fine-tuning approaches that adjust parameters solely based on the classification error on training data , we employ the encoder-decoder framework of a UMT as a regularization component on the shared network parameters .",2,0.7206715,49.66104122786477,41
1542,"In particular , the cross-lingual encoder of our model learns a shared representation , which is effective for both reconstructing input sentences of two languages and generating more representative views from the input for classification .",3,0.7008544,37.0992029549023,36
1542,Extensive experiments on five language pairs verify that our model significantly outperforms other models for 8/11 sentiment classification tasks .,3,0.916992,19.429704870134188,20
1543,"We present an efficient annotation framework for argument quality , a feature difficult to be measured reliably as per previous work .",1,0.34640774,158.10692150176968,22
1543,A stochastic transitivity model is combined with an effective sampling strategy to infer high-quality labels with low effort from crowdsourced pairwise judgments .,2,0.73212314,49.073451026397294,23
1543,"The model ’s capabilities are showcased by compiling Webis-ArgQuality-20 , an argument quality corpus that comprises scores for rhetorical , logical , dialectical , and overall quality inferred from a total of 41,859 pairwise judgments among 1,271 arguments .",3,0.50791585,134.4773519305178,42
1543,"With up to 93 % cost savings , our approach significantly outperforms existing annotation procedures .",3,0.9345568,60.23279735349565,16
1543,"Furthermore , novel insight into argument quality is provided through statistical analysis , and a new aggregation method to infer overall quality from individual quality dimensions is proposed .",3,0.47824654,88.0631810611991,29
1544,This paper studies the task of comparative preference classification ( CPC ) .,1,0.80102426,135.51003495127523,13
1544,"Given two entities in a sentence , our goal is to classify whether the first ( or the second ) entity is preferred over the other or no comparison is expressed at all between the two entities .",2,0.48073816,35.28954385133781,38
1544,Existing works either do not learn entity-aware representations well and fail to deal with sentences involving multiple entity pairs or use sequential modeling approaches that are unable to capture long-range dependencies between the entities .,0,0.7934446,29.491359959628006,38
1544,Some also use traditional machine learning approaches that do not generalize well .,0,0.7519202,17.02273670792835,13
1544,This paper proposes a novel Entity-aware Dependency-based Deep Graph Attention Network ( ED-GAT ) that employs a multi-hop graph attention over a dependency graph sentence representation to leverage both the semantic information from word embeddings and the syntactic information from the dependency graph to solve the problem .,1,0.7960432,16.55322621612157,54
1544,Empirical evaluation shows that the proposed model achieves the state-of-the-art performance in comparative preference classification .,3,0.93644553,9.3186920928199,22
1545,"We present OpinionDigest , an abstractive opinion summarization framework , which does not rely on gold-standard summaries for training .",2,0.33104163,39.833114210454724,20
1545,"The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews , and trains a Transformer model to reconstruct the original reviews from these extractions .",2,0.82050157,29.82425785714311,30
1545,"At summarization time , we merge extractions from multiple reviews and select the most popular ones .",2,0.7246196,61.338246990772824,17
1545,"The selected opinions are used as input to the trained Transformer model , which verbalizes them into an opinion summary .",2,0.6739391,68.93587386681114,21
1545,"OpinionDigest can also generate customized summaries , tailored to specific user needs , by filtering the selected opinions according to their aspect and / or sentiment .",3,0.49396345,77.00678447095437,27
1545,Automatic evaluation on Yelp data shows that our framework outperforms competitive baselines .,3,0.92161864,17.741414327932993,13
1545,Human studies on two corpora verify that OpinionDigest produces informative summaries and shows promising customization capabilities .,3,0.78323555,142.64677851917395,17
1546,"Affective tasks such as sentiment analysis , emotion classification , and sarcasm detection have been popular in recent years due to an abundance of user-generated data , accurate computational linguistic models , and a broad range of relevant applications in various domains .",0,0.95266014,27.093013090115655,43
1546,"At the same time , many studies have highlighted the importance of text preprocessing , as an integral step to any natural language processing prediction model and downstream task .",0,0.91205394,50.37265033484099,30
1546,"While preprocessing in affective systems is well-studied , preprocessing in word vector-based models applied to affective systems , is not .",0,0.8392995,60.00509606010331,22
1546,"To address this limitation , we conduct a comprehensive analysis of the role of preprocessing techniques in affective analysis based on word vector models .",1,0.6690101,39.68019412861492,25
1546,"Our analysis is the first of its kind and provides useful insights of the importance of each preprocessing technique when applied at the training phase , commonly ignored in pretrained word vector models , and / or at the downstream task phase .",3,0.9340852,58.03017615825895,43
1547,"Generative dialogue systems tend to produce generic responses , which often leads to boring conversations .",0,0.9004892,47.73420704247191,16
1547,"For alleviating this issue , Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs .",0,0.91430396,108.08872352885403,18
1547,"While this paradigm works to a certain extent , it usually retrieves knowledge facts only based on the entity word itself , without considering the specific dialogue context .",0,0.7910144,60.06335108185693,29
1547,"Thus , the introduction of the context-irrelevant knowledge facts can impact the quality of generations .",0,0.57454705,101.60258263608091,16
1547,"To this end , this paper proposes a novel commonsense knowledge-aware dialogue generation model , ConKADI .",1,0.8264353,41.43969961968511,19
1547,We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context ;,2,0.78179944,89.32696227027434,23
1547,"furthermore , two techniques , Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI .",2,0.5298885,110.7787301842278,25
1547,We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation .,2,0.8625823,35.928729738096585,17
1547,"Extensive evaluations over both an open-released English dataset and our Chinese dataset demonstrate that our approach ConKADI outperforms the state-of-the-art approach CCM , in most experiments .",3,0.8854058,43.36552473129121,32
1548,"Maintaining a consistent personality in conversations is quite natural for human beings , but is still a non-trivial task for machines .",0,0.89840657,23.823498289600263,22
1548,The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models .,2,0.38208556,34.08874060077966,26
1548,"Despite the success of existing persona-based models on generating human-like responses , their one-stage decoding framework can hardly avoid the generation of inconsistent persona words .",0,0.7799002,54.14659906373828,27
1548,"In this work , we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one .",1,0.6099746,58.1774337523854,38
1548,We carry out evaluations by both human and automatic metrics .,2,0.77766985,37.262231068986885,11
1548,Experiments on the Persona-Chat dataset show that our approach achieves good performance .,3,0.90355515,14.159129785980008,15
1549,Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems .,0,0.799885,22.69399502711355,19
1549,Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task .,0,0.8318561,22.618334177862714,23
1549,"However , fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective , resulting in similar dialogue models for different tasks .",3,0.60198087,89.27871583216016,24
1549,"In this paper , we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting .",1,0.89333504,26.20934803272429,23
1549,"In our approach , each dialogue model consists of a shared module , a gating module , and a private module .",2,0.729889,45.372615620903765,22
1549,"The first two modules are shared among all the tasks , while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task .",3,0.45844623,40.47049564155739,31
1549,"The extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency , response quality , and diversity .",3,0.9273279,17.9575518130889,26
1550,Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses .,0,0.9222932,10.474936812584366,28
1550,"In this paper , we leverage the power of pre-trained language models for improving video-grounded dialogue , which is very challenging and involves complex features of different dynamics : ( 1 ) Video features which can extend across both spatial and temporal dimensions ;",1,0.5469774,51.75572792045447,44
1550,and ( 2 ) Dialogue features which involve semantic dependencies over multiple dialogue turns .,2,0.447036,239.9155712621009,15
1550,"We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task , combining both visual and textual representation into a structured sequence , and fine-tuning a large pre-trained GPT-2 network .",2,0.54990757,21.091705561182476,48
1550,Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information : spatio-temporal level in video and token-sentence level in dialogue context .,3,0.5663531,38.56857884196752,31
1550,"We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues ( AVSD ) benchmark from DSTC7 , which supports a potential direction in this line of research .",3,0.9386631,76.11532629864395,27
1551,The task of named entity recognition ( NER ) is normally divided into nested NER and flat NER depending on whether named entities are nested or not .,0,0.9539817,22.123066519162798,28
1551,"Models are usually separately developed for the two tasks , since sequence labeling models , the most widely used backbone for flat NER , are only able to assign a single label to a particular token , which is unsuitable for nested NER where a token may be assigned several labels .",0,0.82443136,45.83358933076427,52
1551,"In this paper , we propose a unified framework that is capable of handling both flat and nested NER tasks .",1,0.85342205,17.62921764529731,21
1551,"Instead of treating the task of NER as a sequence labeling problem , we propose to formulate it as a machine reading comprehension ( MRC ) task .",2,0.45399716,12.915754403736896,28
1551,"For example , extracting entities with the per label is formalized as extracting answer spans to the question “ which person is mentioned in the text "" .",3,0.40853757,128.80599551324815,28
1551,This formulation naturally tackles the entity overlapping issue in nested NER : the extraction of two overlapping entities with different categories requires answering two independent questions .,2,0.39636227,182.78726711490378,27
1551,"Additionally , since the query encodes informative prior knowledge , this strategy facilitates the process of entity extraction , leading to better performances for not only nested NER , but flat NER .",3,0.74778134,112.07904998962668,33
1551,We conduct experiments on both nested and flat NER datasets .,2,0.7634927,41.84294467880119,11
1551,Experiment results demonstrate the effectiveness of the proposed formulation .,3,0.96014166,10.51167820200436,10
1551,"We are able to achieve a vast amount of performance boost over current SOTA models on nested NER datasets , i.e. , + 1.28 , + 2.55 , + 5.44 , + 6.37 , respectively on ACE04 , ACE05 , GENIA and KBP17 , along with SOTA results on flat NER datasets , i.e. , +0.24 , +1.95 , +0.21 , +1.49 respectively on English CoNLL 2003 , English OntoNotes 5.0 , Chinese MSRA and Chinese OntoNotes 4.0 .",3,0.60755366,21.942327413605565,81
1552,"Unlike widely used Named Entity Recognition ( NER ) data sets in generic domains , biomedical NER data sets often contain mentions consisting of discontinuous spans .",0,0.91436476,64.59454218760926,27
1552,Conventional sequence tagging techniques encode Markov assumptions that are efficient but preclude recovery of these mentions .,0,0.8522834,356.7509795145559,17
1552,"We propose a simple , effective transition-based model with generic neural encoding for discontinuous NER .",3,0.44050828,105.68009964300396,18
1552,"Through extensive experiments on three biomedical data sets , we show that our model can effectively recognize discontinuous mentions without sacrificing the accuracy on continuous mentions .",3,0.86256063,30.86378683346271,27
1553,"While traditional systems for Open Information Extraction were statistical and rule-based , recently neural models have been introduced for the task .",0,0.91700804,44.41051843874439,22
1553,"Our work builds upon CopyAttention , a sequence generation OpenIE model ( Cui et .",2,0.4523895,328.4183189382733,15
1553,al .,4,0.9227275,207.69517318273193,2
1553,18 ) .,4,0.5686902,113.81711599780589,3
1553,"Our analysis reveals that CopyAttention produces a constant number of extractions per sentence , and its extracted tuples often express redundant information .",3,0.9712809,73.18714359369707,23
1553,"We present IMoJIE , an extension to CopyAttention , which produces the next extraction conditioned on all previously extracted tuples .",2,0.40288326,122.21972691451852,21
1553,"This approach overcomes both shortcomings of CopyAttention , resulting in a variable number of diverse extractions per sentence .",3,0.53698635,89.78842538707313,19
1553,"We train IMoJIE on training data bootstrapped from extractions of several non-neural systems , which have been automatically filtered to reduce redundancy and noise .",2,0.8272431,83.32761750704405,25
1553,"IMoJIE outperforms CopyAttention by about 18 F1 pts , and a BERT-based strong baseline by 2 F1 pts , establishing a new state of the art for the task .",3,0.9186554,29.513045389856966,32
1554,Event Detection ( ED ) is a fundamental task in automatically structuring texts .,0,0.95541763,106.60529544479895,14
1554,"Due to the small scale of training data , previous methods perform poorly on unseen / sparsely labeled trigger words and are prone to overfitting densely labeled trigger words .",0,0.8746778,37.35684128659219,30
1554,"To address the issue , we propose a novel Enrichment Knowledge Distillation ( EKD ) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations .",1,0.47490215,35.43862862123925,35
1554,"Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines , is especially effective for unseen / sparsely labeled trigger words .",3,0.9289974,70.20908498178862,24
1554,The source code is released on https://github.com/shuaiwa16/ekd.git .,3,0.49313587,11.843990319202826,8
1555,"Exploiting sentence-level labels , which are easy to obtain , is one of the plausible methods to improve low-resource named entity recognition ( NER ) , where token-level labels are costly to annotate .",0,0.8404353,35.64487219245744,38
1555,Current models for jointly learning sentence and token labeling are limited to binary classification .,0,0.8176048,96.07556354737018,15
1555,We present a joint model that supports multi-class classification and introduce a simple variant of self-attention that allows the model to learn scaling factors .,2,0.3989212,29.3807828065864,25
1555,"Vietnamese , Thai , and Indonesian , respectively .",2,0.4975713,48.519458416739184,9
1556,Cross-domain NER is a challenging yet practical problem .,0,0.9063956,20.059474231971784,9
1556,Entity mentions can be highly different across domains .,0,0.8151237,84.67774252489947,9
1556,"However , the correlations between entity types can be relatively more stable across domains .",3,0.78793335,71.20275702411094,15
1556,"We investigate a multi-cell compositional LSTM structure for multi-task learning , modeling each entity type using a separate cell state .",2,0.60855937,64.83817500377154,21
1556,"With the help of entity typed units , cross-domain knowledge transfer can be made in an entity type level .",3,0.45953146,88.3694968336227,20
1556,"Theoretically , the resulting distinct feature distributions for each entity type make it more powerful for cross-domain transfer .",3,0.8037561,68.0268199587268,19
1556,"Empirically , experiments on four few-shot and zero-shot datasets show our method significantly outperforms a series of multi-task learning methods and achieves the best results .",3,0.87976456,16.719620636040347,26
1557,"This paper presents Pyramid , a novel layered model for Nested Named Entity Recognition ( nested NER ) .",1,0.81082433,50.29704554262889,19
1557,"In our approach , token or text region embeddings are recursively inputted into L flat NER layers , from bottom to top , stacked in a pyramid shape .",2,0.7449607,144.9115176387063,29
1557,"Each time an embedding passes through a layer of the pyramid , its length is reduced by one .",3,0.3881125,39.610012087993766,19
1557,"Its hidden state at layer l represents an l-gram in the input text , which is labeled only if its corresponding text region represents a complete entity mention .",2,0.5327144,111.85166528204414,29
1557,We also design an inverse pyramid to allow bidirectional interaction between layers .,2,0.6514518,57.57539698226088,13
1557,"The proposed method achieves state-of-the-art F1 scores in nested NER on ACE-2004 , ACE-2005 , GENIA , and NNE , which are 80.27 , 79.42 , 77.78 , and 93.70 with conventional embeddings , and 87.74 , 86.34 , 79.31 , and 94.68 with pre-trained contextualized embeddings .",3,0.8084714,19.979983540112112,57
1557,"In addition , our model can be used for the more general task of Overlapping Named Entity Recognition .",3,0.901107,19.08199103057806,19
1557,A preliminary experiment confirms the effectiveness of our method in overlapping NER .,3,0.94908595,47.68731875091467,13
1558,The goal of Knowledge graph embedding ( KGE ) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples .,0,0.8269636,39.87213730484869,29
1558,The conventional shallow models are limited to their expressiveness .,0,0.83467674,98.44673948352253,10
1558,"ConvE ( Dettmers et al. , 2018 ) takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings .",3,0.48437697,114.19123099658792,31
1558,"However , there is no structural information in the embedding space of ConvE , and the performance is still limited by the number of interactions .",0,0.6458634,26.960492632439205,26
1558,"The recent KBGAT ( Nathani et al. , 2019 ) provides another way to learn embeddings by adaptively utilizing structural information .",0,0.86891747,85.85170608116043,22
1558,"In this paper , we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding ( ReInceptionE ) .",1,0.8164068,62.59255058290986,34
1558,"Specifically , we first explore the Inception network to learn query embedding , which aims to further increase the interactions between head and relation embeddings .",2,0.7200854,59.89046892075251,26
1558,"Then , we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information .",2,0.5455382,26.020784840619715,26
1558,Experimental results on both WN18 RR and FB15k-237 datasets demonstrate that ReInceptionE achieves competitive performance compared with state-of-the-art methods .,3,0.95522934,29.47706881461909,27
1559,Distant supervision based methods for entity and relation extraction have received increasing popularity due to the fact that these methods require light human annotation efforts .,0,0.9480439,51.23880788594636,26
1559,"In this paper , we consider the problem of shifted label distribution , which is caused by the inconsistency between the noisy-labeled training set subject to external knowledge graph and the human-annotated test set , and exacerbated by the pipelined entity-then-relation extraction manner with noise propagation .",1,0.6869414,58.3632628000566,53
1559,We propose a joint extraction approach to address this problem by re-labeling noisy instances with a group of cooperative multiagents .,2,0.50770915,64.69305748086622,21
1559,"To handle noisy instances in a fine-grained manner , each agent in the cooperative group evaluates the instance by calculating a continuous confidence score from its own perspective ;",2,0.5777087,102.29716948272028,29
1559,"To leverage the correlations between these two extraction tasks , a confidence consensus module is designed to gather the wisdom of all agents and re-distribute the noisy training set with confidence-scored labels .",2,0.6309697,88.48578950325667,34
1559,"Further , the confidences are used to adjust the training losses of extractors .",2,0.5048186,101.40070732058285,14
1559,"Experimental results on two real-world datasets verify the benefits of re-labeling noisy instance , and show that the proposed model significantly outperforms the state-of-the-art entity and relation extraction methods .",3,0.9213565,11.385337198156503,36
1560,"Recently , many works have tried to augment the performance of Chinese named entity recognition ( NER ) using word lexicons .",0,0.94462305,51.418886203959,22
1560,"As a representative , Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets .",3,0.5238486,47.6949142103143,18
1560,"However , Lattice-LSTM has a complex model architecture .",0,0.6932283,38.918313721003045,10
1560,This limits its application in many industrial areas where real-time NER responses are needed .,0,0.6097714,52.9104566931556,16
1560,"In this work , we propose a simple but effective method for incorporating the word lexicon into the character representations .",1,0.7597909,17.16201198358785,21
1560,"This method avoids designing a complicated sequence modeling architecture , and for any neural NER model , it requires only subtle adjustment of the character representation layer to introduce the lexicon information .",2,0.4665054,120.45071825486148,33
1560,"Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-of-the-art methods , along with a better performance .",3,0.9200191,15.731083556299655,39
1560,The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT .,3,0.98020935,10.088083497732491,19
1561,"In this paper , we propose a new adversarial augmentation method for Neural Machine Translation ( NMT ) .",1,0.8954927,16.27744452983828,19
1561,"The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions , in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs .",2,0.7007431,106.93923342906595,45
1561,"We then discuss our approach , AdvAug , to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning .",1,0.33006766,49.7011124944944,24
1561,"Experiments on Chinese-English , English-French , and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer ( up to 4.9 BLEU points ) , and substantially outperforms other data augmentation techniques ( e.g.back-translation ) without using extra corpora .",3,0.8917677,27.562103686105438,47
1562,The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns .,0,0.92720664,30.321004108517528,28
1562,Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation .,0,0.90762836,33.562164050957435,20
1562,"In this work , we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context .",1,0.78217846,18.695025367888466,36
1562,"Our experiments and evaluation , using generic and pronoun-focused automatic metrics , show that the use of future context not only achieves significant improvements over the context-agnostic Transformer , but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context .",3,0.9229506,42.194898213279075,47
1562,We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU .,3,0.7963764,31.70735556748072,24
1563,"Although neural machine translation ( NMT ) has achieved significant progress in recent years , most previous NMT models only depend on the source text to generate translation .",0,0.9457875,13.258057163042384,29
1563,"Inspired by the success of template-based and syntax-based approaches in other fields , we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure .",2,0.5747775,32.88396030434038,36
1563,"In order to learn the syntactic structure of the target sentences , we adopt constituency-based parse tree to generate candidate templates .",2,0.74172443,36.19190510091001,24
1563,We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text .,2,0.80000716,21.583342257987354,18
1563,Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrates the effectiveness of soft target templates .,3,0.93856335,13.851395984522764,22
1564,"In this paper , we show that neural machine translation ( NMT ) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts .",1,0.8916125,23.939965732133935,32
1564,"Such NMT systems better translate human-produced translations , i.e. , translationese , but may largely worsen the translation quality of original texts .",3,0.5896896,111.35488923861526,23
1564,Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training .,3,0.9689947,34.39919789129608,42
1564,"We also show that , in contrast to high-resource configurations , NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations .",3,0.9731569,29.245312629497395,28
1564,We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown .,3,0.98840135,28.060154646471123,28
1565,Speech translation ( ST ) aims to learn transformations from speech in the source language to the text in the target language .,0,0.92806625,24.75561480998295,23
1565,"Previous works show that multitask learning improves the ST performance , in which the recognition decoder generates the text of the source language , and the translation decoder obtains the final translations based on the output of the recognition decoder .",0,0.8891315,27.622303460547233,41
1565,"Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy , we propose to improve the multitask ST model by utilizing word embedding as the intermediate .",2,0.4204075,44.15809364498058,35
1566,Measuring the scholarly impact of a document without citations is an important and challenging problem .,0,0.94788235,34.97323562958029,16
1566,"Existing approaches such as Document Influence Model ( DIM ) are based on dynamic topic models , which only consider the word frequency change .",0,0.79811543,72.86931846106721,25
1566,"In this paper , we use both frequency changes and word semantic shifts to measure document influence by developing a neural network framework .",1,0.46832785,84.87489271463195,24
1566,Our model has three steps .,2,0.62297267,70.40386359313607,6
1566,"Firstly , we train the word embeddings for different time periods .",2,0.8657872,19.51652884738596,12
1566,"Subsequently , we propose an unsupervised method to align vectors for different time periods .",2,0.55655545,26.31618840387657,15
1566,"Finally , we compute the influence value of documents .",2,0.7434353,213.2689559870938,10
1566,Our experimental results show that our model outperforms DIM .,3,0.97445786,32.03978942603,10
1567,Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation .,0,0.8951109,30.02681416294906,18
1567,"Despite promising results , paraphrases generated by these models mostly suffer from lack of quality and diversity .",0,0.7720054,52.92119300764144,18
1567,"To address these problems , we propose a novel retrieval-based method for paraphrase generation .",1,0.58418494,13.902767846178325,17
1567,Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index .,2,0.7480762,35.31886930047984,17
1567,"With its novel editor module , the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences .",2,0.62290215,60.636789153638134,27
1567,"In order to have fine-grained control over the editing process , our model uses the newly introduced concept of Micro Edit Vectors .",2,0.62546307,39.03524478883366,24
1567,It both extracts and exploits these vectors using the attention mechanism in the Transformer architecture .,2,0.45500436,38.94122994354637,16
1567,"Experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics , and human evaluation of relevance , grammaticality , and diversity of generated paraphrases .",3,0.97303885,24.059021585140368,31
1568,"We study the problem of multilingual masked language modeling , i.e .",1,0.6968268,24.03744625388894,12
1568,"the training of a single model on concatenated text from multiple languages , and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer .",1,0.46813303,24.118457963965295,33
1568,"We show , contrary to what was previously hypothesized , that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains .",3,0.95425266,32.90274281863815,36
1568,The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder .,3,0.48715872,21.130345893228387,19
1568,"To better understand this result , we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively , strongly suggesting that , much like for non-contextual word embeddings , there are universal latent symmetries in the learned embedding spaces .",3,0.955296,38.46934970197451,47
1568,"For multilingual masked language modeling , these symmetries are automatically discovered and aligned during the joint training process .",2,0.3960022,54.52658928016672,19
1569,Pre-trained language models like BERT have proven to be highly performant .,0,0.73432636,5.773983929658328,12
1569,"However , they are often computationally expensive in many practical scenarios , for such heavy models can hardly be readily implemented with limited resources .",0,0.8202555,67.32629392125978,25
1569,"To improve their efficiency with an assured model performance , we propose a novel speed-tunable FastBERT with adaptive inference time .",2,0.50483125,76.90777843003984,21
1569,"The speed at inference can be flexibly adjusted under varying demands , while redundant calculation of samples is avoided .",3,0.7588656,128.3478857129393,20
1569,"Moreover , this model adopts a unique self-distillation mechanism at fine-tuning , further enabling a greater computational efficacy with minimal loss in performance .",3,0.82838297,40.794542386206786,24
1569,Our model achieves promising results in twelve English and Chinese datasets .,3,0.87987447,32.36403968739743,12
1569,It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff .,3,0.80368096,58.85925345335511,28
1570,Open-domain code generation aims to generate code in a general-purpose programming language ( such as Python ) from natural language ( NL ) intents .,0,0.86754745,30.78821939598099,27
1570,"Motivated by the intuition that developers usually retrieve resources on the web when writing code , we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation : automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation .",2,0.604056,56.609951298532806,53
1570,Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2 % absolute BLEU score on the code generation testbed CoNaLa .,3,0.9696424,18.931124045812755,42
1570,The code and resources are available at https://github.com/neulab/external-knowledge-codegen .,3,0.57263726,11.174450965598913,9
1571,"Verifying the correctness of a textual statement requires not only semantic reasoning about the meaning of words , but also symbolic reasoning about logical operations like count , superlative , aggregation , etc .",0,0.8574882,50.102950799814295,34
1571,"In this work , we propose LogicalFactChecker , a neural network approach capable of leveraging logical operations for fact checking .",1,0.76549584,48.069368258036974,21
1571,"It achieves the state-of-the-art performance on TABFACT , a large-scale , benchmark dataset built for verifying a textual statement with semi-structured tables .",3,0.6065194,35.3174967571847,27
1571,This is achieved by a graph module network built upon the Transformer-based architecture .,2,0.5750602,32.20780254719351,16
1571,"With a textual statement and a table as the input , LogicalFactChecker automatically derives a program ( a.k.a .",2,0.5439168,83.11860266671667,19
1571,logical form ) of the statement in a semantic parsing manner .,2,0.41484377,105.61748069962815,12
1571,"A heterogeneous graph is then constructed to capture not only the structures of the table and the program , but also the connections between inputs with different modalities .",2,0.5832739,32.469537231669804,29
1571,"Such a graph reveals the related contexts of each word in the statement , the table and the program .",2,0.457507,120.16359694082365,20
1571,The graph is used to obtain graph-enhanced contextual representations of words in Transformer-based architecture .,2,0.66375166,25.41994726296732,17
1571,"After that , a program-driven module network is further introduced to exploit the hierarchical structure of the program , where semantic compositionality is dynamically modeled along the program structure with a set of function-specific modules .",2,0.66495055,53.52458937900136,37
1571,Ablation experiments suggest that both the heterogeneous graph and the module network are important to obtain strong results .,3,0.9485344,54.85252435913378,19
1572,Adversarial attacks are carried out to reveal the vulnerability of deep neural networks .,2,0.5368234,21.52201359952275,14
1572,Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input .,0,0.8835101,35.68801065721604,22
1572,"Word-level attacking , which can be regarded as a combinatorial optimization problem , is a well-studied class of textual attack methods .",0,0.9247602,27.365217320486572,26
1572,"However , existing word-level attack models are far from perfect , largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed .",0,0.87831444,91.62738923059989,26
1572,"In this paper , we propose a novel attack model , which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately .",1,0.84351444,61.10863949607111,34
1572,We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets .,2,0.70750654,30.758710122028386,19
1572,Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods .,3,0.96775925,26.788487165371308,25
1572,"Also , further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training .",3,0.9702829,73.14653318601114,23
1572,All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack .,3,0.52494633,13.567247615555731,14
1573,Existing datasets for regular expression ( regex ) generation from natural language are limited in complexity ;,0,0.9159276,120.46909897818651,17
1573,"compared to regex tasks that users post on StackOverflow , the regexes in these datasets are simple , and the language used to describe them is not diverse .",3,0.71017975,52.605454636561035,29
1573,"We introduce StructuredRegex , a new regex synthesis dataset differing from prior ones in three aspects .",2,0.62987053,102.90308592965484,17
1573,"First , to obtain structurally complex and realistic regexes , we generate the regexes using a probabilistic grammar with pre-defined macros observed from real-world StackOverflow posts .",2,0.8904795,48.23176403646973,28
1573,"Second , to obtain linguistically diverse natural language descriptions , we show crowdworkers abstract depictions of the underlying regex and ask them to describe the pattern they see , rather than having them paraphrase synthetic language .",2,0.82956296,113.3662493342818,37
1573,"Third , we augment each regex example with a collection of strings that are and are not matched by the ground truth regex , similar to how real users give examples .",2,0.8266662,61.04517887274019,32
1573,Our quantitative and qualitative analysis demonstrates the advantages of StructuredRegex over prior datasets .,3,0.98268396,52.97499567875765,14
1573,"Further experimental results using various multimodal synthesis techniques highlight the challenge presented by our dataset , including non-local constraints and multi-modal inputs .",3,0.96547705,43.04967202382232,23
1574,"With the great success of pre-trained language models , the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding ( NLU ) tasks .",0,0.9471904,25.854910237010692,27
1574,"At the fine-tune stage , target task data is usually introduced in a completely random order and treated equally .",0,0.5508988,45.15505457649791,21
1574,"However , examples in NLU tasks can vary greatly in difficulty , and similar to human learning procedure , language models can benefit from an easy-to-difficult curriculum .",0,0.80300325,67.15988183097791,31
1574,"Based on this idea , we propose our Curriculum Learning approach .",2,0.5006384,38.942232662228776,12
1574,"By reviewing the trainset in a crossed way , we are able to distinguish easy examples from difficult ones , and arrange a curriculum for language models .",3,0.3943699,150.40897984504463,28
1574,"Without any manual model architecture design or use of external data , our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks .",3,0.8847037,37.769689650339686,30
1575,"Despite the success of language models using neural networks , it remains unclear to what extent neural models have the generalization ability to perform inferences .",0,0.92952305,16.99961065473998,26
1575,"In this paper , we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language , namely , the regularity for performing arbitrary inferences with generalization on composition .",1,0.86898935,63.6867136304278,36
1575,We consider four aspects of monotonicity inferences and test whether the models can systematically interpret lexical and logical phenomena on different training / test splits .,2,0.45734957,86.75590849854848,26
1575,A series of experiments show that three neural models systematically draw inferences on unseen combinations of lexical and logical phenomena when the syntactic structures of the sentences are similar between the training and test sets .,3,0.8066508,29.187719742190556,36
1575,"However , the performance of the models significantly decreases when the structures are slightly changed in the test set while retaining all vocabularies and constituents already appearing in the training set .",3,0.9594181,39.154366156141414,32
1575,This indicates that the generalization ability of neural models is limited to cases where the syntactic structures are nearly the same as those in the training set .,3,0.95574594,13.477890677509754,28
1576,Generating inferential texts about an event in different perspectives requires reasoning over different contexts that the event occurs .,0,0.85668755,66.68154871491666,19
1576,"Existing works usually ignore the context that is not explicitly provided , resulting in a context-independent semantic representation that struggles to support the generation .",0,0.86514235,43.979708677583204,25
1576,"To address this , we propose an approach that automatically finds evidence for an event from a large text corpus , and leverages the evidence to guide the generation of inferential texts .",1,0.53746957,24.666675371260247,33
1576,"Our approach works in an encoderdecoder manner and is equipped with Vector Quantised-Variational Autoencoder , where the encoder outputs representations from a distribution over discrete variables .",2,0.6676845,40.04564169370486,28
1576,"Such discrete representations enable automatically selecting relevant evidence , which not only facilitates evidence-aware generation , but also provides a natural way to uncover rationales behind the generation .",0,0.5239621,62.30269839981136,31
1576,Our approach provides state-of-the-art performance on both Event2mind and Atomic datasets .,3,0.8880628,34.48177841560017,16
1576,"More importantly , we find that with discrete representations , our model selectively uses evidence to generate different inferential texts .",3,0.9761865,125.27437411792687,21
1577,"Given a sentence and its relevant answer , how to ask good questions is a challenging task , which has many real applications .",0,0.9109611,70.85698213201715,24
1577,"Inspired by human ’s paraphrasing capability to ask questions of the same meaning but with diverse expressions , we propose to incorporate paraphrase knowledge into question generation ( QG ) to generate human-like questions .",0,0.4083012,32.937779867159875,35
1577,"Specifically , we present a two-hand hybrid model leveraging a self-built paraphrase resource , which is automatically conducted by a simple back-translation method .",2,0.7413353,55.72456249723487,27
1577,"On the one hand , we conduct multi-task learning with sentence-level paraphrase generation ( PG ) as an auxiliary task to supplement paraphrase knowledge to the task-share encoder .",2,0.8257687,39.30857730573745,33
1577,"On the other hand , we adopt a new loss function for diversity training to introduce more question patterns to QG .",2,0.6273697,96.20993356449249,22
1577,"Extensive experimental results show that our proposed model obtains obvious performance gain over several strong baselines , and further human evaluation validates that our model can ask questions of high quality by leveraging paraphrase knowledge .",3,0.95072955,28.32988826670329,36
1578,"Knowledge inference on knowledge graph has attracted extensive attention , which aims to find out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications .",0,0.9517247,48.580482942315705,34
1578,"However , researchers have mainly poured attention to knowledge inference on binary facts .",0,0.9542747,355.39708103860926,14
1578,"The studies on n-ary facts are relatively scarcer , although they are also ubiquitous in the real world .",0,0.87174,65.68181393426148,19
1578,"Therefore , this paper addresses knowledge inference on n-ary facts .",1,0.8143767,124.4696449575566,11
1578,We represent each n-ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute-value pair ( s ) .,2,0.8297144,159.97406849525365,25
1578,"We further propose a neural network model , NeuInfer , for knowledge inference on n-ary facts .",2,0.43260682,74.95831817547887,17
1578,"Besides handling the common task to infer an unknown element in a whole fact , NeuInfer can cope with a new type of task , flexible knowledge inference .",3,0.49768436,147.77119139735905,29
1578,It aims to infer an unknown element in a partial fact consisting of the primary triple coupled with any number of its auxiliary description ( s ) .,0,0.65117455,200.36875709673248,28
1578,Experimental results demonstrate the remarkable superiority of NeuInfer .,3,0.96243006,44.47192000248784,9
1579,Chinese short text matching usually employs word sequences rather than character sequences to get better performance .,0,0.8343798,81.0507353712686,17
1579,"However , Chinese word segmentation can be erroneous , ambiguous or inconsistent , which consequently hurts the final matching performance .",0,0.87231004,181.06116386472684,21
1579,"To address this problem , we propose neural graph matching networks , a novel sentence matching framework capable of dealing with multi-granular input information .",1,0.44707477,54.55056683074826,25
1579,"Instead of a character sequence or a single word sequence , paired word lattices formed from multiple word segmentation hypotheses are used as input and the model learns a graph representation according to an attentive graph matching mechanism .",2,0.64608186,61.4803829359778,39
1579,Experiments on two Chinese datasets show that our models outperform the state-of-the-art short text matching models .,3,0.9268991,9.723386732410297,23
1580,Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables ;,0,0.5253035,162.84645610551632,22
1580,thus they have attracted much attention in mining dispersed document topics .,0,0.9451457,189.17814413540768,12
1580,"However , the existing parameter inference method like Monte Carlo sampling is quite time-consuming .",0,0.84344983,76.53535715275852,17
1580,"In this paper , we propose two efficient neural mixed counting models , i.e. , the Negative Binomial-Neural Topic Model ( NB-NTM ) and the Gamma Negative Binomial-Neural Topic Model ( GNB-NTM ) for dispersed topic discovery .",1,0.69565153,29.83097107295583,46
1580,Neural variational inference algorithms are developed to infer model parameters by using the reparameterization of Gamma distribution and the Gaussian approximation of Poisson distribution .,2,0.7378557,31.85438991107505,25
1580,Experiments on real-world datasets indicate that our models outperform state-of-the-art baseline models in terms of perplexity and topic coherence .,3,0.92566484,7.021600471644235,26
1580,The results also validate that both NB-NTM and GNB-NTM can produce explainable intermediate variables by generating dispersed proportions of document topics .,3,0.9904535,122.48068666731825,26
1581,Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence .,0,0.9201415,17.79636930518657,20
1581,"In this work , we present a method suitable for reasoning about the semantic-level structure of evidence .",1,0.87609565,31.841549898434227,20
1581,"Unlike most previous works , which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences , our approach operates on rich semantic structures of evidence obtained by semantic role labeling .",2,0.5203342,66.61385710505967,38
1581,"We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT , GPT or XLNet .",2,0.48835912,40.44383481758093,24
1581,"Specifically , using XLNet as the backbone , we first utilize the graph structure to re-define the relative distances of words , with the intuition that semantically related words should have short distances .",2,0.81334054,53.00430590202695,34
1581,"Then , we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph .",2,0.8885751,26.020617337518033,23
1581,"We evaluate our system on FEVER , a benchmark dataset for fact checking , and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy .",3,0.8253547,46.708445737672776,33
1581,"Our model is the state-of-the-art system in terms of both official evaluation metrics , namely claim verification accuracy and FEVER score .",2,0.4528967,24.66858088681416,28
1582,"In this paper , we study the challenging problem of automatic generation of citation texts in scholarly papers .",1,0.9008581,32.83628759888688,19
1582,"Given the context of a citing paper A and a cited paper B , the task aims to generate a short text to describe B in the given context of A .",2,0.4640281,41.28479491542707,32
1582,One big challenge for addressing this task is the lack of training data .,0,0.94478023,13.55662583900549,14
1582,"Usually , explicit citation texts are easy to extract , but it is not easy to extract implicit citation texts from scholarly papers .",0,0.9053194,46.69814592429049,24
1582,We thus first train an implicit citation extraction model based on BERT and leverage the model to construct a large training dataset for the citation text generation task .,2,0.8434478,39.38599031186562,29
1582,Then we propose and train a multi-source pointer-generator network with cross attention mechanism for citation text generation .,2,0.5835175,56.99249746627265,20
1582,Empirical evaluation results on a manually labeled test dataset verify the efficacy of our model .,3,0.899434,22.259452553098892,16
1582,This pilot study confirms the feasibility of automatically generating citation texts in scholarly papers and the technique has the great potential to help researchers prepare their scientific papers .,3,0.98450744,45.73205362166508,29
1583,"In this paper , we argue that elementary discourse unit ( EDU ) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization .",1,0.84654087,45.831513137733666,30
1583,"To well handle the problem of composing EDUs into an informative and fluent summary , we propose a novel summarization method that first designs an EDU selection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence .",2,0.6549603,30.986638017306852,51
1583,"We also design the reinforcement learning mechanism to use EDU fusion results to reward the EDU selection action , boosting the final summarization performance .",2,0.72916156,165.6691130332633,25
1583,Experiments on CNN / Daily Mail have demonstrated the effectiveness of our model .,3,0.87975127,19.888755161848717,14
1584,This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems .,1,0.741868,24.940604617627542,18
1584,"Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences , we formulate the extractive summarization task as a semantic text matching problem , in which a source document and candidate summaries will be ( extracted from the original text ) matched in a semantic space .",2,0.6497107,39.38861030716932,54
1584,"Notably , this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset .",0,0.49108955,46.30170808430208,38
1584,"Besides , even instantiating the framework with a simple form of a matching model , we have driven the state-of-the-art extractive result on CNN / DailyMail to a new level ( 44.41 in ROUGE-1 ) .",3,0.8540784,37.71691225889903,43
1584,Experiments on the other five datasets also show the effectiveness of the matching framework .,3,0.94553363,26.13964807868342,15
1584,We believe the power of this matching-based summarization framework has not been fully exploited .,3,0.93487126,30.433117194529448,17
1584,"To encourage more instantiations in the future , we have released our codes , processed dataset , as well as generated summaries in https://github.com/maszhongming/MatchSum .",3,0.5965064,59.888612679453985,25
1585,"As a crucial step in extractive document summarization , learning cross-sentence relations has been explored by a plethora of approaches .",0,0.9378276,28.539892978587446,21
1585,"An intuitive way is to put them in the graph-based neural network , which has a more complex structure for capturing inter-sentence relationships .",0,0.3306641,28.430646370249928,25
1585,"In this paper , we present a heterogeneous graph-based neural network for extractive summarization ( HETERSUMGRAPH ) , which contains semantic nodes of different granularity levels apart from sentences .",1,0.8149423,60.77489428188807,32
1585,These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations .,3,0.585444,39.062448521636966,15
1585,"Besides , our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes .",3,0.5339731,92.95153851893407,21
1585,"To our knowledge , we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits .",3,0.815264,27.05703187532825,36
1585,The code will be released on Github .,3,0.5067337,13.778745477343334,8
1586,Cross-lingual summarization is the task of generating a summary in one language given a text in a different language .,0,0.9304166,9.116336168348248,20
1586,Previous works on cross-lingual summarization mainly focus on using pipeline methods or training an end-to-end model using the translated parallel data .,0,0.82769126,22.094238803224,24
1586,"However , it is a big challenge for the model to directly learn cross-lingual summarization as it requires learning to understand different languages and learning how to summarize at the same time .",0,0.8035679,21.695405616012852,33
1586,"In this paper , we propose to ease the cross-lingual summarization training by jointly learning to align and summarize .",1,0.87436557,38.53389994972887,20
1586,We design relevant loss functions to train this framework and propose several methods to enhance the isomorphism and cross-lingual transfer between languages .,2,0.4732829,33.0544850737689,23
1586,Experimental results show that our model can outperform competitive models in most cases .,3,0.9598747,8.503770092093918,14
1586,"In addition , we show that our model even has the ability to generate cross-lingual summaries without access to any cross-lingual corpus .",3,0.95833147,16.33886972976439,23
1587,Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries .,0,0.90014434,80.85984614277633,23
1587,"In this paper , we develop a neural abstractive multi-document summarization ( MDS ) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph , to more effectively process multiple input documents and produce abstractive summaries .",1,0.84520745,30.195982426179032,44
1587,"Our model utilizes graphs to encode documents in order to capture cross-document relations , which is crucial to summarizing long documents .",2,0.6890917,36.97676757633307,22
1587,"Our model can also take advantage of graphs to guide the summary generation process , which is beneficial for generating coherent and concise summaries .",3,0.803183,26.01081718169144,25
1587,"Furthermore , pre-trained language models can be easily combined with our model , which further improve the summarization performance significantly .",3,0.9062462,28.62213404028402,21
1587,Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines .,3,0.9404832,15.682779106369432,21
1588,"In this paper , we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization , which jointly learn semantic representations for words , sentences , and documents .",1,0.8356315,22.28315059920047,30
1588,The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary .,2,0.569037,11.718896465242741,22
1588,"We employ attention mechanisms to interact between different granularity of semantic representations , which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization .",2,0.60247403,31.250390132721236,31
1588,Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset .,3,0.9635198,11.675374278006828,24
1589,"We present a constituency parsing algorithm that , like a supertagger , works by assigning labels to each word in a sentence .",2,0.40881884,65.29257392251536,23
1589,"In order to maximally leverage current neural architectures , the model scores each word ’s tags in parallel , with minimal task-specific structure .",2,0.6251983,144.98111723799903,25
1589,"After scoring , a left-to-right reconciliation phase extracts a tree in ( empirically ) linear time .",3,0.42822468,243.88945486885333,18
1589,Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies .,3,0.9386338,16.004910390622605,30
1590,Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation ( QE ) for machine translation .,0,0.93905735,12.53233926584512,30
1590,A carefully engineered ensemble of such models won the QE shared task at WMT19 .,0,0.5385877,185.59882935356862,15
1590,"Our in-depth analysis , however , shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets : ( i ) The distributions of quality scores are imbalanced and skewed towards good quality scores ;",3,0.9634336,39.11511296049354,49
1590,( iii ) QE models can perform well on these datasets while looking at only source or translated sentences ;,3,0.8746335,266.2301276636222,20
1590,They contain statistical artifacts that correlate well with human-annotated QE labels .,0,0.74403095,53.45211626723128,12
1590,"Our findings suggest that although QE models might capture fluency of translated sentences and complexity of source sentences , they cannot model adequacy of translations effectively .",3,0.9901714,85.74185980833943,27
1591,"While natural language understanding ( NLU ) is advancing rapidly , today ’s technology differs from human-like language understanding in fundamental ways , notably in its inferior efficiency , interpretability , and generalization .",0,0.96190035,70.6168248762849,35
1591,This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics ( ECL ) .,1,0.7370724,57.32402859145635,21
1591,"According to ECL , natural language is inherently executable ( like programming languages ) , driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction .",0,0.81558853,141.91365254293598,34
1591,"This position paper argues that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems , and proposes a system architecture along with a roadmap towards realizing this vision .",1,0.69988286,67.72859446616282,34
1592,Language technologies contribute to promoting multilingualism and linguistic diversity around the world .,0,0.8363343,39.20829495710433,13
1592,"However , only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications .",0,0.9285744,35.86071102695148,26
1592,"In this paper we look at the relation between the types of languages , resources , and their representation in NLP conferences to understand the trajectory that different languages have followed over time .",1,0.9202088,32.18796628098712,34
1592,"Our quantitative investigation underlines the disparity between languages , especially in terms of their resources , and calls into question the “ language agnostic ” status of current models and systems .",3,0.9872405,62.28374742443552,32
1592,"Through this paper , we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here , so that no language is left behind .",1,0.75204515,59.38884685247539,29
1593,"In this paper , we trace the history of neural networks applied to natural language understanding tasks , and identify key contributions which the nature of language has made to the development of neural network architectures .",1,0.8740669,17.131057259063667,37
1593,"We focus on the importance of variable binding and its instantiation in attention-based models , and argue that Transformer is not a sequence model but an induced-structure model .",1,0.40235427,49.11543695776194,31
1593,This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding .,3,0.46831053,32.242422437149955,19
1594,Corpus query systems exist to address the multifarious information needs of any person interested in the content of annotated corpora .,0,0.92881,73.77135807804,21
1594,In this role they play an important part in making those resources usable for a wider audience .,0,0.6027462,37.29125753934904,18
1594,"Over the past decades , several such query systems and languages have emerged , varying greatly in their expressiveness and technical details .",0,0.9511333,68.30502168491671,23
1594,This paper offers a broad overview of the history of corpora and corpus query tools .,1,0.7772997,58.97019160687281,16
1594,It focusses strongly on the query side and hints at exciting directions for future development .,3,0.6426865,56.69426144455591,16
1595,Recent studies in dialogue state tracking ( DST ) leverage historical information to determine states which are generally represented as slot-value pairs .,0,0.92930937,77.26712556657212,23
1595,"However , most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history .",0,0.81994003,48.684953343595,31
1595,"Besides , existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately , which limits the learning of hard slots and eventually hurts overall performance .",0,0.79733914,50.83144686542113,29
1595,"In this paper , we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations .",1,0.87844133,41.35007644707044,36
1595,We further propose an adaptive objective to alleviate the slot imbalance problem by dynamically adjust weights of different slots during training .,3,0.46874374,44.23314908572243,22
1595,Experimental results show that our approach reaches 52.68 % and 58.55 % joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new state-of-the-art performance with considerable improvements ( + 1.24 % and + 5.98 % ) .,3,0.9320863,10.995876113419364,46
1596,Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm .,0,0.81926435,15.017549076922005,18
1596,"As such , a reliable training corpus is the crux of building a robust and well-behaved dialogue model .",0,0.799089,39.36372269436908,21
1596,"However , due to the open-ended nature of human conversations , the quality of user-generated training data varies greatly , and effective training samples are typically insufficient while noisy samples frequently appear .",0,0.89669025,42.17404888717169,33
1596,This impedes the learning of those data-driven neural dialogue models .,0,0.76616925,32.653074800383564,11
1596,"Therefore , effective dialogue learning requires not only more reliable learning samples , but also fewer noisy samples .",0,0.5927252,88.39018894495294,19
1596,"In this paper , we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously .",1,0.897014,61.40394470702005,37
1596,"In particular , the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data .",2,0.5797857,66.10353274275438,26
1596,"Note that , the proposed data manipulation framework is fully data-driven and learnable .",3,0.6947664,45.877079861577,14
1596,"It not only manipulates training samples to optimize the dialogue generation model , but also learns to increase its manipulation skills through gradient descent with validation samples .",2,0.48505086,72.40716370012177,28
1596,Extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments .,3,0.9323821,18.05994148835226,23
1597,Recent studies have shown remarkable success in end-to-end task-oriented dialog system .,0,0.9236859,12.543689620418686,15
1597,"However , most neural models rely on large training data , which are only available for a certain number of task domains , such as navigation and scheduling .",0,0.9062739,40.619352420617965,29
1597,This makes it difficult to scalable for a new domain with limited labeled data .,0,0.78809416,45.065690057140245,15
1597,"However , there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains .",0,0.946181,23.479583764048876,29
1597,"To this end , we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge .",1,0.7051011,47.008265062544815,29
1597,"In addition , we propose a novel Dynamic Fusion Network ( DF-Net ) which automatically exploit the relevance between the target domain and each domain .",2,0.6210105,54.09593986297473,28
1597,"Results show that our models outperforms existing methods on multi-domain dialogue , giving the state-of-the-art in the literature .",3,0.98114014,17.400259647907994,25
1597,"Besides , with little training data , we show its transferability by outperforming prior best model by 13.9 % on average .",3,0.9206751,47.062642353658624,22
1598,Training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users .,0,0.7717623,18.74037476913648,23
1598,Human demonstrations can be used to accelerate learning progress .,0,0.5044174,36.15956147708183,10
1598,"However , how to effectively leverage demonstrations to learn dialogue policy remains less explored .",0,0.90878123,140.5696407441489,15
1598,"In this paper , we present Sˆ2Agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping .",1,0.8344367,166.67589813863742,21
1598,"We use an imitation model to distill knowledge from demonstrations , based on which policy shaping estimates feedback on how the agent should act in policy space .",2,0.85550165,114.99183501189178,28
1598,Reward shaping is then incorporated to bonus state-actions similar to demonstrations explicitly in value space encouraging better exploration .,2,0.4557363,900.5658953293984,19
1598,The effectiveness of the proposed Sˆ2Agentt is demonstrated in three dialogue domains and a challenging domain adaptation task with both user simulator evaluation and human evaluation .,3,0.73182374,83.47305042942436,27
1599,Dialogue state tracker is responsible for inferring user intentions through dialogue history .,0,0.42671508,70.00236011201665,13
1599,"Previous methods have difficulties in handling dialogues with long interaction context , due to the excessive information .",0,0.83938307,105.92604736213046,18
1599,We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing ( SAS ) to reduce redundant information ’s interference and improve long dialogue context tracking .,1,0.41287795,173.0970574265154,29
1599,"Specially , we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module .",2,0.8421616,48.84049665772181,30
1599,Our model yields a significantly improved performance compared to previous state-of the-art models on the MultiWOZ dataset .,3,0.9408527,8.71157071384362,22
1600,Automatic evaluation of open-domain dialogue response generation is very challenging because there are many appropriate responses for a given context .,0,0.9317695,18.183404863823892,21
1600,Existing evaluation models merely compare the generated response with the ground truth response and rate many of the appropriate responses as inappropriate if they deviate from the ground truth .,0,0.8472756,42.46783101153963,30
1600,One approach to resolve this problem is to consider the similarity of the generated response with the conversational context .,0,0.77672434,19.861413505516783,20
1600,"In this paper , we propose an automatic evaluation model based on that idea and learn the model parameters from an unlabeled conversation corpus .",1,0.7932222,28.546317093539603,25
1600,Our approach considers the speakers in defining the different levels of similar context .,2,0.5656057,214.7758767751191,14
1600,We use a Twitter conversation corpus that contains many speakers and conversations to test our evaluation model .,2,0.8089717,84.50349128521388,18
1600,Experiments show that our model outperforms the other existing evaluation metrics in terms of high correlation with human annotation scores .,3,0.95506966,16.812695009332867,21
1600,We also show that our model trained on Twitter can be applied to movie dialogues without any additional training .,3,0.94449013,19.60631161940883,20
1600,We provide our code and the learned parameters so that they can be used for automatic evaluation of dialogue response generation models .,3,0.6688397,24.366306661437147,23
1601,"Due to its great importance in deep natural language understanding and various down-stream applications , text-level parsing of discourse rhetorical structure ( DRS ) has been drawing more and more attention in recent years .",0,0.9629004,45.674258691218974,37
1601,"However , all the previous studies on text-level discourse parsing adopt bottom-up approaches , which much limit the DRS determination on local information and fail to well benefit from global information of the overall discourse .",0,0.88319004,92.44626350123241,40
1601,"In this paper , we justify from both computational and perceptive points-of-view that the top-down architecture is more suitable for text-level DRS parsing .",1,0.7289193,47.80228950379013,29
1601,"On the basis , we propose a top-down neural architecture toward text-level DRS parsing .",2,0.41078293,37.10853575051291,17
1601,"In particular , we cast discourse parsing as a recursive split point ranking task , where a split point is classified to different levels according to its rank and the elementary discourse units ( EDUs ) associated with it are arranged accordingly .",2,0.7927022,77.69199353135117,43
1601,"In this way , we can determine the complete DRS as a hierarchical tree structure via an encoder-decoder with an internal stack .",3,0.5702601,50.515143318704425,23
1601,Experimentation on both the English RST-DT corpus and the Chinese CDTB corpus shows the great effectiveness of our proposed top-down approach towards text-level DRS parsing .,3,0.9210122,33.15211450682757,30
1602,"An in-depth exploration of protein-protein interactions ( PPI ) is essential to understand the metabolism in addition to the regulations of biological entities like proteins , carbohydrates , and many more .",0,0.9499097,37.51995492947657,33
1602,Most of the recent PPI tasks in BioNLP domain have been carried out solely using textual data .,0,0.90102124,37.788082328521014,18
1602,"In this paper , we argue that incorporating multimodal cues can improve the automatic identification of PPI .",1,0.85607713,29.050355305085976,18
1602,"As a first step towards enabling the development of multimodal approaches for PPI identification , we have developed two multi-modal datasets which are extensions and multi-modal versions of two popular benchmark PPI corpora ( BioInfer and HRPD50 ) .",2,0.6417629,35.65350761248513,39
1602,"Besides , existing textual modalities , two new modalities , 3D protein structure and underlying genomic sequence , are also added to each instance .",2,0.47611514,187.7632401889302,25
1602,"Further , a novel deep multi-modal architecture is also implemented to efficiently predict the protein interactions from the developed datasets .",3,0.52272934,54.89792331348977,21
1602,A detailed experimental analysis reveals the superiority of the multi-modal approach in comparison to the strong baselines including unimodal approaches and state-of the-art methods over both the generated multi-modal datasets .,3,0.9217528,17.541101351422995,35
1602,The developed multi-modal datasets are available for use at https://github.com/sduttap16/MM_PPI_NLP .,3,0.7309148,22.647251757755427,11
1603,"In this paper , we propose a novel bipartite flat-graph network ( BiFlaG ) for nested named entity recognition ( NER ) , which contains two subgraph modules : a flat NER module for outermost entities and a graph module for all the entities located in inner layers .",1,0.80187184,37.17299162949717,50
1603,Bidirectional LSTM ( BiLSTM ) and graph convolutional network ( GCN ) are adopted to jointly learn flat entities and their inner dependencies .,2,0.73277324,22.717771136184897,24
1603,"Different from previous models , which only consider the unidirectional delivery of information from innermost layers to outer ones ( or outside-to-inside ) , our model effectively captures the bidirectional interaction between them .",3,0.49448344,33.517519478479024,37
1603,"We first use the entities recognized by the flat NER module to construct an entity graph , which is fed to the next graph module .",2,0.8170359,50.31284115731194,26
1603,The richer representation learned from graph module carries the dependencies of inner entities and can be exploited to improve outermost entity predictions .,3,0.71207803,168.67110058631062,23
1603,Experimental results on three standard nested NER datasets demonstrate that our BiFlaG outperforms previous state-of-the-art models .,3,0.94829875,16.874973251948557,22
1604,"Knowledge graph ( KG ) entity typing aims at inferring possible missing entity type instances in KG , which is a very significant but still under-explored subtask of knowledge graph completion .",0,0.9044134,37.67030642557242,32
1604,"In this paper , we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge in KGs .",1,0.84987855,58.80090453335547,34
1604,"Specifically , we present two distinct knowledge-driven effective mechanisms of entity type inference .",1,0.44358832,87.61250646749443,15
1604,"Accordingly , we build two novel embedding models to realize the mechanisms .",2,0.6499668,112.01242581024223,13
1604,"Afterward , a joint model via connecting them is used to infer missing entity type instances , which favors inferences that agree with both entity type instances and triple knowledge in KGs .",2,0.71048325,137.27033745895125,33
1604,Experimental results on two real-world datasets ( Freebase and YAGO ) demonstrate the effectiveness of our proposed mechanisms and models for improving KG entity typing .,3,0.8913615,28.8649960602285,26
1604,The source code and data of this paper can be obtained from : https://github.com/Adam1679/ConnectE .,3,0.5299067,14.881502794429846,15
1605,Continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations .,0,0.9147615,47.54758753621689,25
1605,Some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem .,0,0.9077707,41.896071369620046,32
1605,"However , these memory-based methods usually suffer from overfitting the few memorized examples of old relations , which may gradually cause inevitable confusion among existing relations .",0,0.88401353,92.16013135346704,29
1605,"Inspired by the mechanism in human long-term memory formation , we introduce episodic memory activation and reconsolidation ( EMAR ) to continual relation learning .",0,0.31895825,52.368941887701084,25
1605,"Every time neural models are activated to learn both new and memorized data , EMAR utilizes relation prototypes for memory reconsolidation exercise to keep a stable understanding of old relations .",0,0.39413467,134.1370294220027,31
1605,The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models .,3,0.9846274,29.695124268472338,28
1606,One great challenge in neural sequence labeling is the data sparsity problem for rare entity words and phrases .,0,0.9307452,43.33778333833093,19
1606,"Most of test set entities appear only few times and are even unseen in training corpus , yielding large number of out-of-vocabulary ( OOV ) and low-frequency ( LF ) entities during evaluation .",3,0.75671655,42.79245161705031,36
1606,"In this work , we propose approaches to address this problem .",1,0.8496146,31.172159284593594,12
1606,"For OOV entities , we introduce local context reconstruction to implicitly incorporate contextual information into their representations .",2,0.71426505,118.56594493516221,18
1606,"For LF entities , we present delexicalized entity identification to explicitly extract their frequency-agnostic and entity-type-specific representations .",2,0.67144406,92.32621856820028,22
1606,Extensive experiments on multiple benchmark datasets show that our model has significantly outperformed all previous methods and achieved new start-of-the-art results .,3,0.9319846,9.942592416182054,28
1606,"Notably , our methods surpass the model fine-tuned on pre-trained language models without external resource .",3,0.9216809,34.97482827524969,16
1607,Interpretable rationales for model predictions play a critical role in practical applications .,0,0.8653471,31.286255889116614,13
1607,"In this study , we develop models possessing interpretable inference process for structured prediction .",1,0.89275575,180.40825712376662,15
1607,"Specifically , we present a method of instance-based learning that learns similarities between spans .",2,0.51267636,48.36223931751068,17
1607,"At inference time , each span is assigned a class label based on its similar spans in the training set , where it is easy to understand how much each training instance contributes to the predictions .",2,0.5854068,38.51873487127403,37
1607,"Through empirical analysis on named entity recognition , we demonstrate that our method enables to build models that have high interpretability without sacrificing performance .",3,0.79775375,23.665649094597367,25
1608,Electronic Medical Records ( EMRs ) have become key components of modern medical care systems .,0,0.978452,18.620475496566417,16
1608,"Despite the merits of EMRs , many doctors suffer from writing them , which is time-consuming and tedious .",0,0.91819084,59.51247367398412,21
1608,"We believe that automatically converting medical dialogues to EMRs can greatly reduce the burdens of doctors , and extracting information from medical dialogues is an essential step .",3,0.8897426,44.70040436430872,28
1608,"To this end , we annotate online medical consultation dialogues in a window-sliding style , which is much easier than the sequential labeling annotation .",2,0.82926077,77.74053951458684,26
1608,We then propose a Medical Information Extractor ( MIE ) towards medical dialogues .,2,0.3899667,71.0111857191963,14
1608,"MIE is able to extract mentioned symptoms , surgeries , tests , other information and their corresponding status .",3,0.58596516,237.7969005276629,19
1608,"To tackle the particular challenges of the task , MIE uses a deep matching architecture , taking dialogue turn-interaction into account .",2,0.54755336,118.2798195363452,22
1608,The experimental results demonstrate MIE is a promising solution to extract medical information from doctor-patient dialogues .,3,0.984429,30.419899960452845,17
1609,"Named Entity Recognition ( NER ) is a fundamental task in Natural Language Processing , concerned with identifying spans of text expressing references to entities .",0,0.9565295,24.05212202500283,26
1609,"NER research is often focused on flat entities only ( flat NER ) , ignoring the fact that entity references can be nested , as in [ Bank of [ China ] ] ( Finkel and Manning , 2009 ) .",0,0.88892865,132.2640042359328,41
1609,"In this paper , we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model ( Dozat and Manning , 2017 ) .",2,0.48092481,46.45603865698608,35
1609,"The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans , so that the model is able to predict named entities accurately .",2,0.5868571,45.0728786994478,33
1609,"We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them , with accuracy gains of up to 2.2 percentage points .",3,0.8923208,38.046277344599815,37
1610,Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment .,0,0.92354405,55.29644738841063,13
1610,"This paper presents Neighborhood Matching Network ( NMN ) , a novel entity alignment framework for tackling the structural heterogeneity challenge .",1,0.8584357,60.09386089421301,22
1610,NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference .,2,0.4733879,65.58642134690822,17
1610,It provides two innovative components for better learning representations for entity alignment .,3,0.5182797,162.25574995121661,13
1610,It first uses a novel graph sampling method to distill a discriminative neighborhood for each entity .,2,0.77940875,35.32137032939444,17
1610,It then adopts a cross-graph neighborhood matching module to jointly encode the neighborhood difference for a given entity pair .,2,0.71698564,57.87951009934625,20
1610,Such strategies allow NMN to effectively construct matching-oriented entity representations while ignoring noisy neighbors that have a negative impact on the alignment task .,3,0.6306846,76.65230308533577,25
1610,Extensive experiments performed on three entity alignment datasets show that NMN can well estimate the neighborhood similarity in more tough cases and significantly outperforms 12 previous state-of-the-art methods .,3,0.8917762,33.265494370955224,35
1611,Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences .,0,0.85685456,78.11059340574417,26
1611,Efforts thus far have focused on improving extraction accuracy but little is known about their explanability .,0,0.9230649,23.890251701435744,17
1611,In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models .,2,0.5312693,22.062008348148243,29
1611,We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation .,3,0.9696238,29.52752293447569,26
1611,We also propose to automatically generate “ distractor ” sentences to augment the bags and train the model to ignore the distractors .,2,0.452133,42.33066525146797,23
1611,Evaluations on the widely used FB-NYT dataset show that our methods achieve new state-of-the-art accuracy while improving model explanability .,3,0.95046926,20.220567456707712,28
1612,We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images .,1,0.57555413,36.271429282815355,21
1612,We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document .,2,0.5522748,25.614988088204072,40
1612,"These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable , as we show using loss cases .",3,0.72825545,75.20136999831442,32
1613,"To better tackle the named entity recognition ( NER ) problem on languages with little / no labeled data , cross-lingual NER must effectively leverage knowledge learned from source languages with rich labeled data .",0,0.9073682,39.21079094880281,35
1613,Previous works on cross-lingual NER are mostly based on label projection with pairwise texts or direct model transfer .,0,0.855863,66.59524602347726,19
1613,"However , such methods either are not applicable if the labeled data in the source languages is unavailable , or do not leverage information contained in unlabeled data in the target language .",0,0.8823851,22.493661677091367,33
1613,"In this paper , we propose a teacher-student learning method to address such limitations , where NER models in the source languages are used as teachers to train a student model on unlabeled data in the target language .",1,0.8131806,23.231491864378928,40
1613,The proposed method works for both single-source and multi-source cross-lingual NER .,3,0.81099993,12.643734892668386,12
1613,"For the latter , we further propose a similarity measuring method to better weight the supervision from different teacher models .",2,0.64247465,74.28680391773122,21
1613,Extensive experiments for 3 target languages on benchmark datasets well demonstrate that our method outperforms existing state-of-the-art methods for both single-source and multi-source cross-lingual NER .,3,0.90112364,9.655284744176452,32
1614,Opinion entity extraction is a fundamental task in fine-grained opinion mining .,0,0.93492675,21.540684041806845,13
1614,Related studies generally extract aspects and / or opinion expressions without recognizing the relations between them .,0,0.9121463,165.57031701973912,17
1614,"However , the relations are crucial for downstream tasks , including sentiment classification , opinion summarization , etc .",0,0.83542895,67.89644811639542,19
1614,"In this paper , we explore Aspect-Opinion Pair Extraction ( AOPE ) task , which aims at extracting aspects and opinion expressions in pairs .",1,0.85499656,52.03742578120139,25
1614,"To deal with this task , we propose Synchronous Double-channel Recurrent Network ( SDRN ) mainly consisting of an opinion entity extraction unit , a relation detection unit , and a synchronization unit .",2,0.5114188,43.702914740151314,34
1614,The opinion entity extraction unit and the relation detection unit are developed as two channels to extract opinion entities and relations simultaneously .,2,0.536945,57.8420978147659,23
1614,"Furthermore , within the synchronization unit , we design Entity Synchronization Mechanism ( ESM ) and Relation Synchronization Mechanism ( RSM ) to enhance the mutual benefit on the above two channels .",2,0.7885579,31.529690003818814,33
1614,"To verify the performance of SDRN , we manually build three datasets based on SemEval 2014 and 2015 benchmarks .",2,0.85037565,39.16449608056427,20
1614,Extensive experiments demonstrate that SDRN achieves state-of-the-art performances .,3,0.8603945,12.1535933388472,13
1615,We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning .,2,0.7396459,60.69655465180624,21
1615,"Using an annotation protocol specifically devised for capturing image–caption coherence relations , we annotate 10,000 instances from publicly-available image–caption pairs .",2,0.89334166,50.03005287141112,23
1615,"We introduce a new task for learning inferences in imagery and text , coherence relation prediction , and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step , and also train coherence-aware , controllable image captioning models .",3,0.3892793,59.359544116632115,47
1615,The results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations .,3,0.9866426,37.556470140359146,25
1616,"In human cognition , world knowledge supports the perception of object colours : knowing that trees are typically green helps to perceive their colour in certain contexts .",0,0.9348497,141.38653954688334,28
1616,We go beyond previous studies on colour terms using isolated colour swatches and study visual grounding of colour terms in realistic objects .,2,0.39098626,98.24574696998258,23
1616,Our models integrate processing of visual information and object-specific knowledge via hard-coded ( late ) or learned ( early ) fusion .,2,0.5573111,132.8486753304188,24
1616,"We find that both models consistently outperform a bottom-up baseline that predicts colour terms solely from visual inputs , but show interesting differences when predicting atypical colours of so-called colour diagnostic objects .",3,0.9719986,72.67388702818455,35
1616,Our models also achieve promising results when tested on new object categories not seen during training .,3,0.94941074,41.07001885529205,17
1617,"Given an untrimmed video and a text query , natural language video localization ( NLVL ) is to locate a matching span from the video that semantically corresponds to the query .",0,0.8470823,79.17380115447533,32
1617,"Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture , or as a regression task to directly regress the target video span .",0,0.72930264,149.80587454791515,28
1617,"In this work , we address NLVL task with a span-based QA approach by treating the input video as text passage .",1,0.44511443,77.55274979323096,22
1617,"We propose a video span localizing network ( VSLNet ) , on top of the standard span-based QA framework , to address NLVL .",1,0.4212285,125.9528884819503,24
1617,The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting ( QGH ) strategy .,3,0.6292631,131.119187662798,26
1617,The QGH guides VSLNet to search for matching video span within a highlighted region .,3,0.42801657,580.5101797667307,15
1617,"Through extensive experiments on three benchmark datasets , we show that the proposed VSLNet outperforms the state-of-the-art methods ;",3,0.86980695,16.230027705952956,24
1617,and adopting span-based QA framework is a promising direction to solve NLVL .,3,0.8123897,99.16447381698876,13
1618,Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image .,0,0.94423103,21.982389831405165,20
1618,"We critically examine RefCOCOg , a standard benchmark for this task , using a human study and show that 83.7 % of test instances do not require reasoning on linguistic structure , i.e. , words are enough to identify the target object , the word order does n’t matter .",3,0.76486254,82.56077107541356,50
1618,"To measure the true progress of existing models , we split the test set into two sets , one which requires reasoning on linguistic structure and the other which does n’t .",2,0.8369608,49.51668364606736,32
1618,"Additionally , we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes .",2,0.8582872,54.9817024041209,23
1618,"Using these datasets , we empirically show that existing methods fail to exploit linguistic structure and are 12 % to 23 % lower in performance than the established progress for this task .",3,0.85083944,52.24377245401414,33
1618,"We also propose two methods , one based on contrastive learning and the other based on multi-task learning , to increase the robustness of ViLBERT , the current state-of-the-art model for this task .",2,0.48568964,13.281826791428445,40
1618,Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv .,3,0.529554,20.262505019408668,8
1619,Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks .,0,0.882866,5.092627512705813,23
1619,Evidence has shown that they are overparameterized ;,0,0.8589349,73.9592306154831,8
1619,attention heads can be pruned without significant performance loss .,3,0.7696169,61.10196705914872,10
1619,"In this work , we instead “ reallocate ” them — the model learns to activate different heads on different inputs .",2,0.6065384,119.56415079244,22
1619,"Drawing connections between multi-head attention and mixture of experts , we propose the mixture of attentive experts model ( MAE ) .",2,0.3965094,110.81328208481715,22
1619,MAE is trained using a block coordinate descent algorithm that alternates between updating ( 1 ) the responsibilities of the experts and ( 2 ) their parameters .,2,0.7080345,82.60743538955325,28
1619,Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks .,3,0.88544136,10.130675207416575,17
1619,"Particularly , on the WMT14 English to German translation dataset , MAE improves over “ transformer-base ” by 0.8 BLEU , with a comparable number of parameters .",3,0.95098746,47.52181593999862,30
1619,Our analysis shows that our model learns to specialize different experts to different inputs .,3,0.97735435,72.40239921007185,15
1620,Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect .,0,0.9437542,28.303531467734885,20
1620,One sentence may contain various sentiments for different aspects .,0,0.8268544,128.20371589088344,10
1620,Many sophisticated methods such as attention mechanism and Convolutional Neural Networks ( CNN ) have been widely employed for handling this challenge .,0,0.9118034,29.817823406596098,23
1620,"Recently , semantic dependency tree implemented by Graph Convolutional Networks ( GCN ) is introduced to describe the inner connection between aspects and the associated emotion words .",0,0.8706425,76.83307620600218,28
1620,But the improvement is limited due to the noise and instability of dependency trees .,3,0.6113662,30.15763461739447,15
1620,"To this end , we propose a dependency graph enhanced dual-transformer network ( named DGEDT ) by jointly considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner .",2,0.5926845,54.76625223702874,43
1620,"Specifically , a dual-transformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning .",2,0.6238762,56.566831751031614,25
1620,The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa .,2,0.41868582,24.192306742369734,21
1620,The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin .,3,0.96903574,18.934125782344367,25
1621,"We propose Differentiable Window , a new neural module and general purpose component for dynamic window selection .",1,0.40651584,118.22219256138574,18
1621,"While universally applicable , we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling more focused attentions over the input regions .",3,0.91458136,109.53830227378509,29
1621,"We propose two variants of Differentiable Window , and integrate them within the Transformer architecture in two novel ways .",2,0.48400566,48.08601197421764,20
1621,"We evaluate our proposed approach on a myriad of NLP tasks , including machine translation , sentiment analysis , subject-verb agreement and language modeling .",2,0.42228144,24.294567870049363,27
1621,Our experimental results demonstrate consistent and sizable improvements across all tasks .,3,0.982398,33.63738553073367,12
1622,"Despite achieving prominent performance on many important tasks , it has been reported that neural networks are vulnerable to adversarial examples .",0,0.9543925,28.763164940996344,22
1622,"Previously studies along this line mainly focused on semantic tasks such as sentiment analysis , question answering and reading comprehension .",0,0.9217746,34.83263561517737,21
1622,"In this study , we show that adversarial examples also exist in dependency parsing : we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels , and design algorithms to construct such examples in both of the black-box and white-box settings .",1,0.813333,55.38976116364407,60
1622,"Our experiments with one of state-of-the-art parsers on the English Penn Treebank ( PTB ) show that up to 77 % of input examples admit adversarial perturbations , and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage , while suffering little to no performance drop on the clean input data .",3,0.8373505,28.98075487560158,71
1623,It is commonly believed that knowledge of syntactic structure should improve language modeling .,0,0.897744,24.7122598550943,14
1623,"However , effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic .",0,0.94013095,43.76600916138664,19
1623,"In this paper , we make use of a multi-task objective , i.e. , the models simultaneously predict words as well as ground truth parse trees in a form called “ syntactic distances ” , where information between these two separate objectives shares the same intermediate representation .",2,0.46359605,60.035776671896514,48
1623,"Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals , the model is able to achieve lower perplexity and induce trees with better quality .",3,0.92896247,23.982817740837106,39
1624,Neural architecture search ( NAS ) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell .,0,0.9493404,109.59812416605631,28
1624,"In this paper , we extend the search space of NAS .",1,0.8340723,55.17087042868876,12
1624,"In particular , we present a general approach to learn both intra-cell and inter-cell architectures ( call it ESS ) .",1,0.4169949,55.49360431193649,21
1624,"For a better search result , we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously .",2,0.675165,59.70760619596036,20
1624,We implement our model in a differentiable architecture search system .,2,0.69649357,134.32949966609073,11
1624,"For recurrent neural language modeling , it outperforms a strong baseline significantly on the PTB and WikiText data , with a new state-of-the-art on PTB .",3,0.90303534,21.336598784753875,31
1624,"Moreover , the learned architectures show good transferability to other systems .",3,0.8918442,49.5018461079479,12
1624,"E.g. , they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition ( NER ) tasks and CoNLL chunking task , indicating a promising line of research on large-scale pre-learned architectures .",3,0.7433057,26.586856190372803,40
1625,"As NLP models become larger , executing a trained model requires significant computational resources incurring monetary and environmental costs .",0,0.75626016,75.02232529931185,20
1625,"To better respect a given inference budget , we propose a modification to contextual representation fine-tuning which , during inference , allows for an early ( and fast ) “ exit ” from neural network calculations for simple instances , and late ( and accurate ) exit for hard instances .",2,0.5601323,120.87447347392326,51
1625,"To achieve this , we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions .",2,0.78206754,64.80717241526517,24
1625,We test our proposed modification on five different datasets in two tasks : three text classification datasets and two natural language inference benchmarks .,2,0.7884343,31.755107827850047,24
1625,"Our method presents a favorable speed / accuracy tradeoff in almost all cases , producing models which are up to five times faster than the state of the art , while preserving their accuracy .",3,0.8495013,39.721709564532695,35
1625,Our method also requires almost no additional training resources ( in either time or parameters ) compared to the baseline BERT model .,3,0.8514914,65.30487298128041,23
1625,"Finally , our method alleviates the need for costly retraining of multiple models at different levels of efficiency ;",3,0.90021574,39.325844127831495,19
1625,"we allow users to control the inference speed / accuracy tradeoff using a single trained model , by setting a single variable at inference time .",2,0.5827808,62.27676850225626,26
1625,We publicly release our code .,2,0.5355345,24.464572423421483,6
1626,"Polysynthetic languages have exceptionally large and sparse vocabularies , thanks to the number of morpheme slots and combinations in a word .",0,0.93374205,37.62265486310672,22
1626,"This complexity , together with a general scarcity of written data , poses a challenge to the development of natural language technologies .",0,0.9405104,38.09105969438058,23
1626,"To address this challenge , we offer linguistically-informed approaches for bootstrapping a neural morphological analyzer , and demonstrate its application to Kunwinjku , a polysynthetic Australian language .",1,0.65727097,75.02250416668707,30
1626,We generate data from a finite state transducer to train an encoder-decoder model .,2,0.900638,16.400874736849183,14
1626,"We improve the model by “ hallucinating ” missing linguistic structure into the training data , and by resampling from a Zipf distribution to simulate a more natural distribution of morphemes .",2,0.6024543,42.83337305538399,32
1626,"The best model accounts for all instances of reduplication in the test set and achieves an accuracy of 94.7 % overall , a 10 percentage point improvement over the FST baseline .",3,0.93045557,26.764004701534432,32
1626,This process demonstrates the feasibility of bootstrapping a neural morph analyzer from minimal resources .,3,0.9027301,52.2534640321345,15
1627,Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation ( CWS ) .,0,0.9413226,28.525518816322087,19
1627,"Nevertheless , the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary ( OOV ) problem .",0,0.6026901,86.13964948498281,31
1627,"In order to simultaneously alleviate the issues , this paper intuitively couples distant annotation and adversarial training for cross-domain CWS .",1,0.4282496,95.99537951641612,21
1627,"1 ) We rethink the essence of “ Chinese words ” and design an automatic distant annotation mechanism , which does not need any supervision or pre-defined dictionaries on the target domain .",2,0.69392294,73.36656619247903,33
1627,The method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain .,3,0.8954316,33.57088719375358,18
1627,2 ) We further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information .,2,0.58885956,58.73300658533815,25
1627,"Experiments on multiple real-world datasets across various domains show the superiority and robustness of our model , significantly outperforming previous state-of-the-arts cross-domain CWS methods .",3,0.8943258,14.49754092928543,30
1628,This paper describes a language-independent model for fully unsupervised morphological analysis that exploits a universal framework leveraging morphological typology .,1,0.8652988,40.90648838010107,22
1628,"By modeling morphological processes including suffixation , prefixation , infixation , and full and partial reduplication with constrained stem change rules , our system effectively constrains the search space and offers a wide coverage in terms of morphological typology .",3,0.6250569,61.235729526764096,40
1628,"The system is tested on nine typologically and genetically diverse languages , and shows superior performance over leading systems .",3,0.5275208,55.42413370139903,20
1628,We also investigate the effect of an oracle that provides only a handful of bits per language to signal morphological type .,2,0.3772747,49.50138582585516,22
1629,The noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties .,0,0.935523,41.725222599106864,18
1629,"Class membership is far from deterministic , but the phonological form of a noun and / or its meaning can often provide imperfect clues .",0,0.83960277,86.54262720333541,25
1629,"Here , we investigate the strength of those clues .",1,0.8714774,58.026883409310656,10
1629,"More specifically , we operationalize this by measuring how much information , in bits , we can glean about declension class from knowing the form and / or meaning of nouns .",2,0.7793396,88.06717037227034,32
1629,"We know that form and meaning are often also indicative of grammatical gender — which , as we quantitatively verify , can itself share information with declension class — so we also control for gender .",3,0.58759683,92.67197298539617,36
1629,We find for two Indo-European languages ( Czech and German ) that form and meaning respectively share significant amounts of information with class ( and contribute additional information above and beyond gender ) .,3,0.9689659,158.27777494200336,34
1629,"The three-way interaction between class , form , and meaning ( given gender ) is also significant .",3,0.9652143,126.70375340956599,20
1629,"First , we introduce a new method that provides additional quantitative support for a classic linguistic finding that form and meaning are relevant for the classification of nouns into declensions .",2,0.5160184,55.65148557188513,31
1629,"Secondly , we show not only that individual declensions classes vary in the strength of their clues within a language , but also that these variations themselves vary across languages .",3,0.8872407,48.006364654698565,31
1630,We propose the task of unsupervised morphological paradigm completion .,1,0.46270394,40.255886161722394,10
1630,"Given only raw text and a lemma list , the task consists of generating the morphological paradigms , i.e. , all inflected forms , of the lemmas .",2,0.6452298,50.28112301928122,28
1630,"From a natural language processing ( NLP ) perspective , this is a challenging unsupervised task , and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators .",0,0.9239782,31.96591507693413,36
1630,"From a cognitive science perspective , this can shed light on how children acquire morphological knowledge .",0,0.50768733,35.42082212473491,17
1630,"We further introduce a system for the task , which generates morphological paradigms via the following steps : ( i ) EDIT TREE retrieval , ( ii ) additional lemma retrieval , ( iii ) paradigm size discovery , and ( iv ) inflection generation .",2,0.6934832,89.79977193520772,46
1630,We perform an evaluation on 14 typologically diverse languages .,2,0.7830978,29.10245193774763,10
1630,"Our system outperforms trivial baselines with ease and , for some languages , even obtains a higher accuracy than minimally supervised systems .",3,0.9338552,62.31684112779686,23
1631,"Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers , which are a long answer ( typically a paragraph ) and a short answer ( one or more entities inside the long answer ) .",0,0.93247724,39.2142500763673,40
1631,"Despite the effectiveness of existing methods on this benchmark , they treat these two sub-tasks individually during training while ignoring their dependencies .",0,0.48515984,38.33337090613626,23
1631,"To address this issue , we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature , which are different levels of granularity : documents , paragraphs , sentences , and tokens .",1,0.66145283,38.43237757951622,40
1631,We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously .,2,0.728375,38.03560234681601,19
1631,"The long and short answers can be extracted from paragraph-level representation and token-level representation , respectively .",3,0.39726576,34.09181289229726,21
1631,"In this way , we can model the dependencies between the two-grained answers to provide evidence for each other .",3,0.5429986,37.163660344244924,21
1631,"We jointly train the two sub-tasks , and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria .",3,0.74075663,26.351467142882655,26
1632,Question Answering ( QA ) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models .,0,0.96837413,10.268126476362728,23
1632,Recent research works have attempted to extend these successes to the settings with few or no labeled data available .,0,0.9023773,50.534055543517084,20
1632,"In this work , we introduce two approaches to improve unsupervised QA .",1,0.7591553,24.83865605801856,13
1632,"First , we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs ( named as RefQA ) .",2,0.91566086,35.06340497598652,27
1632,"Second , we take advantage of the QA model to extract more appropriate answers , which iteratively refines data over RefQA .",2,0.7070482,56.800983588320065,22
1632,"We conduct experiments on SQuAD 1.1 , and NewsQA by fine-tuning BERT without access to manually annotated data .",2,0.79257125,22.088255532428832,19
1632,"Our approach outperforms previous unsupervised approaches by a large margin , and is competitive with early supervised models .",3,0.8374966,16.41108763879861,19
1632,We also show the effectiveness of our approach in the few-shot learning setting .,3,0.8344259,10.369933190867602,14
1633,This paper focuses on generating multi-hop reasoning questions from the raw text in a low resource circumstance .,1,0.8820927,50.92130494311965,18
1633,Such questions have to be syntactically valid and need to logically correlate with the answers by deducing over multiple relations on several sentences in the text .,0,0.8053684,46.262469369689356,27
1633,"Specifically , we first build a multi-hop generation model and guide it to satisfy the logical rationality by the reasoning chain extracted from a given text .",2,0.86657524,48.59856657365758,27
1633,"Since the labeled data is limited and insufficient for training , we propose to learn the model with the help of a large scale of unlabeled data that is much easier to obtain .",2,0.47000355,16.402266853204964,34
1633,Such data contains rich expressive forms of the questions with structural patterns on syntax and semantics .,0,0.61423606,105.84001417307141,17
1633,These patterns can be estimated by the neural hidden semi-Markov model using latent variables .,0,0.47401723,64.46755113325528,15
1633,"With latent patterns as a prior , we can regularize the generation model and produce the optimal results .",3,0.560327,80.37755435050053,19
1633,Experimental results on the HotpotQA data set demonstrate the effectiveness of our model .,3,0.9434614,7.463424040517112,14
1633,"Moreover , we apply the generated results to the task of machine reading comprehension and achieve significant performance improvements .",3,0.85143244,18.150668460849737,20
1634,Recent studies have revealed that reading comprehension ( RC ) systems learn to exploit annotation artifacts and other biases in current datasets .,0,0.9542314,65.06857683292439,23
1634,This prevents the community from reliably measuring the progress of RC systems .,0,0.699643,64.72017860342976,13
1634,"To address this issue , we introduce R4C , a new task for evaluating RC systems ’ internal reasoning .",1,0.67149603,83.35062649796743,20
1634,R4C requires giving not only answers but also derivations : explanations that justify predicted answers .,0,0.5411674,177.24587946017485,16
1634,"We present a reliable , crowdsourced framework for scalably annotating RC datasets with derivations .",3,0.4688693,133.36195302410712,15
1634,"We create and publicly release the R4C dataset , the first , quality-assured dataset consisting of 4.6 k questions , each of which is annotated with 3 reference derivations ( i.e .",2,0.7737233,44.75945339371496,34
1634,13.8 k derivations ) .,3,0.72941333,340.81013419675287,5
1634,"Experiments show that our automatic evaluation metrics using multiple reference derivations are reliable , and that R4C assesses different skills from an existing benchmark .",3,0.95702684,85.52650372703687,25
1635,"In this paper , we study machine reading comprehension ( MRC ) on long texts : where a model takes as inputs a lengthy document and a query , extracts a text span from the document as an answer .",1,0.71303785,46.114597613617036,40
1635,"State-of-the-art models ( e.g. , BERT ) tend to use a stack of transformer layers that are pre-trained from a large number of unlabeled language corpora to encode the joint contextual information of query and document .",0,0.725335,17.86165251871865,41
1635,"However , these transformer models can only take as input a fixed-length ( e.g. , 512 ) text .",0,0.6236249,59.80574020639566,21
1635,"To deal with even longer text inputs , previous approaches usually chunk them into equally-spaced segments and predict answers based on each segment independently without considering the information from other segments .",0,0.85444,59.15629618715989,33
1635,"As a result , they may form segments that fail to cover complete answers or retain insufficient contexts around the correct answer required for question answering .",0,0.7646256,100.40824713054914,27
1635,"Moreover , they are less capable of answering questions that need cross-segment information .",0,0.72534984,47.894751122323214,14
1635,We propose to let a model learn to chunk in a more flexible way via reinforcement learning : a model can decide the next segment that it wants to process in either direction .,2,0.575093,52.2311063536043,34
1635,We also apply recurrent mechanisms to enable information to flow across segments .,2,0.5595208,106.00952188908373,13
1635,"Experiments on three MRC tasks – CoQA , QuAC , and TriviaQA – demonstrate the effectiveness of our proposed recurrent chunking mechanisms : we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions .",3,0.8375081,39.30453822645701,52
1636,Reading long documents to answer open-domain questions remains challenging in natural language understanding .,0,0.95233375,23.356194008077743,14
1636,"In this paper , we introduce a new model , called RikiNet , which reads Wikipedia pages for natural question answering .",1,0.76013255,74.67964957756186,22
1636,RikiNet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor .,0,0.47937468,106.24068266117959,14
1636,The reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms .,0,0.6136678,78.92020590548725,17
1636,"The representations are then fed into the predictor to obtain the span of the short answer , the paragraph of the long answer , and the answer type in a cascaded manner .",2,0.61793596,34.29721722549042,33
1636,"On the Natural Questions ( NQ ) dataset , a single RikiNet achieves 74.3 F1 and 57.9 F1 on long-answer and short-answer tasks .",3,0.8508071,31.364638475750386,25
1636,"To our best knowledge , it is the first single model that outperforms the single human performance .",3,0.8862072,32.45714570996234,18
1636,"Furthermore , an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks , achieving the best performance on the official NQ leaderboard .",3,0.9456122,38.58931099198577,28
1637,We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing .,1,0.32109198,29.255459560034875,20
1637,"The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena , such as quantification , negation and modality , in a way that is consistent with the state-of-the-art underspecification approach .",3,0.7889917,29.562976068749848,43
1637,"Moreover , the well-formedness of such a graph is clear , since model-theoretic interpretation is available .",3,0.5758501,60.81574043845575,19
1637,We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for English Resource Semantics .,3,0.5429794,19.113043868023254,25
1637,"At the core of this parser is a novel neural graph rewriting system which combines the strengths of Hyperedge Replacement Grammar , a knowledge-intensive model , and Graph Neural Networks , a data-intensive model .",2,0.3721967,40.41944649290151,36
1637,"Our parser achieves an accuracy of 92.39 % in terms of elementary dependency match , which is a 2.88 point improvement over the best data-driven model in the literature .",3,0.9230907,31.983547931418425,30
1637,"The output of our parser is highly coherent : at least 91 % graphs are valid , in that they allow at least one sound scope-resolved logical form .",3,0.92822164,170.1895511613646,29
1638,This paper is concerned with semantic parsing for English as a second language ( ESL ) .,1,0.78797334,39.69949823356613,17
1638,"Motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface during second language acquisition , we formulate the task based on the divergence between literal and intended meanings .",1,0.4052596,60.986814919892915,35
1638,"We combine the complementary strengths of English Resource Grammar , a linguistically-precise hand-crafted deep grammar , and TLE , an existing manually annotated ESL UD-TreeBank with a novel reranking model .",2,0.79350096,94.89926059315292,36
1638,"Experiments demonstrate that in comparison to human annotations , our method can obtain a very promising SemBanking quality .",3,0.9683312,86.7897130990381,19
1638,"By means of the newly created corpus , we evaluate state-of-the-art semantic parsing as well as grammatical error correction models .",2,0.7274087,16.49862453119568,27
1638,The evaluation profiles the performance of neural NLP techniques for handling ESL data and suggests some research directions .,3,0.938065,98.69580530923616,19
1639,"Semantic dependency parsing , which aims to find rich bi-lexical relationships , allows words to have multiple dependency heads , resulting in graph-structured representations .",0,0.8785943,70.90808686239303,25
1639,We propose an approach to semi-supervised learning of semantic dependency parsers based on the CRF autoencoder framework .,1,0.42003173,14.829855299715415,18
1639,Our encoder is a discriminative neural semantic dependency parser that predicts the latent parse graph of the input sentence .,2,0.6675564,26.783588881625835,20
1639,Our decoder is a generative neural model that reconstructs the input sentence conditioned on the latent parse graph .,2,0.68757224,23.45617913244336,19
1639,Our model is arc-factored and therefore parsing and learning are both tractable .,3,0.54835427,124.16394399450986,13
1639,Experiments show our model achieves significant and consistent improvement over the supervised baseline .,3,0.9546633,19.987277941835192,14
1640,One daunting problem for semantic parsing is the scarcity of annotation .,0,0.9499972,46.35115691266639,12
1640,"Aiming to reduce nontrivial human labor , we propose a two-stage semantic parsing framework , where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance .",2,0.6417477,17.03113589179534,37
1640,The downstream naive semantic parser accepts the intermediate output and returns the target logical form .,2,0.42403337,178.11965431392184,16
1640,"Furthermore , the entire training process is split into two phases : pre-training and cycle learning .",2,0.4678742,33.77829059248916,17
1640,Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model .,2,0.6224681,54.63815356050401,15
1640,Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training .,3,0.9535774,40.80597224489901,19
1641,Discourse representation tree structure ( DRTS ) parsing is a novel semantic parsing task which has been concerned most recently .,0,0.9538632,132.38035349536057,21
1641,"State-of-the-art performance can be achieved by a neural sequence-to-sequence model , treating the tree construction as an incremental sequence generation problem .",0,0.4783221,15.30637393351584,28
1641,"Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model , which could be potentially useful for the DRTS parsing .",3,0.7255436,95.8828410908347,31
1641,"In this work , we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information , where graph attention network ( GAT ) is exploited for effectively modeling .",1,0.5942417,56.74102238382732,37
1641,Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature .,3,0.9369791,11.322543304182494,23
1642,"We tackle the task of Term Set Expansion ( TSE ) : given a small seed set of example terms from a semantic class , finding more members of that class .",0,0.43058175,166.20621472117213,32
1642,"The task is of great practical utility , and also of theoretical utility as it requires generalization from few examples .",3,0.47604355,49.22549386746033,21
1642,Previous approaches to the TSE task can be characterized as either distributional or pattern-based .,0,0.891234,35.455007007665756,17
1642,"We harness the power of neural masked language models ( MLM ) and propose a novel TSE algorithm , which combines the pattern-based and distributional approaches .",2,0.52289104,35.6476172812837,29
1642,"Due to the small size of the seed set , fine-tuning methods are not effective , calling for more creative use of the MLM .",3,0.8260239,31.660935698990027,26
1642,"The gist of the idea is to use the MLM to first mine for informative patterns with respect to the seed set , and then to obtain more members of the seed class by generalizing these patterns .",2,0.45508888,46.20292456928721,38
1642,Our method outperforms state-of-the-art TSE algorithms .,3,0.8680176,14.882904330990602,11
1642,Implementation is available at : https://github.com/ guykush / TermSetExpansion-MPB / .,3,0.5871351,114.56258781383205,11
1643,"Recently , the character-word lattice structure has been proved to be effective for Chinese named entity recognition ( NER ) by incorporating the word information .",0,0.915272,31.507183834842415,28
1643,"However , since the lattice structure is complex and dynamic , the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed .",0,0.8306532,41.421238023924126,34
1643,"In this paper , we propose FLAT : Flat-LAttice Transformer for Chinese NER , which converts the lattice structure into a flat structure consisting of spans .",1,0.8625085,39.888015917143015,27
1643,Each span corresponds to a character or latent word and its position in the original lattice .,2,0.430074,58.289532684249735,17
1643,"With the power of Transformer and well-designed position encoding , FLAT can fully leverage the lattice information and has an excellent parallel ability .",3,0.6271739,85.17377364398533,26
1643,Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency .,3,0.8681388,23.689228848093883,17
1644,"Entity embeddings , which represent different aspects of each entity with a single vector like word embeddings , are a key component of neural entity linking models .",0,0.8598067,27.370078412483462,28
1644,Existing entity embeddings are learned from canonical Wikipedia articles and local contexts surrounding target entities .,2,0.42574346,78.99155093886084,16
1644,"Such entity embeddings are effective , but too distinctive for linking models to learn contextual commonality .",3,0.66437525,84.95028411651569,17
1644,"We propose a simple yet effective method , FGS2EE , to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality .",3,0.3307711,30.115838560494733,31
1644,"FGS2EE first uses the embeddings of semantic type words to generate semantic embeddings , and then combines them with existing entity embeddings through linear aggregation .",2,0.77768457,48.43990157905145,26
1644,Extensive experiments show the effectiveness of such embeddings .,3,0.6447515,17.173308909400227,9
1644,"Based on our entity embeddings , we achieved new sate-of-the-art performance on entity linking .",3,0.89836997,43.32726609638907,21
1645,We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval : document translation ( DT ) and query translation ( QT ) .,1,0.6642473,28.860550662004044,26
1645,Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013 – 2015 containing English documents and queries in several European languages .,2,0.8611612,127.62122973613455,31
1645,We exploit the Statistical Machine Translation ( SMT ) and Neural Machine Translation ( NMT ) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English ( for the QT approach ) and the English documents to all the query languages ( for the DT approach ) .,2,0.83562607,24.54861556921278,56
1645,The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages .,3,0.9872535,57.089353911381664,27
1645,"NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages , but still , QT provides generally better retrieval results than DT .",3,0.93723357,122.90753953553875,30
1646,Showing items that do not match search query intent degrades customer experience in e-commerce .,3,0.49146727,69.16018942709152,15
1646,These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs .,3,0.56063354,98.94186839833054,24
1646,"Mitigating the problem requires a large labeled dataset , which is expensive and time-consuming to obtain .",0,0.70996934,15.13412796821562,19
1646,"In this paper , we develop a deep , end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier .",1,0.8121406,35.03476775458063,30
1646,We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples .,2,0.8304443,22.12482301406718,24
1646,This not only makes the classifier more robust but also boosts the overall ranking performance .,3,0.7257079,13.661062552980672,16
1646,"Our model achieves a relative gain compared to baselines by over 26 % in F-score , and over 17 % in Area Under PR curve .",3,0.92633355,51.34748909744336,26
1646,"On live search traffic , our model gains significant improvement in multiple countries .",3,0.9144585,153.02765666948966,14
1647,Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky ( i.e .,0,0.85547,79.82201678766867,18
1647,"over-confident ) predictions , a common sign of overfitting .",3,0.47676015,202.07358171183384,10
1647,"This class of techniques , of which label smoothing is one , has a connection to entropy regularization .",0,0.58386475,93.77272208250803,19
1647,"Despite the consistent success of label smoothing across architectures and data sets in language generation tasks , two problems remain open : ( 1 ) there is little understanding of the underlying effects entropy regularizers have on models , and ( 2 ) the full space of entropy regularization techniques is largely unexplored .",0,0.80242866,45.341611945406456,54
1647,"We introduce a parametric family of entropy regularizers , which includes label smoothing as a special case , and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks .",2,0.5382072,29.35563888790277,43
1647,We also find that variance in model performance can be explained largely by the resulting entropy of the model .,3,0.97107273,22.06489101244907,20
1647,"Lastly , we find that label smoothing provably does not allow for sparsity in an output distribution , an undesirable property for language generation models , and therefore advise the use of other entropy regularization methods in its place .",3,0.976504,68.81255300006053,40
1648,"Self-attention mechanisms have made striking state-of-the-art ( SOTA ) progress in various sequence learning tasks , standing on the multi-headed dot product attention by attending to all the global contexts at different locations .",0,0.94282573,45.1317741459189,37
1648,"Through a pseudo information highway , we introduce a gated component self-dependency units ( SDU ) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations .",2,0.7787152,87.83009965317768,37
1648,"The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections , leading to a clear margin of convergence speed with gradient descent algorithms .",3,0.63661903,145.91052665970372,33
1648,"We may unveil the role of gating mechanism to aid in the context-based Transformer modules , with hypothesizing that SDU gates , especially on shallow layers , could push it faster to step towards suboptimal points during the optimization process .",3,0.9213744,144.47622589614193,42
1649,Knowledge graph ( KG ) embeddings learn low-dimensional representations of entities and relations to predict missing facts .,0,0.52177227,29.700555024789832,18
1649,KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space .,0,0.9048516,42.481694563094216,16
1649,"For hierarchical data , hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations .",0,0.8050241,35.23349438557083,16
1649,"However , existing hyperbolic embedding methods do not account for the rich logical patterns in KGs .",0,0.85921264,40.81321118994404,17
1649,"In this work , we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns .",1,0.76823354,34.01533190352065,21
1649,Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns .,2,0.70261353,82.39009043722668,15
1649,Experimental results on standard KG benchmarks show that our method improves over previous Euclidean-and hyperbolic-based efforts by up to 6.1 % in mean reciprocal rank ( MRR ) in low dimensions .,3,0.93214047,40.635354214196276,36
1649,"Furthermore , we observe that different geometric transformations capture different types of relations while attention-based transformations generalize to multiple relations .",3,0.96351904,38.106247183820216,23
1649,"In high dimensions , our approach yields new state-of-the-art MRRs of 49.6 % on WN18 RR and 57.7 % on YAGO3-10 .",3,0.91791725,38.77034754590031,28
1650,Effective projection-based cross-lingual word embedding ( CLWE ) induction critically relies on the iterative self-learning procedure .,0,0.8720618,42.13108533259994,20
1650,It gradually expands the initial small seed dictionary to learn improved cross-lingual mappings .,3,0.4245617,93.06999999960355,14
1650,"In this work , we present ClassyMap , a classification-based approach to self-learning , yielding a more robust and a more effective induction of projection-based CLWEs .",1,0.7213787,74.46487442005093,31
1650,"Unlike prior self-learning methods , our approach allows for integration of diverse features into the iterative process .",3,0.50536287,36.76934803027349,19
1650,We show the benefits of ClassyMap for bilingual lexicon induction : we report consistent improvements in a weakly supervised setup ( 500 seed translation pairs ) on a benchmark with 28 language pairs .,3,0.8585232,103.42057831847427,34
1651,Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines .,0,0.9497726,60.9303952402445,20
1651,"This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages , gender bias included .",0,0.7549105,41.99035171551807,29
1651,"Exclusively fed with textual data , machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities .",0,0.9160468,46.82271981494156,33
1651,"We present the first thorough investigation of gender bias in speech translation , contributing with : i ) the release of a benchmark useful for future studies , and ii ) the comparison of different technologies ( cascade and end-to-end ) on two language directions ( English-Italian / French ) .",1,0.4101777,65.60302996618692,53
1652,Neural machine translation ( NMT ) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages .,0,0.94997585,45.78240037318536,30
1652,The keys lie in the assessment of data difficulty and model competence .,0,0.757001,135.48444934387481,13
1652,"We propose uncertainty-aware curriculum learning , which is motivated by the intuition that : 1 ) the higher the uncertainty in a translation pair , the more complex and rarer the information it contains ;",2,0.52144545,58.981440351036355,37
1652,and 2 ) the end of the decline in model uncertainty indicates the completeness of current training stage .,3,0.65935546,150.42497438710225,19
1652,"Specifically , we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty .",2,0.72127676,76.4900074302281,30
1652,Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed .,3,0.9348057,17.745877415562312,25
1652,Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule .,3,0.9716219,83.55922868925971,18
1653,"Exploiting natural language processing in the clinical domain requires de-identification , i.e. , anonymization of personal information in texts .",0,0.89449126,39.03576596905836,20
1653,"However , current research considers de-identification and downstream tasks , such as concept extraction , only in isolation and does not study the effects of de-identification on other tasks .",0,0.8787717,31.555883508142234,30
1653,"In this paper , we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction .",1,0.90716535,53.24488140810964,27
1653,"In particular , we propose a stacked model with restricted access to privacy sensitive information and a multitask model .",2,0.6707746,63.67389954289915,20
1653,We set the new state of the art on benchmark datasets in English ( 96.1 % F1 for de-identification and 88.9 % F1 for concept extraction ) and Spanish ( 91.4 % F1 for concept extraction ) .,2,0.7027899,11.891247912489742,38
1654,"In this paper , we present CorefQA , an accurate and extensible approach for the coreference resolution task .",1,0.88015735,23.717140514872014,19
1654,"A query is generated for each candidate mention using its surrounding context , and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query .",2,0.7769467,43.955441258372076,36
1654,The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage ;,3,0.76587796,267.169294367099,18
1654,"( 2 ) In the question answering framework , encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions ;",3,0.61876386,125.50301096801283,39
1654,and ( 3 ) A plethora of existing question answering datasets can be used for data augmentation to improve the model ’s generalization capability .,3,0.58823776,29.45585225354761,25
1654,"Experiments demonstrate significant performance boost over previous models , with 83.1 ( + 3.5 ) F1 score on the CoNLL-2012 benchmark and 87.5 ( + 2.5 ) F1 score on the GAP benchmark .",3,0.92311394,12.831667287393683,35
1655,The inability to correctly resolve rumours circulating online can have harmful real-world consequences .,0,0.9155935,72.33983396625426,14
1655,We present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification .,1,0.4589179,59.01367988456207,21
1655,We show that these estimates can be used to filter out model predictions likely to be erroneous so that these difficult instances can be prioritised by a human fact-checker .,3,0.8561931,28.163370975315697,30
1655,"We propose two methods for uncertainty-based instance rejection , supervised and unsupervised .",2,0.53861177,92.46447108302344,15
1655,We also show how uncertainty estimates can be used to interpret model performance as a rumour unfolds .,3,0.71256566,67.41502269788361,18
1656,Entity linking ( EL ) is concerned with disambiguating entity mentions in a text against knowledge bases ( KB ) .,0,0.9148096,59.59604804577361,21
1656,"It is crucial in a considerable number of fields like humanities , technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge .",0,0.931815,102.55558550399732,27
1656,"The use of EL in such domains requires handling noisy texts , low resource settings and domain-specific KBs .",0,0.66308904,180.12133287665714,19
1656,"Existing approaches are mostly inappropriate for this , as they depend on training data .",0,0.86851376,62.905554350290586,15
1656,"However , in the above scenario , there exists hardly annotated data , and it needs to be created from scratch .",0,0.8317464,53.37465083906903,22
1656,"We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach : we use recommenders that suggest potential concepts and adaptive candidate ranking , thereby speeding up the overall annotation process and making it less tedious for users .",2,0.5448156,55.77786391456818,38
1656,We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy .,3,0.73447865,39.589571617427595,24
1656,"In a user study , the annotation speed improves by 35 % compared to annotating without interactive support ;",3,0.89903325,135.4730149149099,19
1656,users report that they strongly prefer our system .,3,0.9268101,51.24276611476808,9
1656,An open-source and ready-to-use implementation based on the text annotation platform INCEpTION ( https://inception-project.github.io) is made available .,0,0.48310566,26.912044271419372,20
1657,Transfer learning using ImageNet pre-trained models has been the de facto approach in a wide range of computer vision tasks .,0,0.90836835,15.394704259316747,21
1657,"However , fine-tuning still requires task-specific training data .",0,0.7579518,22.304209529084016,10
1657,"In this paper , we propose N3 ( Neural Networks from Natural Language )-a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model .",1,0.8703814,41.198856122100224,33
1657,"N3 leverages language descriptions to generate parameter adaptations as well as a new task-specific classification layer for a pre-trained neural network , effectively “ fine-tuning ” the network for a new task using only language descriptions as input .",2,0.54891604,34.25095792232549,41
1657,"To the best of our knowledge , N3 is the first method to synthesize entire neural networks from natural language .",3,0.8204206,17.38238445746667,21
1657,Experimental results show that N3 can out-perform previous natural-language based zero-shot learning methods across 4 different zero-shot image classification benchmarks .,3,0.97225577,27.12551728587676,24
1657,We also demonstrate a simple method to help identify keywords in language descriptions leveraged by N3 when synthesizing model parameters .,3,0.8019729,112.49866991767752,21
1658,"Question-answer driven Semantic Role Labeling ( QA-SRL ) was proposed as an attractive open and natural flavour of SRL , potentially attainable from laymen .",0,0.90703297,103.73538886113845,29
1658,"Recently , a large-scale crowdsourced QA-SRL corpus and a trained parser were released .",0,0.920448,42.61653513766048,16
1658,"Trying to replicate the QA-SRL annotation for new texts , we found that the resulting annotations were lacking in quality , particularly in coverage , making them insufficient for further research and evaluation .",3,0.9748498,68.14753035593648,36
1658,"In this paper , we present an improved crowdsourcing protocol for complex semantic annotation , involving worker selection and training , and a data consolidation phase .",1,0.8962857,127.17982572322869,27
1658,"Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage , producing a new gold evaluation dataset .",3,0.9048844,167.9997350542114,22
1658,We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations .,3,0.971168,107.57438932040877,19
1659,Many efforts of research are devoted to semantic role labeling ( SRL ) which is crucial for natural language understanding .,0,0.9291595,35.48747343873756,21
1659,Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English .,0,0.8945394,25.99854738310695,20
1659,"While for the low-resource languages with no annotated SRL dataset , it is still challenging to obtain competitive performances .",0,0.6040445,31.834164111727535,20
1659,"Cross-lingual SRL is one promising way to address the problem , which has achieved great advances with the help of model transferring and annotation projection .",0,0.7905073,55.86951245279802,26
1659,"In this paper , we propose a novel alternative based on corpus translation , constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations .",1,0.8367255,52.14558789106331,31
1659,"Experimental results on Universal Proposition Bank show that the translation-based method is highly effective , and the automatic pseudo datasets can improve the target-language SRL performances significantly .",3,0.9673777,57.83732645332261,32
1660,We address the task of unsupervised Semantic Textual Similarity ( STS ) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings .,2,0.4559014,20.16072203022409,22
1660,"We apply , extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level , including dimensionality reduction ( Yin and Schütze , 2016 ) , generalized Canonical Correlation Analysis ( Rastogi et al. , 2015 ) and cross-view auto-encoders ( Bollegala and Bao , 2018 ) .",2,0.7143356,44.568936468878434,52
1660,"Our sentence meta-embeddings set a new unsupervised State of The Art ( SoTA ) on the STS Benchmark and on the STS12-STS16 datasets , with gains of between 3.7 % and 6.4 % Pearson ’s r over single-source systems .",3,0.83460385,39.427113335299026,43
1661,"Transition-based parsers implemented with Pointer Networks have become the new state of the art in dependency parsing , excelling in producing labelled syntactic trees and outperforming graph-based models in this task .",0,0.8964949,31.401496312524717,36
1661,"In order to further test the capabilities of these powerful neural networks on a harder NLP problem , we propose a transition system that , thanks to Pointer Networks , can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing .",2,0.5006074,54.998641400802164,43
1661,"In addition , we enhance our approach with deep contextualized word embeddings extracted from BERT .",2,0.5617219,23.4816383085561,16
1661,"The resulting system not only outperforms all existing transition-based models , but also matches the best fully-supervised accuracy to date on the SemEval 2015 Task 18 datasets among previous state-of-the-art graph-based parsers .",3,0.89186484,22.289893457190267,45
1662,Semantic similarity detection is a fundamental task in natural language understanding .,0,0.9383722,13.050412745500454,12
1662,Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks .,0,0.54394305,40.685347109787116,22
1662,There is currently no standard way of combining topics with pretrained contextual representations such as BERT .,0,0.8896585,25.41122757313658,17
1662,We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets .,3,0.6121418,20.615449954215737,33
1662,We find that the addition of topics to BERT helps particularly with resolving domain-specific cases .,3,0.98612404,67.28839015041075,16
1663,Aspect term extraction aims to extract aspect terms from review texts as opinion targets for sentiment analysis .,0,0.75719523,72.6563197716174,18
1663,One of the big challenges with this task is the lack of sufficient annotated data .,0,0.95380163,10.73599284148261,16
1663,"While data augmentation is potentially an effective technique to address the above issue , it is uncontrollable as it may change aspect words and aspect labels unexpectedly .",0,0.73568314,66.41375940122849,28
1663,"In this paper , we formulate the data augmentation as a conditional generation task : generating a new sentence while preserving the original opinion targets and labels .",1,0.49952093,58.175491903440935,28
1663,We propose a masked sequence-to-sequence method for conditional augmentation of aspect term extraction .,1,0.46608225,33.50781956628474,16
1663,"Unlike existing augmentation approaches , ours is controllable and allows to generate more diversified sentences .",3,0.57749236,56.66050611816542,16
1663,Experimental results confirm that our method alleviates the data scarcity problem significantly .,3,0.98089635,16.366207751744703,13
1663,It also effectively boosts the performances of several current models for aspect term extraction .,3,0.8389282,96.83251029824258,15
1664,"Predicting the persuasiveness of arguments has applications as diverse as writing assistance , essay scoring , and advertising .",0,0.879916,73.62565861701381,19
1664,"While clearly relevant to the task , the personal characteristics of an argument ’s source and audience have not yet been fully exploited toward automated persuasiveness prediction .",0,0.88892007,56.49846914113937,28
1664,"In this paper , we model debaters ’ prior beliefs , interests , and personality traits based on their previous activity , without dependence on explicit user profiles or questionnaires .",1,0.54437643,116.50526646138488,31
1664,"Using a dataset of over 60,000 argumentative discussions , comprising more than three million individual posts collected from the subreddit r/ChangeMyView , we demonstrate that our modeling of debater ’s characteristics enhances the prediction of argument persuasiveness as well as of debaters ’ resistance to persuasion .",3,0.68927735,36.29015656508587,47
1665,An educated and informed consumption of media content has become a challenge in modern times .,0,0.9570094,36.55753076753626,16
1665,"With the shift from traditional news outlets to social media and similar venues , a major concern is that readers are becoming encapsulated in “ echo chambers ” and may fall prey to fake news and disinformation , lacking easy access to dissenting views .",0,0.91465324,46.83141691609394,45
1665,We suggest a novel task aiming to alleviate some of these concerns – that of detecting articles that most effectively counter the arguments – and not just the stance – made in a given text .,1,0.41242698,64.86940894801096,36
1665,We study this problem in the context of debate speeches .,1,0.5402582,48.97633176908875,11
1665,"Given such a speech , we aim to identify , from among a set of speeches on the same topic and with an opposing stance , the ones that directly counter it .",1,0.4559142,50.75071259883576,33
1665,"We provide a large dataset of 3,685 such speeches ( in English ) , annotated for this relation , which hopefully would be of general interest to the NLP community .",3,0.5304821,48.88312240677099,31
1665,"We explore several algorithms addressing this task , and while some are successful , all fall short of expert human performance , suggesting room for further research .",3,0.6888513,98.76015979921733,28
1665,All data collected during this work is freely available for research .,3,0.49352393,26.50899051374387,12
1666,Neural network-based sequence-to-sequence ( seq2seq ) models strongly suffer from the low-diversity problem when it comes to open-domain dialogue generation .,0,0.9106879,18.84785701869289,24
1666,"As bland and generic utterances usually dominate the frequency distribution in our daily chitchat , avoiding them to generate more interesting responses requires complex data filtering , sampling techniques or modifying the training objective .",3,0.5250307,165.93839607229614,35
1666,"In this paper , we propose a new perspective to diversify dialogue generation by leveraging non-conversational text .",1,0.913839,27.658106590913487,18
1666,"Compared with bilateral conversations , non-conversational text are easier to obtain , more diverse and cover a much broader range of topics .",0,0.5306615,45.87989100038622,23
1666,"We collect a large-scale non-conversational corpus from multi sources including forum comments , idioms and book snippets .",2,0.84280497,71.84964777575259,18
1666,We further present a training paradigm to effectively incorporate these text via iterative back translation .,3,0.4589461,100.94755608005944,16
1666,The resulting model is tested on two conversational datasets from different domains and is shown to produce significantly more diverse responses without sacrificing the relevance with context .,3,0.747802,30.625581637653422,28
1667,The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations .,0,0.9498328,26.38140854196767,29
1667,"In this paper , we propose a Chinese multi-domain knowledge-driven conversation dataset , KdConv , which grounds the topics in multi-turn conversations to knowledge graphs .",1,0.8003899,52.37668361167857,26
1667,"Our corpus contains 4.5 K conversations from three domains ( film , music , and travel ) , and 86 K utterances with an average turn number of 19.0 .",2,0.52488834,76.30371032696604,30
1667,These conversations contain in-depth discussions on related topics and natural transition between multiple topics .,2,0.43574718,71.75364529863276,17
1667,"To facilitate the following research on this corpus , we provide several benchmark models .",2,0.3774465,137.1952804159103,15
1667,"Comparative results show that the models can be enhanced by introducing background knowledge , yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research .",3,0.9879451,43.280001040066445,32
1667,"Results also show that there are obvious performance differences between different domains , indicating that it is worth further explore transfer learning and domain adaptation .",3,0.9897683,34.27593885373372,26
1667,The corpus and benchmark models are publicly available .,2,0.3485293,38.87541342749684,9
1668,A Dialogue State Tracker ( DST ) is a core component of a modular task-oriented dialogue system .,0,0.7934854,28.388955950394354,20
1668,Tremendous progress has been made in recent years .,0,0.9311904,6.2728287988522995,9
1668,"However , the major challenges remain .",0,0.893793,78.95784696581094,7
1668,The state-of-the-art accuracy for DST is below 50 % for a multi-domain dialogue task .,3,0.6144381,17.560458531965953,19
1668,A learnable DST for any new domain requires a large amount of labeled in-domain data and training from scratch .,0,0.67948425,34.303334237080186,22
1668,"In this paper , we propose a Meta-Reinforced Multi-Domain State Generator ( MERET ) .",1,0.8291212,64.29725875958452,15
1668,Our first contribution is to improve the DST accuracy .,1,0.34964824,37.18512668969729,10
1668,"We enhance a neural model based DST generator with a reward manager , which is built on policy gradient reinforcement learning ( RL ) to fine-tune the generator .",2,0.79455507,65.21033878823769,31
1668,"With this change , we are able to improve the joint accuracy of DST from 48.79 % to 50.91 % on the MultiWOZ corpus .",3,0.9236152,22.056344063288822,25
1668,"Second , we explore to train a DST meta-learning model with a few domains as source domains and a new domain as target domain .",2,0.6894843,23.94216331004773,25
1668,We apply the model-agnostic meta-learning algorithm ( MAML ) to DST and the obtained meta-learning model is used for new domain adaptation .,2,0.81575227,27.907569297944335,24
1668,Our experimental results show this solution is able to outperform the traditional training approach with extremely less training data in target domain .,3,0.9775588,32.10573490911557,23
1669,"Based on the recently proposed transferable dialogue state generator ( TRADE ) that predicts dialogue states from utterance-concatenated dialogue context , we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation .",2,0.57695293,19.886592996356864,54
1669,"By enabling the model to learn a better representation of the long dialogue context , our approaches attempt to solve the problem that the performance of the baseline significantly drops when the input dialogue context sequence is long .",2,0.41648924,36.63265255855904,39
1669,"In our experiments , our proposed model achieves a 7.03 % relative improvement over the baseline , establishing a new state-of-the-art joint goal accuracy of 52.04 % on the MultiWOZ 2.0 dataset .",3,0.9243021,16.143296231379484,39
1670,Generating fluent and informative responses is of critical importance for task-oriented dialogue systems .,0,0.8884823,16.05143607691033,16
1670,Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation .,0,0.8483809,138.95520978983643,17
1670,There are at least two shortcomings with such approaches .,0,0.8294007,23.300852597915746,10
1670,"First , the inherent structures of multi-domain dialogue acts are neglected .",0,0.6295768,95.94792344238988,12
1670,"Second , the semantic associations between acts and responses are not taken into account for response generation .",0,0.47641465,52.07622354749782,18
1670,"To address these issues , we propose a neural co-generation model that generates dialogue acts and responses concurrently .",1,0.60380334,42.176039839373146,19
1670,"Unlike those pipeline approaches , our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed .",3,0.558132,95.49227124011269,30
1670,We train the two modules jointly using an uncertainty loss to adjust their task weights adaptively .,2,0.85044676,116.86865544132507,17
1670,Extensive experiments are conducted on the large-scale MultiWOZ dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations .,3,0.8990781,7.62005005064836,37
1671,Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data .,0,0.8491742,17.48790192207865,23
1671,"In current dominant approaches , owing to the lack of fine-grained control on the influence from the target style , they are unable to yield desirable output sentences .",0,0.8332473,44.00441964284377,30
1671,"In this paper , we propose a novel attentional sequence-to-sequence ( Seq2seq ) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer .",1,0.88626534,22.107944280284773,34
1671,"Specifically , we first pretrain a style classifier , where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation .",2,0.8894856,35.719591843034124,30
1671,"In a denoising auto-encoding manner , we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously .",2,0.84155285,69.14119662211994,28
1671,"In this way , this model is endowed with the ability to automatically predict the style relevance of each output word .",3,0.4779728,50.648305597568374,22
1671,"Then , we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer .",2,0.8149922,116.19214100004301,26
1671,"Particularly , we fine-tune this model using a carefully-designed objective function involving style transfer , style relevance consistency , content preservation and fluency modeling loss terms .",2,0.6550393,159.1036313535539,29
1671,Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation .,3,0.97328746,7.1039090205933935,26
1672,The graph-to-sequence ( Graph2Seq ) learning aims to transduce graph-structured representations to word sequences for text generation .,0,0.82013947,22.77598457365463,18
1672,Recent studies propose various models to encode graph structure .,0,0.93219364,88.63756228141334,10
1672,"However , most previous works ignore the indirect relations between distance nodes , or treat indirect relations and direct relations in the same way .",0,0.89478487,48.974335065460885,25
1672,"In this paper , we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph , including direct relations , indirect relations and multiple possible relations between nodes .",1,0.72666323,30.863404193824124,38
1672,Experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of AMR-to-text generation and syntax-based neural machine translation .,3,0.96766436,11.825204191163616,33
1673,The neural attention model has achieved great success in data-to-text generation tasks .,0,0.85639787,12.551228319486068,16
1673,"Though usually excelling at producing fluent text , it suffers from the problem of information missing , repetition and “ hallucination ” .",0,0.8840626,94.76970358015566,23
1673,"Due to the black-box nature of the neural attention architecture , avoiding these problems in a systematic way is non-trivial .",0,0.8439933,18.21931890870435,23
1673,"To address this concern , we propose to explicitly segment target text into fragment units and align them with their data correspondences .",1,0.41627,63.733588992365696,23
1673,The segmentation and correspondence are jointly learned as latent variables without any human annotations .,2,0.6097088,44.00140869362279,15
1673,We further impose a soft statistical constraint to regularize the segmental granularity .,2,0.7133234,85.23378168079213,13
1673,"The resulting architecture maintains the same expressive power as neural attention models , while being able to generate fully interpretable outputs with several times less computational cost .",3,0.82397276,45.16760924889912,28
1673,"On both E2E and WebNLG benchmarks , we show the proposed model consistently outperforms its neural attention counterparts .",3,0.9189574,58.68149799784093,19
1674,Visual question answering aims to answer the natural language question about a given image .,0,0.90134734,19.150775779388066,15
1674,Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question .,0,0.86917907,21.93081599402031,30
1674,"To simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question , we propose a novel dual channel graph convolutional network ( DC-GCN ) for better combining visual and textual advantages .",2,0.5726687,30.665204407774635,43
1674,"The DC-GCN model consists of three parts : an I-GCN module to capture the relations between objects in an image , a Q-GCN module to capture the syntactic dependency relations between words in a question , and an attention alignment module to align image representations and question representations .",2,0.67220455,18.22902125045795,53
1674,Experimental results show that our model achieves comparable performance with the state-of-the-art approaches .,3,0.96756035,3.9130614209682113,19
1675,"We introduce a new neural network architecture , Multimodal Neural Graph Memory Networks ( MN-GMN ) , for visual question answering .",1,0.44391745,39.62137456404331,24
1675,"The MN-GMN uses graph structure with different region features as node attributes and applies a recently proposed powerful graph neural network model , Graph Network ( GN ) , to reason about objects and their interactions in an image .",2,0.6225126,158.54154523868473,42
1675,The input module of the MN-GMN generates a set of visual features plus a set of encoded region-grounded captions ( RGCs ) for the image .,2,0.580891,58.908080954366014,28
1675,The RGCs capture object attributes and their relationships .,2,0.3690657,138.08623479611515,9
1675,Two GNs are constructed from the input module using the visual features and encoded RGCs .,2,0.69451576,90.25042190906288,16
1675,Each node of the GNs iteratively computes a question-guided contextualized representation of the visual / textual information assigned to it .,2,0.56121755,56.92709487546854,23
1675,"Then , to combine the information from both GNs , the nodes write the updated representations to an external spatial memory .",2,0.6310797,150.17750064371543,22
1675,The final states of the memory cells are fed into an answer module to predict an answer .,2,0.4957202,43.587503760467584,18
1675,"Experiments show MN-GMN rivals the state-of-the-art models on Visual7W , VQA-v2.0 , and CLEVR datasets .",3,0.92122436,44.36974047968881,24
1676,"We propose a novel large-scale referring expression recognition dataset , Refer360° , consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360° scenes .",2,0.64462,121.67638016274383,29
1676,Refer360° differs from existing related datasets in three ways .,0,0.5557519,140.5583133286319,10
1676,"First , we propose a more realistic scenario where instructors and the followers have partial , yet dynamic , views of the scene – followers continuously modify their field-of-view ( FoV ) while interpreting instructions that specify a final target location .",2,0.657585,100.434056895715,44
1676,"Second , instructions to find the target location consist of multiple steps for followers who will start at random FoVs .",2,0.35340023,238.9483239184059,21
1676,"As a result , intermediate instructions are strongly grounded in object references , and followers must identify intermediate FoVs to find the final target location correctly .",0,0.5623954,194.20868876963652,27
1676,"Third , the target locations are neither restricted to predefined objects nor chosen by annotators ;",3,0.41595587,133.60783505835104,16
1676,"instead , they are distributed randomly across scenes .",0,0.500014,109.9733156107576,9
1676,"This “ point anywhere ” approach leads to more linguistically complex instructions , as shown in our analyses .",3,0.961205,161.83864604465828,19
1676,"Our examination of the dataset shows that Refer360° manifests linguistically rich phenomena in a language grounding task that poses novel challenges for computational modeling of language , vision , and navigation .",3,0.9692629,149.73524395921362,32
1677,Pretrained language models are now ubiquitous in Natural Language Processing .,0,0.9354533,10.374997871202051,11
1677,"Despite their success , most available models have either been trained on English data or on the concatenation of data in multiple languages .",0,0.9060009,27.994481686926033,24
1677,This makes practical use of such models –in all languages except English– very limited .,0,0.74093467,112.09887926560114,15
1677,"In this paper , we investigate the feasibility of training monolingual Transformer-based language models for other languages , taking French as an example and evaluating our language models on part-of-speech tagging , dependency parsing , named entity recognition and natural language inference tasks .",1,0.8626642,17.127745161433587,47
1677,We show that the use of web crawled data is preferable to the use of Wikipedia data .,3,0.9607281,28.336879904023696,18
1677,"More surprisingly , we show that a relatively small web crawled dataset ( 4GB ) leads to results that are as good as those obtained using larger datasets ( 130 + GB ) .",3,0.97722185,48.48836219425647,34
1677,Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks .,3,0.9369621,45.118551731698325,19
1678,Advances in variational inference enable parameterisation of probabilistic models by deep neural networks .,0,0.87263954,26.79763472904426,14
1678,This combines the statistical transparency of the probabilistic modelling framework with the representational power of deep learning .,0,0.4754584,33.38200769737745,18
1678,"Yet , due to a problem known as posterior collapse , it is difficult to estimate such models in the context of language modelling effectively .",0,0.93360585,49.529175649338804,26
1678,"We concentrate on one such model , the variational auto-encoder , which we argue is an important building block in hierarchical probabilistic models of language .",1,0.34383413,30.689326077438388,26
1678,"This paper contributes a sober view of the problem , a survey of techniques to address it , novel techniques , and extensions to the model .",1,0.8174201,91.98219274737542,27
1678,"To establish a ranking of techniques , we perform a systematic comparison using Bayesian optimisation and find that many techniques perform reasonably similar , given enough resources .",2,0.53782886,68.76174538608888,28
1678,"Still , a favourite can be named based on convenience .",0,0.6177121,248.12181878019462,11
1678,We also make several empirical observations and recommendations of best practices that should help researchers interested in this exciting field .,3,0.7559233,47.69561923945307,21
1679,"The ability to control for the kinds of information encoded in neural representation has a variety of use cases , especially in light of the challenge of interpreting these models .",0,0.854624,33.75360797628013,31
1679,"We present Iterative Null-space Projection ( INLP ) , a novel method for removing information from neural representations .",1,0.5281511,61.17619156782223,19
1679,"Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove , followed by projection of the representations on their null-space .",2,0.7732457,84.45954152271788,30
1679,"By doing so , the classifiers become oblivious to that target property , making it hard to linearly separate the data according to it .",0,0.7349135,74.61301734157817,25
1679,"While applicable for multiple uses , we evaluate our method on bias and fairness use-cases , and show that our method is able to mitigate bias in word embeddings , as well as to increase fairness in a setting of multi-class classification .",3,0.71427214,29.884317975932277,43
1680,Simplified Chinese to Traditional Chinese character conversion is a common preprocessing step in Chinese NLP .,0,0.87109107,35.4943274076567,16
1680,"Despite this , current approaches have insufficient performance because they do not take into account that a simplified Chinese character can correspond to multiple traditional characters .",0,0.86106616,46.930458815757085,27
1680,"Here , we propose a model that can disambiguate between mappings and convert between the two scripts .",1,0.84280336,23.378322767313968,18
1680,"The model is based on subword segmentation , two language models , as well as a method for mapping between subword sequences .",2,0.73398876,33.23426046244777,23
1680,We further construct benchmark datasets for topic classification and script conversion .,2,0.61697155,154.3606671459294,12
1680,Our proposed method outperforms previous Chinese Character conversion approaches by 6 points in accuracy .,3,0.9002858,59.7403850871699,15
1680,"These results are further confirmed in a downstream application , where 2 kenize is used to convert pretraining dataset for topic classification .",3,0.9777454,131.37408925811306,23
1680,An error analysis reveals that our method ’s particular strengths are in dealing with code mixing and named entities .,3,0.95481515,65.57919744088966,20
1681,"We present the first study that examines the evolution of morphological families , i.e. , sets of morphologically related words such as “ trump ” , “ antitrumpism ” , and “ detrumpify ” , in social media .",1,0.66113967,29.279135463126632,39
1681,We introduce the novel task of Morphological Family Expansion Prediction ( MFEP ) as predicting the increase in the size of a morphological family .,1,0.5476688,66.02776910089449,25
1681,We create a ten-year Reddit corpus as a benchmark for MFEP and evaluate a number of baselines on this benchmark .,2,0.791305,59.02896183696626,21
1681,Our experiments demonstrate very good performance on MFEP .,3,0.9759721,188.57292215021576,9
1682,"Historical text normalization , the task of mapping historical word forms to their modern counterparts , has recently attracted a lot of interest ( Bollmann , 2019 ; Tang et al. , 2018 ; Lusetti et al. , 2018 ; Bollmann et al. , 2018 ; Robertson and Goldwater , 2018 ; Bollmannet al. , 2017 ; Korchagina , 2017 ) .",0,0.8761867,42.35794374792443,62
1682,"Yet , virtually all approaches suffer from the two limitations : 1 ) They consider a fully supervised setup , often with impractically large manually normalized datasets ;",0,0.85287553,148.77444881029035,28
1682,2 ) Normalization happens on words in isolation .,3,0.42535958,200.92654931438562,9
1682,"By utilizing a simple generative normalization model and obtaining powerful contextualization from the target-side language model , we train accurate models with unlabeled historical data .",2,0.7010224,79.59317352748279,28
1682,"In realistic training scenarios , our approach often leads to reduction in manually normalized data at the same accuracy levels .",3,0.8322627,96.78464038200276,21
1683,Question answering and conversational systems are often baffled and need help clarifying certain ambiguities .,0,0.8515896,62.35821811813867,15
1683,"However , limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions .",0,0.91791433,45.04082340782538,20
1683,"In order to overcome these limitations , we devise a novel bootstrapping framework ( based on self-supervision ) that assists in the creation of a diverse , large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange .",2,0.6841092,29.890738269438657,40
1683,The framework utilises a neural network based architecture for classifying clarification questions .,2,0.6596777,60.42818822113047,13
1683,It is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall .,0,0.66692793,14.732589991115365,24
1683,We quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering .,3,0.7886356,12.394391855279698,22
1683,"The final dataset , ClarQ , consists of ~2 M examples distributed across 173 domains of stackexchange .",2,0.614226,193.91313059087486,19
1683,We release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems .,3,0.5148197,54.88057046048581,28
1684,The goal of this work is to build conversational Question Answering ( QA ) interfaces for the large body of domain-specific information available in FAQ sites .,1,0.8937821,28.611749722631878,27
1684,"We present DoQA , a dataset with 2,437 dialogues and 10,917 QA pairs .",2,0.5853101,27.485024968087185,14
1684,The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing .,2,0.89316505,43.92612857344621,18
1684,"Compared to previous work , DoQA comprises well-defined information needs , leading to more coherent and natural conversations with less factoid questions and is multi-domain .",3,0.6548926,102.39194235249285,28
1684,"In addition , we introduce a more realistic information retrieval ( IR ) scenario where the system needs to find the answer in any of the FAQ documents .",2,0.5948316,58.275025700692325,29
1684,"The results of an existing , strong , system show that , thanks to transfer learning from a Wikipedia QA dataset and fine tuning on a single FAQ domain , it is possible to build high quality conversational QA systems for FAQs without in-domain training data .",3,0.9757086,57.7577331574152,49
1684,The good results carry over into the more challenging IR scenario .,3,0.97743064,72.06825382957074,12
1684,"In both cases , there is still ample room for improvement , as indicated by the higher human upperbound .",3,0.84685975,55.11418055857291,20
1685,"Question answering ( QA ) models have shown rapid progress enabled by the availability of large , high-quality benchmark datasets .",0,0.9679863,33.66724839129634,21
1685,"Such annotated datasets are difficult and costly to collect , and rarely exist in languages other than English , making building QA systems that work well in other languages challenging .",0,0.898493,28.98948986343217,31
1685,"In order to develop such systems , it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress .",0,0.87195134,45.87219085925702,22
1685,"We present MLQA , a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area .",1,0.6910677,51.703545705051106,19
1685,"MLQA contains QA instances in 7 languages , English , Arabic , German , Spanish , Hindi , Vietnamese and Simplified Chinese .",0,0.49562448,36.30672940222552,23
1685,"MLQA has over 12 K instances in English and 5 K in each other language , with each instance parallel between 4 languages on average .",3,0.60106134,48.56262602584736,26
1685,We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA .,2,0.70333743,13.22749743618384,19
1685,"In all cases , transfer results are shown to be significantly behind training-language performance .",3,0.95961195,104.2079116410319,17
1686,"Multiple-choice question answering ( MCQA ) is one of the most challenging tasks in machine reading comprehension since it requires more advanced reading comprehension skills such as logical reasoning , summarization , and arithmetic operations .",0,0.9650985,12.890254828490656,36
1686,"Unfortunately , most existing MCQA datasets are small in size , which increases the difficulty of model learning and generalization .",0,0.9088689,30.08460664287267,21
1686,"To address this challenge , we propose a multi-source meta transfer ( MMT ) for low-resource MCQA .",1,0.65856975,42.697561358270534,18
1686,"In this framework , we first extend meta learning by incorporating multiple training sources to learn a generalized feature representation across domains .",2,0.6989265,66.89712518687533,23
1686,"To bridge the distribution gap between training sources and the target , we further introduce the meta transfer that can be integrated into the multi-source meta training .",2,0.6579038,46.67552771384108,28
1686,"More importantly , the proposed MMT is independent of backbone language models .",3,0.84333664,136.34881875478828,13
1686,"Extensive experiments demonstrate the superiority of MMT over state-of-the-arts , and continuous improvements can be achieved on different backbone networks on both supervised and unsupervised domain adaptation settings .",3,0.9227473,25.273377953290662,32
1687,Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims .,0,0.82798374,39.4287865986326,27
1687,"This paper presents Kernel Graph Attention Network ( KGAT ) , which conducts more fine-grained fact verification with kernel-based attentions .",1,0.83481705,56.190652622128695,24
1687,"Given a claim and a set of potential evidence sentences that form an evidence graph , KGAT introduces node kernels , which better measure the importance of the evidence node , and edge kernels , which conduct fine-grained evidence propagation in the graph , into Graph Attention Networks for more accurate fact verification .",2,0.44937962,70.84684668437853,55
1687,"KGAT achieves a 70.38 % FEVER score and significantly outperforms existing fact verification models on FEVER , a large-scale benchmark for fact verification .",3,0.9141758,34.31330534111283,24
1687,"Our analyses illustrate that , compared to dot-product attentions , the kernel-based attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph , which is the main source of KGAT ’s effectiveness .",3,0.9739328,96.69579529759136,39
1687,All source codes of this work are available at https://github.com/thunlp/KernelGAT .,3,0.4682903,7.1059671689118655,11
1688,"Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata , social network spread , language used in claims , and , more recently , evidence supporting or denying claims .",0,0.90225387,81.37742655204481,39
1688,A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process – generating justifications for verdicts on claims .,0,0.8261758,34.67875734851416,31
1688,"This paper provides the first study of how these explanations can be generated automatically based on available claim context , and how this task can be modelled jointly with veracity prediction .",3,0.54998136,41.70939822393785,32
1688,"Our results indicate that optimising both objectives at the same time , rather than training them separately , improves the performance of a fact checking system .",3,0.9902401,38.375096625448066,27
1688,"The results of a manual evaluation further suggest that the informativeness , coverage and overall quality of the generated explanations are also improved in the multi-task model .",3,0.98481894,29.213569558156582,28
1689,"The discovery of supporting evidence for addressing complex mathematical problems is a semantically challenging task , which is still unexplored in the field of natural language processing for mathematical text .",0,0.9399862,35.27415017004878,31
1689,The natural language premise selection task consists in using conjectures written in both natural language and mathematical formulae to recommend premises that most likely will be useful to prove a particular statement .,0,0.55875987,58.89131387257205,33
1689,"We propose an approach to solve this task as a link prediction problem , using Deep Convolutional Graph Neural Networks .",1,0.37934145,34.97443636094903,21
1689,"This paper also analyses how different baselines perform in this task and shows that a graph structure can provide higher F1-score , especially when considering multi-hop premise selection .",3,0.68612707,46.43355985224365,31
1690,"We review motivations , definition , approaches , and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them .",1,0.71692616,64.72122788546095,26
1690,An existing rationale for such research is based on the lack of parallel data for many of the world ’s languages .,0,0.93079525,27.70746865760773,22
1690,"However , we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice .",3,0.67575425,29.203534633850584,20
1690,"We also discuss different training signals that have been used in previous work , which depart from the pure unsupervised setting .",3,0.3778294,43.077239261307184,22
1690,We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices .,1,0.51776624,34.13173720026036,19
1690,"Finally , we provide a unified outlook for different types of research in this area ( i.e. , cross-lingual word embeddings , deep multilingual pretraining , and unsupervised machine translation ) and argue for comparable evaluation of these models .",3,0.65667176,31.878352535172322,40
1691,Measuring what linguistic information is encoded in neural models of language has become popular in NLP .,0,0.94389987,30.15480902459839,17
1691,Researchers approach this enterprise by training “ probes ” — supervised models designed to extract linguistic structure from another model ’s output .,0,0.8878014,95.39706105743366,23
1691,"One such probe is the structural probe ( Hewitt and Manning , 2019 ) , designed to quantify the extent to which syntactic information is encoded in contextualised word representations .",0,0.9125505,36.768865876170594,31
1691,"The structural probe has a novel design , unattested in the parsing literature , the precise benefit of which is not immediately obvious .",3,0.50468457,125.41477136647549,24
1691,"To explore whether syntactic probes would do better to make use of existing techniques , we compare the structural probe to a more traditional parser with an identical lightweight parameterisation .",2,0.45154008,89.09164047717815,31
1691,"The parser outperforms structural probe on UUAS in seven of nine analysed languages , often by a substantial amount ( e.g .",3,0.9193898,179.45649721030028,22
1691,by 11.1 points in English ) .,3,0.8160388,71.4386728511016,7
1691,"Under a second less common metric , however , there is the opposite trend — the structural probe outperforms the parser .",3,0.91687423,145.4709160900565,22
1691,This begs the question : which metric should we prefer ? .,0,0.9013351,89.3577633412834,12
1692,"It has been exactly a decade since the first establishment of SPMRL , a research initiative unifying multiple research efforts to address the peculiar challenges of Statistical Parsing for Morphologically-Rich Languages ( MRLs ) .",0,0.9312644,65.02301396026157,37
1692,"Here we reflect on parsing MRLs in that decade , highlight the solutions and lessons learned for the architectural , modeling and lexical challenges in the pre-neural era , and argue that similar challenges re-emerge in neural architectures for MRLs .",1,0.69409585,54.32582145442746,41
1692,"We then aim to offer a climax , suggesting that incorporating symbolic ideas proposed in SPMRL terms into nowadays neural architectures has the potential to push NLP for MRLs to a new level .",1,0.5274409,168.97926261544882,34
1692,"We sketch a strategies for designing Neural Models for MRLs ( NMRL ) , and showcase preliminary support for these strategies via investigating the task of multi-tagging in Hebrew , a morphologically-rich , high-fusion , language .",1,0.40828413,136.18708946765526,39
1693,"Over its three decade history , speech translation has experienced several shifts in its primary research themes ;",0,0.9571452,223.53336351600996,18
1693,"moving from loosely coupled cascades of speech recognition and machine translation , to exploring questions of tight coupling , and finally to end-to-end models that have recently attracted much attention .",0,0.6866439,42.74097010853985,33
1693,"This paper provides a brief survey of these developments , along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer , and from training cascaded models separately towards different objectives .",1,0.8120017,76.89135091079406,43
1693,Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations .,0,0.9169644,29.482480779638802,30
1693,"However , a closer look reveals that many end-to-end models fall short of solving these issues , due to compromises made to address data scarcity .",0,0.67740047,35.899437137314855,28
1693,This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions .,3,0.5408616,38.599285523893506,29
1694,"In addition to the traditional task of machines answering questions , question answering ( QA ) research creates interesting , challenging questions that help systems how to answer questions and reveal the best systems .",0,0.9620933,85.87442928018685,35
1694,"We argue that creating a QA dataset — and the ubiquitous leaderboard that goes with it — closely resembles running a trivia tournament : you write questions , have agents ( either humans or machines ) answer the questions , and declare a winner .",3,0.49477124,95.99556261324662,45
1694,"However , the research community has ignored the hard-learned lessons from decades of the trivia community creating vibrant , fair , and effective question answering competitions .",0,0.913758,120.13838825945476,28
1694,"After detailing problems with existing QA datasets , we outline the key lessons — removing ambiguity , discriminating skill , and adjudicating disputes — that can transfer to QA research and how they might be implemented .",1,0.43463767,81.52800795713908,37
1695,"Distributional semantic models have become a mainstay in NLP , providing useful features for downstream tasks .",0,0.9498364,22.61945048004566,17
1695,"However , assessing long-term progress requires explicit long-term goals .",0,0.9362319,55.58070418017466,10
1695,"In this paper , I take a broad linguistic perspective , looking at how well current models can deal with various semantic challenges .",1,0.83210593,53.26715233845575,24
1695,"Given stark differences between models proposed in different subfields , a broad perspective is needed to see how we could integrate them .",0,0.4748641,53.92403226989883,23
1695,"I conclude that , while linguistic insights can guide the design of model architectures , future progress will require balancing the often conflicting demands of linguistic expressiveness and computational tractability .",3,0.9713249,49.383776706273736,31
1696,Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community .,0,0.9682133,19.173266905505496,22
1696,"In this paper , we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation .",1,0.9116749,35.39733597439759,30
1696,Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning .,2,0.75412506,103.77813532929252,21
1696,The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features .,2,0.4420452,117.23345736574345,17
1696,"During generation , the model further incorporates visual relationships using multi-task learning for jointly predicting word and object / predicate tag sequences .",2,0.66405225,137.17879561483804,23
1696,"We perform extensive experiments on the MSCOCO dataset , showing that the proposed framework significantly outperforms the baselines , resulting in the state-of-the-art performance under a wide range of evaluation metrics .",3,0.7978807,9.541286669635122,38
1696,The code of our paper has been made publicly available .,3,0.5752573,15.372246897085196,11
1697,The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language Generation shared tasks with the goal of exploring approaches to surface realization from Universal-Dependency-like trees to surface strings for several languages .,0,0.68865347,131.02300083180347,38
1697,"In the 2018 shared task there was very little difference in the absolute performance of systems trained with and without additional , synthetically created data , and a new rule prohibiting the use of synthetic data was introduced for the 2019 shared task .",3,0.6001394,38.55109303104527,44
1697,"Contrary to the findings of the 2018 shared task , we show , in experiments on the English 2018 dataset , that the use of synthetic data can have a substantial positive effect – an improvement of almost 8 BLEU points for a previously state-of-the-art system .",3,0.9558631,28.62938209738877,53
1697,"We analyse the effects of synthetic data , and we argue that its use should be encouraged rather than prohibited so that future research efforts continue to explore systems that can take advantage of such data .",3,0.5875951,25.021032904453474,37
1698,We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives .,2,0.5245854,31.143017093289767,24
1698,"Since it does not need to model fluency , the sentence-level language model can focus on longer range dependencies , which are crucial for multi-sentence coherence .",3,0.596312,33.89605854056799,29
1698,"Rather than dealing with individual words , our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence , which is more efficient than predicting word embeddings .",2,0.5407783,21.638122546652863,39
1698,Notably this allows us to consider a large number of candidates for the next sentence during training .,3,0.77462536,33.18775734216493,18
1698,We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks .,3,0.8990518,17.141516418186374,33
1699,"In this work , we explore the implicit event argument detection task , which studies event arguments beyond sentence boundaries .",1,0.8685084,125.160510341778,21
1699,The addition of cross-sentence argument candidates imposes great challenges for modeling .,0,0.7606201,92.5970138375893,12
1699,"To reduce the number of candidates , we adopt a two-step approach , decomposing the problem into two sub-problems : argument head-word detection and head-to-span expansion .",2,0.7766525,34.84645745657831,31
1699,"Evaluated on the recent RAMS dataset ( Ebner et al. , 2020 ) , our model achieves overall better performance than a strong sequence labeling baseline .",3,0.90800905,58.64560868862926,27
1699,"We further provide detailed error analysis , presenting where the model mainly makes errors and indicating directions for future improvements .",3,0.7214189,90.10117182157862,21
1699,"It remains a challenge to detect implicit arguments , calling for more future work of document-level modeling for this task .",0,0.558186,60.627739786332434,23
1700,Machine reading is an ambitious goal in NLP that subsumes a wide range of text understanding capabilities .,0,0.9530674,38.226297427048145,18
1700,"Within this broad framework , we address the task of machine reading the time of historical events , compile datasets for the task , and develop a model for tackling it .",1,0.37819922,68.29521870633262,32
1700,"Given a brief textual description of an event , we show that good performance can be achieved by extracting relevant sentences from Wikipedia , and applying a combination of task-specific and general-purpose feature embeddings for the classification .",3,0.66404676,21.79894849353582,41
1700,"Furthermore , we establish a link between the historical event ordering task and the event focus time task from the information retrieval literature , showing they also provide a challenging test case for machine reading algorithms .",3,0.8062662,79.63671750533034,37
1701,Unsupervised relation extraction ( URE ) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases ( KBs ) .,0,0.76961195,56.218953990383035,27
1701,"URE methods can be categorised into generative and discriminative approaches , which rely either on hand-crafted features or surface form .",0,0.7687694,40.34779340794957,22
1701,"However , we demonstrate that by using only named entities to induce relation types , we can outperform existing methods on two popular datasets .",3,0.9297735,30.45486339867928,25
1701,"We conduct a comparison and evaluation of our findings with other URE techniques , to ascertain the important features in URE .",1,0.60726154,69.474192507511,22
1701,We conclude that entity types provide a strong inductive bias for URE .,3,0.9883931,101.73648385556085,13
1702,"Extracting information from full documents is an important problem in many domains , but most previous work focus on identifying relationships within a sentence or a paragraph .",0,0.93230075,31.634828467421173,28
1702,It is challenging to create a large-scale information extraction ( IE ) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections .,0,0.94285613,36.850413382394485,44
1702,"In this paper , we introduce SciREX , a document level IE dataset that encompasses multiple IE tasks , including salient entity identification and document level N-ary relation identification from scientific articles .",1,0.77565706,107.54320619749761,33
1702,"We annotate our dataset by integrating automatic and human annotations , leveraging existing scientific knowledge resources .",2,0.76838607,55.999483330248324,17
1702,We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE .,2,0.61721367,23.367896463601753,26
1702,"Analyzing the model performance shows a significant gap between human performance and current baselines , inviting the community to use our dataset as a challenge to develop document-level IE models .",3,0.9500203,43.93761874324041,33
1702,Our data and code are publicly available at https://github.com/allenai/SciREX .,3,0.59666425,11.507956917831997,10
1703,We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems .,2,0.34478882,18.104440953241582,15
1703,"Our approach exploits the characteristic structure of training corpora related to so-called “ trigger ” words , which are responsible for flipping the answer in pronoun disambiguation .",2,0.60977,65.86312191589549,28
1703,We achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions .,2,0.5307795,98.52431997683016,12
1703,"To this end , we leverage a mutual exclusive loss regularized by a contrastive margin .",2,0.8619109,96.87892562468932,16
1703,"Our architecture is based on the recently introduced transformer networks , BERT , that exhibits strong performance on many NLP benchmarks .",2,0.594037,44.92745963537556,22
1703,Empirical results show that our method alleviates the limitation of current supervised approaches for commonsense reasoning .,3,0.97518706,13.369638402240188,17
1703,This study opens up avenues for exploiting inexpensive self-supervision to achieve performance gain in commonsense reasoning tasks .,3,0.9748514,40.88948291603776,18
1704,Deep attention models have advanced the modelling of sequential data across many domains .,0,0.93662876,52.274722038585416,14
1704,"For language modelling in particular , the Transformer-XL — a Transformer augmented with a long-range memory of past activations — has been shown to be state-of-the-art across a variety of well-studied benchmarks .",0,0.84292257,19.107275697165566,44
1704,"The Transformer-XL incorporates a long-range memory at every layer of the network , which renders its state to be thousands of times larger than RNN predecessors .",3,0.41576618,46.226371789425784,29
1704,However it is unclear whether this is necessary .,0,0.924597,17.039587970483836,9
1704,We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network .,3,0.57339233,37.847509838471275,39
1705,"Learning disentangled representations of natural language is essential for many NLP tasks , e.g. , conditional text generation , style transfer , personalized dialogue systems , etc .",0,0.885425,57.92909931600719,28
1705,"Similar problems have been studied extensively for other forms of data , such as images and videos .",0,0.87464106,26.82586364938056,18
1705,"However , the discrete nature of natural language makes the disentangling of textual representations more challenging ( e.g. , the manipulation over the data space cannot be easily achieved ) .",0,0.9088297,38.74775360040474,31
1705,"Inspired by information theory , we propose a novel method that effectively manifests disentangled representations of text , without any supervision on semantics .",2,0.44385058,40.389073247335325,24
1705,A new mutual information upper bound is derived and leveraged to measure dependence between style and content .,2,0.50772923,72.33331483337676,18
1705,"By minimizing this upper bound , the proposed method induces style and content embeddings into two independent low-dimensional spaces .",2,0.47909492,66.6360320408214,20
1705,Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation .,3,0.92817724,27.786350380599913,25
1706,We consider a task based on CVPR 2018 challenge dataset on advertisement ( Ad ) understanding .,2,0.69013554,558.3912551329855,17
1706,The task involves detecting the viewer ’s interpretation of an Ad image captured as text .,2,0.471898,223.8375621824515,16
1706,Recent results have shown that the embedded scene-text in the image holds a vital cue for this task .,0,0.8828702,75.3862338476431,19
1706,"Motivated by this , we fine-tune the base BERT model for a sentence-pair classification task .",2,0.6227299,21.517729433983693,19
1706,"Despite utilizing the scene-text as the only source of visual information , we could achieve a hit-or-miss accuracy of 84.95 % on the challenge test data .",3,0.9653314,32.56327282664458,29
1706,"To enable BERT to process other visual information , we append image captions to the scene-text .",2,0.6568225,76.22748662877765,17
1706,"This achieves an accuracy of 89.69 % , which is an improvement of 4.7 % .",3,0.90214634,24.324161242417006,16
1706,This is the best reported result for this task .,3,0.8851675,25.92546417316283,10
1707,"We present InstaMap , an instance-based method for learning projection-based cross-lingual word embeddings .",1,0.38637155,18.668474623598083,18
1707,"Unlike prior work , it deviates from learning a single global linear projection .",0,0.4470404,120.05437842746326,14
1707,"InstaMap is a non-parametric model that learns a non-linear projection by iteratively : ( 1 ) finding a globally optimal rotation of the source embedding space relying on the Kabsch algorithm , and then ( 2 ) moving each point along an instance-specific translation vector estimated from the translation vectors of the point ’s nearest neighbours in the training dictionary .",2,0.52659166,46.05186129323125,62
1707,We report performance gains with InstaMap over four representative state-of-the-art projection-based models on bilingual lexicon induction across a set of 28 diverse language pairs .,3,0.58667636,36.13034771732236,32
1707,"We note prominent improvements , especially for more distant language pairs ( i.e. , languages with non-isomorphic monolingual spaces ) .",3,0.9652924,69.44709918877044,21
1708,We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models .,1,0.56676775,39.04199274952921,21
1708,"Under this protocol , synthetic training examples are constructed by taking real training examples and replacing ( possibly discontinuous ) fragments with other fragments that appear in at least one similar environment .",2,0.5886438,99.0075163780977,33
1708,The protocol is model-agnostic and useful for a variety of tasks .,3,0.5241848,14.429958785981158,13
1708,"Applied to neural sequence-to-sequence models , it reduces error rate by as much as 87 % on diagnostic tasks from the SCAN dataset and 16 % on a semantic parsing task .",3,0.85365444,28.67535644827854,34
1708,"Applied to n-gram language models , it reduces perplexity by roughly 1 % on small corpora in several languages .",3,0.7427671,50.13998350408124,20
1709,"When translating natural language questions into SQL queries to answer questions from a database , contemporary semantic parsing models struggle to generalize to unseen database schemas .",0,0.9114238,37.85531601371871,27
1709,"The generalization challenge lies in ( a ) encoding the database relations in an accessible way for the semantic parser , and ( b ) modeling alignment between database columns and their mentions in a given query .",0,0.716524,72.24314207474279,38
1709,"We present a unified framework , based on the relation-aware self-attention mechanism , to address schema encoding , schema linking , and feature representation within a text-to-SQL encoder .",1,0.3476339,42.754871858080975,35
1709,"On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2 % , surpassing its best counterparts by 8.7 % absolute improvement .",3,0.9098856,69.99578461474918,26
1709,"Further augmented with BERT , it achieves the new state-of-the-art performance of 65.6 % on the Spider leaderboard .",3,0.88816047,16.709144020682327,25
1709,"In addition , we observe qualitative improvements in the model ’s understanding of schema linking and alignment .",3,0.9570135,93.75242397918039,18
1709,Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql .,3,0.8364717,11.083325381965459,8
1710,"Temporal common sense ( e.g. , duration and frequency of events ) is crucial for understanding natural language .",0,0.9323433,35.35458258414246,19
1710,"However , its acquisition is challenging , partly because such information is often not expressed explicitly in text , and human annotation on such concepts is costly .",0,0.9442401,92.52321827777735,28
1710,"This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense , extracted from a large corpus , to build TacoLM , a temporal common sense language model .",1,0.61841655,68.3595988512902,36
1710,Our method is shown to give quality predictions of various dimensions of temporal common sense ( on UDST and a newly collected dataset from RealNews ) .,3,0.75101614,184.9571625884927,27
1710,"It also produces representations of events for relevant tasks such as duration comparison , parent-child relations , event coreference and temporal QA ( on TimeBank , HiEVE and MCTACO ) that are better than using the standard BERT .",3,0.5975402,130.84518813926397,39
1710,"Thus , it will be an important component of temporal NLP .",3,0.7156627,58.31605477910069,12
1711,"Large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the Winograd Schema Challenge , a widely employed test of commonsense reasoning ability .",0,0.7541973,27.430440822187528,29
1711,"We show , however , with a new diagnostic dataset , that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding .",3,0.9210681,96.95753599798222,29
1711,"Our results highlight interesting differences between humans and language models : language models are more sensitive to number or gender alternations and synonym replacements than humans , and humans are more stable and consistent in their predictions , maintain a much higher absolute performance , and perform better on non-associative instances than associative ones .",3,0.9854858,47.501131709710606,55
1712,Natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text .,0,0.94272095,28.62186107964445,32
1712,"However , evaluation results on existing data sets are seldom reported by taking the timestamp of the document into account .",0,0.83604616,74.20141345942557,21
1712,"We analyze and propose methods that make better use of temporally-diverse training data , with a focus on the task of named entity recognition .",1,0.6039159,23.131532448173925,27
1712,"To support these experiments , we introduce a novel data set of English tweets annotated with named entities .",2,0.66844,24.208981704184186,19
1712,"We empirically demonstrate the effect of temporal drift on performance , and how the temporal information of documents can be used to obtain better models compared to those that disregard temporal information .",3,0.7275178,30.455887217629467,33
1712,"Our analysis gives insights into why this information is useful , in the hope of informing potential avenues of improvement for named entity recognition as well as other NLP tasks under similar experimental setups .",3,0.95716006,34.67710377592831,35
1713,We tackle the task of building supervised event trigger identification models which can generalize better across domains .,1,0.36494374,51.96954347274102,18
1713,Our work leverages the adversarial domain adaptation ( ADA ) framework to introduce domain-invariance .,2,0.5981064,42.86945799121229,15
1713,"ADA uses adversarial training to construct representations that are predictive for trigger identification , but not predictive of the example ’s domain .",2,0.4771114,96.50735072615905,23
1713,"It requires no labeled data from the target domain , making it completely unsupervised .",0,0.5435807,29.266747360063498,15
1713,Experiments with two domains ( English literature and news ) show that ADA leads to an average F1 score improvement of 3.9 on out-of-domain data .,3,0.88950986,27.62439778310791,27
1713,"Our best performing model ( BERT-A ) reaches 44-49 F1 across both domains , using no labeled target data .",3,0.92412937,91.21038961401801,23
1713,"Preliminary experiments reveal that finetuning on 1 % labeled data , followed by self-training leads to substantial improvement , reaching 51.5 and 67.2 F1 on literature and news respectively .",3,0.93779653,48.97437009464755,30
1714,"Approaches to Grounded Language Learning are commonly focused on a single task-based final performance measure which may not depend on desirable properties of the learned hidden representations , such as their ability to predict object attributes or generalize to unseen situations .",0,0.8869921,51.052653077048944,44
1714,"To remedy this , we present GroLLA , an evaluation framework for Grounded Language Learning with Attributes based on three sub-tasks : 1 ) Goal-oriented evaluation ;",2,0.4835897,83.00026048667125,27
1714,2 ) Object attribute prediction evaluation ;,2,0.6530023,13654.414583139951,7
1714,and 3 ) Zero-shot evaluation .,2,0.74552375,148.8243997884374,6
1714,"as an instance of this framework for evaluating the quality of learned neural representations , in particular with respect to attribute grounding .",2,0.41901737,56.6585068336697,23
1714,dataset by including a semantic layer on top of the perceptual one .,2,0.64513296,41.8232564208778,13
1714,images with several attributes from resources such as VISA and ImSitu .,2,0.3702983,166.61463243253314,12
1714,We then compare several hidden state representations from current state-of-the-art approaches to Grounded Language Learning .,2,0.68410194,28.306979950641583,21
1714,"By using diagnostic classifiers , we show that current models ’ learned representations are not expressive enough to encode object attributes ( average F1 of 44.27 ) .",3,0.85019994,85.42452746844478,28
1714,"In addition , they do not learn strategies nor representations that are robust enough to perform well when novel scenes or objects are involved in gameplay ( zero-shot best accuracy 50.06 % ) .",3,0.7869367,108.49676656002316,34
1715,This work deals with the challenge of learning and reasoning over language and vision data for the related downstream tasks such as visual question answering ( VQA ) and natural language for visual reasoning ( NLVR ) .,1,0.79485375,27.61588976235591,38
1715,"We design a novel cross-modality relevance module that is used in an end-to-end framework to learn the relevance representation between components of various input modalities under the supervision of a target task , which is more generalizable to unobserved data compared to merely reshaping the original representation space .",2,0.7655343,28.08134337937059,51
1715,"In addition to modeling the relevance between the textual entities and visual entities , we model the higher-order relevance between entity relations in the text and object relations in the image .",2,0.68480504,29.35660475609236,34
1715,Our proposed approach shows competitive performance on two different language and vision tasks using public benchmarks and improves the state-of-the-art published results .,3,0.92713225,26.643070969916938,29
1715,The learned alignments of input spaces and their relevance representations by NLVR task boost the training efficiency of VQA task .,3,0.7249926,102.07497847617522,21
1716,We explore learning web-based tasks from a human teacher through natural language explanations and a single demonstration .,2,0.70869607,76.09101279005658,18
1716,"Our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context , rather than mapping explanations to demonstrations .",2,0.41064194,115.02649429815806,25
1716,"By leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations , we ensure that all considered interpretations are consistent with executable actions in any context , thus simplifying the problem of search over logical forms .",3,0.44293666,129.13999703115712,42
1716,We present a dataset of explanations paired with demonstrations for web-based tasks .,2,0.45103306,62.18030199549822,13
1716,"Our methods show better task completion rates than a supervised semantic parsing baseline ( 40 % relative improvement on average ) , and are competitive with simple exploration-and-demonstration based methods , while requiring no exploration of the environment .",3,0.9115106,92.38161809373611,43
1716,"In learning to align explanations with demonstrations , basic properties of natural language syntax emerge as learned behavior .",0,0.80521876,204.0289917605123,19
1716,This is an interesting example of pragmatic language acquisition without any linguistic annotation .,3,0.78338546,48.16566511688857,14
1717,"We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning , with an end goal of teaching agents to communicate with humans in natural language .",1,0.61622864,23.306803144064574,32
1717,"Our starting point is a language model that has been trained on generic , not task-specific language data .",2,0.5322545,29.238034113957635,20
1717,"We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model , turning it into a task-conditional language model .",2,0.8214804,45.759647472367064,35
1717,"We introduce a new way for combining the two types of learning based on the idea of reranking language model samples , and show that this method outperforms others in communicating with humans in a visual referential communication task .",3,0.5289112,42.083178644821636,40
1717,"Finally , we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them .",1,0.43416917,37.160842809799206,24
1718,"Transformers are ubiquitous in Natural Language Processing ( NLP ) tasks , but they are difficult to be deployed on hardware due to the intensive computation .",0,0.94593227,26.366895635539127,27
1718,"To enable low-latency inference on resource-constrained hardware platforms , we propose to design Hardware-Aware Transformers ( HAT ) with neural architecture search .",1,0.32111236,42.91482177857572,26
1718,We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers .,2,0.76518273,36.38785924598113,15
1718,"Then we train a SuperTransformer that covers all candidates in the design space , and efficiently produces many SubTransformers with weight sharing .",2,0.7717822,124.1546490032354,23
1718,"Finally , we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware .",2,0.75814235,110.09840213729746,26
1718,"Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware ( CPU , GPU , IoT device ) .",3,0.7148398,61.88031888394481,26
1718,"When running WMT ’14 translation task on Raspberry Pi-4 , HAT can achieve 3 × speedup , 3.7 × smaller size over baseline Transformer ;",3,0.9118598,144.2111676552111,26
1718,"2.7 × speedup , 3.6 × smaller size over Evolved Transformer with 12,041 × less search cost and no performance loss .",3,0.8838892,103.48253622712885,22
1718,HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers .,3,0.43346205,16.590883998977613,6
1719,Recent work has questioned the importance of the Transformer ’s multi-headed attention for achieving high translation quality .,0,0.9027343,33.04947326281987,18
1719,We push further in this direction by developing a “ hard-coded ” attention variant without any learned parameters .,3,0.44833064,67.59702134047447,20
1719,"Surprisingly , replacing all learned self-attention heads in the encoder and decoder with fixed , input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs .",3,0.92580867,43.66006957577355,28
1719,"However , additionally , hard-coding cross attention ( which connects the decoder to the encoder ) significantly lowers BLEU , suggesting that it is more important than self-attention .",3,0.9657249,48.770237859929075,31
1719,Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer .,3,0.6765152,61.205836566885935,25
1719,"Taken as a whole , our results offer insight into which components of the Transformer are actually important , which we hope will guide future work into the development of simpler and more efficient attention-based models .",3,0.98755324,23.989005360083034,39
1720,"Transfer learning improves quality for low-resource machine translation , but it is unclear what exactly it transfers .",0,0.93493634,60.987105728381394,18
1720,"We perform several ablation studies that limit information transfer , then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning .",2,0.8341415,71.38476891433886,30
1720,"Word embeddings play an important role in transfer learning , particularly if they are properly aligned .",0,0.791238,28.20553034441919,17
1720,"Although transfer learning can be performed without embeddings , results are sub-optimal .",3,0.71265835,34.763335088659446,13
1720,"In contrast , transferring only the embeddings but nothing else yields catastrophic results .",3,0.5123048,103.22739573794648,14
1720,"We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences , finding even randomly generated sequences as parents yield noticeable but smaller gains .",3,0.7010873,179.70088460079208,28
1720,"Finally , transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs .",3,0.869876,45.44037398993189,24
1721,Most data selection research in machine translation focuses on improving a single domain .,0,0.92768836,83.93120239466876,14
1721,We perform data selection for multiple domains at once .,2,0.7575894,89.65528560365948,10
1721,This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches .,2,0.49193943,74.3363768199675,27
1721,"Both the choice of features and the use of curriculum are crucial for balancing and improving all domains , including out-of-domain .",3,0.5034238,47.98829543910309,24
1721,"In large-scale experiments , the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training .",3,0.85708624,58.6977575124148,22
1722,Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men .,0,0.8038385,26.411761782030442,20
1722,"In Neural Machine Translation ( NMT ) gender bias has been shown to reduce translation quality , particularly when the target language has grammatical gender .",0,0.9396662,21.542348072644636,26
1722,"Ideally we would reduce system bias by simply debiasing all data prior to training , but achieving this effectively is itself a challenge .",0,0.5359448,61.31877067883134,24
1722,"Rather than attempt to create a ‘ balanced ’ dataset , we use transfer learning on a small set of trusted , gender-balanced examples .",2,0.7946671,110.48636929010567,26
1722,This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch .,3,0.855383,55.27335436870322,20
1722,"A known pitfall of transfer learning on new domains is ‘ catastrophic forgetting ’ , which we address at adaptation and inference time .",0,0.43945026,92.18601881458936,24
1722,During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction .,3,0.9596833,61.03472977627556,21
1722,"At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al , 2019 on WinoMT with no degradation of general test set BLEU .",2,0.48915893,123.24005511450484,32
1722,We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability .,3,0.66901094,66.37703404310756,18
1723,"Machine translation has an undesirable propensity to produce “ translationese ” artifacts , which can lead to higher BLEU scores while being liked less by human raters .",0,0.9095354,84.97163422621098,28
1723,"Motivated by this , we model translationese and original ( i.e .",2,0.60077626,126.86432277228249,12
1723,"There is no data with original source and original target , so we train a sentence-level classifier to distinguish translationese from original target text , and use this classifier to tag the training data for an NMT model .",2,0.7390278,37.03147570347242,41
1723,"Using this technique we bias the model to produce more natural outputs at test time , yielding gains in human evaluation scores on both accuracy and fluency .",3,0.4992112,39.978072934262364,28
1723,"Additionally , we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score , increasing it while decreasing human-rated quality .",3,0.94505244,65.49457131067604,28
1723,"We analyze these outputs using metrics measuring the degree of translationese , and present an analysis of the volatility of heuristic-based train-data tagging .",2,0.636543,115.94767584159366,28
1724,"The notion of “ in-domain data ” in NLP is often over-simplistic and vague , as textual data varies in many nuanced linguistic aspects such as topic , style or level of formality .",0,0.9359032,48.01198477778902,36
1724,"In addition , domain labels are many times unavailable , making it challenging to build domain-specific systems .",0,0.9081975,112.42836520599135,18
1724,We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision – suggesting a simple data-driven definition of domains in textual data .,3,0.9412883,63.86415877038962,29
1724,"We harness this property and propose domain data selection methods based on such models , which require only a small set of in-domain monolingual data .",2,0.5231262,51.40767024312546,27
1724,"We evaluate our data selection methods for neural machine translation across five diverse domains , where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection .",2,0.55630803,55.74462762308933,37
1725,We present Neural Machine Translation ( NMT ) training using document-level metrics with batch-level documents .,2,0.5557231,61.99689202865337,19
1725,"Previous sequence-objective approaches to NMT training focus exclusively on sentence-level metrics like sentence BLEU which do not correspond to the desired evaluation metric , typically document BLEU .",0,0.8311792,50.30005556197533,31
1725,Meanwhile research into document-level NMT training focuses on data or model architecture rather than training procedure .,0,0.820531,63.256006909169535,18
1725,"We find that each of these lines of research has a clear space in it for the other , and propose merging them with a scheme that allows a document-level evaluation metric to be used in the NMT training objective .",3,0.842195,36.72547161918388,42
1725,We first sample pseudo-documents from sentence samples .,2,0.9172993,110.38357795550127,8
1725,We then approximate the expected document BLEU gradient with Monte Carlo sampling for use as a cost function in Minimum Risk Training ( MRT ) .,2,0.8357688,102.45548245142768,26
1725,This two-level sampling procedure gives NMT performance gains over sequence MRT and maximum-likelihood training .,3,0.63838345,93.40972024068523,16
1725,We demonstrate that training is more robust for document-level metrics than with sequence metrics .,3,0.9621487,60.38198758509012,17
1725,"We further demonstrate improvements on NMT with TER and Grammatical Error Correction ( GEC ) using GLEU , both metrics used at the document level for evaluations .",3,0.88247037,75.70045219285772,28
1726,"Variational Neural Machine Translation ( VNMT ) is an attractive framework for modeling the generation of target translations , conditioned not only on the source sentence but also on some latent random variables .",0,0.94393027,34.29397924920356,34
1726,The latent variable modeling may introduce useful statistical dependencies that can improve translation accuracy .,3,0.6257559,114.3906397311052,15
1726,"Unfortunately , learning informative latent variables is non-trivial , as the latent space can be prohibitively large , and the latent codes are prone to be ignored by many translation models at training time .",0,0.80161345,25.79840853089059,35
1726,Previous works impose strong assumptions on the distribution of the latent code and limit the choice of the NMT architecture .,0,0.84362507,48.07591272350201,21
1726,"In this paper , we propose to apply the VNMT framework to the state-of-the-art Transformer and introduce a more flexible approximate posterior based on normalizing flows .",1,0.81701744,27.894205249591774,32
1726,"We demonstrate the efficacy of our proposal under both in-domain and out-of-domain conditions , significantly outperforming strong baselines .",3,0.92826766,10.922035524214953,23
1727,"This work treats the paradigm discovery problem ( PDP ) , the task of learning an inflectional morphological system from unannotated sentences .",1,0.47146225,44.73335855034984,23
1727,We formalize the PDP and develop evaluation metrics for judging systems .,2,0.64890903,135.6952222280779,12
1727,"Using currently available resources , we construct datasets for the task .",2,0.7059005,63.61305266109719,12
1727,We also devise a heuristic benchmark for the PDP and report empirical results on five diverse languages .,2,0.43479452,53.992291978792096,18
1727,Our benchmark system first makes use of word embeddings and string similarity to cluster forms by cell and by paradigm .,2,0.61192304,104.50373901363504,21
1727,"Then , we bootstrap a neural transducer on top of the clustered data to predict words to realize the empty paradigm slots .",2,0.8276041,77.23607245402292,23
1727,An error analysis of our system suggests clustering by cell across different inflection classes is the most pressing challenge for future work .,3,0.9742231,73.43264554510614,23
1728,"Hindi grapheme-to-phoneme ( G2P ) conversion is mostly trivial , with one exception : whether a schwa represented in the orthography is pronounced or unpronounced ( deleted ) .",0,0.8499138,65.931056664749,31
1728,Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis .,0,0.92743963,50.51043443245391,18
1728,"We present the first statistical schwa deletion classifier for Hindi , which relies solely on the orthography as the input and outperforms previous approaches .",3,0.42157817,58.83175492532465,25
1728,We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries .,2,0.8524675,28.61815589647525,16
1728,"Our best Hindi model achieves state of the art performance , and also achieves good performance on a closely related language , Punjabi , without modification .",3,0.88830113,53.03184953948269,27
1729,"In this theme paper , we focus on Automated Writing Evaluation ( AWE ) , using Ellis Page ’s seminal 1966 paper to frame the presentation .",1,0.79039216,159.86952062627032,27
1729,We discuss some of the current frontiers in the field and offer some thoughts on the emergent uses of this technology .,1,0.5404516,22.495608496523484,22
1730,Building on Petroni et al .,0,0.56551254,39.96269202803132,6
1730,"2019 , we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models ( PLMs ) .",2,0.59885895,75.61498718179512,20
1730,( 1 ) Negation .,3,0.29457936,586.1625528665065,5
1730,We find that PLMs do not distinguish between negated ( ‘ ‘ Birds cannot [ MASK ] ” ) and non-negated ( ‘ ‘ Birds can [ MASK ] ” ) cloze questions .,3,0.97975516,25.64187337951524,34
1730,( 2 ) Mispriming .,0,0.4345429,902.206926769576,5
1730,Birds can [ MASK ] ” ) .,0,0.5480093,1049.6064453400934,8
1730,We find that PLMs are easily distracted by misprimes .,3,0.9818886,94.97209818463114,10
1730,These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge .,3,0.9896988,18.29872278963927,20
1731,"The field of natural language processing is experiencing a period of unprecedented growth , and with it a surge of published papers .",0,0.9622887,19.802980422750103,23
1731,"This represents an opportunity for us to take stock of how we cite the work of other researchers , and whether this growth comes at the expense of “ forgetting ” about older literature .",0,0.37205395,41.423262569905816,35
1731,"In this paper , we address this question through bibliographic analysis .",1,0.91499794,28.57862289283367,12
1731,"By looking at the age of outgoing citations in papers published at selected ACL venues between 2010 and 2019 , we find that there is indeed a tendency for recent papers to cite more recent work , but the rate at which papers older than 15 years are cited has remained relatively stable .",3,0.878944,30.650475969685225,54
1732,"Most NLP models today treat language as universal , even though socio-and psycholingustic research shows that the communicated message is influenced by the characteristics of the speaker as well as the target audience .",0,0.9227368,66.55464364853924,34
1732,"This paper surveys the landscape of personalization in natural language processing and related fields , and offers a path forward to mitigate the decades of deviation of the NLP tools from sociolingustic findings , allowing to flexibly process the “ natural ” language of each user rather than enforcing a uniform NLP treatment .",1,0.6915488,81.21074123868557,54
1732,"It outlines a possible direction to incorporate these aspects into neural NLP models by means of socially contextual personalization , and proposes to shift the focus of our evaluation strategies accordingly .",3,0.71215016,80.78215262213138,32
1733,"Many tasks aim to measure machine reading comprehension ( MRC ) , often focusing on question types presumed to be difficult .",0,0.9380844,72.63511998245131,22
1733,"Rarely , however , do task designers start by considering what systems should in fact comprehend .",0,0.87915426,263.2747907530055,17
1733,In this paper we make two key contributions .,1,0.7083688,14.66764300213103,9
1733,"First , we argue that existing approaches do not adequately define comprehension ;",3,0.55959713,115.19379531382465,13
1733,they are too unsystematic about what content is tested .,0,0.8031562,76.3142989349029,10
1733,"Second , we present a detailed definition of comprehension — a “ Template of Understanding ” — for a widely useful class of texts , namely short narratives .",1,0.42071328,100.77321544697287,29
1733,We then conduct an experiment that strongly suggests existing systems are not up to the task of narrative understanding as we define it .,2,0.66846746,31.4376853981612,24
1734,"Disparities in authorship and citations across gender can have substantial adverse consequences not just on the disadvantaged genders , but also on the field of study as a whole .",0,0.85659575,32.72163219233641,30
1734,Measuring gender gaps is a crucial step towards addressing them .,0,0.7706164,29.81033843959153,11
1734,"In this work , we examine female first author percentages and the citations to their papers in Natural Language Processing ( 1965 to 2019 ) .",1,0.74016815,137.61874080729876,26
1734,We determine aggregate-level statistics using an existing manually curated author--gender list as well as first names strongly associated with a gender .,2,0.8632469,250.72789969114274,25
1734,We find that only about 29 % of first authors are female and only about 25 % of last authors are female .,3,0.9811539,20.23365577338858,23
1734,"Notably , this percentage has not improved since the mid 2000s .",3,0.53216493,39.09946744884532,12
1734,"We also show that , on average , female first authors are cited less than male first authors , even when controlling for experience and area of research .",3,0.97870797,45.141373312697176,29
1734,"Finally , we discuss the ethical considerations involved in automatic demographic analysis .",3,0.4984582,57.56639274533535,13
1735,"We present BART , a denoising autoencoder for pretraining sequence-to-sequence models .",2,0.30826762,16.817537924999428,14
1735,"BART is trained by ( 1 ) corrupting text with an arbitrary noising function , and ( 2 ) learning a model to reconstruct the original text .",2,0.6471806,59.82684695344589,28
1735,"It uses a standard Tranformer-based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left-to-right decoder ) , and other recent pretraining schemes .",2,0.53390354,38.33142426595852,46
1735,"We evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme , where spans of text are replaced with a single mask token .",2,0.80277354,65.10985598432684,40
1735,BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks .,3,0.8814933,63.69512617505376,18
1735,"It matches the performance of RoBERTa on GLUE and SQuAD , and achieves new state-of-the-art results on a range of abstractive dialogue , question answering , and summarization tasks , with gains of up to 3.5 ROUGE .",3,0.8552384,14.49214981826381,44
1735,"BART also provides a 1.1 BLEU increase over a back-translation system for machine translation , with only target language pretraining .",3,0.91292226,44.82581514967075,22
1735,"We also replicate other pretraining schemes within the BART framework , to understand their effect on end-task performance .",2,0.64116347,76.18112056261349,19
1736,Text generation has made significant advances in the last few years .,0,0.9589827,14.537719357730754,12
1736,"Yet , evaluation metrics have lagged behind , as the most popular choices ( e.g. , BLEU and ROUGE ) may correlate poorly with human judgment .",0,0.9154895,36.32137864524055,27
1736,"We propose BLEURT , a learned evaluation metric for English based on BERT .",2,0.46173838,87.68903337137534,14
1736,BLEURT can model human judgment with a few thousand possibly biased training examples .,3,0.5130552,148.14737704269064,14
1736,A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize .,2,0.48291063,17.300422029865416,23
1736,BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set .,3,0.45719653,24.04791329027219,26
1736,"In contrast to a vanilla BERT-based approach , it yields superior results even when the training data is scarce and out-of-distribution .",3,0.82484335,15.656967164980715,25
1737,Large-scale pre-trained language model such as BERT has achieved great success in language understanding tasks .,0,0.82504565,5.91026953376201,16
1737,"However , it remains an open question how to utilize BERT for language generation .",0,0.8642683,19.322036885049567,15
1737,"In this paper , we present a novel approach , Conditional Masked Language Modeling ( C-MLM ) , to enable the finetuning of BERT on target generation tasks .",1,0.86853516,29.75211519632529,30
1737,The finetuned BERT ( teacher ) is exploited as extra supervision to improve conventional Seq2Seq models ( student ) for better text generation performance .,2,0.5435586,70.25342424666819,25
1737,"By leveraging BERT ’s idiosyncratic bidirectional nature , distilling knowledge learned in BERT can encourage auto-regressive Seq2Seq models to plan ahead , imposing global sequence-level supervision for coherent text generation .",3,0.7339037,73.84551310386007,33
1737,Experiments show that the proposed approach significantly outperforms strong Transformer baselines on multiple language generation tasks such as machine translation and text summarization .,3,0.9280289,8.317967867345446,24
1737,Our proposed model also achieves new state of the art on IWSLT German-English and English-Vietnamese MT datasets .,3,0.9160261,14.028675798489004,20
1738,Neural networks lack the ability to reason about qualitative physics and so cannot generalize to scenarios and tasks unseen during training .,0,0.8997376,45.22569037725015,22
1738,"We propose ESPRIT , a framework for commonsense reasoning about qualitative physics in natural language that generates interpretable descriptions of physical events .",1,0.44451976,56.165873830395036,23
1738,We use a two-step approach of first identifying the pivotal physical events in an environment and then generating natural language descriptions of those events using a data-to-text approach .,2,0.85803443,20.09792928003306,32
1738,Our framework learns to generate explanations of how the physical simulation will causally evolve so that an agent or a human can easily reason about a solution using those interpretable descriptions .,2,0.42750898,59.59499660347188,32
1738,Human evaluations indicate that ESPRIT produces crucial fine-grained details and has high coverage of physical concepts compared to even human annotations .,3,0.7378429,103.68267268967057,23
1738,"Dataset , code and documentation are available at https://github.com/salesforce/esprit .",3,0.55908525,17.655969848936326,10
1739,"We present a novel iterative , edit-based approach to unsupervised sentence simplification .",1,0.5558067,28.6996097711297,13
1739,"Our model is guided by a scoring function involving fluency , simplicity , and meaning preservation .",2,0.65640473,114.94956701725724,17
1739,"Then , we iteratively perform word and phrase-level edits on the complex sentence .",2,0.7917415,35.62040520212575,15
1739,"Compared with previous approaches , our model does not require a parallel training set , but is more controllable and interpretable .",3,0.88735145,21.79983204807682,22
1739,Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches .,3,0.9100725,13.890270500542647,25
1740,Neural natural language generation ( NLG ) models have recently shown remarkable progress in fluency and coherence .,0,0.9647455,20.520096913972928,18
1740,"However , existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference , an important aspect of human thinking and language .",0,0.9206223,50.18605263307591,31
1740,"In this paper , we suggest a new NLG task where a model is tasked with generating natural language statements that can be logically entailed by the facts in an open-domain semi-structured table .",1,0.857958,22.54985994654245,34
1740,"To facilitate the study of the proposed logical NLG problem , we use the existing TabFact dataset ~ ( CITATION ) featured with a wide range of logical / symbolic inferences as our testbed , and propose new automatic metrics to evaluate the fidelity of generation models w.r.t .",2,0.68046296,70.53017081681593,49
1740,logical inference .,0,0.4314579,102.83598277356363,3
1740,The new task poses challenges to the existing monotonic generation frameworks due to the mismatch between sequence order and logical order .,0,0.78309107,26.211891415898094,22
1740,"1 ) Pre-Trained LM can significantly boost both the fluency and logical fidelity metrics , 2 ) RL and Adversarial Training are trading fluency for fidelity , 3 ) Coarse-to-Fine generation can help partially alleviate the fidelity issue while maintaining high language fluency .",3,0.8991988,84.30354560481449,48
1740,The code and data are available at https://github.com/wenhuchen/LogicNLG .,3,0.54499966,11.452585291404107,9
1741,"The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus , which are extracted by aligning sentences between parallel articles .",0,0.7222665,39.00622789567496,34
1741,"To evaluate and improve sentence alignment quality , we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora , Newsela and Wikipedia .",2,0.8313405,66.62097261262144,29
1741,We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity .,2,0.49611852,35.291992317998435,33
1741,Experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in F1 .,3,0.9583836,12.753693953187385,24
1741,"We apply our CRF aligner to construct two new text simplification datasets , Newsela-Auto and Wiki-Auto , which are much larger and of better quality compared to the existing datasets .",2,0.64490736,43.752540127401524,32
1741,A Transformer-based seq2seq model trained on our datasets establishes a new state-of-the-art for text simplification in both automatic and human evaluation .,3,0.83688307,9.307010797179807,30
1742,Different texts shall by nature correspond to different number of keyphrases .,0,0.70396954,98.56388512672581,12
1742,This desideratum is largely missing from existing neural keyphrase generation models .,0,0.81812286,44.10251916882541,12
1742,"In this study , we address this problem from both modeling and evaluation perspectives .",1,0.90602285,29.96003110234076,15
1742,We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences .,2,0.6061878,25.17296778720895,16
1742,Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states .,2,0.40158886,77.19748527361575,15
1742,"In contrast to previous approaches , our model is capable of generating diverse keyphrases and controlling number of outputs .",3,0.8504931,32.15287591511686,20
1742,We further propose two evaluation metrics tailored towards the variable-number generation .,3,0.46919805,79.00891692267862,14
1742,"We also introduce a new dataset StackEx that expands beyond the only existing genre ( i.e. , academic writing ) in keyphrase generation tasks .",2,0.4846457,103.67105499911789,25
1742,"With both previous and new evaluation metrics , our model outperforms strong baselines on all datasets .",3,0.9008405,18.266366695707404,17
1743,We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence .,1,0.40302995,13.90804913133465,15
1743,"Our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm : reversal of valence and semantic incongruity with the context , which could include shared commonsense or world knowledge between the speaker and the listener .",2,0.7495818,49.386272859449974,42
1743,"While prior works on sarcasm generation predominantly focus on context incongruity , we show that combining valence reversal and semantic incongruity based on the commonsense knowledge generates sarcasm of higher quality .",3,0.8881997,28.863027889246716,32
1743,"Human evaluation shows that our system generates sarcasm better than humans 34 % of the time , and better than a reinforced hybrid baseline 90 % of the time .",3,0.95402104,59.746623942282376,30
1744,The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs .,0,0.9124432,16.522467428520404,21
1744,"As a crucial defect , the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs .",0,0.85490096,41.98804918425161,32
1744,We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information .,1,0.33066016,54.84805191304209,21
1744,"In particular , we introduce two types of autoencoding losses , each individually focusing on different aspects ( a.k.a .",2,0.7502136,44.8702322150882,20
1744,views ) of input graphs .,2,0.38364828,278.44113077754236,6
1744,The losses are then back-propagated to better calibrate our model via multi-task training .,2,0.6745871,25.187027701126645,16
1744,Experiments on two benchmarks for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline .,3,0.77850336,6.428500947140961,28
1745,"Most existing joint neural models for Information Extraction ( IE ) use local task-specific classifiers to predict labels for individual instances ( e.g. , trigger , relation ) regardless of their interactions .",0,0.87326956,67.05537177884656,34
1745,"For example , a victim of a die event is likely to be a victim of an attack event in the same sentence .",3,0.5316779,30.697946602648504,24
1745,"In order to capture such cross-subtask and cross-instance inter-dependencies , we propose a joint neural framework , OneIE , that aims to extract the globally optimal IE result as a graph from an input sentence .",2,0.6213697,49.98100475066565,36
1745,OneIE performs end-to-end IE in four stages : ( 1 ) Encoding a given sentence as contextualized word representations ;,2,0.52684635,82.47684172795503,21
1745,( 2 ) Identifying entity mentions and event triggers as nodes ;,2,0.5540225,441.2994134070564,12
1745,( 3 ) Computing label scores for all nodes and their pairwise links using local classifiers ;,2,0.77099603,330.96304462870455,17
1745,( 4 ) Searching for the globally optimal graph with a beam decoder .,2,0.6745205,115.11812845240235,14
1745,"At the decoding stage , we incorporate global features to capture the cross-subtask and cross-instance interactions .",2,0.78881395,42.20595565425972,17
1745,Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks .,3,0.9564401,15.603833669044674,25
1745,"In addition , as OneIE does not use any language-specific feature , we prove it can be easily applied to new languages or trained in a multilingual manner .",3,0.8770252,59.944582206629796,30
1746,Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions .,0,0.85533357,65.32141030333756,18
1746,This is problematic when the information needed to recognize an event argument is spread across multiple sentences .,0,0.7182129,44.45822312405331,18
1746,We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers .,3,0.7784485,35.290015020725725,32
1746,"We first investigate how end-to-end neural sequence models ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ’ performance .",2,0.46522126,50.95279521341152,41
1746,"To dynamically aggregate information captured by neural representations learned at different levels of granularity ( e.g. , the sentence-and paragraph-level ) , we propose a novel multi-granularity reader .",2,0.61299604,47.362962339834276,33
1746,"We evaluate our models on the MUC-4 event extraction dataset , and show that our best system performs substantially better than prior work .",3,0.7692264,30.280317192370756,26
1746,We also report findings on the relationship between context length and neural model performance on the task .,3,0.80245197,21.870981932130977,18
1747,This paper studies the task of Relation Extraction ( RE ) that aims to identify the semantic relations between two entity mentions in text .,1,0.824201,16.007913760494343,25
1747,"In the deep learning models for RE , it has been beneficial to incorporate the syntactic structures from the dependency trees of the input sentences .",0,0.8642813,31.94928979489866,26
1747,"In such models , the dependency trees are often used to directly structure the network architectures or to obtain the dependency relations between the word pairs to inject the syntactic information into the models via multi-task learning .",0,0.7841122,29.73007689812726,38
1747,The major problem with these approaches is the lack of generalization beyond the syntactic structures in the training data or the failure to capture the syntactic importance of the words for RE .,0,0.8428523,20.97507278685082,33
1747,"In order to overcome these issues , we propose a novel deep learning model for RE that uses the dependency trees to extract the syntax-based importance scores for the words , serving as a tree representation to introduce syntactic information into the models with greater generalization .",2,0.5656435,41.01315778094735,49
1747,"In particular , we leverage Ordered-Neuron Long-Short Term Memory Networks ( ON-LSTM ) to infer the model-based importance scores for RE for every word in the sentences that are then regulated to be consistent with the syntax-based scores to enable syntactic information injection .",2,0.84315425,82.98838805337192,52
1747,"We perform extensive experiments to demonstrate the effectiveness of the proposed method , leading to the state-of-the-art performance on three RE benchmark datasets .",3,0.5672842,10.149585633833548,30
1748,Linguistic Code-switching ( CS ) is still an understudied phenomenon in natural language processing .,0,0.97544664,23.707614415788182,15
1748,"The NLP community has mostly focused on monolingual and multi-lingual scenarios , but little attention has been given to CS in particular .",0,0.943419,16.697169116228558,23
1748,"This is partly because of the lack of resources and annotated data , despite its increasing occurrence in social media platforms .",0,0.87776977,33.72034801402866,22
1748,"In this paper , we aim at adapting monolingual models to code-switched text in various tasks .",1,0.9305814,21.23054406807091,17
1748,"Specifically , we transfer English knowledge from a pre-trained ELMo model to different code-switched language pairs ( i.e. , Nepali-English , Spanish-English , and Hindi-English ) using the task of language identification .",2,0.89741486,23.52432612262442,34
1748,"Our method , CS-ELMo , is an extension of ELMo with a simple yet effective position-aware attention mechanism inside its character convolutions .",2,0.5169947,65.63657280216262,27
1748,"We show the effectiveness of this transfer learning step by outperforming multilingual BERT and homologous CS-unaware ELMo models and establishing a new state of the art in CS tasks , such as NER and POS tagging .",3,0.9228989,36.718406178862885,39
1748,"Our technique can be expanded to more English-paired code-switched languages , providing more resources to the CS community .",3,0.9518568,63.69655368458887,21
1749,Concept graphs are created as universal taxonomies for text understanding in the open-domain knowledge .,0,0.6150877,74.71184798561858,15
1749,The nodes in concept graphs include both entities and concepts .,2,0.39352012,131.36463034749443,11
1749,"The edges are from entities to concepts , showing that an entity is an instance of a concept .",3,0.430984,40.92554016424208,19
1749,"In this paper , we propose the task of learning interpretable relationships from open-domain facts to enrich and refine concept graphs .",1,0.9021455,49.92712401971032,22
1749,The Bayesian network structures are learned from open-domain facts as the interpretable relationships between relations of facts and concepts of entities .,2,0.5614403,44.972106084464436,22
1749,We conduct extensive experiments on public English and Chinese datasets .,2,0.831837,29.183697778203992,11
1749,"Compared to the state-of-the-art methods , the learned network structures help improving the identification of concepts for entities based on the relations of entities on both datasets .",3,0.9060657,27.611228583803253,32
1750,"We present a novel document-level model for finding argument spans that fill an event ’s roles , connecting related ideas in sentence-level semantic role labeling and coreference resolution .",1,0.4207576,112.11218983062919,32
1750,"Because existing datasets for cross-sentence linking are small , development of our neural model is supported through the creation of a new resource , Roles Across Multiple Sentences ( RAMS ) , which contains 9,124 annotated events across 139 types .",2,0.47822022,72.99914377314323,41
1750,We demonstrate strong performance of our model on RAMS and other event-related datasets .,3,0.94061303,38.95397010419192,14
1751,"Nowadays , the interpretability of machine learning models is becoming increasingly important , especially in the medical domain .",0,0.9413148,26.548040383473293,19
1751,"Aiming to shed some light on how to rationalize medical relation prediction , we present a new interpretable framework inspired by existing theories on how human memory works , e.g. , theories of recall and recognition .",1,0.7028687,38.356628561359166,37
1751,"Given the corpus-level statistics , i.e. , a global co-occurrence graph of a clinical text corpus , to predict the relations between two entities , we first recall rich contexts associated with the target entities , and then recognize relational interactions between these contexts to form model rationales , which will contribute to the final prediction .",2,0.7125691,61.783641570206846,57
1751,"We conduct experiments on a real-world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models , but also present rationales to justify its prediction .",3,0.6718648,33.15590077736097,39
1751,We further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making .,3,0.7761116,68.95632277697209,20
1752,"Named-entities are inherently multilingual , and annotations in any given language may be limited .",0,0.8978773,52.51146028509324,15
1752,"This motivates us to consider polyglot named-entity recognition ( NER ) , where one model is trained using annotated data drawn from more than one language .",0,0.7285505,32.969851329947446,27
1752,"However , a straightforward implementation of this simple idea does not always work in practice : naive training of NER models using annotated data drawn from multiple languages consistently underperforms models trained on monolingual data alone , despite having access to more training data .",3,0.5936821,42.054232178607435,45
1752,"The starting point of this paper is a simple solution to this problem , in which polyglot models are fine-tuned on monolingual data to consistently and significantly outperform their monolingual counterparts .",1,0.5450142,11.700197170218463,32
1752,"To explain this phenomena , we explore the sources of multilingual transfer in polyglot NER models and examine the weight structure of polyglot models compared to their monolingual counterparts .",1,0.7189309,24.588617210548893,30
1752,We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters .,3,0.98127306,24.06917665611553,23
1753,"In many documents , such as semi-structured webpages , textual semantics are augmented with additional information conveyed using visual elements including layout , font size , and color .",0,0.8900791,58.69311147716269,29
1753,Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template .,0,0.91909444,40.21941204553609,31
1753,"In this work , we propose a solution for “ zero-shot ” open-domain relation extraction from webpages with a previously unseen template , including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals .",1,0.7795439,74.15334490989541,45
1753,"Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them , enabling generalization to new templates .",2,0.7511352,38.9120510018398,33
1753,Experiments show this approach provides a 31 % F1 gain over a baseline for zero-shot extraction in a new subject vertical .,3,0.9166709,60.85658502860049,22
1754,Traditional named entity recognition models use gazetteers ( lists of entities ) as features to improve performance .,0,0.8908445,48.31033401588577,18
1754,"Although modern neural network models do not require such hand-crafted features for strong performance , recent work has demonstrated their utility for named entity recognition on English data .",0,0.91280055,30.27772554220324,30
1754,"However , designing such features for low-resource languages is challenging , because exhaustive entity gazetteers do not exist in these languages .",0,0.82707036,45.53565977994878,22
1754,"To address this problem , we propose a method of “ soft gazetteers ” that incorporates ubiquitously available information from English knowledge bases , such as Wikipedia , into neural named entity recognition models through cross-lingual entity linking .",1,0.38655952,35.845110927030085,39
1754,Our experiments on four low-resource languages show an average improvement of 4 points in F1 score .,3,0.94727767,12.840018855259506,17
1755,We reframe suicide risk assessment from social media as a ranking problem whose goal is maximizing detection of severely at-risk individuals given the time available .,2,0.46544638,72.94350586862517,26
1755,"Building on measures developed for resource-bounded document retrieval , we introduce a well founded evaluation paradigm , and demonstrate using an expert-annotated test collection that meaningful improvements over plausible cascade model baselines can be achieved using an approach that jointly ranks individuals and their social media posts .",3,0.42530334,81.33037117571982,48
1756,Hierarchical Topic modeling ( HTM ) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration .,0,0.90875113,48.33947202687362,23
1756,"Despite advantages over traditional topic modeling , HTM poses its own challenges , such as ( 1 ) topic incoherence , ( 2 ) unreasonable ( hierarchical ) structure , and ( 3 ) issues related to the definition of the “ ideal ” number of topics and depth of the hierarchy .",0,0.79664105,79.44836297963698,53
1756,"In this paper , we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM , a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM .",1,0.9051489,24.947937487630863,40
1756,"CluHTM ’s novel contributions include : ( i ) the exploration of richer text representation that encapsulates both , global ( dataset level ) and local semantic information – when combined , these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure ;",3,0.58267033,150.57115574851076,53
1756,( ii ) the exploitation of a stability analysis metric for defining the number of topics and the “ shape ” the hierarchical structure .,2,0.572825,144.77442062701454,25
1756,"In our evaluation , considering twelve datasets and seven state-of-the-art baselines , CluHTM outperformed the baselines in the vast majority of the cases , with gains of around 500 % over the strongest state-of-the-art baselines .",3,0.9197449,24.140111764484686,48
1756,We also provide qualitative and quantitative statistical analyses of why our solution works so well .,3,0.5821341,31.14910624908034,16
1757,"Entity set expansion , aiming at expanding a small seed entity set with new entities belonging to the same semantic class , is a critical task that benefits many downstream NLP and IR applications , such as question answering , query understanding , and taxonomy construction .",0,0.9314344,66.4247176228896,47
1757,Existing set expansion methods bootstrap the seed entity set by adaptively selecting context features and extracting new entities .,0,0.736213,134.28748727452438,19
1757,A key challenge for entity set expansion is to avoid selecting ambiguous context features which will shift the class semantics and lead to accumulative errors in later iterations .,0,0.6512292,96.4381640287347,29
1757,"In this study , we propose a novel iterative set expansion framework that leverages automatically generated class names to address the semantic drift issue .",1,0.90773374,36.72860641340954,25
1757,"In each iteration , we select one positive and several negative class names by probing a pre-trained language model , and further score each candidate entity based on selected class names .",2,0.8301352,53.37587250185745,32
1757,Experiments on two datasets show that our framework generates high-quality class names and outperforms previous state-of-the-art methods significantly .,3,0.9190994,8.655979310233114,25
1758,"In classification , there are usually some good features that are indicative of class labels .",0,0.84889174,87.76956109913198,16
1758,"For example , in sentiment classification , words like good and nice are indicative of the positive sentiment and words like bad and terrible are indicative of the negative sentiment .",0,0.58496827,26.18202380550924,31
1758,"However , there are also many common features ( e.g. , words ) that are not indicative of any specific class ( e.g. , voice and screen , which are common to both sentiment classes and are not discriminative for classification ) .",0,0.54792887,58.29167290890433,43
1758,"Although deep learning has made significant progresses in generating discriminative features through its powerful representation learning , we believe there is still room for improvement .",0,0.8148435,30.041071016666255,26
1758,"In this paper , we propose a novel angle to further improve this representation learning , i.e. , feature projection .",1,0.8638419,78.39769071374475,21
1758,This method projects existing features into the orthogonal space of the common features .,2,0.5512963,70.85461706454878,14
1758,The resulting projection is thus perpendicular to the common features and more discriminative for classification .,3,0.731504,119.29432986387272,16
1758,"We apply this new method to improve CNN , RNN , Transformer , and Bert based text classification and obtain markedly better results .",3,0.83193916,85.36926986792304,24
1759,"Existing Visual Question Answering ( VQA ) methods tend to exploit dataset biases and spurious statistical correlations , instead of producing right answers for the right reasons .",0,0.95216596,34.00900677973133,28
1759,"To address this issue , recent bias mitigation methods for VQA propose to incorporate visual cues ( e.g. , human attention maps ) to better ground the VQA models , showcasing impressive gains .",0,0.8812432,99.04581146755261,34
1759,"However , we show that the performance improvements are not a result of improved visual grounding , but a regularization effect which prevents over-fitting to linguistic priors .",3,0.9696378,42.48773153345549,28
1759,"For instance , we find that it is not actually necessary to provide proper , human-based cues ;",3,0.88418317,157.28458870480446,20
1759,"random , insensible cues also result in similar improvements .",3,0.87929195,448.41390694897336,10
1759,"Based on this observation , we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on VQA-CPv2 .",3,0.70150566,17.932099735548782,34
1760,"Visual Dialogue involves “ understanding ” the dialogue history ( what has been discussed previously ) and the current question ( what is asked ) , in addition to grounding information in the image , to accurately generate the correct response .",0,0.85823387,76.54189001905222,42
1760,"In this paper , we show that co-attention models which explicitly encode dialoh history outperform models that do n’t , achieving state-of-the-art performance ( 72 % NDCG on val set ) .",1,0.589492,86.54580480068239,38
1760,"However , we also expose shortcomings of the crowdsourcing dataset collection procedure , by showing that dialogue history is indeed only required for a small amount of the data , and that the current evaluation metric encourages generic replies .",3,0.9108869,77.14942555762211,40
1760,"To that end , we propose a challenging subset ( VisdialConv ) of the VisdialVal set and the benchmark NDCG of 63 % .",2,0.5510111,181.01135444658556,24
1761,"We present a new problem : grounding natural language instructions to mobile user interface actions , and create three new datasets for it .",1,0.5274274,153.61325817618547,24
1761,"For full task evaluation , we create PixelHelp , a corpus that pairs English instructions with actions performed by people on a mobile UI emulator .",2,0.82801837,160.94701179028303,26
1761,"To scale training , we decouple the language and action data by ( a ) annotating action phrase spans in How-To instructions and ( b ) synthesizing grounded descriptions of actions for mobile user interfaces .",2,0.7570809,72.41607206603435,38
1761,We use a Transformer to extract action phrase tuples from long-range natural language instructions .,2,0.8563305,42.535104911305346,15
1761,A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions .,2,0.6149461,204.11608364439653,22
1761,"Given a starting screen and instruction , our model achieves 70.59 % accuracy on predicting complete ground-truth action sequences in PixelHelp .",3,0.86383593,177.45205275170107,23
1762,"We present the task of Spatio-Temporal Video Question Answering , which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts ( people and objects ) to answer natural language questions about videos .",1,0.60148436,43.20129929783158,37
1762,"We first augment the TVQA dataset with 310.8 K bounding boxes , linking depicted objects to visual concepts in questions and answers .",2,0.85343474,108.94707118138959,23
1762,We name this augmented version as TVQA + .,3,0.4504753,272.46095186368154,9
1762,"We then propose Spatio-Temporal Answerer with Grounded Evidence ( STAGE ) , a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos .",2,0.49551135,39.613223097490014,30
1762,Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA + dataset can contribute to the question answering task .,3,0.9477652,37.06161247979335,28
1762,"Moreover , by performing this joint task , our model is able to produce insightful and interpretable spatio-temporal attention visualizations .",3,0.877754,35.46449269089082,21
1763,Unsupervised machine translation ( MT ) has recently achieved impressive results with monolingual corpora only .,0,0.9644812,27.05417427794098,16
1763,"However , it is still challenging to associate source-target sentences in the latent space .",0,0.9232016,37.39492734705595,16
1763,"As people speak different languages biologically share similar visual systems , the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT ( MMT ) .",0,0.93025184,67.13586787986478,32
1763,"In this paper , we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT .",1,0.9396239,35.80967905983106,22
1763,Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision .,2,0.72448,47.35797144591069,32
1763,The experimental results on the widely used Multi30 K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time .,3,0.9548864,16.296875868141047,40
1764,"In many languages like Arabic , diacritics are used to specify pronunciations as well as meanings .",0,0.9033499,20.85230263474595,17
1764,"Such diacritics are often omitted in written text , increasing the number of possible pronunciations and meanings for a word .",0,0.8971,29.7162156059998,21
1764,This results in a more ambiguous text making computational processing on such text more difficult .,0,0.73094636,131.21475685604,16
1764,Diacritic restoration is the task of restoring missing diacritics in the written text .,0,0.95246804,28.640209826102332,14
1764,"Most state-of-the-art diacritic restoration models are built on character level information which helps generalize the model to unseen data , but presumably lose useful information at the word level .",0,0.86739993,36.62825966777124,34
1764,"Thus , to compensate for this loss , we investigate the use of multi-task learning to jointly optimize diacritic restoration with related NLP problems namely word segmentation , part-of-speech tagging , and syntactic diacritization .",1,0.5269493,35.01483496034736,36
1764,We use Arabic as a case study since it has sufficient data resources for tasks that we consider in our joint modeling .,2,0.711666,60.00343654880159,23
1764,Our joint models significantly outperform the baselines and are comparable to the state-of-the-art models that are more complex relying on morphological analyzers and / or a lot more data ( e.g .,3,0.9412207,17.511003360410935,38
1764,dialectal data ) .,3,0.4274713,401.270427164747,4
1765,"Lexica distinguishing all morphologically related forms of each lexeme are crucial to many language technologies , yet building them is expensive .",0,0.91230875,158.03177428455257,22
1765,We propose a frugal paradigm completion approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible .,1,0.4739868,82.8192323218272,25
1765,It induces typological information during training which it uses to determine the best sources at test time .,3,0.35768995,95.71031016873118,18
1765,We evaluate our language-agnostic approach on 7 diverse languages .,2,0.66163206,36.47994498672027,12
1765,"Compared to popular alternative approaches , ours reduces manual labor by 16-63 % and is the most robust to typological variation .",3,0.87992245,71.24412271783477,24
1766,Contextual features always play an important role in Chinese word segmentation ( CWS ) .,0,0.89506906,48.789008628796786,15
1766,"Wordhood information , being one of the contextual features , is proved to be useful in many conventional character-based segmenters .",0,0.72598535,85.21573826129475,23
1766,"However , this feature receives less attention in recent neural models and it is also challenging to design a framework that can properly integrate wordhood information from different wordhood measures to existing neural frameworks .",0,0.8747281,57.70119156852958,35
1766,"In this paper , we therefore propose a neural framework , WMSeg , which uses memory networks to incorporate wordhood information with several popular encoder-decoder combinations for CWS .",1,0.8112747,89.01541754492722,29
1766,Experimental results on five benchmark datasets indicate the memory mechanism successfully models wordhood information for neural segmenters and helps WMSeg achieve state-of-the-art performance on all those datasets .,3,0.94544595,53.65501493480117,34
1766,Further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments .,3,0.9576466,37.28602115006063,28
1767,"Chinese word segmentation ( CWS ) and part-of-speech ( POS ) tagging are important fundamental tasks for Chinese language processing , where joint learning of them is an effective one-step solution for both tasks .",0,0.9162268,35.27969280387124,35
1767,Previous studies for joint CWS and POS tagging mainly follow the character-based tagging paradigm with introducing contextual information such as n-gram features or sentential representations from recurrent neural models .,0,0.8575857,64.00344931066316,32
1767,"However , for many cases , the joint tagging needs not only modeling from context features but also knowledge attached to them ( e.g. , syntactic relations among words ) ;",0,0.72870183,101.26691002050912,31
1767,limited efforts have been made by existing research to meet such needs .,0,0.92742926,42.39548796642287,13
1767,"In this paper , we propose a neural model named TwASP for joint CWS and POS tagging following the character-based sequence labeling paradigm , where a two-way attention mechanism is used to incorporate both context feature and their corresponding syntactic knowledge for each input character .",1,0.7232385,40.7811905308936,49
1767,"Particularly , we use existing language processing toolkits to obtain the auto-analyzed syntactic knowledge for the context , and the proposed attention module can learn and benefit from them although their quality may not be perfect .",2,0.5114043,64.0591405884654,37
1767,"Our experiments illustrate the effectiveness of the two-way attentions for joint CWS and POS tagging , where state-of-the-art performance is achieved on five benchmark datasets .",3,0.8553064,26.682700047270806,33
1768,The written forms of Semitic languages are both highly ambiguous and morphologically rich : a word can have multiple interpretations and is one of many inflected forms of the same concept or lemma .,0,0.9314849,33.65119040009367,34
1768,"This is further exacerbated for dialectal content , which is more prone to noise and lacks a standard orthography .",0,0.6614483,52.27356296779498,20
1768,"The morphological features can be lexicalized , like lemmas and diacritized forms , or non-lexicalized , like gender , number , and part-of-speech tags , among others .",0,0.52283525,33.75419544796194,29
1768,"Joint modeling of the lexicalized and non-lexicalized features can identify more intricate morphological patterns , which provide better context modeling , and further disambiguate ambiguous lexical choices .",3,0.7119853,47.15428039781637,28
1768,"However , the different modeling granularity can make joint modeling more difficult .",0,0.62003124,77.03899437898264,13
1768,"Our approach models the different features jointly , whether lexicalized ( on the character-level ) , or non-lexicalized ( on the word-level ) .",2,0.75236756,31.055546565192675,27
1768,"We use Arabic as a test case , and achieve state-of-the-art results for Modern Standard Arabic with 20 % relative error reduction , and Egyptian Arabic with 11 % relative error reduction .",3,0.6203006,24.47527796885136,39
1769,Informal romanization is an idiosyncratic process used by humans in informal digital communication to encode non-Latin script languages into Latin character sets found on common keyboards .,0,0.9405167,62.45299513902599,27
1769,"Character substitution choices differ between users but have been shown to be governed by the same main principles observed across a variety of languages — namely , character pairs are often associated through phonetic or visual similarity .",0,0.73660284,74.39641171427014,38
1769,We propose a noisy-channel WFST cascade model for deciphering the original non-Latin script from observed romanized text in an unsupervised fashion .,1,0.37421829,56.27635086278331,24
1769,We train our model directly on romanized data from two languages : Egyptian Arabic and Russian .,2,0.8487191,47.62177294601874,17
1769,"We demonstrate that adding inductive bias through phonetic and visual priors on character mappings substantially improves the model ’s performance on both languages , yielding results much closer to the supervised skyline .",3,0.9512397,81.67754379428143,33
1769,"Finally , we introduce a new dataset of romanized Russian , collected from a Russian social network website and partially annotated for our experiments .",2,0.6645573,36.993362914111074,25
1770,"We improve upon pairwise annotation for active learning in coreference resolution , by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent .",2,0.67696446,98.2939177204232,29
1770,"This simple modification , when combined with a novel mention clustering algorithm for selecting which examples to label , is much more efficient in terms of the performance obtained per annotation budget .",3,0.8262532,77.11709593382616,33
1770,"In experiments with existing benchmark coreference datasets , we show that the signal from this additional question leads to significant performance gains per human-annotation hour .",3,0.83742404,75.37746330023819,26
1770,Future work can use our annotation protocol to effectively develop coreference models for new domains .,3,0.9756999,72.3474921108139,16
1770,Our code is publicly available .,3,0.42964724,8.875162332047664,6
1771,"This paper introduces two tasks : determining ( a ) the duration of possession relations and ( b ) co-possessions , i.e. , whether multiple possessors possess a possessee at the same time .",1,0.71199656,47.24895810587612,34
1771,We present new annotations on top of corpora annotating possession existence and experimental results .,3,0.57894653,155.5697750449693,15
1771,"Regarding possession duration , we derive the time spans we work with empirically from annotations indicating lower and upper bounds .",2,0.74705815,138.64810785625681,21
1771,"Regarding co-possessions , we use a binary label .",2,0.8162908,111.87102757369107,9
1771,"Cohen ’s kappa coefficients indicate substantial agreement , and experimental results show that text is more useful than the image for solving these tasks .",3,0.8520476,80.33923648231844,25
1772,Language models pretrained on text from a wide variety of sources form the foundation of today ’s NLP .,0,0.8793014,41.251420745569135,19
1772,"In light of the success of these broad-coverage models , we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task .",1,0.69924486,21.47030665244838,30
1772,"We present a study across four domains ( biomedical and computer science publications , news , and reviews ) and eight classification tasks , showing that a second phase of pretraining in-domain ( domain-adaptive pretraining ) leads to performance gains , under both high-and low-resource settings .",3,0.43317267,49.17671862774876,52
1772,"Moreover , adapting to the task ’s unlabeled data ( task-adaptive pretraining ) improves performance even after domain-adaptive pretraining .",3,0.7425084,52.29288416998002,22
1772,"Finally , we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative , especially when resources for domain-adaptive pretraining might be unavailable .",3,0.97244173,63.974188066817625,32
1772,"Overall , we consistently find that multi-phase adaptive pretraining offers large gains in task performance .",3,0.9855158,40.81178081272017,17
1773,Word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity ( STS ) tasks .,0,0.91712505,32.36650124508942,22
1773,"Recent work has increasingly adopted a statistical view on these embeddings , with some of the top approaches being essentially various correlations ( which include the famous cosine similarity ) .",0,0.9129507,108.0491475031064,31
1773,"Another excellent candidate for a similarity measure is mutual information ( MI ) , which can capture arbitrary dependencies between the variables and has a simple and intuitive expression .",0,0.8085253,76.87166451256657,30
1773,"Unfortunately , its use in the context of dense word embeddings has so far been avoided due to difficulties with estimating MI for continuous data .",0,0.8707194,49.3799738475142,26
1773,"In this work we go through a vast literature on estimating MI in such cases and single out the most promising methods , yielding a simple and elegant similarity measure for word embeddings .",1,0.6664441,71.5928450909253,34
1773,"We show that mutual information is a viable alternative to correlations , gives an excellent signal that correlates well with human judgements of similarity and rivals existing state-of-the-art unsupervised methods .",3,0.94945645,26.58153213111997,37
1774,"We study the task of cross-database semantic parsing ( XSP ) , where a system that maps natural language utterances to executable SQL queries is evaluated on databases unseen during training .",2,0.36650357,38.482871322322296,32
1774,"Recently , several datasets , including Spider , were proposed to support development of XSP systems .",0,0.8874223,179.3605116369314,17
1774,"We propose a challenging evaluation setup for cross-database semantic parsing , focusing on variation across database schemas and in-domain language use .",1,0.43332106,55.21079337578317,23
1774,"We re-purpose eight semantic parsing datasets that have been well-studied in the setting where in-domain training data is available , and instead use them as additional evaluation data for XSP systems instead .",2,0.7754889,44.173235662619284,36
1774,"We build a system that performs well on Spider , and find that it struggles to generalize to our re-purposed set .",3,0.72781605,46.309491365563815,22
1774,"Our setup uncovers several generalization challenges for cross-database semantic parsing , demonstrating the need to use and develop diverse training and evaluation datasets .",3,0.94755,48.24581828775507,24
1775,"The focus of a negation is the set of tokens intended to be negated , and a key component for revealing affirmative alternatives to negated utterances .",0,0.835471,45.24538377319731,27
1775,"In this paper , we experiment with neural networks to predict the focus of negation .",1,0.75905,33.049520540504425,16
1775,Our main novelty is leveraging a scope detector to introduce the scope of negation as an additional input to the network .,3,0.3979183,62.569960894570706,22
1775,Experimental results show that doing so obtains the best results to date .,3,0.91875964,12.339264186751011,13
1775,"Additionally , we perform a detailed error analysis providing insights into the main error categories , and analyze errors depending on whether the model takes into account scope and context information .",2,0.75730145,47.452922366567805,32
1776,Recent neural network-driven semantic role labeling ( SRL ) systems have shown impressive improvements in F1 scores .,0,0.936157,35.72155914271883,20
1776,"These improvements are due to expressive input representations , which , at least at the surface , are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear SRL models .",3,0.82732767,136.96656736057733,32
1776,Introducing the benefits of structure to inform neural models presents a methodological challenge .,0,0.857364,78.00864311124101,14
1776,"In this paper , we present a structured tuning framework to improve models using softened constraints only at training time .",1,0.80000323,72.68116465060463,21
1776,Our framework leverages the expressiveness of neural networks and provides supervision with structured loss components .,2,0.52698725,68.61664716414417,16
1776,"We start with a strong baseline ( RoBERTa ) to validate the impact of our approach , and show that our framework outperforms the baseline by learning to comply with declarative constraints .",3,0.50824815,28.445766208614423,33
1776,"Additionally , our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios .",3,0.97813624,21.782339138346803,19
1777,Recent years have witnessed the burgeoning of pretrained language models ( LMs ) for text-based natural language ( NL ) understanding tasks .,0,0.9655808,31.850744674038406,25
1777,"Such models are typically trained on free-form NL text , hence may not be suitable for tasks like semantic parsing over structured data , which require reasoning over both free-form NL questions and structured tabular data ( e.g. , database tables ) .",0,0.76924825,36.610230755266485,43
1777,"In this paper we present TaBERT , a pretrained LM that jointly learns representations for NL sentences and ( semi-) structured tables .",1,0.7152846,84.12625342759407,25
1777,TaBERT is trained on a large corpus of 26 million tables and their English contexts .,2,0.53344965,38.211973124386695,16
1777,"In experiments , neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions , while performing competitively on the text-to-SQL dataset Spider .",3,0.835384,71.18927930808103,40
1778,"We introduce a transductive model for parsing into Universal Decompositional Semantics ( UDS ) representations , which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores .",2,0.7391417,39.30072444211031,38
1778,"We also introduce a strong pipeline model for parsing into the UDS graph structure , and show that our transductive parser performs comparably while additionally performing attribute prediction .",3,0.7043707,92.66503550296308,29
1778,"By analyzing the attribute prediction errors , we find the model captures natural relationships between attribute groups .",3,0.8560247,140.24726715902602,18
1779,This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks .,3,0.89296454,9.887656724720207,24
1779,"We train a Transformer-based masked language model on one hundred languages , using more than two terabytes of filtered CommonCrawl data .",2,0.88952166,39.101052227671936,24
1779,"Our model , dubbed XLM-R , significantly outperforms multilingual BERT ( mBERT ) on a variety of cross-lingual benchmarks , including + 14.6 % average accuracy on XNLI , + 13 % average F1 score on MLQA , and + 2.4 % F1 score on NER .",3,0.7989314,23.14498189518498,48
1779,"XLM-R performs particularly well on low-resource languages , improving 15.7 % in XNLI accuracy for Swahili and 11.4 % for Urdu over previous XLM models .",3,0.92861736,29.856358405244517,27
1779,"We also present a detailed empirical analysis of the key factors that are required to achieve these gains , including the trade-offs between ( 1 ) positive transfer and capacity dilution and ( 2 ) the performance of high and low resource languages at scale .",3,0.4880962,30.05794308584545,47
1779,"Finally , we show , for the first time , the possibility of multilingual modeling without sacrificing per-language performance ;",3,0.93599427,53.53339537033037,20
1779,XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks .,3,0.85273314,20.875303880361855,16
1779,We will make our code and models publicly available .,3,0.5896733,11.530130973908799,10
1780,"Concept normalization , the task of linking textual mentions of concepts to concepts in an ontology , is challenging because ontologies are large .",0,0.93585485,52.428170195363755,24
1780,"In most cases , annotated datasets cover only a small sample of the concepts , yet concept normalizers are expected to predict all concepts in the ontology .",0,0.7925953,63.284094723664,28
1780,"In this paper , we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT .",1,0.83140427,21.217362287196064,21
1780,"The ranker considers pairings of concept mentions and candidate concepts , allowing it to make predictions for any concept , not just those seen during training .",2,0.59697455,70.5668385778504,27
1780,We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training .,3,0.48422605,33.90506247165687,26
1780,Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets .,3,0.8200144,12.674734776322598,17
1781,We propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction .,1,0.5661569,57.19417430822848,20
1781,"At training , our novel multi-level learning-to-rank loss compares positive types against negative siblings according to the type tree .",2,0.7004276,162.67105882564633,24
1781,"During prediction , we define a coarse-to-fine decoder that restricts viable candidates at each level of the ontology based on already predicted parent type ( s ) .",2,0.79923266,91.91800334285385,30
1781,"Our approach significantly outperform prior work on strict accuracy , demonstrating the effectiveness of our method .",3,0.9243922,43.89684590217282,17
1782,Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input .,0,0.94429904,18.866083201362198,29
1782,"However , domain transfer of NER models with data from multiple genres has not been widely studied .",0,0.9202076,49.12800343016849,18
1782,"To this end , we conduct NER experiments in three predictive setups on data from : a ) multiple domains ;",2,0.85985196,197.30738829646097,21
1782,b) multiple domains where the genre label is unknown at inference time ;,3,0.431873,131.5923342661329,14
1782,c ) domains not encountered in training .,3,0.56777847,164.44838628655663,8
1782,We introduce a new architecture tailored to this task by using shared and private domain parameters and multi-task learning .,2,0.52341896,47.841647982999994,20
1782,"This consistently outperforms all other baseline and competitive methods on all three experimental setups , with differences ranging between + 1.95 to + 3.11 average F1 across multiple genres when compared to standard approaches .",3,0.9409344,36.26791845299022,35
1782,"These results illustrate the challenges that need to be taken into account when building real-world NLP applications that are robust to various types of text and the methods that can help , at least partially , alleviate these issues .",3,0.98783445,21.03974321557889,41
1783,Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce .,0,0.92945725,35.75669069259459,14
1783,"State-of-the-art approaches for knowledge extraction were each designed for a single category of product , and thus do not apply to real-life e-Commerce scenarios , which often contain thousands of diverse categories .",0,0.8069906,29.354925005752495,38
1783,"This paper proposes TXtract , a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy .",1,0.81435823,52.14593600158045,25
1783,"Through category conditional self-attention and multi-task learning , our approach is both scalable , as it trains a single model for thousands of categories , and effective , as it extracts category-specific attribute values .",3,0.58997995,43.19633498674691,36
1783,"Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10 % in F1 and 15 % in coverage across all categories .",3,0.9221558,16.752847262990432,37
1784,"Training neural models for named entity recognition ( NER ) in a new domain often requires additional human annotations ( e.g. , tens of thousands of labeled instances ) that are usually expensive and time-consuming to collect .",0,0.9300774,25.397950791525084,40
1784,"Thus , a crucial research question is how to obtain supervision in a cost-effective way .",0,0.8217304,36.94355519142197,16
1784,"In this paper , we introduce “ entity triggers , ” an effective proxy of human explanations for facilitating label-efficient learning of NER models .",1,0.83378536,133.4877335567772,27
1784,An entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence .,0,0.75754964,29.28008485234141,27
1784,We crowd-sourced 14 k entity triggers for two well-studied NER datasets .,2,0.9073201,55.17800022755556,13
1784,"Our proposed model , Trigger Matching Network , jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging .",2,0.47646293,116.6847886274287,29
1784,Our framework is significantly more cost-effective than the traditional neural NER frameworks .,3,0.88945526,32.357080433833026,13
1784,Experiments show that using only 20 % of the trigger-annotated sentences results in a comparable performance as using 70 % of conventional annotated sentences .,3,0.94524735,26.335174911187952,27
1785,This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders ( CVAEs ) .,1,0.86029345,15.57877564700418,23
1785,"It thus improves performance of machine translation models that use noisy or monolingual data , as well as in conventional settings .",3,0.62021846,44.40710913936288,22
1785,"Extending Transformer and conditional VAEs , our proposed latent variable model measurably prevents posterior collapse by ( 1 ) using a modified evidence lower bound ( ELBO ) objective which promotes mutual information between the latent variable and the target , and ( 2 ) guiding the latent variable with an auxiliary bag-of-words prediction task .",2,0.46193168,72.87182027352235,56
1785,"As a result , the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro↔En and De↔En .",3,0.92951936,31.96131216245942,23
1785,"With latent variables being effectively utilized , our model demonstrates improved robustness over non-latent Transformer in handling uncertainty : exploiting noisy source-side monolingual data ( up to + 3.2 BLEU ) , and training with weakly aligned web-mined parallel data ( up to + 4.7 BLEU ) .",3,0.8589715,46.912582050328304,48
1786,"When training multilingual machine translation ( MT ) models that can translate to / from multiple languages , we are faced with imbalanced training sets : some languages have much more training data than others .",0,0.8917266,34.38424177557639,36
1786,"Standard practice is to up-sample less resourced languages to increase representation , and the degree of up-sampling has a large effect on the overall performance .",0,0.79362226,35.59362123000555,26
1786,"In this paper , we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages .",1,0.85714436,47.09458704157077,32
1786,"Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance , but also offers flexible control over the performance of which languages are optimized .",3,0.8626684,19.5454180593297,47
1787,Neural Machine Translation ( NMT ) models are sensitive to small perturbations in the input .,0,0.9285175,13.115194935290768,16
1787,Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input .,0,0.6872761,24.87345437793073,19
1787,This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input .,1,0.8384491,41.41712998108056,23
1787,We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed .,2,0.40756893,50.90551249121108,26
1787,Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used .,3,0.98688936,31.980558886054258,22
1788,Web-crawled data provides a good source of parallel corpora for training machine translation models .,0,0.90232646,25.625353915191525,15
1788,"It is automatically obtained , but extremely noisy , and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods .",0,0.90866035,42.85948357750819,30
1788,"In this paper , we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models .",1,0.855855,14.598289851981846,23
1788,We measure sentence parallelism by leveraging the multilingual capability of BERT and use the Generative Pre-training ( GPT ) language model as a domain filter to balance data domains .,2,0.8508004,54.17124901381468,30
1788,"We evaluate the proposed method on the WMT 2018 Parallel Corpus Filtering shared task , and on our own web-crawled Japanese-Chinese parallel corpus .",2,0.6275082,58.62388437034797,26
1788,Our method significantly outperforms baselines and achieves a new state-of-the-art .,3,0.91834134,5.212707755159929,15
1788,"In an unsupervised setting , our method achieves comparable performance to the top-1 supervised method .",3,0.89078635,16.908972214749994,18
1788,We also evaluate on a web-crawled Japanese-Chinese parallel corpus that we make publicly available .,2,0.64659303,39.99571966274158,17
1789,Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network ( RNN ) based neural machine translation ( NMT ) .,0,0.75046974,26.057115042326185,30
1789,"However , it is challenging to extend them into the advanced Transformer architecture , which is more complicated than RNN .",0,0.62015325,40.14559619737897,21
1789,This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer .,1,0.77007043,26.73018745042526,28
1789,"In addition , to further reduce the bias problem in the gate mechanism , this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information .",2,0.42946717,59.57044927838871,36
1789,Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline .,3,0.89687717,14.5111243544385,25
1790,The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories .,0,0.92274374,55.068970389701,27
1790,"In this paper , we propose a novel multi-perspective cross-lingual neural framework for code–text matching , inspired in part by a previous model for monolingual text-to-text matching , to capture both global and local similarities .",1,0.8734079,18.752603351578383,40
1790,Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space .,3,0.9556125,15.982020119546048,39
1791,"While automated essay scoring ( AES ) can reliably grade essays at scale , automated writing evaluation ( AWE ) additionally provides formative feedback to guide essay revision .",0,0.90665346,94.71702701637203,29
1791,"However , a neural AES typically does not provide useful feature representations for supporting AWE .",0,0.87065506,274.6603543513692,16
1791,"This paper presents a method for linking AWE and neural AES , by extracting Topical Components ( TCs ) representing evidence from a source text using the intermediate output of attention layers .",1,0.8078616,109.44119394871507,33
1791,We evaluate performance using a feature-based AES requiring TCs .,2,0.76330465,614.6637338760349,12
1791,"Results show that performance is comparable whether using automatically or manually constructed TCs for 1 ) representing essays as rubric-based features , 2 ) grading essays .",3,0.9873468,167.12833933697527,28
1792,"In traditional approaches to entity linking , linking decisions are based on three sources of information – the similarity of the mention string to an entity ’s name , the similarity of the context of the document to the entity , and broader information about the knowledge base ( KB ) .",0,0.89948094,35.5658059178901,52
1792,"In some domains , there is little contextual information present in the KB and thus we rely more heavily on mention string similarity .",0,0.4182582,63.4235415097623,24
1792,"We consider one example of this , concept linking , which seeks to link mentions of medical concepts to a medical concept ontology .",2,0.39179865,91.55768437292612,24
1792,"We propose an approach to concept linking that leverages recent work in contextualized neural models , such as ELMo ( Peters et al .",2,0.37684125,48.589159001755206,24
1792,"2018 ) , which create a token representation that integrates the surrounding context of the mention and concept name .",2,0.357701,108.1176417403601,20
1792,We find a neural ranking approach paired with contextualized embeddings provides gains over a competitive baseline ( Leaman et al .,3,0.9259374,57.26804811257638,21
1792,2013 ) .,4,0.79494923,130.21561410372792,3
1792,"Additionally , we find that a pre-training step using synonyms from the ontology offers a useful initialization for the ranker .",3,0.97009665,53.08377765614603,21
1793,The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence .,0,0.95600134,45.38059976711534,26
1793,"The Fact Extraction and VERification ( FEVER ) dataset provides such a resource for evaluating endto-end fact-checking , requiring retrieval of evidence from Wikipedia to validate a veracity prediction .",0,0.73550916,72.46059578653309,32
1793,"We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking – multiple propositions , temporal reasoning , and ambiguity and lexical variation – and introduce a resource with these types of claims .",3,0.62005156,105.9721220213426,42
1793,Then we present a system designed to be resilient to these “ attacks ” using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions .,2,0.54096854,93.74880297331595,34
1793,"We find that in handling these attacks we obtain state-of-the-art results on FEVER , largely due to improved evidence retrieval .",3,0.980164,28.684047258667302,27
1794,"In this paper , we aim to learn associations between visual attributes of fonts and the verbal context of the texts they are typically applied to .",1,0.9472042,34.649352064765,27
1794,"Compared to related work leveraging the surrounding visual context , we choose to focus only on the input text , which can enable new applications for which the text is the only visual element in the document .",2,0.5127418,42.58562772486706,38
1794,"We introduce a new dataset , containing examples of different topics in social media posts and ads , labeled through crowd-sourcing .",2,0.76032627,58.26260592350431,23
1794,"Due to the subjective nature of the task , multiple fonts might be perceived as acceptable for an input text , which makes this problem challenging .",0,0.8701264,45.3432983808686,27
1794,"To this end , we investigate different end-to-end models to learn label distributions on crowd-sourced data , to capture inter-subjectivity across all annotations .",2,0.649241,42.15010449401904,26
1795,News framing refers to the practice in which aspects of specific issues are highlighted in the news to promote a particular interpretation .,0,0.8902024,49.45919968098631,23
1795,"In NLP , although recent works have studied framing in English news , few have studied how the analysis can be extended to other languages and in a multi-label setting .",0,0.86430275,35.86764563363121,31
1795,"In this work , we explore multilingual transfer learning to detect multiple frames from just the news headline in a genuinely low-resource context where there are few / no frame annotations in the target language .",1,0.6098513,53.558021546638955,36
1795,We propose a novel method that can leverage elementary resources consisting of a dictionary and few annotations to detect frames in the target language .,2,0.33715478,46.91466247505179,25
1795,Our method performs comparably or better than translating the entire target language headline to the source language for which we have annotated data .,3,0.91035795,44.429517898816876,24
1795,"This work opens up an exciting new capability of scaling up frame analysis to many languages , even those without existing translation technologies .",3,0.9729893,95.03542943429127,24
1795,"Lastly , we apply our method to detect frames on the issue of U.S .",3,0.5124277,60.404766649618345,15
1795,gun violence in multiple languages and obtain exciting insights on the relationship between different frames of the same problem across different countries with different languages .,1,0.36075872,62.70565131751483,26
1796,"Given the complexity of combinations of tasks , languages , and domains in natural language processing ( NLP ) research , it is computationally prohibitive to exhaustively test newly proposed models on each possible experimental setting .",0,0.88413775,39.80444373084807,37
1796,"In this work , we attempt to explore the possibility of gaining plausible judgments of how well an NLP model can perform under an experimental setting , without actually training or testing the model .",1,0.9170139,34.07965539066289,35
1796,"To do so , we build regression models to predict the evaluation score of an NLP experiment given the experimental settings as input .",2,0.8332382,34.487722780772735,24
1796,"Experimenting on ~9 different NLP tasks , we find that our predictors can produce meaningful predictions over unseen languages and different modeling architectures , outperforming reasonable baselines as well as human experts .",3,0.9068252,39.792117975153005,33
1796,% we represent experimental settings using an array of features .,2,0.6433648,162.1047948135658,11
1796,"Going further , we outline how our predictor can be used to find a small subset of representative experiments that should be run in order to obtain plausible predictions for all other experimental settings .",3,0.7196349,35.37563653635094,35
1797,"It is appealing to have a system that generates a story or scripts automatically from a storyline , even though this is still out of our reach .",0,0.83973336,57.537522802523114,28
1797,"In dialogue systems , it would also be useful to drive dialogues by a dialogue plan .",0,0.6164974,73.25410924601235,17
1797,"In this paper , we address a key problem involved in these applications-guiding a dialogue by a narrative .",1,0.922423,83.27931532755235,21
1797,The proposed model ScriptWriter selects the best response among the candidates that fit the context as well as the given narrative .,2,0.49425793,82.43748377090418,22
1797,It keeps track of what in the narrative has been said and what is to be said .,0,0.72387755,20.302786883871338,18
1797,"A narrative plays a different role than the context ( i.e. , previous utterances ) , which is generally used in current dialogue systems .",0,0.65182734,68.35650226372677,25
1797,"Due to the unavailability of data for this new application , we construct a new large-scale data collection GraphMovie from a movie website where end-users can upload their narratives freely when watching a movie .",2,0.6694611,41.520418646204064,35
1797,Experimental results on the dataset show that our proposed approach based on narratives significantly outperforms the baselines that simply use the narrative as a kind of context .,3,0.9595529,19.31239279462731,28
1798,Most of recent work in cross-lingual word embeddings is severely Anglocentric .,0,0.80689585,21.53260710148799,12
1798,"The vast majority of lexicon induction evaluation dictionaries are between English and another language , and the English embedding space is selected by default as the hub when learning in a multilingual setting .",0,0.61882836,48.32318990473637,34
1798,"With this work , however , we challenge these practices .",3,0.425702,613.4120310721465,11
1798,"First , we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance .",3,0.6974038,65.5958667879035,21
1798,"Second , we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation , and create new dictionaries for under-represented languages .",2,0.8213546,69.973125577829,27
1798,Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field .,3,0.56231844,43.76996406473057,27
1798,"Finally , in our analysis we identify general guidelines for strong cross-lingual embedding baselines , that extend to language pairs that do not include English .",3,0.9498309,42.32318743099113,26
1799,"Intelligent features in email service applications aim to increase productivity by helping people organize their folders , compose their emails and respond to pending tasks .",0,0.91879517,83.67820824525211,26
1799,"In this work , we explore a new application , Smart-To-Do , that helps users with task management over emails .",1,0.8414709,75.2438029347111,25
1799,We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action .,2,0.41282713,37.96383937982546,26
1799,"We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning , obtaining BLEU and ROUGE scores of 0.23 and 0.63 for this task .",2,0.7157085,15.3292028647425,32
1799,"To the best of our knowledge , this is the first work to address the problem of composing To-Do items from emails .",3,0.88102305,15.459545621602611,25
1800,"Natural language inference ( NLI ) is an increasingly important task for natural language understanding , which requires one to infer whether a sentence entails another .",0,0.96125007,24.06782813725239,27
1800,"However , the ability of NLI models to make pragmatic inferences remains understudied .",0,0.9425423,20.03646423715488,14
1800,"We create an IMPlicature and PRESupposition diagnostic dataset ( IMPPRES ) , consisting of 32 K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types .",2,0.88035905,167.13168647659217,27
1800,"We use IMPPRES to evaluate whether BERT , InferSent , and BOW NLI models trained on MultiNLI ( Williams et al. , 2018 ) learn to make pragmatic inferences .",2,0.62337035,122.31376695218921,30
1800,"Although MultiNLI appears to contain very few pairs illustrating these inference types , we find that BERT learns to draw pragmatic inferences .",3,0.9744751,152.80300340301093,23
1800,It reliably treats scalar implicatures triggered by “ some ” as entailments .,3,0.5395807,137.14118890104035,13
1800,"For some presupposition triggers like “ only ” , BERT reliably recognizes the presupposition as an entailment , even when the trigger is embedded under an entailment canceling operator like negation .",3,0.7239502,64.7622557995221,32
1800,BOW and InferSent show weaker evidence of pragmatic reasoning .,3,0.8877124,295.00960173156614,10
1800,"We conclude that NLI training encourages models to learn some , but not all , pragmatic inferences .",3,0.98328006,90.52255280375749,18
1801,"Several recent studies have shown that strong natural language understanding ( NLU ) models are prone to relying on unwanted dataset biases without learning the underlying task , resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios .",0,0.9278485,18.278516944797342,51
1801,"We propose two learning strategies to train neural models , which are more robust to such biases and transfer better to out-of-domain datasets .",2,0.46349362,26.07644308320519,27
1801,"The biases are specified in terms of one or more bias-only models , which learn to leverage the dataset biases .",2,0.6080813,80.4943448470875,21
1801,"During training , the bias-only models ’ predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples .",2,0.6512761,55.79108416921308,39
1801,"We experiment on large-scale natural language inference and fact verification benchmarks , evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data .",2,0.86462545,21.41176473745201,36
1801,Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets .,3,0.9852224,33.38686297384736,21
1801,Our code and data are publicly available in https://github.com/rabeehk/robust-nli .,3,0.5814687,11.057324948265068,10
1802,"Models for natural language understanding ( NLU ) tasks often rely on the idiosyncratic biases of the dataset , which make them brittle against test cases outside the training distribution .",0,0.95147145,44.221507828833346,31
1802,"Recently , several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance .",0,0.8072998,15.342369331061203,20
1802,"However , their improvements come at the expense of performance drop when models are evaluated on the in-distribution data , which contain examples with higher diversity .",3,0.5130127,54.585017244575276,27
1802,This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the out-of-distribution data .,3,0.62513185,32.44540870020256,39
1802,"In this paper , we address this trade-off by introducing a novel debiasing method , called confidence regularization , which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples .",1,0.6815954,49.40543385789139,41
1802,"We evaluate our method on three NLU tasks and show that , in contrast to its predecessors , it improves the performance on out-of-distribution datasets ( e.g. , 7 pp gain on HANS dataset ) while maintaining the original in-distribution accuracy .",3,0.7999591,29.476288730165617,43
1803,The recent growth in the popularity and success of deep learning models on NLP classification tasks has accompanied the need for generating some form of natural language explanation of the predicted labels .,0,0.95910174,29.228394961588272,33
1803,"Such generated natural language ( NL ) explanations are expected to be faithful , i.e. , they should correlate well with the model ’s internal decision making .",0,0.7439958,69.25500057542104,28
1803,"We propose Natural-language Inference over Label-specific Explanations ( NILE ) , a novel NLI method which utilizes auto-generated label-specific NL explanations to produce labels along with its faithful explanation .",2,0.35002235,75.35363700660972,32
1803,We demonstrate NILE ’s effectiveness over previously reported methods through automated and human evaluation of the produced labels and explanations .,3,0.7904107,103.31027072125603,21
1803,Our evaluation of NILE also supports the claim that accurate systems capable of providing testable explanations of their decisions can be designed .,3,0.9882261,81.86259503702288,23
1803,We discuss the faithfulness of NILE ’s explanations in terms of sensitivity of the decisions to the corresponding explanations .,3,0.5560169,77.218580646107,20
1803,"We argue that explicit evaluation of faithfulness , in addition to label and explanation accuracy , is an important step in evaluating model ’s explanations .",3,0.902953,69.11934156915827,26
1803,"Further , we demonstrate that task-specific probes are necessary to establish such sensitivity .",3,0.9485382,48.99493655173673,15
1804,Question-answering ( QA ) data often encodes essential information in many facets .,0,0.95725584,60.95030039632493,15
1804,"We suggest that simply further pre-training BERT is often not the best option , and propose the question-answer driven sentence encoding ( QuASE ) framework .",3,0.592655,98.43777376668255,28
1804,"QuASE learns representations from QA data , using BERT or other state-of-the-art contextual language models .",2,0.52394676,32.59564019537011,22
1804,"In particular , we observe the need to distinguish between two types of sentence encodings , depending on whether the target task is a single-or multi-sentence input ;",3,0.929517,29.121879463452007,30
1804,"in both cases , the resulting encoding is shown to be an easy-to-use plugin for many downstream tasks .",3,0.84760463,31.303909393384725,23
1804,This work may point out an alternative way to supervise NLP tasks .,3,0.9657353,37.57266275183483,13
1805,"While deep learning models are making fast progress on the task of Natural Language Inference , recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases , and without deep understanding of the language semantics .",0,0.9154372,34.37548759483855,42
1805,"Using contradiction-word bias and word-overlapping bias as our two bias examples , this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases .",1,0.59496355,53.40163573605312,34
1805,"First , we debias the dataset through data augmentation and enhancement , but show that the model bias cannot be fully removed via this method .",2,0.6681471,50.134568496965365,26
1805,"Next , we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance .",2,0.54012483,63.83343928178099,22
1805,The first approach aims to remove the label bias at the embedding level .,2,0.53725857,29.40908932361335,14
1805,The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models .,2,0.6821661,24.48240981886192,36
1805,"We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests , and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy .",3,0.8134977,50.38210291512309,39
1806,"We introduce Uncertain Natural Language Inference ( UNLI ) , a refinement of Natural Language Inference ( NLI ) that shifts away from categorical labels , targeting instead the direct prediction of subjective probability assessments .",2,0.49427098,48.169328521228806,36
1806,"We demonstrate the feasibility of collecting annotations for UNLI by relabeling a portion of the SNLI dataset under a probabilistic scale , where items even with the same categorical label differ in how likely people judge them to be true given a premise .",3,0.6031712,46.797631201425844,44
1806,"We describe a direct scalar regression modeling approach , and find that existing categorically-labeled NLI data can be used in pre-training .",3,0.58011216,44.09456013809539,24
1806,"Our best models correlate well with humans , demonstrating models are capable of more subtle inferences than the categorical bin assignment employed in current NLI tasks .",3,0.968133,71.4984471769288,27
1807,"An interesting and frequent type of multi-word expression ( MWE ) is the headless MWE , for which there are no true internal syntactic dominance relations ;",0,0.8932049,94.43917626205686,27
1807,"examples include many named entities ( “ Wells Fargo ” ) and dates ( “ July 5 , 2020 ” ) as well as certain productive constructions ( “ blow for blow ” , “ day after day ” ) .",0,0.61353827,31.67750911417308,41
1807,"Despite their special status and prevalence , current dependency-annotation schemes require treating such flat structures as if they had internal syntactic heads , and most current parsers handle them in the same fashion as headed constructions .",0,0.8646739,101.66335445364962,39
1807,"Meanwhile , outside the context of parsing , taggers are typically used for identifying MWEs , but taggers might benefit from structural information .",0,0.847243,91.05464560327627,24
1807,We empirically compare these two common strategies — parsing and tagging — for predicting flat MWEs .,2,0.5983995,74.62813963397699,17
1807,"Additionally , we propose an efficient joint decoding algorithm that combines scores from both strategies .",2,0.45686406,53.815696738199726,16
1807,"Experimental results on the MWE-Aware English Dependency Corpus and on six non-English dependency treebanks with frequent flat structures show that : ( 1 ) tagging is more accurate than parsing for identifying flat-structure MWEs , ( 2 ) our joint decoder reconciles the two different views and , for non-BERT features , leads to higher accuracies , and ( 3 ) most of the gains result from feature sharing between the parsers and taggers .",3,0.34993815,49.75127373822747,78
1808,"Neural encoders have allowed dependency parsers to shift from higher-order structured models to simpler first-order ones , making decoding faster and still achieving better accuracy than non-neural parsers .",0,0.77548176,35.075938506550976,31
1808,"This has led to a belief that neural encoders can implicitly encode structural constraints , such as siblings and grandparents in a tree .",0,0.9357823,44.03592631323264,24
1808,"We tested this hypothesis and found that neural parsers may benefit from higher-order features , even when employing a powerful pre-trained encoder , such as BERT .",3,0.89206034,28.71067625931957,29
1808,"While the gains of higher-order features are small in the presence of a powerful encoder , they are consistent for long-range dependencies and long sentences .",3,0.8894979,35.18291040521559,28
1808,"In particular , higher-order models are more accurate on full sentence parses and on the exact match of modifier lists , indicating that they deal better with larger , more complex structures .",3,0.95462483,93.08375857910907,35
1809,Virtual adversarial training ( VAT ) is a powerful technique to improve model robustness in both supervised and semi-supervised settings .,0,0.8794194,21.076559665979623,21
1809,It is effective and can be easily adopted on lots of image classification and text classification tasks .,3,0.84468853,21.106404408994404,18
1809,"However , its benefits to sequence labeling tasks such as named entity recognition ( NER ) have not been shown as significant , mostly , because the previous approach can not combine VAT with the conditional random field ( CRF ) .",0,0.8973328,90.08278528274572,42
1809,"CRF can significantly boost accuracy for sequence models by putting constraints on label transitions , which makes it an essential component in most state-of-the-art sequence labeling model architectures .",0,0.65694714,33.67380700532206,35
1809,"In this paper , we propose SeqVAT , a method which naturally applies VAT to sequence labeling models with CRF .",1,0.86919373,126.21098592429061,21
1809,"Empirical studies show that SeqVAT not only significantly improves the sequence labeling performance over baselines under supervised settings , but also outperforms state-of-the-art approaches under semi-supervised settings .",3,0.6781414,14.065898054088526,34
1810,"A recent advance in monolingual dependency parsing is the idea of a treebank embedding vector , which allows all treebanks for a particular language to be used as training data while at the same time allowing the model to prefer training data from one treebank over others and to select the preferred treebank at test time .",0,0.8981021,20.93226926309303,57
1810,"We build on this idea by 1 ) introducing a method to predict a treebank vector for sentences that do not come from a treebank used in training , and 2 ) exploring what happens when we move away from predefined treebank embedding vectors during test time and instead devise tailored interpolations .",2,0.7455607,42.90174766962865,53
1810,"We show that 1 ) there are interpolated vectors that are superior to the predefined ones , and 2 ) treebank vectors can be predicted with sufficient accuracy , for nine out of ten test languages , to match the performance of an oracle approach that knows the most suitable predefined treebank embedding for the test set .",3,0.9089586,51.36880718374062,58
1811,This work investigates the use of interactively updated label suggestions to improve upon the efficiency of gathering annotations on the task of opinion mining in German Covid-19 social media data .,1,0.8832025,73.26134017109663,32
1811,"We develop guidelines to conduct a controlled annotation study with social science students and find that suggestions from a model trained on a small , expert-annotated dataset already lead to a substantial improvement – in terms of inter-annotator agreement ( +. 14 Fleiss ’ κ ) and annotation quality – compared to students that do not receive any label suggestions .",3,0.85066485,61.531208788449945,61
1811,We further find that label suggestions from interactively trained models do not lead to an improvement over suggestions from a static model .,3,0.9805366,35.876959440461476,23
1811,"Nonetheless , our analysis of suggestion bias shows that annotators remain capable of reflecting upon the suggested label in general .",3,0.98676735,77.83360274423566,21
1811,"Finally , we confirm the quality of the annotated data in transfer learning experiments between different annotator groups .",3,0.95992213,33.68875931879714,19
1811,"To facilitate further research in opinion mining on social media data , we release our collected data consisting of 200 expert and 2,785 student annotations .",2,0.6741852,71.90345702849197,26
1812,"Humor is an important social phenomenon , serving complex social and psychological functions .",0,0.9691928,75.90297291753971,14
1812,"However , despite being studied for millennia humor is computationally not well understood , often considered an AI-complete problem .",0,0.9510966,165.94361844362552,22
1812,"In this work , we introduce a novel setting in humor mining : automatically detecting funny and unusual scientific papers .",1,0.8399293,165.73026814222018,21
1812,"We are inspired by the Ig Nobel prize , a satirical prize awarded annually to celebrate funny scientific achievements ( example past winner : “ Are cows more likely to lie down the longer they stand ? ” ) .",0,0.47763592,75.15469632085576,40
1812,This challenging task has unique characteristics that make it particularly suitable for automatic learning .,0,0.9218766,26.928032099948133,15
1812,"We construct a dataset containing thousands of funny papers and use it to learn classifiers , combining findings from psychology and linguistics with recent advances in NLP .",2,0.8271769,47.74061481890242,28
1812,"We use our models to identify potentially funny papers in a large dataset of over 630,000 articles .",2,0.7721369,49.89441187100335,18
1812,"The results demonstrate the potential of our methods , and more broadly the utility of integrating state-of-the-art NLP methods with insights from more traditional disciplines .",3,0.98809737,18.783227642321528,32
1813,This paper presents a novel task to generate poll questions for social media posts .,1,0.8621515,31.7628842047521,15
1813,It offers an easy way to hear the voice from the public and learn from their feelings to important social topics .,0,0.55569506,67.4035475569628,22
1813,"While most related work tackles formal languages ( e.g. , exam papers ) , we generate poll questions for short and colloquial social media messages exhibiting severe data sparsity .",0,0.5007003,175.68197893167778,30
1813,"To deal with that , we propose to encode user comments and discover latent topics therein as contexts .",2,0.4136184,122.39042837256729,19
1813,They are then incorporated into a sequence-to-sequence ( S2S ) architecture for question generation and its extension with dual decoders to additionally yield poll choices ( answers ) .,2,0.574514,58.36387505826031,30
1813,"For experiments , we collect a large-scale Chinese dataset from Sina Weibo containing over 20 K polls .",2,0.90436107,50.62400345420822,18
1813,The results show that our model outperforms the popular S2S models without exploiting topics from comments and the dual decoder design can further benefit the prediction of both questions and answers .,3,0.983201,49.12782773505108,32
1813,Human evaluations further exhibit our superiority in yielding high-quality polls helpful to draw user engagements .,3,0.9738724,373.1407093905179,16
1814,Detecting online hate is a difficult task that even state-of-the-art models struggle with .,0,0.9099851,14.0003347451927,20
1814,"Typically , hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score .",0,0.82656187,32.53593283316108,27
1814,"However , this approach makes it difficult to identify specific model weak points .",0,0.75940156,57.193301601045356,14
1814,It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets .,0,0.53265023,80.42045371276748,22
1814,"To enable more targeted diagnostic insights , we introduce HateCheck , a suite of functional tests for hate speech detection models .",1,0.5155972,66.55781729691857,22
1814,We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders .,2,0.67794764,66.35643241624142,22
1814,We craft test cases for each functionality and validate their quality through a structured annotation process .,2,0.86231536,58.585460154328324,17
1814,"To illustrate HateCheck ’s utility , we test near-state-of-the-art transformer models as well as two popular commercial models , revealing critical model weaknesses .",2,0.6413055,93.54758789387417,30
1815,Recent studies constructing direct interactions between the claim and each single user response ( a comment or a relevant article ) to capture evidence have shown remarkable success in interpretable claim verification .,0,0.9054317,128.54761755578167,33
1815,"Owing to different single responses convey different cognition of individual users ( i.e. , audiences ) , the captured evidence belongs to the perspective of individual cognition .",0,0.500322,180.13447430333343,28
1815,"However , individuals ’ cognition of social things is not always able to truly reflect the objective .",0,0.92680824,154.01547867129736,18
1815,There may be one-sided or biased semantics in their opinions on a claim .,0,0.73121864,52.715337854543016,14
1815,"The captured evidence correspondingly contains some unobjective and biased evidence fragments , deteriorating task performance .",3,0.6820278,229.10736812628764,16
1815,"In this paper , we propose a Dual-view model based on the views of Collective and Individual Cognition ( CICD ) for interpretable claim verification .",1,0.9221613,52.27114520392025,26
1815,"From the view of the collective cognition , we not only capture the word-level semantics based on individual users , but also focus on sentence-level semantics ( i.e. , the overall responses ) among all users and adjust the proportion between them to generate global evidence .",2,0.7230381,55.309685407972076,50
1815,"From the view of individual cognition , we select the top-k articles with high degree of difference and interact with the claim to explore the local key evidence fragments .",2,0.8353695,109.96162225445875,30
1815,"To weaken the bias of individual cognition-view evidence , we devise inconsistent loss to suppress the divergence between global and local evidence for strengthening the consistent shared evidence between the both .",2,0.7055231,169.52750004129987,34
1815,Experiments on three benchmark datasets confirm that CICD achieves state-of-the-art performance .,3,0.878196,9.456620411050304,17
1816,"Rap generation , which aims to produce lyrics and corresponding singing beats , needs to model both rhymes and rhythms .",0,0.8551544,137.5499864155979,21
1816,"Previous works for rap generation focused on rhyming lyrics , but ignored rhythmic beats , which are important for rap performance .",0,0.90233386,139.73627138765332,22
1816,"In this paper , we develop DeepRapper , a Transformer-based rap generation system that can model both rhymes and rhythms .",1,0.8557368,49.78683574105522,23
1816,"Since there is no available rap datasets with rhythmic beats , we develop a data mining pipeline to collect a large-scale rap dataset , which includes a large number of rap songs with aligned lyrics and rhythmic beats .",2,0.7067898,35.03029921691872,39
1816,"Second , we design a Transformer-based autoregressive language model which carefully models rhymes and rhythms .",2,0.8703571,35.497467136650926,18
1816,"Specifically , we generate lyrics in the reverse order with rhyme representation and constraint for rhyme enhancement , and insert a beat symbol into lyrics for rhythm / beat modeling .",2,0.8737579,166.9851923500842,31
1816,"To our knowledge , DeepRapper is the first system to generate rap with both rhymes and rhythms .",3,0.91080326,104.04178078284077,18
1816,Both objective and subjective evaluations demonstrate that DeepRapper generates creative and high-quality raps with rhymes and rhythms .,3,0.9644732,91.261551140866,18
1817,"In this paper , we formulate the personalized news headline generation problem whose goal is to output a user-specific title based on both a user ’s reading interests and a candidate news body to be exposed to her .",1,0.6829612,62.049979291862385,39
1817,"To build up a benchmark for this problem , we publicize a large-scale dataset named PENS ( PErsonalized News headlineS ) .",2,0.7063759,152.10940459213205,22
1817,"The training set is collected from user impressions logs of Microsoft News , and the test set is manually created by hundreds of native speakers to enable a fair testbed for evaluating models in an offline mode .",2,0.7582132,71.78386333200574,38
1817,We propose a generic framework as a preparatory solution to our problem .,1,0.3639001,51.97042320602145,13
1817,"At its heart , user preference is learned by leveraging the user behavioral data , and three kinds of user preference injections are proposed to personalize a text generator and establish personalized headlines .",2,0.5870283,134.10479668929918,34
1817,We investigate our dataset by implementing several state-of-the-art user modeling methods in our framework to demonstrate a benchmark score for the proposed dataset .,2,0.5900887,28.70002032527715,30
1817,The dataset is available at https://msnews.github.io/pens.html .,3,0.57999593,14.353014359060461,7
1818,"Text style transfer aims to alter the style ( e.g. , sentiment ) of a sentence while preserving its content .",0,0.9326209,38.56867079673479,21
1818,"A common approach is to map a given sentence to content representation that is free of style , and the content representation is fed to a decoder with a target style .",0,0.849079,27.976620006639234,32
1818,"Previous methods in filtering style completely remove tokens with style at the token level , which incurs the loss of content information .",0,0.8264135,97.74249571908408,23
1818,"In this paper , we propose to enhance content preservation by implicitly removing the style information of each token with reverse attention , and thereby retain the content .",1,0.8574762,73.58880502137762,29
1818,"Furthermore , we fuse content information when building the target style representation , making it dynamic with respect to the content .",2,0.58753306,106.38734174384368,22
1818,"Our method creates not only style-independent content representation , but also content-dependent style representation in transferring style .",3,0.5919306,95.77956841351808,21
1818,Empirical results show that our method outperforms the state-of-the-art baselines by a large margin in terms of content preservation .,3,0.9637911,4.408969675127658,26
1818,"In addition , it is also competitive in terms of style transfer accuracy and fluency .",3,0.8027919,55.34268876585617,16
1819,This paper focuses on Seq2Seq ( S2S ) constrained text generation where the text generator is constrained to mention specific words which are inputs to the encoder in the generated outputs .,1,0.7036832,30.393671116940887,32
1819,"Pre-trained S2S models or a Copy Mechanism are trained to copy the surface tokens from encoders to decoders , but they cannot guarantee constraint satisfaction .",0,0.5685863,51.77219146760073,26
1819,Constrained decoding algorithms always produce hypotheses satisfying all constraints .,0,0.65237975,231.6636999384156,10
1819,"However , they are computationally expensive and can lower the generated text quality .",0,0.82238495,53.15675255720415,14
1819,"In this paper , we propose Mention Flags ( MF ) , which traces whether lexical constraints are satisfied in the generated outputs in an S2S decoder .",1,0.75230825,69.98707386654961,28
1819,"The MF models can be trained to generate tokens in a hypothesis until all constraints are satisfied , guaranteeing high constraint satisfaction .",3,0.44301188,212.7651469723795,23
1819,"Our experiments on the Common Sense Generation task ( CommonGen ) ( Lin et al. , 2020 ) , End2end Restaurant Dialog task ( E2ENLG ) ( Duˇsek et al. , 2020 ) and Novel Object Captioning task ( nocaps ) ( Agrawal et al. , 2019 ) show that the MF models maintain higher constraint satisfaction and text quality than the baseline models and other constrained decoding algorithms , achieving state-of-the-art performance on all three tasks .",3,0.56014526,43.77159204433713,84
1819,These results are achieved with a much lower run-time than constrained decoding algorithms .,3,0.8911239,66.50712050424029,14
1819,We also show that the MF models work well in the low-resource setting .,3,0.9614349,23.47863772634741,14
1820,Concept-to-text Natural Language Generation is the task of expressing an input meaning representation in natural language .,0,0.91782945,24.774131025749018,17
1820,Previous approaches in this task have been able to generalise to rare or unseen instances by relying on a delexicalisation of the input .,0,0.9240642,32.686910792248106,24
1820,"However , this often requires that the input appears verbatim in the output text .",0,0.89518225,39.525184734363435,15
1820,"This poses challenges in multilingual settings , where the task expands to generate the output text in multiple languages given the same input .",0,0.89707565,43.41800695510894,24
1820,"In this paper , we explore the application of multilingual models in concept-to-text and propose Language Agnostic Delexicalisation , a novel delexicalisation method that uses multilingual pretrained embeddings , and employs a character-level post-editing model to inflect words in their correct form during relexicalisation .",1,0.8748353,29.78621878818555,50
1820,"Our experiments across five datasets and five languages show that multilingual models outperform monolingual models in concept-to-text and that our framework outperforms previous approaches , especially in low resource conditions .",3,0.950287,23.733821814337222,34
1821,"Nowadays , open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pre-trained language models .",0,0.9111723,22.814082228699785,22
1821,"However , they generally concatenate the dialogue history directly as the model input to predict the response , which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances .",0,0.72867495,87.43805294764648,35
1821,"In this work , we propose the DialoFlow model , in which we introduce a dynamic flow mechanism to model the context flow , and design three training objectives to capture the information dynamics across dialogue utterances by addressing the semantic influence brought about by each utterance in large-scale pre-training .",1,0.5429807,41.40914218930652,51
1821,Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset demonstrate that our DialoFlow significantly outperforms the DialoGPT on the dialogue generation task .,3,0.938093,32.91328768427487,23
1821,"Besides , we propose the Flow score , an effective automatic metric for evaluating interactive human-bot conversation quality based on the pre-trained DialoFlow , which presents high chatbot-level correlation ( r=0.9 ) with human ratings among 11 chatbots .",2,0.56746536,67.95656373356005,41
1821,Code and pre-trained models will be public .,3,0.44935575,38.32495445404158,8
1822,The goal of dialogue state tracking ( DST ) is to predict the current dialogue state given all previous dialogue contexts .,0,0.8934629,25.77285838386243,22
1822,Existing approaches generally predict the dialogue state at every turn from scratch .,0,0.87009585,68.8207237682917,13
1822,"However , the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn .",3,0.5789587,40.949550431152986,22
1822,"Therefore , the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation .",3,0.75714993,61.315612940758946,30
1822,"To address this problem , we devise the two-stage DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue , and the Slot Value Generator based on the dialogue history .",2,0.6733449,36.54473792641801,37
1822,The Dual Slot Selector determines each slot whether to update slot value or to inherit the slot value from the previous turn from two aspects : ( 1 ) if there is a strong relationship between it and the current turn dialogue utterances ;,2,0.6070953,93.1391685365613,44
1822,( 2 ) if a slot value with high reliability can be obtained for it through the current turn dialogue .,3,0.45681718,117.8477952258197,21
1822,"The slots selected to be updated are permitted to enter the Slot Value Generator to update values by a hybrid method , while the other slots directly inherit the values from the previous turn .",2,0.50632566,96.94916819406602,35
1822,"Empirical results show that our method achieves 56.93 % , 60.73 % , and 58.04 % joint accuracy on MultiWOZ 2.0 , MultiWOZ 2.1 , and MultiWOZ 2.2 datasets respectively and achieves a new state-of-the-art performance with significant improvements .",3,0.91767216,7.578680640985131,46
1823,One of the difficulties in training dialogue systems is the lack of training data .,0,0.90400493,8.26668349631118,15
1823,We explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator .,2,0.3800237,26.059972949198464,20
1823,Our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents .,1,0.8640858,35.53317463318803,23
1823,"In this framework , we first pre-train the two agents on a collection of source domain dialogues , which equips the agents to converse with each other via natural language .",2,0.83175963,29.34839588934018,31
1823,"With further fine-tuning on a small amount of target domain data , the agents continue to interact with the aim of improving their behaviors using reinforcement learning with structured reward functions .",3,0.5308056,50.42342922404171,32
1823,"In experiments on the MultiWOZ dataset , two practical transfer learning problems are investigated : 1 ) domain adaptation and 2 ) single-to-multiple domain transfer .",2,0.71194273,31.36722594021119,27
1823,We demonstrate that the proposed framework is highly effective in bootstrapping the performance of the two agents in transfer learning .,3,0.9313373,21.135137447975644,21
1823,We also show that our method leads to improvements in dialogue system performance on complete datasets .,3,0.9528597,28.080084726149213,17
1824,Maintaining a consistent persona is essential for dialogue agents .,0,0.8110546,27.62916658151888,10
1824,"Although tremendous advancements have been brought , the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models .",0,0.92116094,62.30035149292518,29
1824,This work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERT-over-BERT ( BoB ) model .,1,0.56920534,27.346257529896356,31
1824,"Specifically , the model consists of a BERT-based encoder and two BERT-based decoders , where one decoder is for response generation , and another is for consistency understanding .",2,0.7583034,13.88441996165457,33
1824,"In particular , to learn the ability of consistency understanding from large-scale non-dialogue inference data , we train the second decoder in an unlikelihood manner .",2,0.73451966,82.62126254009787,26
1824,"Under different limited data settings , both automatic and human evaluations demonstrate that the proposed model outperforms strong baselines in response quality and persona consistency .",3,0.93129456,30.71056708462911,26
1825,"Multi-intent SLU can handle multiple intents in an utterance , which has attracted increasing attention .",0,0.9052459,46.538284090385666,16
1825,"However , the state-of-the-art joint models heavily rely on autoregressive approaches , resulting in two issues : slow inference speed and information leakage .",0,0.91306585,28.890208770925728,29
1825,"In this paper , we explore a non-autoregressive model for joint multiple intent detection and slot filling , achieving more fast and accurate .",1,0.86184746,62.06767528073956,24
1825,"Specifically , we propose a Global-Locally Graph Interaction Network ( GL-GIN ) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intent-slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance .",2,0.74160624,33.52632693715691,55
1825,Experimental results on two public datasets show that our framework achieves state-of-the-art performance while being 11.5 times faster .,3,0.9190724,6.794010221364306,25
1826,Both performance and efficiency are crucial factors for sequence labeling tasks in many real-world scenarios .,0,0.79761726,19.87434990283494,16
1826,"Although the pre-trained models ( PTMs ) have significantly improved the performance of various sequence labeling tasks , their computational cost is expensive .",0,0.86574423,25.23008467855104,24
1826,"To alleviate this problem , we extend the recent successful early-exit mechanism to accelerate the inference of PTMs for sequence labeling tasks .",2,0.44011977,76.95660500233268,25
1826,"However , existing early-exit mechanisms are specifically designed for sequence-level tasks , rather than sequence labeling .",0,0.87960076,117.56514070777293,21
1826,"In this paper , we first propose a simple extension of sentence-level early-exit for sequence labeling tasks .",1,0.823078,39.84546216478261,22
1826,"To further reduce the computational cost , we also propose a token-level early-exit mechanism that allows partial tokens to exit early at different layers .",2,0.612907,34.87166486355008,29
1826,"Considering the local dependency inherent in sequence labeling , we employed a window-based criterion to decide for a token whether or not to exit .",2,0.8446947,98.57027718749806,27
1826,"The token-level early-exit brings the gap between training and inference , so we introduce an extra self-sampling fine-tuning stage to alleviate it .",2,0.47939646,45.91311266802994,30
1826,The extensive experiments on three popular sequence labeling tasks show that our approach can save up to 66 % ∼75 % inference cost with minimal performance degradation .,3,0.9034584,44.76743637651386,29
1826,"Compared with competitive compressed models such as DistilBERT , our approach can achieve better performance under the same speed-up ratios of 2 × , 3 × , and 4 × .",3,0.8655108,33.26673957882868,33
1827,"Although the existing Named Entity Recognition ( NER ) models have achieved promising performance , they suffer from certain drawbacks .",0,0.9470548,22.048457510280834,21
1827,"The sequence labeling-based NER models do not perform well in recognizing long entities as they focus only on word-level information , while the segment-based NER models which focus on processing segment instead of single word are unable to capture the word-level dependencies within the segment .",3,0.64194906,25.380947026581044,51
1827,"Moreover , as boundary detection and type prediction may cooperate with each other for the NER task , it is also important for the two sub-tasks to mutually reinforce each other by sharing their information .",0,0.53144234,31.461289548437023,36
1827,"In this paper , we propose a novel Modularized Interaction Network ( MIN ) model which utilizes both segment-level information and word-level dependencies , and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task .",1,0.8549521,39.0826913778061,49
1827,We have conducted extensive experiments based on three NER benchmark datasets .,2,0.7883245,22.95749223845958,12
1827,The performance results have shown that the proposed MIN model has outperformed the current state-of-the-art models .,3,0.971657,13.359419771760873,23
1828,Capturing interactions among event arguments is an essential step towards robust event argument extraction ( EAE ) .,0,0.8835065,61.90927186072145,18
1828,"1 ) The argument role type information of contextual entities is mainly utilized as training signals , ignoring the potential merits of directly adopting it as semantically rich input features ;",3,0.73077035,265.4405391767152,31
1828,"2 ) The argument-level sequential semantics , which implies the overall distribution pattern of argument roles over an event mention , is not well characterized .",0,0.7144797,133.032636112156,27
1828,"To tackle the above two bottlenecks , we formalize EAE as a Seq2Seq-like learning problem for the first time , where a sentence with a specific event trigger is mapped to a sequence of event argument roles .",2,0.7408724,29.098379284946123,40
1828,"A neural architecture with a novel Bi-directional Entity-level Recurrent Decoder ( BERD ) is proposed to generate argument roles by incorporating contextual entities ’ argument role predictions , like a word-by-word text generation process , thereby distinguishing implicit argument distribution patterns within an event more accurately .",2,0.4615958,85.58043481196965,52
1829,"Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks ( i.e. , entity detection and relation classification ) .",0,0.71226966,73.54797173801259,25
1829,We argue that this setting may hinder the information interaction between entities and relations .,3,0.7425548,63.22603217569846,15
1829,"In this work , we propose to eliminate the different treatment on the two sub-tasks ’ label spaces .",1,0.62130576,102.96929997589716,19
1829,The input of our model is a table containing all word pairs from a sentence .,2,0.7351139,29.08783605222568,16
1829,Entities and relations are represented by squares and rectangles in the table .,3,0.34096807,37.35056268593799,13
1829,"We apply a unified classifier to predict each cell ’s label , which unifies the learning of two sub-tasks .",2,0.84870106,48.45118623943499,20
1829,"For testing , an effective ( yet fast ) approximate decoder is proposed for finding squares and rectangles from tables .",2,0.49983603,144.78443087745214,21
1829,"Experiments on three benchmarks ( ACE04 , ACE05 , SciERC ) show that , using only half the number of parameters , our model achieves competitive accuracy with the best extractor , and is faster .",3,0.88237107,49.04877018752688,36
1830,"Continual learning has gained increasing attention in recent years , thanks to its biological interpretation and efficiency in many real-world applications .",0,0.9566384,28.802973824394616,22
1830,"As a typical task of continual learning , continual relation extraction ( CRE ) aims to extract relations between entities from texts , where the samples of different relations are delivered into the model continuously .",0,0.8983297,59.63192186261754,36
1830,Some previous works have proved that storing typical samples of old relations in memory can help the model keep a stable understanding of old relations and avoid forgetting them .,0,0.9016967,47.11111788867127,30
1830,"However , most methods heavily depend on the memory size in that they simply replay these memorized samples in subsequent tasks .",0,0.8526289,78.90951911649015,22
1830,"To fully utilize memorized samples , in this paper , we employ relation prototype to extract useful information of each relation .",2,0.6087712,142.1519143451235,22
1830,"Specifically , the prototype embedding for a specific relation is computed based on memorized samples of this relation , which is collected by K-means algorithm .",2,0.7803113,41.65430346984979,26
1830,"The prototypes of all observed relations at current learning stage are used to re-initialize a memory network to refine subsequent sample embeddings , which ensures the model ’s stable understanding on all observed relations when learning a new task .",2,0.5650321,74.98609556313178,40
1830,"Compared with previous CRE models , our model utilizes the memory information sufficiently and efficiently , resulting in enhanced CRE performance .",3,0.88739324,101.49982821507214,22
1830,Our experiments show that the proposed model outperforms the state-of-the-art CRE models and has great advantage in avoiding catastrophic forgetting .,3,0.97478503,15.463478916256976,26
1830,The code and datasets are released on https://github.com/fd2014cl/RP-CRE .,3,0.54562837,28.337866302833294,9
1831,"Existing multilingual machine translation approaches mainly focus on English-centric directions , while the non-English directions still lag behind .",0,0.91897935,27.692199851346963,19
1831,"In this work , we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions .",1,0.9341041,21.736820624605603,26
1831,Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance .,3,0.460482,22.03574506942242,19
1831,"To this end , we propose mRASP2 , a training method to obtain a single unified multilingual translation model .",2,0.3854037,35.38578434499055,20
1831,"mRASP2 is empowered by two techniques : a ) a contrastive learning scheme to close the gap among representations of different languages , and b ) data augmentation on both multiple parallel and monolingual data to further align token representations .",0,0.49551862,47.841431263073915,41
1831,"For English-centric directions , mRASP2 achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks .",3,0.896886,51.31894823277469,23
1831,"For non-English directions , mRASP2 achieves an improvement of average 10 + BLEU compared with the multilingual baseline .",3,0.9369492,53.39697604854736,19
1832,"Neural Machine Translation ( NMT ) currently exhibits biases such as producing translations that are too short and overgenerating frequent words , and shows poor robustness to copy noise in training data or domain shift .",0,0.9525407,62.952515149895724,36
1832,Recent work has tied these shortcomings to beam search – the de facto standard inference algorithm in NMT – and Eikema & Aziz ( 2020 ) propose to use Minimum Bayes Risk ( MBR ) decoding on unbiased samples instead .,0,0.86050403,136.19838933368408,41
1832,"In this paper , we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search .",1,0.9063915,49.366577873454005,26
1832,"We find that MBR still exhibits a length and token frequency bias , owing to the MT metrics used as utility functions , but that MBR also increases robustness against copy noise in the training data and domain shift .",3,0.9751221,128.05549469004987,40
1833,One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level .,0,0.77844954,35.093646928402634,24
1833,"However , the computational complexity of a self-attention network is O ( n2 ) , increasing quadratically with sequence length .",3,0.5318619,39.83361755222291,21
1833,"By contrast , the complexity of LSTM-based approaches is only O(n ) .",3,0.58610946,43.70321690921415,16
1833,"In practice , however , LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level : to model context , the current LSTM state relies on the full LSTM computation of the preceding state .",0,0.6690253,51.98934727158345,42
1833,This has to be computed n times for a sequence of length n .,3,0.37540537,47.93104302360719,14
1833,The linear transformations involved in the LSTM gate and state computations are the major cost factors in this .,3,0.6527688,77.97985762068143,19
1833,"To enable sequence-level parallelization of LSTMs , we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context .",2,0.7893318,66.3879228926243,34
1833,"This allows us to compute each input step efficiently in parallel , avoiding the formerly costly sequential linear transformations .",2,0.43656468,162.77953448992972,20
1833,We then connect the outputs of each parallel step with computationally cheap element-wise computations .,2,0.8416654,91.60305638357518,16
1833,We call this the Highly Parallelized LSTM .,2,0.41491628,38.99645502044753,8
1833,"To further constrain the number of LSTM parameters , we compute several small HPLSTMs in parallel like multi-head attention in the Transformer .",2,0.6405427,55.64254342375023,23
1833,"The experiments show that our MHPLSTM decoder achieves significant BLEU improvements , while being even slightly faster than the self-attention network in training , and much faster than the standard LSTM .",3,0.95930827,30.324669479038054,32
1834,Word alignment and machine translation are two closely related tasks .,0,0.8966291,27.598513098821137,11
1834,"Neural translation models , such as RNN-based and Transformer models , employ a target-to-source attention mechanism which can provide rough word alignments , but with a rather low accuracy .",0,0.8292574,51.86058424151317,35
1834,"High-quality word alignment can help neural machine translation in many different ways , such as missing word detection , annotation transfer and lexicon injection .",0,0.79395413,54.597538227544526,25
1834,Existing methods for learning word alignment include statistical word aligners ( e.g .,0,0.81986046,61.918984913565296,13
1834,GIZA + + ) and recently neural word alignment models .,0,0.5138326,280.91013686570636,11
1834,This paper presents a bidirectional Transformer based alignment ( BTBA ) model for unsupervised learning of the word alignment task .,1,0.791811,31.588808500920177,21
1834,Our BTBA model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention ( alignment ) .,2,0.6661625,71.19406581179042,31
1834,We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training .,2,0.54530245,36.95863761657889,27
1834,We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA ++ .,3,0.72590417,23.055385785886642,30
1835,Multilingual neural machine translation aims at learning a single translation model for multiple languages .,0,0.91323155,19.023392624065238,15
1835,These jointly trained models often suffer from performance degradationon rich-resource language pairs .,0,0.71676016,71.3415866033701,13
1835,We attribute this degeneration to parameter interference .,3,0.77679837,155.86225036088072,8
1835,"In this paper , we propose LaSS to jointly train a single unified multilingual MT model .",1,0.8562032,51.23107556181114,17
1835,LaSS learns Language Specific Sub-network ( LaSS ) for each language pair to counter parameter interference .,2,0.6355658,235.17816148885547,17
1835,Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU .,3,0.8934308,23.14580963904062,26
1835,"Besides , LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation .",3,0.76067245,75.07800941522271,19
1835,LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs .,3,0.83937037,27.480379325860056,15
1835,Codes and trained models are available at https://github.com/NLP-Playground/LaSS .,3,0.6244637,14.427540408356869,9
1836,"While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years , important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data .",0,0.8659506,15.404242869546536,49
1836,Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field .,0,0.85167444,27.91098950057945,21
1836,One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data .,0,0.9085228,29.25873801491457,27
1836,Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts .,0,0.8827402,26.62293574614067,23
1836,"In this work , we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation .",1,0.92554545,24.83161580141754,28
1836,A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data .,3,0.88170856,17.840300147125127,56
1837,"As a fine-grained task , the annotation cost of aspect term extraction is extremely high .",0,0.720601,41.27361471094473,17
1837,Recent attempts alleviate this issue using domain adaptation that transfers common knowledge across domains .,0,0.93016744,80.32337823211155,15
1837,"Since most aspect terms are domain-specific , they cannot be transferred directly .",0,0.64137334,64.40929343885233,13
1837,Existing methods solve this problem by associating aspect terms with pivot words ( we call this passive domain adaptation because the transfer of aspect terms relies on the links to pivots ) .,0,0.6787791,82.06226862153906,33
1837,"However , all these methods need either manually labeled pivot words or expensive computing resources to build associations .",0,0.7945972,176.23246568416027,19
1837,"In this paper , we propose a novel active domain adaptation method .",1,0.9026457,25.169877099804765,13
1837,Our goal is to transfer aspect terms by actively supplementing transferable knowledge .,1,0.754946,136.44397061661445,13
1837,"To this end , we construct syntactic bridges by recognizing syntactic roles as pivots instead of as links to pivots .",2,0.7933121,46.50148347547023,21
1837,We also build semantic bridges by retrieving transferable semantic prototypes .,2,0.52219385,187.40107287065598,11
1837,Extensive experiments show that our method significantly outperforms previous approaches .,3,0.9244677,7.327560091128832,11
1838,"With the popularity of smartphones , we have witnessed the rapid proliferation of multimodal posts on various social media platforms .",0,0.95903915,14.809319966667996,21
1838,"We observe that the multimodal sentiment expression has specific global characteristics , such as the interdependencies of objects or scenes within the image .",3,0.95624655,46.419546558044786,24
1838,"However , most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset .",0,0.8702189,28.45324095359381,28
1838,"In this paper , we propose Multi-channel Graph Neural Networks with Sentiment-awareness ( MGNNS ) for image-text sentiment detection .",1,0.86387765,40.12880180593282,23
1838,"Specifically , we first encode different modalities to capture hidden representations .",2,0.7759497,57.03061115386698,12
1838,"Then , we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset .",2,0.74468577,19.88346396260791,21
1838,"Finally , we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs .",2,0.58818865,26.961983942387434,24
1838,Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection .,3,0.770598,10.612754088808131,19
1839,Product reviews contain a large number of implicit aspects and implicit opinions .,0,0.9473575,64.78446308110273,13
1839,"However , most of the existing studies in aspect-based sentiment analysis ignored this problem .",0,0.8744789,35.75749205715688,15
1839,"In this work , we introduce a new task , named Aspect-Category-Opinion-Sentiment ( ACOS ) Quadruple Extraction , with the goal to extract all aspect-category-opinion-sentiment quadruples in a review sentence and provide full support for aspect-based sentiment analysis with implicit aspects and opinions .",1,0.7515066,35.283831423041406,52
1839,"We furthermore construct two new datasets , Restaurant-ACOS and Laptop-ACOS , for this new task , both of which contain the annotations of not only aspect-category-opinion-sentiment quadruples but also implicit aspects and opinions .",2,0.74562025,65.32651872188548,42
1839,The former is an extension of the SemEval Restaurant dataset ;,2,0.37807715,100.73554939563356,11
1839,"the latter is a newly collected and annotated Laptop dataset , twice the size of the SemEval Laptop dataset .",2,0.4469956,41.55691337200302,20
1839,We finally benchmark the task with four baseline systems .,2,0.6163611,107.38307367007305,10
1839,Experiments demonstrate the feasibility of the new task and its effectiveness in extracting and describing implicit aspects and implicit opinions .,3,0.92009205,47.25406143980382,21
1839,The two datasets and source code of four systems are publicly released at https://github.com/NUSTM/ACOS .,3,0.47690573,25.98698968187347,15
1840,The product reviews summarization task aims to automatically produce a short summary for a set of reviews of a given product .,0,0.89984566,26.13139797754581,22
1840,"Such summaries are expected to aggregate a range of different opinions in a concise , coherent and informative manner .",0,0.776197,38.43247837256978,20
1840,This challenging task gives rise to two shortcomings in existing work .,0,0.88395023,44.143293603149054,12
1840,"First , summarizers tend to favor generic content that appears in reviews for many different products , resulting in template-like , less informative summaries .",0,0.635752,107.6539266821779,27
1840,"Second , as reviewers often disagree on the pros and cons of a given product , summarizers sometimes yield inconsistent , self-contradicting summaries .",0,0.8678276,40.73653783948181,26
1840,"We propose the PASS system ( Perturb-and-Select Summarizer ) that employs a large pre-trained Transformer-based model ( T5 in our case ) , which follows a few-shot fine-tuning scheme .",2,0.60103875,40.26053174132533,36
1840,"A key component of the PASS system relies on applying systematic perturbations to the model ’s input during inference , which allows it to generate multiple different summaries per product .",0,0.44683215,62.18211066439101,31
1840,"We develop a method for ranking these summaries according to desired criteria , coherence in our case , enabling our system to almost entirely avoid the problem of self-contradiction .",2,0.479886,63.08900510418084,30
1840,"We compare our system against strong baselines on publicly available datasets , and show that it produces summaries which are more informative , diverse and coherent .",3,0.7795045,29.83174631912246,27
1841,"For sentence-level extractive summarization , there is a disproportionate ratio of selected and unselected sentences , leading to flatting the summary features when maximizing the accuracy .",3,0.5147058,62.53378067628067,29
1841,"The imbalanced classification of summarization is inherent , which ca n’t be addressed by common algorithms easily .",0,0.75360316,314.58027807678616,18
1841,"In this paper , we conceptualize the single-document extractive summarization as a rebalance problem and present a deep differential amplifier framework .",1,0.82166195,38.929301429062065,22
1841,"Specifically , we first calculate and amplify the semantic difference between each sentence and all other sentences , and then apply the residual unit as the second item of the differential amplifier to deepen the architecture .",2,0.8897515,66.48888799071587,37
1841,"Finally , to compensate for the imbalance , the corresponding objective loss of minority class is boosted by a weighted cross-entropy .",2,0.61362046,79.84839818409786,22
1841,"In contrast to previous approaches , this model pays more attention to the pivotal information of one sentence , instead of all the informative context modeling by recurrent or Transformer architecture .",3,0.42980716,98.93210279431052,32
1841,We demonstrate experimentally on two benchmark datasets that our summarizer performs competitively against state-of-the-art methods .,3,0.7424388,13.158131822539925,21
1841,Our source code will be available on Github .,3,0.67672384,10.002143932569991,9
1842,"In this paper , we address a novel task , Multiple TimeLine Summarization ( MTLS ) , which extends the flexibility and versatility of Time-Line Summarization ( TLS ) .",1,0.8977914,58.043598118201096,31
1842,"Given any collection of time-stamped news articles , MTLS automatically discovers important yet different stories and generates a corresponding time-line for each story .",0,0.46141648,89.94564912459451,27
1842,"To achieve this , we propose a novel unsupervised summarization framework based on two-stage affinity propagation .",2,0.52660143,23.41866745256118,18
1842,We also introduce a quantitative evaluation measure for MTLS based on previousTLS evaluation methods .,2,0.42888194,93.6178702536475,15
1842,Experimental results show that our MTLS framework demonstrates high effectiveness and MTLS task can give bet-ter results than TLS .,3,0.9808708,144.82696489942956,20
1843,"Recently , opinion summarization , which is the generation of a summary from multiple reviews , has been conducted in a self-supervised manner by considering a sampled review as a pseudo summary .",0,0.92521244,31.913051993074486,33
1843,"However , non-text data such as image and metadata related to reviews have been considered less often .",0,0.9173169,92.1356130961361,18
1843,"To use the abundant information contained in non-text data , we propose a self-supervised multimodal opinion summarization framework called MultimodalSum .",2,0.5599009,20.17089074465339,21
1843,"Our framework obtains a representation of each modality using a separate encoder for each modality , and the text decoder generates a summary .",2,0.66508603,27.5956377893638,24
1843,"To resolve the inherent heterogeneity of multimodal data , we propose a multimodal training pipeline .",2,0.37453786,26.443505404341717,16
1843,We first pretrain the text encoder –decoder based solely on text modality data .,2,0.87518966,81.48191454976076,14
1843,"Subsequently , we pretrain the non-text modality encoders by considering the pretrained text decoder as a pivot for the homogeneous representation of multimodal data .",2,0.8143481,25.63662242158048,25
1843,"Finally , to fuse multimodal representations , we train the entire framework in an end-to-end manner .",2,0.6819479,20.579600884237934,19
1843,We demonstrate the superiority of MultimodalSum by conducting experiments on Yelp and Amazon datasets .,3,0.7005234,40.46468740971329,15
1844,"In recent years , reference-based and supervised summarization evaluation metrics have been widely explored .",0,0.9358041,45.652332317885254,17
1844,"However , collecting human-annotated references and ratings are costly and time-consuming .",0,0.9341159,30.690408998560738,14
1844,"To avoid these limitations , we propose a training-free and reference-free summarization evaluation metric .",2,0.43624207,47.81014265005776,17
1844,Our metric consists of a centrality-weighted relevance score and a self-referenced redundancy score .,2,0.8027433,25.546009952947962,15
1844,"The relevance score is computed between the pseudo reference built from the source document and the given summary , where the pseudo reference content is weighted by the sentence centrality to provide importance guidance .",2,0.69977814,75.812904936898,35
1844,"Besides an F1-based relevance score , we also design an F𝛽-based variant that pays more attention to the recall score .",2,0.6488645,45.69464855703812,23
1844,"As for the redundancy score of the summary , we compute a self-masked similarity score with the summary itself to evaluate the redundant information in the summary .",2,0.73843056,38.05258216981533,28
1844,"Finally , we combine the relevance and redundancy scores to produce the final evaluation score of the given summary .",2,0.68411326,56.54690209203574,20
1844,Extensive experiments show that our methods can significantly outperform existing methods on both multi-document and single-document summarization evaluation .,3,0.94227004,8.732138565926935,19
1844,The source code is released at https://github.com/Chen-Wang-CUHK/Training-Free-and-Ref-Free-Summ-Evaluation .,3,0.5453303,14.51080952318369,8
1845,Short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering .,0,0.9315898,24.404044541962715,32
1845,"However , generating entity descriptions , especially for new and long-tail entities , can be challenging since relevant information is often scattered across multiple sources with varied content and style .",0,0.8970066,56.24402439134708,31
1845,"We introduce DESCGEN : given mentions spread over multiple documents , the goal is to generate an entity summary description .",2,0.69373053,249.54505799050543,21
1845,"DESCGEN consists of 37 K entity descriptions from Wikipedia and Fandom , each paired with nine evidence documents on average .",2,0.632288,176.96913214837897,21
1845,"The documents were collected using a combination of entity linking and hyperlinks into the entity pages , which together provide high-quality distant supervision .",2,0.8763896,71.6044529956558,24
1845,"Compared to other multi-document summarization tasks , our task is entity-centric , more abstractive , and covers a wide range of domains .",3,0.55658996,33.75434030556448,24
1845,"We also propose a two-stage extract-then-generate baseline and show that there exists a large gap ( 19.9 % in ROUGE-L ) between state-of-art models and human performance , suggesting that the data will support significant future work .",3,0.9217332,26.701696335627243,48
1846,"With the recent success of pre-trained models in NLP , a significant focus was put on interpreting their representations .",0,0.8838897,21.63831858689733,20
1846,"One of the most prominent approaches is structural probing ( Hewitt and Manning , 2019 ) , where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures .",0,0.8392531,32.728466971915985,36
1846,"In this work , we introduce a new type of structural probing , where the linear projection is decomposed into 1 .",1,0.6104584,56.65683181178394,22
1846,iso-morphic space rotation ; 2 .,2,0.37349904,913.9507445314491,6
1846,linear scaling that identifies and scales the most relevant dimensions .,0,0.38145816,106.83495344562806,11
1846,"In addition to syntactic dependency , we evaluate our method on two novel tasks ( lexical hypernymy and position in a sentence ) .",2,0.68405277,40.95700037971374,24
1846,We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations .,3,0.52693945,31.970427186041725,22
1846,"Moreover , the orthogonal constraint makes the Structural Probes less vulnerable to memorization .",3,0.57246053,57.70501615131309,14
1847,Backdoor attacks are a kind of insidious security threat against machine learning models .,0,0.93512154,53.548994463702186,14
1847,"After being injected with a backdoor in training , the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference .",3,0.50748,154.83755239224885,34
1847,"As a sort of emergent attack , backdoor attacks in natural language processing ( NLP ) are investigated insufficiently .",0,0.95350033,86.60380612682704,20
1847,"As far as we know , almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers , which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort .",0,0.8626463,70.73288902071857,41
1847,"In this paper , we propose to use the syntactic structure as the trigger in textual backdoor attacks .",1,0.88594806,45.89715538769483,19
1847,We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance ( almost 100 % success rate ) to the insertion-based methods but possesses much higher invisibility and stronger resistance to defenses .,3,0.70184785,62.35955619469294,43
1847,These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks .,3,0.98959816,89.98069661342907,14
1847,All the code and data of this paper can be obtained at https://github.com/thunlp/HiddenKiller .,3,0.58508265,5.435889048595803,14
1848,"Since language models are used to model a wide variety of languages , it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages .",0,0.918838,20.845433048016872,36
1848,Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup .,0,0.9117295,34.62155652755455,19
1848,"Languages vary in many typological dimensions , and it is difficult to single out one or two to investigate without the others acting as confounders .",0,0.877661,40.90178776838334,26
1848,We propose a novel method for investigating the inductive biases of language models using artificial languages .,1,0.53529465,21.529013759835482,17
1848,"These languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated , such as word order .",2,0.45033404,39.95530862778994,28
1848,We then use them to train and test language models .,2,0.71786636,24.742415112212928,11
1848,"This constitutes a fully controlled causal framework , and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models .",3,0.8829289,68.19648581541914,24
1848,"Using this method , we find that commonly used neural architectures exhibit different inductive biases : LSTMs display little preference with respect to word ordering , while transformers display a clear preference for some orderings over others .",3,0.8237067,41.649348115863496,38
1848,"Further , we find that neither the inductive bias of the LSTM nor that of the transformer appear to reflect any tendencies that we see in attested natural languages .",3,0.98341376,26.836278007223974,30
1849,"Despite the success of contextualized language models on various NLP tasks , it is still unclear what these models really learn .",0,0.9323965,9.845182978962931,22
1849,"In this paper , we contribute to the current efforts of explaining such models by exploring the continuum between function and content words with respect to contextualization in BERT , based on linguistically-informed insights .",1,0.87737435,48.6390213382899,37
1849,"In particular , we utilize scoring and visual analytics techniques : we use an existing similarity-based score to measure contextualization and integrate it into a novel visual analytics technique , presenting the model ’s layers simultaneously and highlighting intra-layer properties and inter-layer differences .",2,0.8308662,63.06437178853133,46
1849,We show that contextualization is neither driven by polysemy nor by pure context variation .,3,0.9303913,62.13169520280615,15
1849,We also provide insights on why BERT fails to model words in the middle of the functionality continuum .,3,0.70980483,51.023400211092294,19
1850,Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations .,0,0.9112394,29.6269173812768,20
1850,"Attention has empirically been demonstrated to improve performance in various tasks , while its weights have been extensively used as explanations for model predictions .",0,0.9185065,38.26294358289404,25
1850,"Recent studies ( Jain and Wallace , 2019 ; Serrano and Smith , 2019 ; Wiegreffe and Pinter , 2019 ) have showed that it cannot generally be considered as a faithful explanation ( Jacovi and Goldberg , 2020 ) across encoders and tasks .",0,0.70332354,62.86411394079996,45
1850,"In this paper , we seek to improve the faithfulness of attention-based explanations for text classification .",1,0.9370843,22.524240286312,19
1850,We achieve this by proposing a new family of Task-Scaling ( TaSc ) mechanisms that learn task-specific non-contextualised information to scale the original attention weights .,2,0.5126923,42.393992027759644,29
1850,"Evaluation tests for explanation faithfulness , show that the three proposed variants of TaSc improve attention-based explanations across two attention mechanisms , five encoders and five text classification datasets without sacrificing predictive performance .",3,0.92866904,123.31823793548435,36
1850,"Finally , we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques .",3,0.9843518,72.21352268378988,23
1851,"Car-focused navigation services are based on turns and distances of named streets , whereas navigation instructions naturally used by humans are centered around physical objects called landmarks .",0,0.9126067,127.74330144243271,28
1851,We present a neural model that takes OpenStreetMap representations as input and learns to generate navigation instructions that contain visible and salient landmarks from human natural language instructions .,2,0.526342,46.64500160947074,29
1851,Routes on the map are encoded in a location-and rotation-invariant graph representation that is decoded into natural language instructions .,2,0.44341525,38.37734742811185,24
1851,"Our work is based on a novel dataset of 7,672 crowd-sourced instances that have been verified by human navigation in Street View .",2,0.7473055,30.03928764458566,25
1851,"Our evaluation shows that the navigation instructions generated by our system have similar properties as human-generated instructions , and lead to successful human navigation in Street View .",3,0.97733885,44.83504993417356,28
1852,Vision-language pre-training ( VLP ) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks .,0,0.9106384,28.50934379246728,23
1852,"The most existing pre-training methods mainly adopt a two-step training procedure , which firstly employs a pre-trained object detector to extract region-based visual features , then concatenates the image representation and text embedding as the input of Transformer to train .",0,0.6696995,35.38983415836179,43
1852,"However , these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding , and the computation inefficiency of two-stage pipeline .",0,0.8328003,97.16196216891286,31
1852,"In this paper , we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation , namely E2E-VLP , where we build a unified Transformer framework to jointly learn visual representation , and semantic alignments between image and text .",1,0.7464515,40.701491349382025,48
1852,We incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning .,2,0.7265889,20.61864010500059,25
1852,An extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel VLP paradigm .,2,0.38534677,23.701827130471926,27
1853,"Despite the achievements of large-scale multimodal pre-training approaches , cross-modal retrieval , e.g. , image-text retrieval , remains a challenging task .",0,0.9462585,28.688533868739302,22
1853,"To bridge the semantic gap between the two modalities , previous studies mainly focus on word-region alignment at the object level , lacking the matching between the linguistic relation among the words and the visual relation among the regions .",0,0.92294824,41.07848968378164,40
1853,The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability .,3,0.49644548,46.33571022126615,22
1853,"In this paper , we first propose a novel metric , Intra-modal Self-attention Distance ( ISD ) , to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations .",1,0.8241392,34.29471512606102,34
1853,"In response , we present Inter-modal Alignment on Intra-modal Self-attentions ( IAIS ) , a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment .",1,0.4654376,22.277201150416712,35
1853,"The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin , which demonstrates the superiority of our approach .",3,0.91233647,66.05441035586911,28
1854,"We present Knowledge Enhanced Multimodal BART ( KM-BART ) , which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts .",1,0.5705274,21.219562898124487,36
1854,"We adapt the generative BART architecture ( Lewis et al. , 2020 ) to a multimodal model with visual and textual inputs .",2,0.8115187,53.8340348778519,23
1854,We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation ( VCG ) task .,3,0.5462145,31.02221586061116,21
1854,"In particular , our pretraining task of Knowledge-based Commonsense Generation ( KCG ) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs .",3,0.4476914,38.71980893615677,38
1854,"To the best of our knowledge , we are the first to propose a dedicated task for improving model performance on the VCG task .",3,0.8713234,15.902110489913373,25
1854,"Experimental results show that our model reaches state-of-the-art performance on the VCG task ( Park et al. , 2020 ) by applying these novel pretraining tasks .",3,0.9584641,25.646440569973894,33
1855,Transformers have advanced the field of natural language processing ( NLP ) on a variety of important tasks .,0,0.9557579,15.753888610180939,19
1855,At the cornerstone of the Transformer architecture is the multi-head attention ( MHA ) mechanism which models pairwise interactions between the elements of the sequence .,0,0.91903734,21.05156987649179,26
1855,"Despite its massive success , the current framework ignores interactions among different heads , leading to the problem that many of the heads are redundant in practice , which greatly wastes the capacity of the model .",0,0.75048107,45.61072947696938,37
1855,"To improve parameter efficiency , we re-formulate the MHA as a latent variable model from a probabilistic perspective .",2,0.68225276,33.50102171332352,19
1855,We present cascaded head-colliding attention ( CODA ) which explicitly models the interactions between attention heads through a hierarchical variational distribution .,2,0.5678302,56.60304132584702,23
1855,"We conduct extensive experiments and demonstrate that CODA outperforms the transformer baseline , by 0.6 perplexity on Wikitext-103 in language modeling , and by 0.6 BLEU on WMT14 EN-DE in machine translation , due to its improvements on the parameter efficiency .",3,0.7692979,37.374747772523364,46
1856,"Knowledge distillation is a critical technique to transfer knowledge between models , typically from a large model ( the teacher ) to a more fine-grained one ( the student ) .",0,0.92742425,21.486104329715644,33
1856,The objective function of knowledge distillation is typically the cross-entropy between the teacher and the student ’s output distributions .,0,0.744887,30.82164406680963,20
1856,"However , for structured prediction problems , the output space is exponential in size ;",0,0.7440012,311.9951468596441,15
1856,"therefore , the cross-entropy objective becomes intractable to compute and optimize directly .",0,0.54207295,60.51154811167929,13
1856,"In this paper , we derive a factorized form of the knowledge distillation objective for structured prediction , which is tractable for many typical choices of the teacher and student models .",1,0.762588,52.80743084464474,32
1856,"In particular , we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios : 1 ) the teacher and student share the same factorization form of the output structure scoring function ;",3,0.5374929,128.24180696858465,44
1856,2 ) the student factorization produces more fine-grained substructures than the teacher factorization ;,3,0.85917723,61.3195601387589,15
1856,3 ) the teacher factorization produces more fine-grained substructures than the student factorization ;,3,0.8866132,66.22345126285111,15
1856,4 ) the factorization forms from the teacher and the student are incompatible .,3,0.70120645,113.20494927478441,14
1857,State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model .,0,0.6743783,19.174226894778613,24
1857,"However , such modules are trained separately for each task and thus do not enable sharing information across tasks .",0,0.793198,30.750703039233162,20
1857,"In this paper , we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks , which condition on task , adapter position , and layer id in a transformer model .",1,0.61663485,97.05134178160596,40
1857,This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters .,3,0.6741142,23.50444630262485,38
1857,Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29 % parameters per task .,3,0.9086774,23.817285218627998,23
1857,We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks .,3,0.9508541,29.90427457098526,16
1857,Our code is publicly available in https://github.com/rabeehk/hyperformer .,3,0.5923006,15.607048290817197,8
1858,"Pre-trained multilingual language models , e.g. , multilingual-BERT , are widely used in cross-lingual tasks , yielding the state-of-the-art performance .",0,0.79350847,14.117685104082621,29
1858,"However , such models suffer from a large performance gap between source and target languages , especially in the zero-shot setting , where the models are fine-tuned only on English but tested on other languages for the same task .",0,0.8113124,15.21647512657495,40
1858,"We tackle this issue by incorporating language-agnostic information , specifically , universal syntax such as dependency relations and POS tags , into language models , based on the observation that universal syntax is transferable across different languages .",2,0.4677163,62.06489330274157,40
1858,"Our approach , called COunterfactual SYntax ( COSY ) , includes the design of SYntax-aware networks as well as a COunterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax .",2,0.7041932,45.04842695648404,41
1858,"To evaluate COSY , we conduct cross-lingual experiments on natural language inference and question answering using mBERT and XLM-R as network backbones .",2,0.8048464,33.762485521958666,25
1858,"Our results show that COSY achieves the state-of-the-art performance for both tasks , without using auxiliary training data .",3,0.9868225,19.420863625677637,25
1859,"Recent studies on neural networks with pre-trained weights ( i.e. , BERT ) have mainly focused on a low-dimensional subspace , where the embedding vectors computed from input words ( or their contexts ) are located .",0,0.88772434,38.88510961388196,37
1859,"In this work , we propose a new approach , called OoMMix , to finding and regularizing the remainder of the space , referred to as out-of-manifold , which cannot be accessed through the words .",1,0.7294519,92.77477040642816,38
1859,"Specifically , we synthesize the out-of-manifold embeddings based on two embeddings obtained from actually-observed words , to utilize them for fine-tuning the network .",2,0.85943633,30.757096805787956,27
1859,"A discriminator is trained to detect whether an input embedding is located inside the manifold or not , and simultaneously , a generator is optimized to produce new embeddings that can be easily identified as out-of-manifold by the discriminator .",2,0.5099387,28.816539667147445,41
1859,These two modules successfully collaborate in a unified and end-to-end manner for regularizing the out-of-manifold .,3,0.76347756,35.086067265505946,19
1859,"Our extensive evaluation on various text classification benchmarks demonstrates the effectiveness of our approach , as well as its good compatibility with existing data augmentation techniques which aim to enhance the manifold .",3,0.9180571,21.96201175319352,33
1860,Stereotypical language expresses widely-held beliefs about different social categories .,0,0.74412507,49.16143206918316,12
1860,"Many stereotypes are overtly negative , while others may appear positive on the surface , but still lead to negative consequences .",0,0.82802033,64.35391147146649,22
1860,"In this work , we present a computational approach to interpreting stereotypes in text through the Stereotype Content Model ( SCM ) , a comprehensive causal theory from social psychology .",1,0.8840656,76.41975570483353,31
1860,The SCM proposes that stereotypes can be understood along two primary dimensions : warmth and competence .,0,0.8908866,143.5914839769958,17
1860,"We present a method for defining warmth and competence axes in semantic embedding space , and show that the four quadrants defined by this subspace accurately represent the warmth and competence concepts , according to annotated lexicons .",3,0.5039333,62.07531156331835,38
1860,We then apply our computational SCM model to textual stereotype data and show that it compares favourably with survey-based studies in the psychological literature .,3,0.66837233,67.4821447675443,27
1860,"Furthermore , we explore various strategies to counter stereotypical beliefs with anti-stereotypes .",1,0.33582968,43.21684478261413,13
1860,"It is known that countering stereotypes with anti-stereotypical examples is one of the most effective ways to reduce biased thinking , yet the problem of generating anti-stereotypes has not been previously studied .",0,0.9433845,20.535278666326263,33
1860,"Thus , a better understanding of how to generate realistic and effective anti-stereotypes can contribute to addressing pressing societal concerns of stereotyping , prejudice , and discrimination .",3,0.50082666,42.76900246100433,28
1861,Misinformation has recently become a well-documented matter of public concern .,0,0.96264875,24.855758748939795,13
1861,"Existing studies on this topic have hitherto adopted a coarse concept of misinformation , which incorporates a broad spectrum of story types ranging from political conspiracies to misinterpreted pranks .",0,0.9311566,69.02632892283637,30
1861,This paper aims to structurize these misinformation stories by leveraging fact-check articles .,1,0.9374633,91.15117205652454,13
1861,"Our intuition is that key phrases in a fact-check article that identify the misinformation type ( s ) ( e.g. , doctored images , urban legends ) also act as rationales that determine the verdict of the fact-check ( e.g. , false ) .",3,0.5512663,51.457627660459906,46
1861,"We experiment on rationalized models with domain knowledge as weak supervision to extract these phrases as rationales , and then cluster semantically similar rationales to summarize prevalent misinformation types .",2,0.8559988,149.17794091269374,30
1861,"Using archived fact-checks from Snopes.com , we identify ten types of misinformation stories .",3,0.49368212,47.63463866983185,14
1861,We discuss how these types have evolved over the last ten years and compare their prevalence between the 2016/ 2020 US presidential elections and the H1N1 / COVID-19 pandemics .,1,0.52777,27.45065664148588,31
1862,"While there is an abundance of advice to podcast creators on how to speak in ways that engage their listeners , there has been little data-driven analysis of podcasts that relates linguistic style with engagement .",0,0.9437972,33.26979331136031,36
1862,"In this paper , we investigate how various factors – vocabulary diversity , distinctiveness , emotion , and syntax , among others – correlate with engagement , based on analysis of the creators ’ written descriptions and transcripts of the audio .",1,0.9016998,98.57234529591103,42
1862,"We build models with different textual representations , and show that the identified features are highly predictive of engagement .",3,0.6557317,53.276576482077516,20
1862,"Our analysis tests popular wisdom about stylistic elements in high-engagement podcasts , corroborating some pieces of advice and adding new perspectives on others .",3,0.9160872,118.38725430445936,24
1863,"People debate on a variety of topics on online platforms such as Reddit , or Facebook .",0,0.8770651,23.227742386334484,17
1863,"Debates can be lengthy , with users exchanging a wealth of information and opinions .",0,0.86620814,85.64999978496647,15
1863,"However , conversations do not always go smoothly , and users sometimes engage in unsound argumentation techniques to prove a claim .",0,0.8850893,59.36851742792237,22
1863,These techniques are called fallacies .,0,0.8856795,117.68511299615477,6
1863,Fallacies are persuasive arguments that provide insufficient or incorrect evidence to support the claim .,0,0.9156637,50.984329268834465,15
1863,"In this paper , we study the most frequent fallacies on Reddit , and we present them using the pragma-dialectical theory of argumentation .",1,0.91097605,40.98672578204183,24
1863,"We construct a new annotated dataset of fallacies , using user comments containing fallacy mentions as noisy labels , and cleaning the data via crowdsourcing .",2,0.89446783,108.96884041095745,26
1863,"Finally , we study the task of classifying fallacies using neural models .",2,0.4130492,30.477438966292514,13
1863,We find that generally the models perform better in the presence of conversational context .,3,0.9823874,28.742051061634104,15
1863,We have released the data and the code at github.com/sahaisaumya/ informal_fallacies .,3,0.615001,54.6937017906051,13
1864,Inferring social relations from dialogues is vital for building emotionally intelligent robots to interpret human language better and act accordingly .,0,0.87502056,63.96997847805705,21
1864,"We model the social network as an And-or Graph , named SocAoG , for the consistency of relations among a group and leveraging attributes as inference cues .",2,0.84872156,113.65484926544133,28
1864,"Moreover , we formulate a sequential structure prediction task , and propose an 𝛼-𝛽-𝛾 strategy to incrementally parse SocAoG for the dynamic inference upon any incoming utterance : ( i ) an 𝛼 process predicting attributes and relations conditioned on the semantics of dialogues , ( ii ) a 𝛽 process updating the social relations based on related attributes , and ( iii ) a 𝛾 process updating individual ’s attributes based on interpersonal social relations .",2,0.42648754,37.67556983559435,77
1864,Empirical results on DialogRE and MovieGraph show that our model infers social relations more accurately than the state-of-the-art methods .,3,0.9628208,12.933442006285086,26
1864,"Moreover , the ablation study shows the three processes complement each other , and the case study demonstrates the dynamic relational inference .",3,0.9691003,84.00319193632133,23
1865,"We present a data-driven , end-to-end approach to transaction-based dialog systems that performs at near-human levels in terms of verbal response quality and factual grounding accuracy .",1,0.44220245,31.424324067986962,31
1865,"We show that two essential components of the system produce these results : a sufficiently large and diverse , in-domain labeled dataset , and a neural network-based , pre-trained model that generates both verbal responses and API call predictions .",3,0.89332503,56.37906019728366,43
1865,"In terms of data , we introduce TicketTalk , a movie ticketing dialog dataset with 23,789 annotated conversations .",2,0.83785254,115.01980292617989,19
1865,"The conversations range from completely open-ended and unrestricted to more structured , both in terms of their knowledge base , discourse features , and number of turns .",3,0.36689866,68.03577335736006,28
1865,"In qualitative human evaluations , model-generated responses trained on just 10,000 TicketTalk dialogs were rated to “ make sense ” 86.5 % of the time , almost the same as human responses in the same contexts .",3,0.9348707,79.4871276957226,38
1865,"Our simple , API-focused annotation schema results in a much easier labeling task making it faster and more cost effective .",3,0.9427026,102.87291352470815,23
1865,It is also the key component for being able to predict API calls accurately .,0,0.5736101,38.740095902366086,15
1865,"We handle factual grounding by incorporating API calls in the training data , allowing our model to learn which actions to take and when .",2,0.64069384,65.82619883231793,25
1865,"Trained on the same 10,000-dialog set , the model ’s API call predictions were rated to be correct 93.9 % of the time in our evaluations , surpassing the ratings for the corresponding human labels .",3,0.91391665,69.6193446450564,38
1865,"We show how API prediction and response generation scores improve as the dataset size incrementally increases from 5000 to 21,000 dialogs .",3,0.89193094,66.34197395746665,22
1865,Our analysis also clearly illustrates the benefits of pre-training .,3,0.98516023,29.845953248780603,10
1865,"To facilitate future work on transaction-based dialog systems , we are publicly releasing the TicketTalk dataset at https://git.io/JL8an .",3,0.5209331,46.10234028973723,21
1866,"In this paper , we explore the ability to model and infer personality types of opponents , predict their responses , and use this information to adapt a dialog agent ’s high-level strategy in negotiation tasks .",1,0.91336805,62.7248203934189,39
1866,"Inspired by the idea of incorporating a theory of mind ( ToM ) into machines , we introduce a probabilistic formulation to encapsulate the opponent ’s personality type during both learning and inference .",2,0.5884044,49.22403859120663,34
1866,We test our approach on the CraigslistBargain dataset ( He et al .,2,0.6309634,83.58074729795723,13
1866,2018 ) and show that our method using ToM inference achieves a 20 % higher dialog agreement rate compared to baselines on a mixed population of opponents .,3,0.93088406,111.25675402911722,28
1866,We also demonstrate that our model displays diverse negotiation behavior with different types of opponents .,3,0.9578698,63.417704936984805,16
1867,"In this paper , we propose Inverse Adversarial Training ( IAT ) algorithm for training neural dialogue systems to avoid generic responses and model dialogue history better .",1,0.86824024,43.88390158723317,28
1867,"In contrast to standard adversarial training algorithms , IAT encourages the model to be sensitive to the perturbation in the dialogue history and therefore learning from perturbations .",3,0.46852282,27.64458517966608,28
1867,"By giving higher rewards for responses whose output probability reduces more significantly when dialogue history is perturbed , the model is encouraged to generate more diverse and consistent responses .",3,0.7047675,73.49184544485963,30
1867,"By penalizing the model when generating the same response given perturbed dialogue history , the model is forced to better capture dialogue history and generate more informative responses .",3,0.55406797,43.72711571909101,29
1867,Experimental results on two benchmark datasets show that our approach can better model dialogue history and generate more diverse and consistent responses .,3,0.9393812,12.895551171234617,23
1867,"In addition , we point out a problem of the widely used maximum mutual information ( MMI ) based methods for improving the diversity of dialogue response generation models and demonstrate it empirically .",1,0.3442578,48.1126057295549,34
1868,Knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text .,0,0.92979896,23.650019360496042,21
1868,We discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence .,1,0.5194285,34.24525848340298,24
1868,Existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses .,0,0.901268,35.8614121223058,25
1868,We propose different evaluation measures to disentangle these different styles of responses by quantifying the informativeness and objectivity .,2,0.42504448,43.194038413434185,19
1868,"At training time , additional inputs based on these evaluation measures are given to the dialogue model .",2,0.5054234,81.57474980261814,18
1868,"At generation time , these additional inputs act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence .",3,0.533484,42.96239479481328,26
1868,We also investigate the usage of additional controls at decoding time using resampling techniques .,2,0.39913243,47.95532150150233,15
1868,"In addition to automatic metrics , we perform a human evaluation study where raters judge the output of these controlled generation models to be generally more objective and faithful to the evidence compared to baseline dialogue systems .",2,0.7459073,47.87000113063665,38
1869,Automatically extracting key information from scientific documents has the potential to help scientists work more efficiently and accelerate the pace of scientific progress .,0,0.838734,21.227441438281907,24
1869,"Prior work has considered extracting document-level entity clusters and relations end-to-end from raw scientific text , which can improve literature search and help identify methods and materials for a given problem .",0,0.87131745,63.292786068309205,35
1869,"Despite the importance of this task , most existing works on scientific information extraction ( SciIE ) consider extraction solely based on the content of an individual paper , without considering the paper ’s place in the broader literature .",0,0.9228178,39.60635752039248,40
1869,"In contrast to prior work , we augment our text representations by leveraging a complementary source of document context : the citation graph of referential links between citing and cited papers .",2,0.659941,67.86142680716058,32
1869,"On a test set of English-language scientific documents , we show that simple ways of utilizing the structure and content of the citation graph can each lead to significant gains in different scientific information extraction tasks .",3,0.86840343,38.12965794344027,38
1869,"When these tasks are combined , we observe a sizable improvement in end-to-end information extraction over the state-of-the-art , suggesting the potential for future work along this direction .",3,0.9719285,13.179041792277475,37
1869,We release software tools to facilitate citation-aware SciIE development .,2,0.4957702,203.47141572903914,12
1870,Current event-centric knowledge graphs highly rely on explicit connectives to mine relations between events .,0,0.9248796,85.1717023541419,15
1870,"Unfortunately , due to the sparsity of connectives , these methods severely undermine the coverage of EventKGs .",0,0.5749613,91.58331526808473,18
1870,The lack of high-quality labelled corpora further exacerbates that problem .,0,0.8357534,29.93106584834573,11
1870,"In this paper , we propose a knowledge projection paradigm for event relation extraction : projecting discourse knowledge to narratives by exploiting the commonalities between them .",1,0.90267485,86.88242261053693,27
1870,"Specifically , we propose Multi-tier Knowledge Projection Network ( MKPNet ) , which can leverage multi-tier discourse knowledge effectively for event relation extraction .",2,0.5903266,50.06027585893999,24
1870,"In this way , the labelled data requirement is significantly reduced , and implicit event relations can be effectively extracted .",3,0.7175062,110.92451300220671,21
1870,Intrinsic experimental results show that MKPNet achieves the new state-of-the-art performance and extrinsic experimental results verify the value of the extracted event relations .,3,0.97237486,18.61261042201932,30
1871,"Neural methods have been shown to achieve high performance in Named Entity Recognition ( NER ) , but rely on costly high-quality labeled data for training , which is not always available across languages .",0,0.9424824,23.69496221335456,35
1871,"While previous works have shown that unlabeled data in a target language can be used to improve cross-lingual model performance , we propose a novel adversarial approach ( AdvPicker ) to better leverage such data and further improve results .",2,0.3666545,18.55249402815969,40
1871,We design an adversarial learning framework in which an encoder learns entity domain knowledge from labeled source-language data and better shared features are captured via adversarial training-where a discriminator selects less language-dependent target-language data via similarity to the source language .,2,0.8629094,53.29318074723946,49
1871,Experimental results on standard benchmark datasets well demonstrate that the proposed method benefits strongly from this data selection process and outperforms existing state-of-the-art methods ;,3,0.9346294,18.776506961007307,31
1871,"without requiring any additional external resources ( e.g. , gazetteers or via machine translation ) .",0,0.3538898,42.45751476754263,16
1872,"Nowadays , fake news detection , which aims to verify whether a news document is trusted or fake , has become urgent and important .",0,0.9677136,60.10606915809331,25
1872,"Most existing methods rely heavily on linguistic and semantic features from the news content , and fail to effectively exploit external knowledge which could help determine whether the news document is trusted .",0,0.8327133,39.13181898048838,33
1872,"In this paper , we propose a novel end-to-end graph neural model called CompareNet , which compares the news to the knowledge base ( KB ) through entities for fake news detection .",1,0.831209,46.18508268540231,35
1872,"Considering that fake news detection is correlated with topics , we also incorporate topics to enrich the news representation .",2,0.68720925,62.897755950213664,20
1872,"Specifically , we first construct a directed heterogeneous document graph for each news incorporating topics and entities .",2,0.89337564,102.73501817302453,18
1872,"Based on the graph , we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content .",2,0.7279281,36.44379002643732,34
1872,"The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network , to capture the consistency between the news content and KB .",2,0.7230001,57.690709659195676,34
1872,"Finally , the topic-enriched news representation combining the entity comparison features is fed into a fake news classifier .",2,0.71601266,120.56863332968506,19
1872,Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods .,3,0.8998054,7.443971914072886,19
1873,Named entity recognition ( NER ) remains challenging when entity mentions can be discontinuous .,0,0.9632603,52.54585065898459,15
1873,Existing methods break the recognition process into several sequential steps .,0,0.86664355,51.284260393197826,11
1873,"In training , they predict conditioned on the golden intermediate results , while at inference relying on the model output of the previous steps , which introduces exposure bias .",2,0.38132372,200.90029929617936,30
1873,"To solve this problem , we first construct a segment graph for each sentence , in which each node denotes a segment ( a continuous entity on its own , or a part of discontinuous entities ) , and an edge links two nodes that belong to the same entity .",2,0.8594975,37.0155428018111,51
1873,The nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named Mac .,2,0.5285307,112.37792949132667,26
1873,Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique .,3,0.40202487,34.58476961616106,25
1873,"Experiments on three benchmarks show that our method outperforms the state-of-the-art ( SOTA ) results , with up to 3.5 percentage points improvement on F1 , and achieves 5x speedup over the SOTA model .",3,0.9143007,11.174291114938969,39
1874,"Entity linking ( EL ) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph , a crucial task for text understanding , question answering or conversational systems .",0,0.9419199,40.722476787635244,37
1874,"In the special case of short-text EL , which poses additional challenges due to limited context , prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches .",0,0.8394006,68.15688963769013,35
1874,"Here , we take a different , neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning .",2,0.6398626,32.06422788321185,28
1874,"Even though constrained to use rules , we show that we reach competitive or better performance with SoTA black-box neural approaches .",3,0.9453969,90.99726477976978,24
1874,"Furthermore , our framework has the benefits of extensibility and transferability .",3,0.8788172,25.82140438766631,12
1874,"We show that we can easily blend existing rule templates given by a human expert , with multiple types of features ( priors , BERT encodings , box embeddings , etc ) , and even with scores resulting from previous EL methods , thus improving on such methods .",3,0.88889366,137.3553251228525,49
1874,"As an example of improvement , on the LC-QuAD-1.0 dataset , we show more than 3 % increase in F1 score relative to previous SoTA .",3,0.9341871,44.67922245965904,30
1874,"Finally , we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another , sometimes without finetuning , while still having high accuracy .",3,0.9659426,51.83993959194628,37
1875,"Context-aware machine translation models are designed to leverage contextual information , but often fail to do so .",0,0.93163747,20.504207751051997,20
1875,"As a result , they inaccurately disambiguate pronouns and polysemous words that require context for resolution .",0,0.7924229,53.87936161945097,17
1875,"To answer these questions , we introduce SCAT ( Supporting Context for Ambiguous Translations ) , a new English-French dataset comprising supporting context words for 14 K translations that professional translators found useful for pronoun disambiguation .",1,0.44476038,71.79700855339406,39
1875,"Using SCAT , we perform an in-depth analysis of the context used to disambiguate , examining positional and lexical characteristics of the supporting words .",2,0.8771405,41.60047167350811,26
1875,"Furthermore , we measure the degree of alignment between the model ’s attention scores and the supporting context from SCAT , and apply a guided attention strategy to encourage agreement between the two .",2,0.8037464,63.67969896451907,34
1876,The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages .,0,0.94696856,9.74178773380842,19
1876,"Fortunately , some low-resource languages are linguistically related or similar to high-resource languages ;",0,0.76057744,32.193323324735154,15
1876,these related languages may share many lexical or syntactic structures .,0,0.8436921,28.83820330207788,11
1876,"In this work , we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data , in addition to any parallel data in the related high-resource language .",1,0.53561306,27.93117989359724,36
1876,"Our method , NMT-Adapt , combines denoising autoencoding , back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation .",2,0.759504,49.009516987145574,25
1876,We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines .,3,0.79599094,24.3788872162088,27
1877,"Bilingual lexicons map words in one language to their translations in another , and are typically induced by learning linear projections to align monolingual word embedding spaces .",0,0.8475223,39.63389311644238,28
1877,"In this paper , we show it is possible to produce much higher quality lexicons with methods that combine ( 1 ) unsupervised bitext mining and ( 2 ) unsupervised word alignment .",1,0.7301165,28.835391333728975,33
1877,"Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries , with both unsupervised and semi-supervised schemes .",3,0.80367416,95.54633570954442,37
1877,"Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F1 points averaged over 12 language pairs , while also providing a more interpretable approach that allows for rich reasoning of word meaning in context .",3,0.9208342,28.992192437895465,43
1877,"Further analysis of our output and the standard reference lexicons suggests they are of comparable quality , and new benchmarks may be needed to measure further progress on this task .",3,0.9863935,51.29894726753488,31
1878,We present a simple yet effective approach to build multilingual speech-to-text ( ST ) translation through efficient transfer learning from a pretrained speech encoder and text decoder .,1,0.6168234,22.934143253338537,32
1878,Our key finding is that a minimalistic LNA ( LayerNorm and Attention ) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning 10 50 % of the pretrained parameters .,3,0.9749043,55.5149892526777,33
1878,"This effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling , and mBART for multilingual text generation .",3,0.5630308,30.62221573540116,25
1878,This sets a new state-of-the-art for 36 translation directions ( and surpassing cascaded ST for 26 of them ) on the large-scale multilingual ST benchmark CoVoST 2 ( + 6.4 BLEU on average for En-X directions and + 6.7 BLEU for X-En directions ) .,3,0.805447,33.47652574135429,53
1878,"Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model ( + 5.6 BLEU on average across 28 non-English directions ) , making it an appealing approach for attaining high-quality speech translation with improved parameter and data efficiency .",3,0.91491324,34.60233731447229,43
1879,Learning contextual text embeddings that represent causal graphs has been useful in improving the performance of downstream tasks like causal treatment effect estimation .,0,0.92729765,68.66548123218693,24
1879,"However , existing causal embeddings which are trained to predict direct causal links , fail to capture other indirect causal links of the graph , thus leading to spurious correlations in downstream tasks .",0,0.8785772,35.566280775799164,34
1879,"In this paper , we define the faithfulness property of contextual embeddings to capture geometric distance-based properties of directed acyclic causal graphs .",1,0.7755753,34.4003543092452,25
1879,"By incorporating these faithfulness properties , we learn text embeddings that are 31.3 % more faithful to human validated causal graphs with about 800 K and 200 K causal links and achieve 21.1 % better Precision-Recall AUC in a link prediction fine-tuning task .",3,0.695071,66.77445840334843,46
1879,"Answers with questions of the form “ What causes X ? ” , our faithful embeddings achieved a precision of the first ranked answer ( P@1 ) of 41.07 % , outperforming the existing baseline by 10.2 % .",3,0.914158,74.14751089281062,39
1880,Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens .,0,0.8196174,50.53809187104197,18
1880,We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia .,1,0.36202565,46.052849469573296,26
1880,"In both mid-and long-range contexts , we find that several extremely destructive context manipulations — including shuffling word order within sentences and deleting all words other than nouns — remove less than 15 % of the usable information .",3,0.9654826,61.855184166206485,39
1880,"Our results suggest that long contexts , but not their detailed syntactic and propositional content , are important for the low perplexity of current transformer language models .",3,0.99032927,77.77498478124093,28
1881,"In this paper , we introduce Integrated Directional Gradients ( IDG ) , a method for attributing importance scores to groups of features , indicating their relevance to the output of a neural network model for a given input .",1,0.76676035,35.23499807487762,40
1881,The success of Deep Neural Networks has been attributed to their ability to capture higher level feature interactions .,0,0.9377843,19.22933756291278,19
1881,"Hence , in the last few years capturing the importance of these feature interactions has received increased prominence in ML interpretability literature .",0,0.93183327,80.56415484037721,23
1881,"In this paper , we formally define the feature group attribution problem and outline a set of axioms that any intuitive feature group attribution method should satisfy .",1,0.8038328,47.14017322551623,28
1881,"Earlier , cooperative game theory inspired axiomatic methods only borrowed axioms from solution concepts ( such as Shapley value ) for individual feature attributions and introduced their own extensions to model interactions .",0,0.89063483,170.93200898157252,33
1881,"In contrast , our formulation is inspired by axioms satisfied by characteristic functions as well as solution concepts in cooperative game theory literature .",2,0.51483345,110.82300505120367,24
1881,We believe that characteristic functions are much better suited to model importance of groups compared to just solution concepts .,3,0.9578522,183.7740419750957,20
1881,"We demonstrate that our proposed method , IDG , satisfies all the axioms .",3,0.9335821,96.07877046879979,14
1881,Using IDG we analyze two state-of-the-art text classifiers on three benchmark datasets for sentiment analysis .,2,0.87859553,21.53414215450538,22
1881,Our experiments show that IDG is able to effectively capture semantic interactions in linguistic models via negations and conjunctions .,3,0.9818382,39.0940890020278,20
1882,Sentence embeddings are an important component of many natural language processing ( NLP ) systems .,0,0.9301292,13.354951776244528,16
1882,"Like word embeddings , sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks , such as clustering and retrieval .",0,0.85887593,17.535657070429234,27
1882,"Unlike word embeddings , the highest performing solutions for learning sentence embeddings require labelled data , limiting their usefulness to languages and domains where labelled data is abundant .",0,0.8036577,37.91230017609853,29
1882,"In this paper , we present DeCLUTR : Deep Contrastive Learning for Unsupervised Textual Representations .",1,0.8344128,37.65562486027663,16
1882,"Inspired by recent advances in deep metric learning ( DML ) , we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data .",0,0.40239108,31.84203576563441,31
1882,"When used to extend the pretraining of transformer-based language models , our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders .",3,0.8415235,19.293247848063075,29
1882,"Importantly , our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data .",3,0.9801981,16.582104946035972,28
1882,Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text .,3,0.78734964,18.610866530013464,23
1883,"Due to the scarcity of annotated data , Abstract Meaning Representation ( AMR ) research is relatively limited and challenging for languages other than English .",0,0.965971,19.10718003128509,26
1883,"Upon the availability of English AMR dataset and English-to-X parallel datasets , in this paper we propose a novel cross-lingual pre-training approach via multi-task learning ( MTL ) for both zeroshot AMR parsing and AMR-to-text generation .",1,0.37730965,19.373248576951273,45
1883,"Specifically , we consider three types of relevant tasks , including AMR parsing , AMR-to-text generation , and machine translation .",2,0.70379806,29.544818016714686,25
1883,We hope that knowledge gained while learning for English AMR parsing and text generation can be transferred to the counterparts of other languages .,3,0.7949928,47.6664261093407,24
1883,"With properly pretrained models , we explore four different finetuning methods , i.e. , vanilla fine-tuning with a single task , one-for-all MTL fine-tuning , targeted MTL fine-tuning , and teacher-studentbased MTL fine-tuning .",2,0.77621466,30.330070743565678,37
1883,"Experimental results on AMR parsing and text generation of multiple non-English languages demonstrate that our approach significantly outperforms a strong baseline of pre-training approach , and greatly advances the state of the art .",3,0.9272959,16.302417501734805,34
1883,"In detail , on LDC2020T07 we have achieved 70.45 % , 71.76 % , and 70.80 % in Smatch F1 for AMR parsing of German , Spanish , and Italian , respectively , while for AMR-to-text generation of the languages , we have obtained 25.69 , 31.36 , and 28.42 in BLEU respectively .",3,0.9320856,43.77785406868947,58
1883,We make our code available on github https://github.com/xdqkid/XLPT-AMR .,3,0.51671785,23.55486813140446,9
1884,"Despite the success of sequence-to-sequence ( seq2seq ) models in semantic parsing , recent work has shown that they fail in compositional generalization , i.e. , the ability to generalize to new structures built of components observed during training .",0,0.9266689,23.94277410129018,41
1884,"In this work , we posit that a span-based parser should lead to better compositional generalization .",1,0.7390951,28.117707983393426,17
1884,"we propose SpanBasedSP , a parser that predicts a span tree over an input utterance , explicitly encoding how partial programs compose over spans in the input .",2,0.46217248,144.95236101847863,28
1884,SpanBasedSP extends Pasupat et al .,4,0.36346495,343.9976489304231,6
1884,"( 2019 ) to be comparable to seq2seq models by ( i ) training from programs , without access to gold trees , treating trees as latent variables , ( ii ) parsing a class of non-projective trees through an extension to standard CKY .",0,0.5328479,125.33609606523085,45
1884,"On GeoQuery , SCAN and CLOSURE datasets , SpanBasedSP performs similarly to strong seq2seq baselines on random splits , but dramatically improves performance compared to baselines on splits that require compositional generalization : from 61.0 → 88.9 average accuracy .",3,0.9241465,99.21527124167147,40
1885,"Sequence-to-sequence models excel at handling natural language variation , but have been shown to struggle with out-of-distribution compositional generalization .",0,0.92548335,20.137768738322745,22
1885,"This has motivated new specialized architectures with stronger compositional biases , but most of these approaches have only been evaluated on synthetically-generated datasets , which are not representative of natural language variation .",0,0.90660495,48.73331054630612,35
1885,"To better assess this capability , we propose new train and test splits of non-synthetic datasets .",2,0.47309035,68.87167348266156,17
1885,We demonstrate that strong existing approaches do not perform well across a broad set of evaluations .,3,0.9503112,31.329870511159747,17
1885,"We also propose NQG-T5 , a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model .",3,0.5474544,14.55272474843248,27
1885,"It outperforms existing approaches across several compositional generalization challenges on non-synthetic data , while also being competitive with the state-of-the-art on standard evaluations .",3,0.8501097,23.17750688790074,30
1885,"While still far from solving this problem , our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing .",3,0.9730278,38.33219194323719,33
1886,"We present a targeted , scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena .",2,0.41777223,85.12764861085684,38
1886,Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task .,0,0.65782326,102.73658579695616,17
1886,"We compare human reaction times to by-word probabilities for four contemporary language models , with different architectures and trained on a range of data set sizes .",2,0.7891154,65.93646428746568,28
1886,"We find that across many phenomena , both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model ‘ accuracy ’ scores a la Marvin and Linzen ( 2018 ) about equal .",3,0.9741863,190.24468183417457,39
1886,"However , although language model outputs match humans in direction , we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences .",3,0.95675063,62.765450682059324,31
1886,"Specifically , when models encounter syntactic violations they fail to accurately predict the longer reading times observed in the human data .",3,0.60062885,86.2683500623167,22
1886,These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations .,3,0.9801195,41.125733101809686,19
1887,"Modality is the linguistic ability to describe vents with added information such as how desirable , plausible , or feasible they are .",0,0.8591292,126.7458710974244,23
1887,"Modality is important for many NLP downstream tasks such as the detection of hedging , uncertainty , speculation , and more .",0,0.78813773,55.28255351396355,22
1887,"Previous studies that address modality detection in NLP often restrict modal expressions to a closed syntactic class , and the modal sense labels are vastly different across different studies , lacking an accepted standard .",0,0.8727235,111.95128612084659,35
1887,"Furthermore , these senses are often analyzed independently of the events that they modify .",0,0.90631413,94.6870876207158,15
1887,This work builds on the theoretical foundations of the Georgetown Gradable Modal Expressions ( GME ) work by Rubinstein et al .,0,0.437823,83.52265976837202,22
1887,( 2013 ) to propose an event-based modality detection task where modal expressions can be words of any syntactic class and sense labels are drawn from a comprehensive taxonomy which harmonizes the modal concepts contributed by the different studies .,0,0.3910372,80.2654494587662,40
1887,We present experiments on the GME corpus aiming to detect and classify fine-grained modal concepts and associate them with their modified events .,2,0.39005578,67.88162173381937,24
1887,"We show that detecting and classifying modal expressions is not only feasible , it also improves the detection of modal events in their own right .",3,0.96098864,27.286776469109636,26
1888,Part-of-Speech ( POS ) tags are routinely included as features in many NLP tasks .,0,0.9469528,30.149697746992864,16
1888,"However , the importance and usefulness of POS tags needs to be examined as NLP expands to low-resource languages because linguists who provide many annotated resources do not place priority on early identification and tagging of POS .",0,0.5512471,66.32065585278886,38
1888,This paper describes an empirical study about the effect that POS tags have on two computational morphological tasks with the Transformer architecture .,1,0.8976195,43.937115919975355,23
1888,"Each task is tested twice on identical data except for the presence / absence of POS tags , using published data in ten high-to low-resource languages or unpublished linguistic field data in five low-resource languages .",2,0.821952,77.16436281488423,38
1888,We find that the presence or absence of POS tags does not have a significant bearing on performance .,3,0.9845841,20.687557499307793,19
1888,"In joint segmentation and glossing , the largest average difference is an .09 improvement in F1-scores by removing POS tags .",3,0.9422422,134.93320942898316,23
1888,"In reinflection , the greatest average difference is 1.2 % in accuracy for published data and 5 % for unpublished and noisy field data .",3,0.94889057,80.79066597568827,25
1889,"Parsing spoken dialogue poses unique difficulties , including disfluencies and unmarked boundaries between sentence-like units .",0,0.93105847,144.03867014275002,18
1889,Previous work has shown that prosody can help with parsing disfluent speech ( Tran et al .,0,0.7827499,52.3794184618884,17
1889,"2018 ) , but has assumed that the input to the parser is already segmented into sentence-like units ( SUs ) , which is n’t true in existing speech applications .",0,0.8005133,70.84117146697973,33
1889,"We investigate how prosody affects a parser that receives an entire dialogue turn as input ( a turn-based model ) , instead of gold standard pre-segmented SUs ( an SU-based model ) .",1,0.48136127,109.84721885003134,36
1889,"In experiments on the English Switchboard corpus , we find that when using transcripts alone , the turn-based model has trouble segmenting SUs , leading to worse parse performance than the SU-based model .",3,0.93404526,75.34910978443175,38
1889,"However , prosody can effectively replace gold standard SU boundaries : with prosody , the turn-based model performs as well as the SU-based model ( 91.38 vs .",3,0.9251244,99.48288303013462,32
1889,"91.06 F1 score , respectively ) , despite performing two tasks ( SU segmentation and parsing ) rather than one ( parsing alone ) .",3,0.95642066,102.71434736996243,25
1889,"Analysis shows that pitch and intensity features are the most important for this corpus , since they allow the model to correctly distinguish an SU boundary from a speech disfluency – a distinction that the model otherwise struggles to make .",3,0.9566701,71.88707007331249,41
1890,"We introduce VoxPopuli , a large-scale multilingual corpus providing 400 K hours of unlabeled speech data in 23 languages .",2,0.6138685,49.927695393570175,20
1890,It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning .,0,0.59767777,14.440497045944097,18
1890,VoxPopuli also contains 1.8 K hours of transcribed speeches in 15 languages and their aligned oral interpretations into 15 target languages totaling 17.3 K hours .,0,0.46462744,113.76247698124281,26
1890,We provide speech recognition ( ASR ) baselines and validate the versatility of VoxPopuli unlabeled data in semi-supervised ASR and speech-to-text translation under challenging out-of-domain settings .,3,0.4067814,34.42148833542452,34
1890,The corpus is available at https://github.com/facebookresearch/voxpopuli .,3,0.5417275,18.485302835729378,7
1891,Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal .,0,0.88343793,155.81417214724544,14
1891,"Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences , which are often accompanied by metrics that aggregate an NLP system ’s behavior on these pairs into measurements of harms .",0,0.9004852,69.87693146530404,36
1891,We examine four such benchmarks constructed for two NLP tasks : language modeling and coreference resolution .,2,0.6717879,69.77800807873236,17
1891,We apply a measurement modeling lens — originating from the social sciences — to inventory a range of pitfalls that threaten these benchmarks ’ validity as measurement models for stereotyping .,2,0.81711084,174.49818642391168,31
1891,"We find that these benchmarks frequently lack clear articulations of what is being measured , and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping .",3,0.9542073,34.71359990930699,35
1892,Knowledge Graph ( KG ) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs .,0,0.93110955,59.2215488607932,22
1892,We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed .,2,0.81688267,40.191939200970516,34
1892,We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting .,3,0.42967832,40.780198797615434,26
1892,We find that our model ’s performance improvements stem primarily from its robustness to sparsity .,3,0.9663539,27.60408692013418,16
1892,We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities .,2,0.7794409,38.2122008861739,19
1892,This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion .,3,0.9388669,30.601648694543652,20
1893,"Computing precise evidences , namely minimal sets of sentences that support or refute a given claim , rather than larger evidences is crucial in fact verification ( FV ) , since larger evidences may contain conflicting pieces some of which support the claim while the other refute , thereby misleading FV .",0,0.8896171,84.48709305380049,52
1893,"Despite being important , precise evidences are rarely studied by existing methods for FV .",0,0.8818388,214.03507185429683,15
1893,It is challenging to find precise evidences due to a large search space with lots of local optimums .,0,0.9253722,42.86956020008675,19
1893,"Inspired by the strong exploration ability of the deep Q-learning network ( DQN ) , we propose a DQN-based approach to retrieval of precise evidences .",0,0.38595295,33.456155361717954,28
1893,"In addition , to tackle the label bias on Q-values computed by DQN , we design a post-processing strategy which seeks best thresholds for determining the true labels of computed evidences .",2,0.804064,127.51575156851062,32
1893,Experimental results confirm the effectiveness of DQN in computing precise evidences and demonstrate improvements in achieving accurate claim verification .,3,0.9798507,55.31870595339791,20
1894,"In selective prediction , a classifier is allowed to abstain from making predictions on low-confidence examples .",0,0.4516992,57.56334589871038,17
1894,"Though this setting is interesting and important , selective prediction has rarely been examined in natural language processing ( NLP ) tasks .",0,0.9474799,53.79969928095207,23
1894,"To fill this void in the literature , we study in this paper selective prediction for NLP , comparing different models and confidence estimators .",1,0.83105206,80.54686950276786,25
1894,We further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget .,3,0.52600485,74.64557855151328,19
1894,We show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness .,3,0.9594349,78.87329264243058,17
1894,"We also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios , such as using classifier cascades for accuracy –efficiency trade-offs .",3,0.9645649,68.47756909777586,30
1894,Source code for this paper can be found at https://github.com/castorini/transformers-selective .,4,0.58114207,11.639126610266263,11
1895,Deployed real-world machine learning applications are often subject to uncontrolled and even potentially malicious inputs .,0,0.9313104,40.41131389670216,16
1895,Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety issues .,0,0.8587671,21.758037606703883,17
1895,Prior studies on out-of-domain detection require in-domain task labels and are limited to supervised classification scenarios .,0,0.8339136,31.362342840615746,19
1895,Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data .,1,0.3634191,15.29180911707748,20
1895,We utilize the latent representations of pre-trained transformers and propose a simple yet effective method to transform features across all layers to construct out-of-domain detectors efficiently .,2,0.5961315,29.79403155938748,30
1895,Two domain-specific fine-tuning approaches are further proposed to boost detection accuracy .,2,0.47348014,19.938982495550693,12
1895,Our empirical evaluations of related methods on two datasets validate that our method greatly improves out-of-domain detection ability in a more general scenario .,3,0.9287412,37.368688896387596,27
1896,The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing ( NLP ) .,0,0.9697185,8.432595686523477,24
1896,"While the performance of these models on standard benchmarks has scaled with size , compression techniques such as knowledge distillation have been key in making them practical .",0,0.8428933,38.2085568604815,28
1896,"We present MATE-KD , a novel text-based adversarial training algorithm which improves the performance of knowledge distillation .",1,0.36952424,26.965011812443326,22
1896,MATE-KD first trains a masked language model-based generator to perturb text by maximizing the divergence between teacher and student logits .,2,0.73996884,81.82665157805819,25
1896,Then using knowledge distillation a student is trained on both the original and the perturbed training samples .,2,0.6961397,30.544501468484164,18
1896,"We evaluate our algorithm , using BERT-based models , on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive adversarial learning and data augmentation baselines .",3,0.6812619,38.88312568492136,30
1896,On the GLUE test set our 6 layer RoBERTa based model outperforms BERT-large .,3,0.914155,42.39663017134519,16
1897,"Language model fine-tuning is essential for modern natural language processing , but is computationally expensive and time-consuming .",0,0.9036236,12.272613479247603,20
1897,"Further , the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance .",0,0.5142701,17.99433681393828,19
1897,Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning .,1,0.66686153,26.482256764895727,27
1897,We define the information gain of an example as the improvement on a validation metric after training on that example .,2,0.68801093,55.41221582576481,21
1897,A secondary learner is then trained to approximate this quantity .,2,0.54023826,77.580230825458,11
1897,"During fine-tuning , this learner selects informative examples and skips uninformative ones .",2,0.50862306,45.81466667075453,13
1897,"We show that our method has consistent improvement across datasets , fine-tuning tasks , and language model architectures .",3,0.9289925,40.43003866548613,20
1897,"For example , we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning .",3,0.9234743,27.6211641630741,21
1897,We present statistical evidence that offers insight into the improvements of our method over standard fine-tuning .,3,0.752043,31.843402306883124,17
1897,"The generality of our method leads us to propose a new paradigm for language model fine-tuning — we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning , thereby improving the performance and reducing the overall energy footprint of language model fine-tuning .",3,0.90424865,23.384761423393613,50
1898,Text simplification reduces the language complexity of professional content for accessibility purposes .,0,0.4600206,90.50105942643744,13
1898,"End-to-end neural network models have been widely adopted to directly generate the simplified version of input text , usually functioning as a blackbox .",0,0.9150374,28.36775153686243,27
1898,We show that text simplification can be decomposed into a compact pipeline of tasks to ensure the transparency and explainability of the process .,3,0.89678943,31.972835937426634,24
1898,"The first two steps in this pipeline are often neglected : 1 ) to predict whether a given piece of text needs to be simplified , and 2 ) if yes , to identify complex parts of the text .",0,0.79915786,40.70483936422315,40
1898,"The two tasks can be solved separately using either lexical or deep learning methods , or solved jointly .",0,0.37710926,39.62919701480679,19
1898,"Simply applying explainable complexity prediction as a preliminary step , the out-of-sample text simplification performance of the state-of-the-art , black-box simplification models can be improved by a large margin .",3,0.7825421,24.32921878560315,41
1899,Retrieving relevant contexts from a large corpus is a crucial step for tasks such as open-domain question answering and fact checking .,0,0.90543294,14.689130543017175,22
1899,"Although neural retrieval outperforms traditional methods like tf-idf and BM25 , its performance degrades considerably when applied to out-of-domain data .",0,0.6498746,57.26903119221364,24
1899,"Driven by the question of whether a neural retrieval model can be _universal_ and perform robustly on a wide variety of problems , we propose a multi-task trained model .",2,0.4044066,29.113055996864507,30
1899,"Our approach not only outperforms previous methods in the few-shot setting , but also rivals specialised neural retrievers , even when in-domain training data is abundant .",3,0.8703658,34.10129972991284,28
1899,"With the help of our retriever , we improve existing models for downstream tasks and closely match or improve the state of the art on multiple benchmarks .",3,0.6534777,25.66096071054885,28
1900,NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words .,0,0.9246058,18.241346714855055,17
1900,"To explore this question , we adopt five styles of evaluation : classifier probing , information-theoretic probing , unsupervised relative acceptability judgments , unsupervised language model knowledge probing , and fine-tuning on NLU tasks .",2,0.8022246,63.56778150826823,37
1900,"We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas , a group of RoBERTa models pretrained on 1M , 10M , 100M and 1B words .",2,0.84285337,71.4455201665977,43
1900,We find that these LMs require only about 10 M to 100M words to learn to reliably encode most syntactic and semantic features we test .,3,0.9569566,70.84012430392983,26
1900,They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks .,0,0.8083411,43.67076130947609,26
1900,"The results suggest that , while the ability to encode linguistic features is almost certainly necessary for language understanding , it is likely that other , unidentified , forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models .",3,0.9885389,54.37874434838832,46
1901,"In Neural Machine Translation ( and , more generally , conditional language modeling ) , the generation of a target token is influenced by two types of context : the source and the prefix of the target sequence .",0,0.87347925,67.23883614687912,39
1901,"While many attempts to understand the internal workings of NMT models have been made , none of them explicitly evaluates relative source and target contributions to a generation decision .",0,0.8955959,45.80113498856954,30
1901,We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation ( LRP ) .,3,0.50413406,49.18714294041264,21
1901,"Its underlying ‘ conservation principle ’ makes relevance propagation unique : differently from other methods , it evaluates not an abstract quantity reflecting token importance , but the proportion of each token ’s influence .",0,0.63094425,304.6306315108741,35
1901,We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process .,2,0.70508564,52.54442249698929,27
1901,"We analyze changes in these contributions when conditioning on different types of prefixes , when varying the training objective or the amount of training data , and during the training process .",2,0.5904523,42.918136974448764,32
1901,We find that models trained with more data tend to rely on source information more and to have more sharp token contributions ;,3,0.98390144,98.1860349142546,23
1901,the training process is non-monotonic with several stages of different nature .,0,0.498009,49.39588187181609,12
1902,Recent years have seen numerous NLP datasets introduced to evaluate the performance of fine-tuned models on natural language understanding tasks .,0,0.9463856,12.708267666335704,21
1902,"Recent results from large pretrained models , though , show that many of these datasets are largely saturated and unlikely to be able to detect further progress .",0,0.6563386,59.829471556073905,28
1902,Theory and evaluate 29 datasets using predictions from 18 pretrained Transformer models on individual test examples .,2,0.8577502,122.94763318078888,17
1902,"We find that Quoref , HellaSwag , and MC-TACO are best suited for distinguishing among state-of-the-art models , while SNLI , MNLI , and CommitmentBank seem to be saturated for current strong models .",3,0.9757488,56.766163187562924,42
1902,"We also observe span selection task format , which is used for QA datasets like QAMR or SQuAD2.0 , is effective in differentiating between strong and weak models .",3,0.94779193,67.8244183778616,29
1903,"A growing body of literature has focused on detailing the linguistic knowledge embedded in large , pretrained language models .",0,0.94916195,33.31264649306434,20
1903,Existing work has shown that non-linguistic biases in models can drive model behavior away from linguistic generalizations .,0,0.9004314,30.554305120408642,18
1903,"We hypothesized that competing linguistic processes within a language , rather than just non-linguistic model biases , could obscure underlying linguistic knowledge .",1,0.7044031,91.41390027457264,23
1903,"We tested this claim by exploring a single phenomenon in four languages : English , Chinese , Spanish , and Italian .",2,0.60728794,48.87531441854056,22
1903,"While human behavior has been found to be similar across languages , we find cross-linguistic variation in model behavior .",0,0.7743261,22.510496775672376,20
1903,"We show that competing processes in a language act as constraints on model behavior and demonstrate that targeted fine-tuning can re-weight the learned constraints , uncovering otherwise dormant linguistic knowledge in models .",3,0.88911766,87.2968254379708,33
1903,"Our results suggest that models need to learn both the linguistic constraints in a language and their relative ranking , with mismatches in either producing non-human-like behavior .",3,0.99011046,67.1786827421397,29
1904,Interpretability is an important aspect of the trustworthiness of a model ’s predictions .,0,0.88465196,20.817376762987706,14
1904,"Transformer ’s predictions are widely explained by the attention weights , i.e. , a probability distribution generated at its self-attention unit ( head ) .",0,0.48047632,83.62902492126035,25
1904,Current empirical studies provide shreds of evidence that attention weights are not explanations by proving that they are not unique .,0,0.8528012,115.28523291177805,21
1904,A recent study showed theoretical justifications to this observation by proving the non-identifiability of attention weights .,0,0.8183698,83.84048266032927,17
1904,"For a given input to a head and its output , if the attention weights generated in it are unique , we call the weights identifiable .",2,0.41274443,104.48141699083126,27
1904,"In this work , we provide deeper theoretical analysis and empirical observations on the identifiability of attention weights .",1,0.7084304,65.59755585117172,19
1904,"Ignored in the previous works , we find the attention weights are more identifiable than we currently perceive by uncovering the hidden role of the key vector .",3,0.75011384,80.47001389666848,28
1904,"However , the weights are still prone to be non-unique attentions that make them unfit for interpretation .",0,0.77151495,89.72396995995705,18
1904,"To tackle this issue , we provide a variant of the encoder layer that decouples the relationship between key and value vector and provides identifiable weights up to the desired length of the input .",2,0.50690705,40.385231257404094,35
1904,We prove the applicability of such variations by providing empirical justifications on varied text classification tasks .,3,0.42266497,68.54869082406513,17
1904,The implementations are available at https://github.com/declare-lab/identifiable-transformers .,3,0.6206563,15.394737292803825,7
1905,"Natural Language Generation ( NLG ) is a key component in a task-oriented dialogue system , which converts the structured meaning representation ( MR ) to the natural language .",0,0.95125383,37.0730305789672,32
1905,"For large-scale conversational systems , where it is common to have over hundreds of intents and thousands of slots , neither template-based approaches nor model-based approaches are scalable .",0,0.6451304,28.618012611732578,33
1905,"Recently , neural NLGs started leveraging transfer learning and showed promising results in few-shot settings .",0,0.9208619,79.16662840743645,16
1905,"This paper proposes AugNLG , a novel data augmentation approach that combines a self-trained neural retrieval model with a few-shot learned NLU model , to automatically create MR-to-Text data from open-domain texts .",1,0.83577037,38.858362905388915,37
1905,The proposed system mostly outperforms the state-of-the-art methods on the FewshotWOZ data in both BLEU and Slot Error Rate .,3,0.886757,22.95916171442621,24
1905,We further confirm improved results on the FewshotSGD data and provide comprehensive analysis results on key components of our system .,3,0.9546687,110.76372935014147,21
1905,Our code and data are available at https://github.com/XinnuoXu/AugNLG .,3,0.61845064,13.391864658247512,9
1906,"In this paper we implement and compare 7 different data augmentation strategies for the task of automatic scoring of children ’s ability to understand others ’ thoughts , feelings , and desires ( or “ mindreading ” ) .",1,0.8089401,46.798177918431435,39
1906,We recruit in-domain experts to re-annotate augmented samples and determine to what extent each strategy preserves the original rating .,2,0.8581722,77.64977207853457,21
1906,We also carry out multiple experiments to measure how much each augmentation strategy improves the performance of automatic scoring systems .,2,0.49964043,31.598676121886967,21
1906,"To determine the capabilities of automatic systems to generalize to unseen data , we create UK-MIND-20-a new corpus of children ’s performance on tests of mindreading , consisting of 10,320 question-answer pairs .",2,0.5941353,54.401748950848706,41
1906,"We obtain a new state-of-the-art performance on the MIND-CA corpus , improving macro-F1-score by 6 points .",3,0.9228468,23.622451389660625,28
1906,Results indicate that both the number of training examples and the quality of the augmentation strategies affect the performance of the systems .,3,0.9891299,16.055516135350786,23
1906,The task-specific augmentations generally outperform task-agnostic augmentations .,3,0.9054553,29.418724940878644,10
1906,"Automatic augmentations based on vectors ( GloVe , FastText ) perform the worst .",3,0.79853994,117.88893656557937,14
1906,We find that systems trained on MIND-CA generalize well to UK-MIND-20 .,3,0.97912997,61.195680953940496,18
1906,We demonstrate that data augmentation strategies also improve the performance on unseen data .,3,0.9539255,29.050334526660105,14
1907,Reply suggestion models help users process emails and chats faster .,3,0.6360596,225.4635558867947,11
1907,Previous work only studies English reply suggestion .,0,0.79498947,1154.427847754381,8
1907,"Instead , we present MRS , a multilingual reply suggestion dataset with ten languages .",2,0.5946379,99.47572028058501,15
1907,MRS can be used to compare two families of models : 1 ) retrieval models that select the reply from a fixed set and 2 ) generation models that produce the reply from scratch .,0,0.43488008,43.68778811851355,35
1907,"Therefore , MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks .",3,0.8063702,38.392731446635665,17
1907,We build a generation model and a retrieval model as baselines for MRS .,2,0.7056169,63.094811425461565,14
1907,"The two models have different strengths in the monolingual setting , and they require different strategies to generalize across languages .",0,0.5316273,18.843229097289914,21
1907,MRS is publicly available at https://github.com/zhangmozhi/mrs .,3,0.4926294,9.66878388774867,7
1908,Crowdsourcing is widely used to create data for common natural language understanding tasks .,0,0.9542725,16.97616388888684,14
1908,"Despite the importance of these datasets for measuring and refining model understanding of language , there has been little focus on the crowdsourcing methods used for collecting the datasets .",0,0.921736,29.813373427630705,30
1908,"In this paper , we compare the efficacy of interventions that have been proposed in prior work as ways of improving data quality .",1,0.9321047,24.832876861290035,24
1908,We use multiple-choice question answering as a testbed and run a randomized trial by assigning crowdworkers to write questions under one of four different data collection protocols .,2,0.93008554,35.94680013658366,29
1908,We find that asking workers to write explanations for their examples is an ineffective stand-alone strategy for boosting NLU example difficulty .,3,0.98000973,89.8954396239353,24
1908,"However , we find that training crowdworkers , and then using an iterative process of collecting data , sending feedback , and qualifying workers based on expert judgments is an effective means of collecting challenging data .",3,0.97600776,68.87814337943153,37
1908,"But using crowdsourced , instead of expert judgments , to qualify workers and send feedback does not prove to be effective .",0,0.61307186,150.36882167628738,22
1908,We observe that the data from the iterative protocol with expert assessments is more challenging by several measures .,3,0.9737888,84.51952998013483,19
1908,"Notably , the human–model gap on the unanimous agreement portion of this data is , on average , twice as large as the gap for the baseline protocol data .",3,0.9536472,76.85579442077501,30
1909,Ideology of legislators is typically estimated by ideal point models from historical records of votes .,0,0.81869894,210.55997848551098,16
1909,It represents legislators and legislation as points in a latent space and shows promising results for modeling voting behavior .,3,0.62076324,88.86473508816367,20
1909,"However , it fails to capture more specific attitudes of legislators toward emerging issues and is unable to model newly-elected legislators without voting histories .",3,0.47358397,94.64515226076725,26
1909,"In order to mitigate these two problems , we explore to incorporate both voting behavior and public statements on Twitter to jointly model legislators .",2,0.4535588,96.63343109265222,25
1909,"In addition , we propose a novel task , namely hashtag usage prediction to model the ideology of legislators on Twitter .",2,0.5181617,101.33207125257849,22
1909,"In practice , we construct a heterogeneous graph for the legislative context and use relational graph neural networks to learn the representation of legislators with the guidance of historical records of their voting and hashtag usage .",2,0.70098513,50.92335674334952,37
1909,Experiment results indicate that our model yields significant improvements for the task of roll call vote prediction .,3,0.98783934,29.419566629390964,18
1909,Further analysis further demonstrates that legislator representation we learned captures nuances in statements .,3,0.9730006,447.20723427891886,14
1910,"Languages are dynamic systems : word usage may change over time , reflecting various societal factors .",0,0.9065955,134.69278790427595,17
1910,"However , all languages do not evolve identically : the impact of an event , the influence of a trend or thinking , can differ between communities .",0,0.8557244,144.2365442624916,28
1910,"In this paper , we propose to track these divergences by comparing the evolution of a word and its translation across two languages .",1,0.9056819,23.877223082194654,24
1910,"We investigate several methods of building time-varying and bilingual word embeddings , using contextualised and non-contextualised embeddings .",2,0.59556025,21.591201431700796,20
1910,"We propose a set of scenarios to characterize semantic divergence across two languages , along with a setup to differentiate them in a bilingual corpus .",1,0.43831947,62.53485414898403,26
1910,"We evaluate the different methods by generating a corpus of synthetic semantic change across two languages , English and French , before applying them to newspaper corpora to detect bilingual semantic divergence and provide qualitative insight for the task .",2,0.7181147,81.54527054358581,40
1910,We conclude that BERT embeddings coupled with a clustering step lead to the best performance on synthetic corpora ;,3,0.9847756,46.84543168995434,19
1910,"however , the performance of CBOW embeddings is very competitive and more adapted to an exploratory analysis on a large corpus .",3,0.86556756,66.99059079373401,22
1911,"Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training , i.e .",0,0.9317546,31.260837723148384,20
1911,zero-shot translation .,0,0.34157118,23.712068874778645,3
1911,"Despite being conceptually attractive , it often suffers from low output quality .",0,0.92974186,56.443241043908884,13
1911,The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training .,3,0.79996735,59.90920589977012,23
1911,We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens .,3,0.95659935,81.24192028315143,19
1911,We show that this can be easily alleviated by removing residual connections in an encoder layer .,3,0.8411677,32.286723385818696,17
1911,"With this modification , we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions .",3,0.8381859,41.420428233237416,21
1911,"The improvements are particularly prominent between related languages , where our proposed model outperforms pivot-based translation .",3,0.9272276,74.18054102085985,17
1911,"Moreover , our approach allows easy integration of new languages , which substantially expands translation coverage .",3,0.8272983,92.82574710978639,17
1911,"By thorough inspections of the hidden layer outputs , we show that our approach indeed leads to more language-independent representations .",3,0.82322204,72.47787379843905,23
1912,Commonsense reasoning research has so far been limited to English .,0,0.92753756,40.978166406601204,11
1912,We aim to evaluate and improve popular multilingual language models ( ML-LMs ) to help advance commonsense reasoning ( CSR ) beyond English .,1,0.94011104,58.65265614220006,26
1912,"We collect the Mickey corpus , consisting of 561 k sentences in 11 different languages , which can be used for analyzing and improving ML-LMs .",2,0.8295431,94.06735949629602,28
1912,"We propose Mickey Probe , a language-general probing task for fairly evaluating the common sense of popular ML-LMs across different languages .",1,0.44141987,168.6173828050052,26
1912,"In addition , we also create two new datasets , X-CSQA and X-CODAH , by translating their English versions to 14 other languages , so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning .",2,0.7603322,44.082030467464165,42
1912,"To improve the performance beyond English , we propose a simple yet effective method — multilingual contrastive pretraining ( MCP ) .",1,0.3130831,36.220356877795965,22
1912,"It significantly enhances sentence representations , yielding a large performance gain on both benchmarks ( e.g. , + 2.7 % accuracy for X-CSQA over XLM-R_L ) .",3,0.9008651,81.40661222763023,31
1913,Attention mechanisms have achieved substantial improvements in neural machine translation by dynamically selecting relevant inputs for different predictions .,0,0.92929596,35.082779907589256,19
1913,"However , recent studies have questioned the attention mechanisms ’ capability for discovering decisive inputs .",0,0.9402433,239.36319534181627,16
1913,"In this paper , we propose to calibrate the attention weights by introducing a mask perturbation model that automatically evaluates each input ’s contribution to the model outputs .",1,0.76153034,35.20989739266176,29
1913,"We increase the attention weights assigned to the indispensable tokens , whose removal leads to a dramatic performance decrease .",3,0.5947164,172.59825081794457,20
1913,The extensive experiments on the Transformer-based translation have demonstrated the effectiveness of our model .,3,0.8731143,13.446519672013977,16
1913,We further find that the calibrated attention weights are more uniform at lower layers to collect multiple information while more concentrated on the specific inputs at higher layers .,3,0.98129416,66.42452758051995,29
1913,Detailed analyses also show a great need for calibration in the attention weights with high entropy where the model is unconfident about its decision .,3,0.9723647,81.73504963504674,25
1914,"We propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pre-trained document context signals and assess the impact on translation performance of ( 1 ) different pretraining approaches for generating these signals , ( 2 ) the quantity of parallel data for which document context is available , and ( 3 ) conditioning on source , target , or source and target contexts .",1,0.44719362,35.627395270036956,72
1914,"Experiments on the NIST Chinese-English , and IWSLT and WMT English-German tasks support four general conclusions : that using pre-trained context representations markedly improves sample efficiency , that adequate parallel data resources are crucial for learning to use document context , that jointly conditioning on multiple context representations outperforms any single representation , and that source context is more valuable for translation performance than target side context .",3,0.9277336,52.587183944585504,70
1914,Our best multi-context model consistently outperforms the best existing context-aware transformers .,3,0.9365157,28.026376798870544,13
1915,Recent research in multilingual language models ( LM ) has demonstrated their ability to effectively handle multiple languages in a single model .,0,0.9569078,20.87133256405024,23
1915,This holds promise for low web-resource languages ( LRL ) as multilingual models can enable transfer of supervision from high resource languages to LRLs .,0,0.54869586,49.10025139338983,25
1915,"However , incorporating a new language in an LM still remains a challenge , particularly for languages with limited corpora and in unseen scripts .",0,0.8895813,73.949004034868,25
1915,"In this paper we argue that relatedness among languages in a language family may be exploited to overcome some of the corpora limitations of LRLs , and propose RelateLM .",1,0.8606525,35.62064299477854,30
1915,"We focus on Indian languages , and exploit relatedness along two dimensions : ( 1 ) script ( since many Indic scripts originated from the Brahmic script ) , and ( 2 ) sentence structure .",2,0.6481067,88.07653546511166,36
1915,RelateLM uses transliteration to convert the unseen script of limited LRL text into the script of a Related Prominent Language ( RPL ) ( Hindi in our case ) .,0,0.50638723,182.83660619739246,30
1915,"While exploiting similar sentence structures , RelateLM utilizes readily available bilingual dictionaries to pseudo translate RPL text into LRL corpora .",3,0.36552218,238.5645425982073,21
1915,"Experiments on multiple real-world benchmark datasets provide validation to our hypothesis that using a related language as pivot , along with transliteration and pseudo translation based data augmentation , can be an effective way to adapt LMs for LRLs , rather than direct training or pivoting through English .",3,0.9241841,56.94637105574515,49
1916,The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community .,0,0.8642829,28.02352372628644,27
1916,"However , for many dependency parsing schemes , an important detail of this approach is that the spanning tree must have exactly one edge emanating from the root .",0,0.7474158,69.05697895619687,29
1916,"While work has been done to efficiently solve this problem for finding the one-best dependency tree , no research has attempted to extend this solution to finding the K-best dependency trees .",0,0.8778045,41.23597268868411,33
1916,This is arguably a more important extension as a larger proportion of decoded trees will not be subject to the root constraint of dependency trees .,3,0.7056074,63.56496260310532,26
1916,"Indeed , we show that the rate of root constraint violations increases by an average of 13 times when decoding with K=50 as opposed to K=1 .",3,0.96742076,38.897126598980066,27
1916,"In this paper , we provide a simplification of the K-best spanning tree algorithm of Camerini et al .",1,0.759445,46.736706725022515,19
1916,( 1980 ) .,4,0.5906679,263.8161695033992,4
1916,Our simplification allows us to obtain a constant time speed-up over the original algorithm .,3,0.7730386,33.544092753495825,17
1916,"Furthermore , we present a novel extension of the algorithm for decoding the K-best dependency trees of a graph which are subject to a root constraint .",3,0.4440769,40.859750104712155,27
1917,"This survey builds an interdisciplinary picture of Argument Mining ( AM ) , with a strong focus on its potential to address issues related to Social and Political Science .",1,0.7511301,72.48965976551163,30
1917,"More specifically , we focus on AM challenges related to its applications to social media and in the multilingual domain , and then proceed to the widely debated notion of argument quality .",1,0.6787613,121.56793145883644,33
1917,We propose a novel definition of argument quality which is integrated with that of deliberative quality from the Social Science literature .,1,0.5258898,42.784463793019015,22
1917,"Under our definition , the quality of a contribution needs to be assessed at multiple levels : the contribution itself , its preceding context , and the consequential effect on the development of the upcoming discourse .",0,0.47852683,60.102085437914056,37
1917,The latter has not received the deserved attention within the community .,0,0.8757601,47.91728610619254,12
1917,"We finally define an application of AM for Social Good : ( semi-) automatic moderation , a highly integrative application which ( a ) represents a challenging testbed for the integrated notion of quality we advocate , ( b ) allows the empirical quantification of argument / deliberative quality to benefit from the developments in other NLP fields ( i.e .",3,0.521794,131.9575321256778,63
1917,"hate speech detection , fact checking , debiasing ) , and ( c ) has a clearly beneficial potential at the level of its societal thanks to its real-world application ( even if extremely ambitious ) .",3,0.6165861,185.36144616827826,37
1918,Research on the application of NLP in symbol-based Augmentative and Alternative Communication ( AAC ) tools for improving social interaction support is scarce .,0,0.92261964,47.03083146422609,26
1918,We contribute a novel method for generating context-related vocabulary from photographs of personally relevant events aimed at supporting people with language impairments in retelling their past experiences .,1,0.4268046,34.817831397265046,28
1918,Performance was calculated with information retrieval concepts on the relevance of vocabulary generated for communicating a corpus of 9730 narrative phrases about events depicted in 1946 photographs .,2,0.9083144,220.1638377842323,28
1918,"In comparison to a baseline generation composed of frequent English words , our method generated vocabulary with a 4.6 gain in mean average precision , regardless of the level of contextual information in the input photographs , and 6.9 for photographs in which contextual information was extracted correctly .",3,0.9066512,60.69774130068384,49
1918,We conclude by discussing how our findings provide insights for system optimization and usage .,3,0.9306411,36.529162501748395,15
1919,"Continuity of care is crucial to ensuring positive health outcomes for patients discharged from an inpatient hospital setting , and improved information sharing can help .",0,0.74484664,34.813307527433416,26
1919,"To share information , caregivers write discharge notes containing action items to share with patients and their future caregivers , but these action items are easily lost due to the lengthiness of the documents .",0,0.84812,80.43418327550351,35
1919,"In this work , we describe our creation of a dataset of clinical action items annotated over MIMIC-III , the largest publicly available dataset of real clinical notes .",1,0.85729486,55.31812564012185,31
1919,"This dataset , which we call CLIP , is annotated by physicians and covers 718 documents representing 100K sentences .",2,0.6862949,93.09063863156939,20
1919,"We describe the task of extracting the action items from these documents as multi-aspect extractive summarization , with each aspect representing a type of action to be taken .",2,0.53005433,39.90072335975717,29
1919,"We evaluate several machine learning models on this task , and show that the best models exploit in-domain language model pre-training on 59 K unannotated documents , and incorporate context from neighboring sentences .",3,0.64424366,51.90544977604868,35
1919,We also propose an approach to pre-training data selection that allows us to explore the trade-off between size and domain-specificity of pre-training datasets for this task .,3,0.49706894,14.373814798156815,29
1920,"Emojis have become ubiquitous in digital communication , due to their visual appeal as well as their ability to vividly convey human emotion , among other factors .",0,0.9559968,31.368871259837622,28
1920,This also leads to an increased need for systems and tools to operate on text containing emojis .,0,0.91161865,25.95087480861367,18
1920,"In this study , we assess this support by considering test sets of tweets with emojis , based on which we perform a series of experiments investigating the ability of prominent NLP and text processing tools to adequately process them .",1,0.7225441,40.74698966594873,41
1920,"In particular , we consider tokenization , part-of-speech tagging , dependency parsing , as well as sentiment analysis .",2,0.6602558,49.47288028354008,20
1920,Our findings show that many systems still have notable shortcomings when operating on text containing emojis .,3,0.9900205,42.713770822248215,17
1921,Natural language processing techniques have demonstrated promising results in keyphrase generation .,0,0.91605324,27.366946334807402,12
1921,"However , one of the major challenges in neural keyphrase generation is processing long documents using deep neural networks .",0,0.9109139,28.532191391016035,20
1921,"Generally , documents are truncated before given as inputs to neural networks .",0,0.75614685,101.76540095897852,13
1921,"Consequently , the models may miss essential points conveyed in the target document .",0,0.5322632,124.07942663011399,14
1921,"To overcome this limitation , we propose SEG-Net , a neural keyphrase generation model that is composed of two major components , ( 1 ) a selector that selects the salient sentences in a document and ( 2 ) an extractor-generator that jointly extracts and generates keyphrases from the selected sentences .",2,0.7152983,19.509071348316716,56
1921,"Transformer , a self-attentive architecture , as the basic building block with a novel layer-wise coverage attention to summarize most of the points discussed in the document .",3,0.42877153,63.38577962187536,30
1921,The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin .,3,0.93650836,17.575181089754118,35
1922,We propose a method for generating paraphrases of English questions that retain the original intent but use a different surface form .,1,0.58612645,36.4218572154478,22
1922,"Our model combines a careful choice of training objective with a principled information bottleneck , to induce a latent encoding space that disentangles meaning and form .",2,0.6341275,88.41468016645159,27
1922,"We train an encoder-decoder model to reconstruct a question from a paraphrase with the same meaning and an exemplar with the same surface form , leading to separated encoding spaces .",2,0.809227,31.644423767168817,31
1922,"We use a Vector-Quantized Variational Autoencoder to represent the surface form as a set of discrete latent variables , allowing us to use a classifier to select a different surface form at test time .",2,0.870969,21.015949482905892,35
1922,"Crucially , our method does not require access to an external source of target exemplars .",3,0.76450706,32.70507391543263,16
1922,Extensive experiments and a human evaluation show that we are able to generate paraphrases with a better tradeoff between semantic preservation and syntactic novelty compared to previous methods .,3,0.92784196,19.076168557541564,29
1923,We present AggGen ( pronounced ‘ again ’ ) a data-to-text model which re-introduces two explicit sentence planning stages into neural data-to-text systems : input ordering and input aggregation .,2,0.47315395,107.52264464403733,36
1923,"In contrast to previous work using sentence planning , our model is still end-to-end : AggGen performs sentence planning at the same time as generating text by learning latent alignments ( via semantic facts ) between input representation and target text .",3,0.5343244,98.5460271539938,43
1923,"Experiments on the WebNLG and E2E challenge data show that by using fact-based alignments our approach is more interpretable , expressive , robust to noise , and easier to control , while retaining the advantages of end-to-end systems in terms of fluency .",3,0.91160434,36.686947773451294,47
1923,Our code is available at https://github.com/XinnuoXu/AggGen .,3,0.6055491,15.978812075897066,7
1924,"Publicly available , large pretrained Language Models ( LMs ) generate text with remarkable quality , but only sequentially from left to right .",0,0.84400636,65.2389210912636,24
1924,"As a result , they are not immediately applicable to generation tasks that break the unidirectional assumption , such as paraphrasing or text-infilling , necessitating task-specific supervision .",0,0.6505363,49.558398930770444,30
1924,"In this paper , we present Reflective Decoding , a novel unsupervised algorithm that allows for direct application of unidirectional LMs to non-sequential tasks .",1,0.9100514,26.958654320764296,25
1924,"Our 2-step approach requires no supervision or even parallel corpora , only two off-the-shelf pretrained LMs in opposite directions : forward and backward .",3,0.45752668,45.334845213419925,26
1924,"First , in the contextualization step , we use LMs to generate ensembles of past and future contexts which collectively capture the input ( e.g .",2,0.811214,39.63855197228399,26
1924,the source sentence for paraphrasing ) .,2,0.4578924,128.9772239086355,7
1924,"Second , in the reflection step , we condition on these “ context ensembles ” , generating outputs that are compatible with them .",2,0.78990984,88.48325794072665,24
1924,"Comprehensive empirical results demonstrate that Reflective Decoding outperforms strong unsupervised baselines on both paraphrasing and abductive text infilling , significantly narrowing the gap between unsupervised and supervised methods .",3,0.97299397,22.103085003413636,29
1924,Reflective Decoding surpasses multiple supervised baselines on various metrics including human evaluation .,3,0.8172074,50.08953782783412,13
1925,Recent neural text generation models have shown significant improvement in generating descriptive text from structured data such as table formats .,0,0.9312902,26.90700150748781,21
1925,One of the remaining important challenges is generating more analytical descriptions that can be inferred from facts in a data source .,0,0.8498952,47.785949065332055,22
1925,The use of a template-based generator and a pointer-generator is among the potential alternatives for table-to-text generators .,3,0.4823777,26.790269162218685,26
1925,"In this paper , we propose a framework consisting of a pre-trained model and a copy mechanism .",1,0.7309615,13.74963407245308,18
1925,The pre-trained models are fine-tuned to produce fluent text that is enriched with numerical reasoning .,2,0.55943376,17.073902308916924,16
1925,"However , it still lacks fidelity to the table contents .",0,0.8201697,124.53322691775367,11
1925,The copy mechanism is incorporated in the fine-tuning step by using general placeholders to avoid producing hallucinated phrases that are not supported by a table while preserving high fluency .,2,0.5120782,45.77211924464562,30
1925,"In summary , our contributions are ( 1 ) a new dataset for numerical table-to-text generation using pairs of a table and a paragraph of a table description with richer inference from scientific papers , and ( 2 ) a table-to-text generation framework enriched with numerical reasoning .",3,0.9520966,43.759686232562636,56
1926,"In this paper , we focus on the problem of citing sentence generation , which entails generating a short text to capture the salient information in a cited paper and the connection between the citing and cited paper .",1,0.84765905,35.22062744585381,39
1926,"We present BACO , a BAckground knowledge-and COntent-based framework for citing sentence generation , which considers two types of information : ( 1 ) background knowledge by leveraging structural information from a citation network ;",2,0.49308798,180.89909613600352,39
1926,"and ( 2 ) content , which represents in-depth information about what to cite and why to cite .",0,0.46819398,46.22388105948122,21
1926,"First , a citation network is encoded to provide background knowledge .",2,0.5840687,163.59619110124018,12
1926,"Second , we apply salience estimation to identify what to cite by estimating the importance of sentences in the cited paper .",2,0.87731457,57.507844675985204,22
1926,"During the decoding stage , both types of information are combined to facilitate the text generation , and then we conduct a joint training for the generator and citation function classification to make the model aware of why to cite .",2,0.78596896,72.38569144745684,41
1926,Our experimental results show that our framework outperforms comparative baselines .,3,0.9719254,16.638879033306818,11
1927,"Current dialogue summarization systems usually encode the text with a number of general semantic features ( e.g. , keywords and topics ) to gain more powerful dialogue modeling capabilities .",0,0.8988667,55.91108734284901,30
1927,"However , these features are obtained via open-domain toolkits that are dialog-agnostic or heavily relied on human annotations .",0,0.81121486,36.94401321181784,19
1927,"In this paper , we show how DialoGPT , a pre-trained model for conversational response generation , can be developed as an unsupervised dialogue annotator , which takes advantage of dialogue background knowledge encoded in DialoGPT .",1,0.8317358,23.412894881568683,37
1927,"We apply DialoGPT to label three types of features on two dialogue summarization datasets , SAMSum and AMI , and employ pre-trained and non pre-trained models as our summarizers .",2,0.83224255,68.00265815258993,30
1927,Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-the-art performance on the SAMSum dataset .,3,0.9665954,10.75727187909935,30
1928,"Recent pretrained language models “ solved ” many reading comprehension benchmarks , where questions are written with access to the evidence document .",0,0.84126496,75.56859729991689,23
1928,"However , datasets containing information-seeking queries where evidence documents are provided after the queries are written independently remain challenging .",0,0.91872,111.49723007667977,22
1928,"We analyze why answering information-seeking queries is more challenging and where their prevalent unanswerabilities arise , on Natural Questions and TyDi QA .",1,0.43668422,147.9260792748834,25
1928,"Our controlled experiments suggest two headrooms – paragraph selection and answerability prediction , i.e .",3,0.9637584,207.89046642694504,15
1928,whether the paired evidence document contains the answer to the query or not .,2,0.402239,65.11854966831555,14
1928,"When provided with a gold paragraph and knowing when to abstain from answering , existing models easily outperform a human annotator .",0,0.4990425,64.89360239552808,22
1928,"However , predicting answerability itself remains challenging .",0,0.9401657,312.64222642661633,8
1928,We manually annotate 800 unanswerable examples across six languages on what makes them challenging to answer .,2,0.8005394,44.34346051786078,17
1928,"With this new data , we conduct per-category answerability prediction , revealing issues in the current dataset collection as well as task formulation .",2,0.5779096,175.52799030995467,24
1928,"Together , our study points to avenues for future research in information-seeking question answering , both for dataset creation and model development .",3,0.9842593,43.62108307679319,25
1928,Our code and annotated data is publicly available at https://github.com/AkariAsai/unanswerable_qa .,3,0.61579967,15.778769331702614,11
1929,"Users of medical question answering systems often submit long and detailed questions , making it hard to achieve high recall in answer retrieval .",0,0.9426583,46.07656107857702,24
1929,"To alleviate this problem , we propose a novel Multi-Task Learning ( MTL ) method with data augmentation for medical question understanding .",1,0.45292017,27.570483389624812,23
1929,We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment ( RQE ) using their definitions in the medical domain .,2,0.43873763,35.04566168111147,26
1929,"Based on this equivalence , we propose a data augmentation algorithm to use just one dataset to optimize for both tasks , with a weighted MTL loss .",2,0.6598234,58.04628288269482,28
1929,"We introduce gradually soft parameter-sharing : a constraint for decoder parameters to be close , that is gradually loosened as we move to the highest layer .",2,0.7036131,171.77465813992495,29
1929,We show through ablation studies that our proposed novelties improve performance .,3,0.8759341,93.20683290894067,12
1929,"Our method outperforms existing MTL methods across 4 datasets of medical question pairs , in ROUGE scores , RQE accuracy and human evaluation .",3,0.86848646,114.49585221102632,24
1929,"Finally , we show that our method fares better than single-task learning under 4 low-resource settings .",3,0.95805544,20.489376265058105,19
1930,A common issue in real-world applications of named entity recognition and classification ( NERC ) is the absence of annotated data for the target entity classes during training .,0,0.957884,22.3707003984399,29
1930,Zero-shot learning approaches address this issue by learning models from classes with training data that can predict classes without it .,0,0.8497926,41.39613200329141,21
1930,"This paper presents the first approach for zero-shot NERC , introducing novel architectures that leverage the fact that textual descriptions for many entity classes occur naturally .",1,0.7583545,69.54018142908627,27
1930,We address the zero-shot NERC specific challenge that the not-an-entity class is not well defined as different entity classes are considered in training and testing .,2,0.40337208,65.69691168241432,26
1930,"For evaluation , we adapt two datasets , OntoNotes and MedMentions , emulating the difficulty of real-world zero-shot learning by testing models on the rarest entity classes .",2,0.73504084,64.13429671119846,28
1930,Our proposed approach outperforms baselines adapted from machine reading comprehension and zero-shot text classification .,3,0.8682815,20.020577242552765,15
1930,"Furthermore , we assess the effect of different class descriptions for this task .",2,0.5677117,42.298533104655064,14
1931,"Recently , word enhancement has become very popular for Chinese Named Entity Recognition ( NER ) , reducing segmentation errors and increasing the semantic and boundary information of Chinese words .",0,0.9496718,63.98901534604687,31
1931,"However , these methods tend to ignore the information of the Chinese character structure after integrating the lexical information .",0,0.7918001,45.28152500203778,20
1931,"Chinese characters have evolved from pictographs since ancient times , and their structure often reflects more information about the characters .",0,0.94137555,57.620137066797085,21
1931,This paper presents a novel Multi-metadata Embedding based Cross-Transformer ( MECT ) to improve the performance of Chinese NER by fusing the structural information of Chinese characters .,1,0.8476143,33.81708984512594,28
1931,"Specifically , we use multi-metadata embedding in a two-stream Transformer to integrate Chinese character features with the radical-level embedding .",2,0.87816906,58.76889333296429,22
1931,"With the structural characteristics of Chinese characters , MECT can better capture the semantic information of Chinese characters for NER .",3,0.7577578,47.605074264805914,21
1931,The experimental results obtained on several well-known benchmarking datasets demonstrate the merits and superiority of the proposed MECT method .,3,0.9347575,29.90514441126333,21
1932,"As the sources of information that we consume everyday rapidly diversify , it is becoming increasingly important to develop NLP tools that help to evaluate the credibility of the information we receive .",0,0.94497114,30.838998647403233,33
1932,A critical step towards this goal is to determine the factuality of events in text .,0,0.75063884,16.795399394764363,16
1932,"In this paper , we frame factuality assessment as a modal dependency parsing task that identifies the events and their sources , formally known as conceivers , and then determine the level of certainty that the sources are asserting with respect to the events .",1,0.6655894,47.18832359662793,45
1932,"We crowdsource the first large-scale data set annotated with modal dependency structures that consists of 353 Covid-19 related news articles , 24,016 events , and 2,938 conceivers .",2,0.88554215,61.67714451770949,29
1932,"We also develop the first modal dependency parser that jointly extracts events , conceivers and constructs the modal dependency structure of a text .",3,0.4281406,66.14486919554723,24
1932,We evaluate the joint model against a pipeline model and demonstrate the advantage of the joint model in conceiver extraction and modal dependency structure construction when events and conceivers are automatically extracted .,3,0.7707017,109.94715148136659,33
1932,We believe the dataset and the models will be a valuable resource for a whole host of NLP applications such as fact checking and rumor detection .,3,0.97265244,14.84326170824089,27
1933,The modeling of conversational context plays a vital role in emotion recognition from conversation ( ERC ) .,0,0.9490422,65.11047692326169,18
1933,"In this paper , we put forward a novel idea of encoding the utterances with a directed acyclic graph ( DAG ) to better model the intrinsic structure within a conversation , and design a directed acyclic neural network , namely DAG-ERC , to implement this idea .",1,0.722795,29.902399509559334,50
1933,"In an attempt to combine the strengths of conventional graph-based neural models and recurrence-based neural models , DAG-ERC provides a more intuitive way to model the information flow between long-distance conversation background and nearby context .",3,0.55302995,32.50564742774715,42
1933,Extensive experiments are conducted on four ERC benchmarks with state-of-the-art models employed as baselines for comparison .,2,0.71725285,16.732245925540738,23
1933,The empirical results demonstrate the superiority of this new model and confirm the motivation of the directed acyclic graph architecture for ERC .,3,0.98684907,31.173147759181525,23
1934,Models pre-trained on large-scale regular text corpora often do not work well for user-generated data where the language styles differ significantly from the mainstream text .,0,0.8388157,26.1473584194896,27
1934,"Here we present Context-Aware Rule Injection ( CARI ) , an innovative method for formality style transfer ( FST ) by injecting multiple rules into an end-to-end BERT-based encoder and decoder model .",1,0.77381384,28.456443078527386,38
1934,CARI is able to learn to select optimal rules based on context .,3,0.6726984,38.36344214145771,13
1934,The intrinsic evaluation showed that CARI achieved the new highest performance on the FST benchmark dataset .,3,0.9792843,75.82577557530014,17
1934,Our extrinsic evaluation showed that CARI can greatly improve the regular pre-trained models ’ performance on several tweet sentiment analysis tasks .,3,0.9784539,49.04181266886565,22
1934,Our contributions are as follows : 1 .,3,0.3102796,38.35592440799087,8
1934,"We propose a new method , CARI , to integrate rules for pre-trained language models .",2,0.34305957,48.481622875742026,16
1934,CARI is context-aware and can trained end-to-end with the downstream NLP applications .,3,0.5904111,31.410646392948447,14
1934,2 .,4,0.8740741,90.9251343538622,2
1934,We have achieved new state-of-the-art results for FST on the benchmark GYAFC dataset .,3,0.9441496,14.615838648612744,19
1934,3 .,4,0.66727066,96.48333216766866,2
1934,We are the first to evaluate FST methods with extrinsic evaluation and specifically on sentiment classification tasks .,3,0.7869138,33.90537773339851,18
1934,We show that CARI outperformed existing rule-based FST approaches for sentiment classification .,3,0.9730564,51.18243633576519,14
1935,"Emotion detection in dialogues is challenging as it often requires the identification of thematic topics underlying a conversation , the relevant commonsense knowledge , and the intricate transition patterns between the affective states .",0,0.9288776,67.96690146428116,34
1935,"In this paper , we propose a Topic-Driven Knowledge-Aware Transformer to handle the challenges above .",1,0.891097,24.378009562680013,18
1935,We firstly design a topic-augmented language model ( LM ) with an additional layer specialized for topic detection .,2,0.8323692,50.19025262467463,19
1935,The topic-augmented LM is then combined with commonsense statements derived from a knowledge base based on the dialogue contextual information .,2,0.65321594,56.58887312183161,21
1935,"Finally , a transformer-based encoder-decoder architecture fuses the topical and commonsense information , and performs the emotion label sequence prediction .",2,0.68318427,52.62334273713857,23
1935,"The model has been experimented on four datasets in dialogue emotion detection , demonstrating its superiority empirically over the existing state-of-the-art approaches .",3,0.5202942,25.119986568723558,29
1935,Quantitative and qualitative results show that the model can discover topics which help in distinguishing emotion categories .,3,0.98545825,59.78523956704886,18
1936,"Approaches to computational argumentation tasks such as stance detection and aspect detection have largely focused on the text of independent claims , losing out on potentially valuable context provided by the rest of the collection .",0,0.9090839,46.753613560379094,36
1936,"We introduce a general approach to these tasks motivated by syntopical reading , a reading process that emphasizes comparing and contrasting viewpoints in order to improve topic understanding .",1,0.50592214,81.57576115355619,29
1936,"To capture collection-level context , we introduce the syntopical graph , a data structure for linking claims within a collection .",2,0.75215757,108.00165479954592,23
1936,"A syntopical graph is a typed multi-graph where nodes represent claims and edges represent different possible pairwise relationships , such as entailment , paraphrase , or support .",0,0.71343666,94.00099779308229,28
1936,"Experiments applying syntopical graphs to the problems of detecting stance and aspects demonstrate state-of-the-art performance in each domain , significantly outperforming approaches that do not utilize collection-level information .",3,0.8808607,59.920948060834306,37
1937,"The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites , as users turn to social media to share their views regarding various issues related to the pandemic , e.g .",0,0.9298421,15.971099405392868,44
1937,stay at home mandates and wearing face masks when out in public .,3,0.6591557,132.65826358741106,13
1937,"We set out to make use of this data by collecting the stance expressed by Twitter users , with respect to topics revolving around the pandemic .",2,0.68954486,44.76193991884958,27
1937,"We annotate a new stance detection dataset , called COVID-19-Stance .",2,0.62770665,49.76768107461262,13
1937,"Using this newly annotated dataset , we train several established stance detection models to ascertain a baseline performance for this specific task .",2,0.75273025,73.75739416067401,23
1937,"To further improve the performance , we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets .",2,0.6052075,20.956128137072398,28
1937,"The dataset , code , and other resources are available on GitHub .",3,0.5191581,52.53203419428327,13
1938,Fact verification is a challenging task that requires simultaneously reasoning and aggregating over multiple retrieved pieces of evidence to evaluate the truthfulness of a claim .,0,0.93790853,25.921193376352957,26
1938,"Existing approaches typically ( i ) explore the semantic interaction between the claim and evidence at different granularity levels but fail to capture their topical consistency during the reasoning process , which we believe is crucial for verification ;",0,0.8085398,86.77733999761578,39
1938,"( ii ) aggregate multiple pieces of evidence equally without considering their implicit stances to the claim , thereby introducing spurious information .",0,0.4186935,228.1793430851516,23
1938,"To alleviate the above issues , we propose a novel topic-aware evidence reasoning and stance-aware aggregation model for more accurate fact verification , with the following four key properties : 1 ) checking topical consistency between the claim and evidence ;",2,0.50788295,83.51489392141718,43
1938,2 ) maintaining topical coherence among multiple pieces of evidence ;,0,0.4590596,526.0587021017056,11
1938,3 ) ensuring semantic similarity between the global topic information and the semantic representation of evidence ;,3,0.46111795,217.00015575150275,17
1938,4 ) aggregating evidence based on their implicit stances to the claim .,2,0.5026092,147.1900326636562,13
1938,Extensive experiments conducted on the two benchmark datasets demonstrate the superiority of the proposed model over several state-of-the-art approaches for fact verification .,3,0.8722572,7.457324906395061,29
1938,The source code can be obtained from https://github.com/jasenchn/TARSA .,3,0.6150303,10.756805107733827,9
1939,"We introduce the well-established social scientific concept of social solidarity and its contestation , anti-solidarity , as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic .",1,0.4967227,53.10605716980925,49
1939,"To this end , we annotate 2.3 k English and German tweets for ( anti-) solidarity expressions , utilizing multiple human annotators and two annotation approaches ( experts vs .",2,0.8973445,130.50945233463847,32
1939,crowds ) .,0,0.32402292,185.0564962013611,3
1939,We use these annotations to train a BERT model with multiple data augmentation strategies .,2,0.7748712,19.889366869743718,15
1939,"Our augmented BERT model that combines both expert and crowd annotations outperforms the baseline BERT classifier trained with expert annotations only by over 25 points , from 58 % macro-F1 to almost 85 % .",3,0.90083444,63.54238556210228,37
1939,We use this high-quality model to automatically label over 270k tweets between September 2019 and December 2020 .,2,0.77127135,43.59072542144329,18
1939,"We then assess the automatically labeled data for how statements related to European ( anti-) solidarity discourses developed over time and in relation to one another , before and during the COVID-19 crisis .",2,0.74978423,80.1767039224667,36
1939,Our results show that solidarity became increasingly salient and contested during the crisis .,3,0.9900525,75.25309618022244,14
1939,"While the number of solidarity tweets remained on a higher level and dominated the discourse in the scrutinized time frame , anti-solidarity tweets initially spiked , then decreased to ( almost ) pre-COVID-19 values before rising to a stable higher level until the end of 2020 .",3,0.93125755,59.22581310860396,48
1940,"In conversation , uptake happens when a speaker builds on the contribution of their interlocutor by , for example , acknowledging , repeating or reformulating what they have said .",0,0.89916867,76.30571149764893,30
1940,"In education , teachers ’ uptake of student contributions has been linked to higher student achievement .",0,0.907394,51.25584021576062,17
1940,"Yet measuring and improving teachers ’ uptake at scale is challenging , as existing methods require expensive annotation by experts .",0,0.88506913,145.1761324690748,21
1940,"We propose a framework for computationally measuring uptake , by ( 1 ) releasing a dataset of student-teacher exchanges extracted from US math classroom transcripts annotated for uptake by experts ;",2,0.66854006,130.73480115883206,33
1940,"( 2 ) formalizing uptake as pointwise Jensen-Shannon Divergence ( pJSD ) , estimated via next utterance classification ;",2,0.7323208,428.0764722829755,19
1940,( 3 ) conducting a linguistically-motivated comparison of different unsupervised measures and ( 4 ) correlating these measures with educational outcomes .,2,0.5136749,37.066534557191524,23
1940,"We find that although repetition captures a significant part of uptake , pJSD outperforms repetition-based baselines , as it is capable of identifying a wider range of uptake phenomena like question answering and reformulation .",3,0.97966456,77.07434160861128,37
1940,We apply our uptake measure to three different educational datasets with outcome indicators .,2,0.82515275,191.50443083877633,14
1940,"Unlike baseline measures , pJSD correlates significantly with instruction quality in all three , providing evidence for its generalizability and for its potential to serve as an automated professional development tool for teachers .",3,0.9838344,65.91144203534162,34
1941,The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years .,0,0.9654052,19.825017999614257,20
1941,"So far , much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in linguistics .",0,0.8810992,42.48387223033729,38
1941,"To fill this gap , we offer a survey of code-switching ( C-S ) covering the literature in linguistics with a reflection on the key issues in language technologies .",1,0.85384953,51.130624897327955,31
1941,"From the linguistic perspective , we provide an overview of structural and functional patterns of C-S focusing on the literature from European and Indian contexts as highly multilingual areas .",1,0.6064664,86.33238135946917,30
1941,"From the language technologies perspective , we discuss how massive language models fail to represent diverse C-S types due to lack of appropriate training data , lack of robust evaluation benchmarks for C-S ( across multilingual situations and types of C-S ) and lack of end-to-end systems that cover sociolinguistic aspects of C-S as well .",1,0.4925479,36.86214432317824,58
1941,Our survey will be a step to-wards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and C-S .,3,0.9341972,95.37722997447202,27
1942,We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models .,3,0.39825773,36.97489864250816,25
1942,"We provide a new dataset of 40,000 entries , generated and labelled by trained annotators over four rounds of dynamic data creation .",2,0.69288903,52.78842295471654,23
1942,"It includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate .",0,0.37264282,69.57235343102238,21
1942,"Hateful entries make up 54 % of the dataset , which is substantially higher than comparable datasets .",3,0.9404532,76.27006213712544,18
1942,We show that model performance is substantially improved using this approach .,3,0.92977166,28.69763235092283,12
1942,Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick .,3,0.8146157,54.13468485524816,21
1942,"They also have better performance on HateCheck , a suite of functional tests for online hate detection .",3,0.905012,75.74226381659388,18
1942,"We provide the code , dataset and annotation guidelines for other researchers to use .",3,0.5649837,72.10534301231971,15
1943,"To defend against machine-generated fake news , an effective mechanism is urgently needed .",0,0.867405,40.98739028321918,16
1943,"We contribute a novel benchmark for fake news detection at the knowledge element level , as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative .",3,0.52216434,39.18587555093806,40
1943,"Due to training data scarcity , we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific , hard to detect , known inconsistencies .",2,0.6984358,91.560522193796,37
1943,"Our detection approach outperforms the state-of-the-art ( up to 16.8 % accuracy gain ) , and more critically , yields fine-grained explanations .",3,0.92698777,42.22190802232065,28
1944,"To quantify how well natural language understanding models can capture consistency in a general conversation , we introduce the DialoguE COntradiction DEtection task ( DECODE ) and a new conversational dataset containing both human-human and human-bot contradictory dialogues .",2,0.46863407,72.18023259349619,39
1944,We show that : ( i ) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain ;,3,0.981492,135.2762614966469,37
1944,( ii ) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and out-of-distribution dialogues than standard ( unstructured ) Transformers .,3,0.7725801,56.74919395300173,36
1944,We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots .,3,0.960173,22.357962078593662,40
1945,This paper is concerned with dialogue state tracking ( DST ) in a task-oriented dialogue system .,1,0.76765573,25.5798597930022,19
1945,"Building a DST module that is highly effective is still a challenging issue , although significant progresses have been made recently .",0,0.84719115,55.56570553736807,22
1945,"This paper proposes a new approach to dialogue state tracking , referred to as Seq2Seq-DU , which formalizes DST as a sequence-to-sequence problem .",1,0.8159658,25.26950376190167,28
1945,"Seq2Seq-DU employs two BERT-based encoders to respectively encode the utterances in the dialogue and the descriptions of schemas , an attender to calculate attentions between the utterance embeddings and the schema embeddings , and a decoder to generate pointers to represent the current state of dialogue .",2,0.66209567,33.75149155348739,50
1945,Seq2Seq-DU has the following advantages .,0,0.5241157,69.22337142358366,7
1945,"It can jointly model intents , slots , and slot values ;",3,0.3634262,417.4460727177204,12
1945,it can leverage the rich representations of utterances and schemas based on BERT ;,3,0.55945075,90.74131550094637,14
1945,"it can effectively deal with categorical and non-categorical slots , and unseen schemas .",3,0.5471093,57.96466070222581,14
1945,"In addition , Seq2Seq-DU can also be used in the NLU ( natural language understanding ) module of a dialogue system .",3,0.66888267,51.300317115501336,23
1945,"Experimental results on benchmark datasets in different settings ( SGD , MultiWOZ2.2 , MultiWOZ2.1 , WOZ2.0 , DSTC2 , M2M , SNIPS , and ATIS ) show that Seq2Seq-DU outperforms the existing methods .",3,0.89952207,25.541217066268054,36
1946,"Learning discrete dialog structure graph from human-human dialogs yields basic insights into the structure of conversation , and also provides background knowledge to facilitate dialog generation .",0,0.47056565,79.1247751792282,27
1946,"However , this problem is less studied in open-domain dialogue .",0,0.8995424,26.365462384918082,11
1946,"In this paper , we conduct unsupervised discovery of discrete dialog structure from chitchat corpora , and then leverage it to facilitate coherent dialog generation in downstream systems .",1,0.8308794,48.717848208429075,29
1946,"To this end , we present an unsupervised model , Discrete Variational Auto-Encoder with Graph Neural Network ( DVAE-GNN ) , to discover discrete hierarchical latent dialog states ( at the level of both session and utterance ) and their transitions from corpus as a dialog structure graph .",2,0.655402,46.42006672322882,51
1946,Then we leverage it as background knowledge to facilitate dialog management in a RL based dialog system .,2,0.5884918,66.01098999803955,18
1946,"Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure graph , and the use of dialog structure as background knowledge can significantly improve multi-turn coherence .",3,0.95539325,47.07854464170256,33
1947,We study the learning of a matching model for dialogue response selection .,1,0.6155792,55.919086060148395,13
1947,"Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios , we propose a hierarchical curriculum learning framework that trains the matching model in an “ easy-to-difficult ” scheme .",2,0.50540966,29.321315221690817,42
1947,Our learning framework consists of two complementary curricula : ( 1 ) corpus-level curriculum ( CC ) ;,2,0.7617021,118.79026814791237,18
1947,and ( 2 ) instance-level curriculum ( IC ) .,2,0.5708751,306.84776280806165,12
1947,"In CC , the model gradually increases its ability in finding the matching clues between the dialogue context and a response candidate .",3,0.79391557,120.32322180470189,23
1947,"As for IC , it progressively strengthens the model ’s ability in identifying the mismatching information between the dialogue context and a response candidate .",3,0.86682767,145.16782566180325,25
1947,Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics .,3,0.87948066,12.05629379401292,33
1948,"In this paper , we present a neural model for joint dropped pronoun recovery ( DPR ) and conversational discourse parsing ( CDP ) in Chinese conversational speech .",1,0.8938502,62.951584595722245,29
1948,"We show that DPR and CDP are closely related , and a joint model benefits both tasks .",3,0.9569229,79.71201704689298,18
1948,"We refer to our model as DiscProReco , and it first encodes the tokens in each utterance in a conversation with a directed Graph Convolutional Network ( GCN ) .",2,0.76846826,35.616592243906375,30
1948,The token states for an utterance are then aggregated to produce a single state for each utterance .,2,0.6706123,19.300424977917572,18
1948,The utterance states are then fed into a biaffine classifier to construct a conversational discourse graph .,2,0.6848657,18.671074136704522,17
1948,"A second ( multi-relational ) GCN is then applied to the utterance states to produce a discourse relation-augmented representation for the utterances , which are then fused together with token states in each utterance as input to a dropped pronoun recovery layer .",2,0.58641636,66.84365188803882,45
1948,The joint model is trained and evaluated on a new Structure Parsing-enhanced Dropped Pronoun Recovery ( SPDPR ) data set that we annotated with both two types of information .,2,0.7282493,91.27708799609026,32
1948,Experimental results on the SPDPR dataset and other benchmarks show that DiscProReco significantly outperforms the state-of-the-art baselines of both tasks .,3,0.9557641,19.81929484953658,27
1949,"Knowledge bases ( KBs ) and text often contain complementary knowledge : KBs store structured knowledge that can support long range reasoning , while text stores more comprehensive and timely knowledge in an unstructured way .",0,0.9358842,56.051684593266636,36
1949,"Separately embedding the individual knowledge sources into vector spaces has demonstrated tremendous successes in encoding the respective knowledge , but how to jointly embed and reason with both knowledge sources to fully leverage the complementary information is still largely an open problem .",0,0.88351077,39.31282300351061,43
1949,"We conduct a large-scale , systematic investigation of aligning KB and text embeddings for joint reasoning .",1,0.51977,52.65539595625607,17
1949,"We set up a novel evaluation framework with two evaluation tasks , few-shot link prediction and analogical reasoning , and evaluate an array of KB-text embedding alignment methods .",2,0.8037644,73.83706263910128,32
1949,"We also demonstrate how such alignment can infuse textual information into KB embeddings for more accurate link prediction on emerging entities and events , using COVID-19 as a case study .",3,0.7886984,48.87802959239881,32
1950,"Weak supervision has shown promising results in many natural language processing tasks , such as Named Entity Recognition ( NER ) .",0,0.9162682,15.070770065890725,22
1950,"Existing work mainly focuses on learning deep NER models only with weak supervision , i.e. , without any human annotation , and shows that by merely using weakly labeled data , one can achieve good performance , though still underperforms fully supervised NER with manually / strongly labeled data .",0,0.7190351,62.37644816295324,50
1950,"In this paper , we consider a more practical scenario , where we have both a small amount of strongly labeled data and a large amount of weakly labeled data .",1,0.42488718,16.3401630836663,31
1950,"Unfortunately , we observe that weakly labeled data does not necessarily improve , or even deteriorate the model performance ( due to the extensive noise in the weak labels ) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data .",3,0.94846267,41.51265836795652,51
1950,"To address this issue , we propose a new multi-stage computational framework – NEEDLE with three essential ingredients : ( 1 ) weak label completion , ( 2 ) noise-aware loss function , and ( 3 ) final fine-tuning over the strongly labeled data .",2,0.57249767,53.9815957702643,48
1950,"Through experiments on E-commerce query NER and Biomedical NER , we demonstrate that NEEDLE can effectively suppress the noise of the weak labels and outperforms existing methods .",3,0.87221146,66.96322071877792,28
1950,"In particular , we achieve new SOTA F1-scores on 3 Biomedical NER datasets : BC5CDR-chem 93.74 , BC5CDR-disease 90.69 , NCBI-disease 92.28 .",3,0.87091535,44.992128861052635,30
1951,"Recently , there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types , and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions .",0,0.9222178,52.12871974866914,39
1951,"A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce , and the annotation ability of existing distant or weak supervision approaches is very limited .",0,0.89447707,54.811135474285734,33
1951,"To remedy this problem , in this paper , we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model ( MLM ) .",1,0.6630468,38.18222999480186,30
1951,"Given a mention in a sentence , our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention , which can be used as type labels .",2,0.68083197,51.63373528093064,35
1951,"Experimental results demonstrate that , with the help of these automatically generated labels , the performance of an ultra-fine entity typing model can be improved substantially .",3,0.979685,27.894876957854063,27
1951,We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping .,3,0.93790245,34.69828373354366,22
1952,Recent advances in Named Entity Recognition ( NER ) show that document-level contexts can significantly improve model performance .,0,0.94097334,20.374942356657723,20
1952,"In many application scenarios , however , such contexts are not available .",0,0.85578096,67.49794601234443,13
1952,"In this paper , we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine , with the original sentence as the query .",1,0.8429762,37.30382253787469,36
1952,"We find empirically that the contextual representations computed on the retrieval-based input view , constructed through the concatenation of a sentence and its external contexts , can achieve significantly improved performance compared to the original input view based only on the sentence .",3,0.969331,36.046319828928546,45
1952,"Furthermore , we can improve the model performance of both input views by Cooperative Learning , a training method that encourages the two input views to produce similar contextual representations or output label distributions .",3,0.7942153,76.85125024049327,35
1952,Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains .,3,0.93867046,8.030713760912823,25
1953,"In BART and T5 transformer language models , we identify contextual word representations that function as * models of entities and situations * as they evolve throughout a discourse .",2,0.44448534,178.05282374156943,30
1953,"These neural representations have functional similarities to linguistic models of dynamic semantics : they support a linear readout of each entity ’s current properties and relations , and can be manipulated with predictable effects on language generation .",0,0.69348055,101.18031870376547,38
1953,"Our results indicate that prediction in pretrained neural language models is supported , at least in part , by dynamic representations of meaning and implicit simulation of entity state , and that this behavior can be learned with only text as training data .",3,0.987645,51.51320921870009,44
1954,Targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts .,0,0.7602818,39.51451872830181,20
1954,"To elucidate the mechanisms by which the models accomplish this behavior , this study applies causal mediation analysis to pre-trained neural language models .",1,0.7132558,36.73194289894668,24
1954,"We investigate the magnitude of models ’ preferences for grammatical inflections , as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic structures .",1,0.5688842,53.14238268575096,30
1954,"We uncover similarities and differences across architectures and model sizes — notably , that larger models do not necessarily learn stronger preferences .",3,0.9175442,114.11916112536993,23
1954,We also observe two distinct mechanisms for producing subject-verb agreement depending on the syntactic structure of the input sentence .,3,0.9500994,18.705801624967133,22
1954,"Finally , we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure .",3,0.97994655,29.9182663826435,21
1955,NLP has a rich history of representing our prior understanding of language in the form of graphs .,0,0.95238054,32.983036233923315,18
1955,Recent work on analyzing contextualized text representations has focused on hand-designed probe models to understand how and to what extent do these representations encode a particular linguistic phenomenon .,0,0.93030155,28.238621926367426,31
1955,"However , due to the inter-dependence of various phenomena and randomness of training probe models , detecting how these representations encode the rich information in these linguistic graphs remains a challenging problem .",0,0.88228095,54.83335556517749,33
1955,"In this paper , we propose a new information-theoretic probe , Bird ’s Eye , which is a fairly simple probe method for detecting if and how these representations encode the information in these linguistic graphs .",1,0.8682013,44.76734031611978,39
1955,"Instead of using model performance , our probe takes an information-theoretic view of probing and estimates the mutual information between the linguistic graph embedded in a continuous space and the contextualized word representations .",2,0.63543725,47.4991611740795,36
1955,"Furthermore , we also propose an approach to use our probe to investigate localized linguistic information in the linguistic graphs using perturbation analysis .",3,0.5075894,39.041741425254344,24
1955,We call this probing setup Worm ’s Eye .,0,0.56881946,224.65599734932877,9
1955,"Using these probes , we analyze the BERT models on its ability to encode a syntactic and a semantic graph structure , and find that these models encode to some degree both syntactic as well as semantic information ;",3,0.58788514,38.52014916887154,39
1955,albeit syntactic information to a greater extent .,3,0.64342904,68.36579244725486,8
1956,"Previous literatures show that pre-trained masked language models ( MLMs ) such as BERT can achieve competitive factual knowledge extraction performance on some datasets , indicating that MLMs can potentially be a reliable knowledge source .",0,0.878067,22.981862206409378,36
1956,"In this paper , we conduct a rigorous study to explore the underlying predicting mechanisms of MLMs over different extraction paradigms .",1,0.9368033,36.00414742632928,22
1956,"By investigating the behaviors of MLMs , we find that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts .",3,0.85635424,185.0571138950883,24
1956,"Furthermore , incorporating illustrative cases and external contexts improve knowledge prediction mainly due to entity type guidance and golden answer leakage .",3,0.91032565,511.81351930740374,22
1956,"Our findings shed light on the underlying predicting mechanisms of MLMs , and strongly question the previous conclusion that current MLMs can potentially serve as reliable factual knowledge bases .",3,0.98966765,66.66097972058209,30
1957,We study the problem of generating data poisoning attacks against Knowledge Graph Embedding ( KGE ) models for the task of link prediction in knowledge graphs .,1,0.6699941,38.77037527662374,27
1957,"To poison KGE models , we propose to exploit their inductive abilities which are captured through the relationship patterns like symmetry , inversion and composition in the knowledge graph .",2,0.4920302,112.50258596469693,30
1957,"Specifically , to degrade the model ’s prediction confidence on target facts , we propose to improve the model ’s prediction confidence on a set of decoy facts .",2,0.5873531,27.365869765887332,29
1957,"Thus , we craft adversarial additions that can improve the model ’s prediction confidence on decoy facts through different inference patterns .",2,0.4138287,131.06449188579256,22
1957,Our experiments demonstrate that the proposed poisoning attacks outperform state-of-art baselines on four KGE models for two publicly available datasets .,3,0.9483985,32.34213312986368,25
1957,We also find that the symmetry pattern based attacks generalize across all model-dataset combinations which indicates the sensitivity of KGE models to this pattern .,3,0.9743381,61.41972848413435,26
1958,"A common factor in bias measurement methods is the use of hand-curated seed lexicons , but there remains little guidance for their selection .",0,0.86970687,49.25442053826401,25
1958,"We gather seeds used in prior work , documenting their common sources and rationales , and in case studies of three English-language corpora , we enumerate the different types of social biases and linguistic features that , once encoded in the seeds , can affect subsequent bias measurements .",2,0.8216972,95.0302635023537,51
1958,"Seeds developed in one context are often re-used in other contexts , but documentation and evaluation remain necessary precursors to relying on seeds for sensitive measurements .",0,0.841416,79.54972922322561,27
1959,"Despite inextricable ties between race and language , little work has considered race in NLP research and development .",0,0.930641,31.14262356686024,19
1959,"In this work , we survey 79 papers from the ACL anthology that mention race .",1,0.72083265,169.47884314442342,16
1959,"These papers reveal various types of race-related bias in all stages of NLP model development , highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies .",3,0.87776375,42.936753894448216,31
1959,"However , persistent gaps in research on race and NLP remain : race has been siloed as a niche topic and remains ignored in many NLP tasks ;",0,0.8994984,72.96893606967328,28
1959,"most work operationalizes race as a fixed single-dimensional variable with a ground-truth label , which risks reinforcing differences produced by historical racism ;",0,0.82889664,259.9730233835291,26
1959,and the voices of historically marginalized people are nearly absent in NLP literature .,0,0.89448774,34.44369467926585,14
1959,"By identifying where and how NLP literature has and has not considered race , especially in comparison to related fields , our work calls for inclusion and racial justice in NLP research practices .",3,0.84224576,51.24667577670357,34
1960,Natural Language Processing ( NLP ) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations .,0,0.96305114,56.066333191316836,27
1960,"To guide efforts at debiasing these systems , the NLP community relies on a variety of metrics that quantify bias in models .",0,0.94293,39.77359382521794,23
1960,"Some of these metrics are intrinsic , measuring bias in word embedding spaces , and some are extrinsic , measuring bias in downstream tasks that the word embeddings enable .",0,0.6951406,35.43947355405972,30
1960,We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions .,2,0.81587636,37.4300625701611,18
1960,Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages .,3,0.9892679,58.86860026636778,19
1960,"We urge researchers working on debiasing to focus on extrinsic measures of bias , and to make using these measures more feasible via creation of new challenge sets and annotated test data .",3,0.7490206,49.40837873898989,33
1960,"To aid this effort , we release code , a new intrinsic metric , and an annotated test set focused on gender bias in hate speech .",2,0.530606,79.19777794584772,27
1961,"Text representation models are prone to exhibit a range of societal biases , reflecting the non-controlled and biased nature of the underlying pretraining data , which consequently leads to severe ethical issues and even bias amplification .",0,0.88577986,58.02303749375479,37
1961,Recent work has predominantly focused on measuring and mitigating bias in pretrained language models .,0,0.9260329,19.263652016493793,15
1961,"Surprisingly , the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce : it is limited to only a few types of bias , artificially constructed resources , and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks , e.g. , conversational response generation .",0,0.6345668,91.25689495148926,61
1961,"In this work , we present REDDITBIAS , the first conversational data set grounded in the actual human conversations from Reddit , allowing for bias measurement and mitigation across four important bias dimensions : gender , race , religion , and queerness .",1,0.7642011,77.87830077439442,43
1961,"Further , we develop an evaluation framework which simultaneously 1 ) measures bias on the developed REDDITBIAS resource , and 2 ) evaluates model capability in dialog tasks after model debiasing .",2,0.5460475,178.12627929456661,32
1961,We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods .,2,0.7232813,52.36482175930962,22
1961,Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance .,3,0.9902823,40.586013008069074,27
1962,This paper studies the relative importance of attention heads in Transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks .,1,0.91214293,15.442975586732267,24
1962,Prior research has found that only a few attention heads are important in each mono-lingual Natural Language Processing ( NLP ) task and pruning the remaining heads leads to comparable or improved performance of the model .,0,0.85898346,33.888503226862774,37
1962,"However , the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks .",0,0.8279719,18.985472615447293,18
1962,"Through extensive experiments , we show that ( 1 ) pruning a number of attention heads in a multi-lingual Transformer-based model has , in general , positive effects on its performance in cross-lingual and multi-lingual tasks and ( 2 ) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments .",3,0.84623796,23.452309518548795,61
1962,"Our experiments focus on sequence labeling tasks , with potential applicability on other cross-lingual and multi-lingual tasks .",2,0.4343444,22.569142241411434,18
1962,"For comprehensiveness , we examine two pre-trained multi-lingual models , namely multi-lingual BERT ( mBERT ) and XLM-R , on three tasks across 9 languages each .",2,0.82898486,35.939661714747125,29
1962,We also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings .,3,0.76957273,44.421065630521284,20
1963,Effective adversary generation for neural machine translation ( NMT ) is a crucial prerequisite for building robust machine translation systems .,0,0.94590133,22.91766338074835,21
1963,"In this work , we investigate veritable evaluations of NMT adversarial attacks , and propose a novel method to craft NMT adversarial examples .",1,0.9004134,39.07451099514513,24
1963,"We first show the current NMT adversarial attacks may be improperly estimated by the commonly used mono-directional translation , and we propose to leverage the round-trip translation technique to build valid metrics for evaluating NMT adversarial attacks .",3,0.43854675,42.697235602972064,38
1963,"Our intuition is that an effective NMT adversarial example , which imposes minor shifting on the source and degrades the translation dramatically , would naturally lead to a semantic-destroyed round-trip translation result .",3,0.7542634,99.23362903669262,36
1963,We then propose a promising black-box attack method called Word Saliency speedup Local Search ( WSLS ) that could effectively attack the mainstream NMT architectures .,1,0.4626864,109.47825201726499,28
1963,"Comprehensive experiments demonstrate that the proposed metrics could accurately evaluate the attack effectiveness , and the proposed WSLS could significantly break the state-of-art NMT models with small perturbation .",3,0.96454686,44.22097012742022,32
1963,"Besides , WSLS exhibits strong transferability on attacking Baidu and Bing online translators .",3,0.7521852,191.13149288271418,14
1964,Transfer learning has yielded state-of-the-art ( SoTA ) results in many supervised NLP tasks .,0,0.91999596,15.981117077764413,18
1964,"However , annotated data for every target task in every target language is rare , especially for low-resource languages .",0,0.9270693,33.692028512044175,20
1964,"We propose UXLA , a novel unsupervised data augmentation framework for zero-resource transfer learning scenarios .",1,0.5061896,48.959660040602536,16
1964,"In particular , UXLA aims to solve cross-lingual adaptation problems from a source language task distribution to an unknown target language task distribution , assuming no training label in the target language .",0,0.44247296,52.764716743644556,33
1964,"At its core , UXLA performs simultaneous self-training with data augmentation and unsupervised sample selection .",3,0.37132356,57.08480796406285,16
1964,"To show its effectiveness , we conduct extensive experiments on three diverse zero-resource cross-lingual transfer tasks .",2,0.68292385,22.09952818399412,17
1964,"UXLA achieves SoTA results in all the tasks , outperforming the baselines by a good margin .",3,0.9435737,50.75969152831559,17
1964,"With an in-depth framework dissection , we demonstrate the cumulative contributions of different components to its success .",3,0.6655269,58.562361864944805,19
1965,Recent work on non-autoregressive neural machine translation ( NAT ) aims at improving the efficiency by parallel decoding without sacrificing the quality .,0,0.9505085,24.01532914545861,23
1965,"However , existing NAT methods are either inferior to Transformer or require multiple decoding passes , leading to reduced speedup .",0,0.82759917,75.66660097618254,21
1965,We propose the Glancing Language Model ( GLM ) for single-pass parallel generation models .,1,0.34178272,69.30841993650392,17
1965,"With GLM , we develop Glancing Transformer ( GLAT ) for machine translation .",2,0.41898605,80.05949625833989,14
1965,"With only single-pass parallel decoding , GLAT is able to generate high-quality translation with 8×-15 × speedup .",3,0.8451528,65.27069044101567,22
1965,"Note that GLAT does not modify the network architecture , which is a training method to learn word interdependency .",3,0.61656404,94.03650444686895,20
1965,"Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods , and is nearly comparable to Transformer , reducing the gap to 0.25-0.9 BLEU points .",3,0.9212551,31.625356718115533,34
1966,Dense video event captioning aims to generate a sequence of descriptive captions for each event in a long untrimmed video .,0,0.9273626,30.452612570229345,21
1966,Video-level context provides important information and facilities the model to generate consistent and less redundant captions between events .,3,0.47450334,72.97673040915373,20
1966,"In this paper , we introduce a novel Hierarchical Context-aware Network for dense video event captioning ( HCN ) to capture context from various aspects .",1,0.87970304,37.888842200791004,28
1966,"In detail , the model leverages local and global context with different mechanisms to jointly learn to generate coherent captions .",2,0.52145535,38.39580715943066,21
1966,The local context module performs full interaction between neighbor frames and the global context module selectively attends to previous or future events .,2,0.36294642,113.02954128912351,23
1966,"According to our extensive experiment on both Youcook2 and Activitynet Captioning datasets , the video-level HCN model outperforms the event-level context-agnostic model by a large margin .",3,0.91089404,51.14623110692428,30
1966,The code is available at https://github.com/KirkGuo/HCN .,3,0.58248764,8.031943074008378,7
1967,Generating image captions with user intention is an emerging need .,0,0.9105017,50.21880033901123,11
1967,"The recently published Localized Narratives dataset takes mouse traces as another input to the image captioning task , which is an intuitive and efficient way for a user to control what to describe in the image .",0,0.90450954,42.49295886511652,37
1967,"However , how to effectively employ traces to improve generation quality and controllability is still under exploration .",0,0.74190515,51.412683413917605,18
1967,"This paper aims to solve this problem by proposing a novel model called LoopCAG , which connects Contrastive constraints and Attention Guidance in a Loop manner , engaged explicit spatial and temporal constraints to the generating process .",1,0.902109,133.59700493664926,38
1967,"Precisely , each generated sentence is temporally aligned to the corresponding trace sequence through a contrastive learning strategy .",2,0.60740805,40.76452872468463,19
1967,"Besides , each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance .",2,0.5190548,199.14240581967468,21
1967,"Comprehensive experimental results demonstrate that our LoopCAG model learns better correspondence among the three modalities ( vision , language , and traces ) and achieves SOTA performance on trace-controlled image captioning task .",3,0.9760849,91.38086537036675,34
1967,"Moreover , the controllability and explainability of LoopCAG are validated by analyzing spatial and temporal sensitivity during the generation process .",3,0.67141885,62.04447621704722,21
1968,"Understanding manipulated media , from automatically generated ‘ deepfakes ’ to manually edited ones , raises novel research challenges .",0,0.94517386,218.7880570315986,20
1968,"Because the vast majority of edited or manipulated images are benign , such as photoshopped images for visual enhancements , the key challenge is to understand the complex layers of underlying intents of media edits and their implications with respect to disinformation .",0,0.90201783,44.39963500763331,43
1968,"In this paper , we study Edited Media Frames , a new formalism to understand visual media manipulation as structured annotations with respect to the intents , emotional reactions , attacks on individuals , and the overall implications of disinformation .",1,0.8950602,108.73283245380243,41
1968,"We introduce a dataset for our task , EMU , with 56 k question-answer pairs written in rich natural language .",2,0.7749335,62.41101958023911,23
1968,"We evaluate a wide variety of vision-and-language models for our task , and introduce a new model PELICAN , which builds upon recent progress in pretrained multimodal representations .",2,0.52446383,36.42546110742598,32
1968,"Our model obtains promising results on our dataset , with humans rating its answers as accurate 48.2 % of the time .",3,0.9321576,39.04529734881056,22
1968,"At the same time , there is still much work to be done – and we provide analysis that highlights areas for further progress .",3,0.57429874,18.64927437124994,25
1969,"We propose PIGLeT : a model that learns physical commonsense knowledge through interaction , and then uses this knowledge to ground language .",1,0.50126845,89.0760083974296,23
1969,"We factorize PIGLeT into a physical dynamics model , and a separate language model .",2,0.8555313,247.60674302628715,15
1969,"Our dynamics model learns not just what objects are but also what they do : glass cups break when thrown , plastic ones do n’t .",3,0.4298231,128.94174263623427,26
1969,"We then use it as the interface to our language model , giving us a unified model of linguistic form and grounded meaning .",2,0.57915443,68.56719391546993,24
1969,"PIGLeT can read a sentence , simulate neurally what might happen next , and then communicate that result through a literal symbolic representation , or natural language .",0,0.679304,169.71555031407212,28
1969,"Experimental results show that our model effectively learns world dynamics , along with how to communicate them .",3,0.9726573,55.25411755284818,18
1969,"It is able to correctly forecast what happens next given an English sentence over 80 % of the time , outperforming a 100x larger , text-to-text approach by over 10 % .",3,0.9117584,52.64918208071584,36
1969,"Likewise , its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives .",3,0.6749456,95.49359174287702,21
1969,We present comprehensive analysis showing room for future work .,3,0.82594633,161.837179808379,10
1970,"Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space , but such spaces are not well-suited to modeling these types ’ complex interdependencies .",0,0.8978702,32.172038564811544,32
1970,"We study the ability of box embeddings , which embed concepts as d-dimensional hyperrectangles , to capture hierarchies of types even when these relationships are not defined explicitly in the ontology .",1,0.46126673,41.416359768406394,32
1970,Our model represents both types and entity mentions as boxes .,2,0.72128415,154.4415064887394,11
1970,Each mention and its context are fed into a BERT-based model to embed that mention in our box space ;,2,0.72884935,68.63032505544577,22
1970,"essentially , this model leverages typological clues present in the surface text to hypothesize a type representation for the mention .",0,0.47736353,76.50580199000338,21
1970,Box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves .,3,0.4242515,76.11855658800182,28
1970,We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks .,3,0.5459856,15.003606070877847,25
1970,"In addition to competitive typing performance , our box-based model shows better performance in prediction consistency ( predicting a supertype and a subtype together ) and confidence ( i.e. , calibration ) , demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does .",3,0.97090346,51.925167251336156,51
1971,"Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language : glyph and pinyin , which carry significant syntax and semantic information for language understanding .",0,0.9095917,59.888212881166154,30
1971,"In this work , we propose ChineseBERT , which incorporates both the glyph and pinyin information of Chinese characters into language model pretraining .",1,0.55762,40.02870778835804,24
1971,"The glyph embedding is obtained based on different fonts of a Chinese character , being able to capture character semantics from the visual features , and the pinyin embedding characterizes the pronunciation of Chinese characters , which handles the highly prevalent heteronym phenomenon in Chinese ( the same character has different pronunciations with different meanings ) .",2,0.40728295,60.03586255382536,57
1971,"Pretrained on large-scale unlabeled Chinese corpus , the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps .",3,0.7660286,25.37963393076523,23
1971,"The proposed model achieves new SOTA performances on a wide range of Chinese NLP tasks , including machine reading comprehension , natural language inference , text classification , sentence pair matching , and competitive performances in named entity recognition and word segmentation .",3,0.836689,24.875678337466788,43
1972,Knowledge distillation has been proven to be effective in model acceleration and compression .,0,0.85446286,25.899454745814378,14
1972,It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network .,2,0.47755685,16.021617342794965,27
1972,"But this way ignores the knowledge inside the large neural networks , e.g. , parameters .",0,0.577602,135.3011626209847,16
1972,Our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge .,3,0.98846775,46.88087259063874,22
1972,"In this paper , we propose Weight Distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator .",1,0.75809795,24.003302460829257,29
1972,"On the WMT16 En-Ro , NIST12 Zh-En , and WMT14 En-De machine translation tasks , our experiments show that weight distillation learns a small network that is 1.88 2.94 x faster than the large network but with competitive BLEU performance .",3,0.89310086,52.0676200123913,42
1972,"When fixing the size of small networks , weight distillation outperforms knowledge distillation by 0.51 1.82 BLEU points .",3,0.89772826,44.89001698725628,19
1973,It is a common belief that training deep transformers from scratch requires large datasets .,0,0.9178976,52.66081957422331,15
1973,"Consequently , for small datasets , people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning .",0,0.67220974,48.64600290000919,22
1973,"This work shows that this does not always need to be the case : with proper initialization and optimization , the benefits of very deep transformers can carry over to challenging tasks with small datasets , including Text-to-SQL semantic parsing and logical reading comprehension .",3,0.95864284,46.64400072705855,47
1973,"In particular , we successfully train 48 layers of transformers , comprising 24 fine-tuned layers from pre-trained RoBERTa and 24 relation-aware layers trained from scratch .",3,0.5781165,53.40800208656659,30
1973,"With fewer training steps and no task-specific pre-training , we obtain the state of the art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider .",3,0.7883442,19.46358797830061,29
1973,"We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme ( DT-Fixup ) , inspired by the prior T-Fixup work .",2,0.68962455,110.741390324333,27
1973,Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding .,3,0.96699756,58.81784219989938,24
1974,"Transformer-based language models ( TLMs ) , such as BERT , ALBERT and GPT-3 , have shown strong performance in a wide range of NLP tasks and currently dominate the field of NLP .",0,0.92464554,15.514950740332715,38
1974,"However , many researchers wonder whether these models can maintain their dominance forever .",0,0.95416385,132.0120969987035,14
1974,"Of course , we do not have answers now , but , as an attempt to find better neural architectures and training schemes , we pretrain a simple CNN using a GAN-style learning scheme and Wikipedia data , and then integrate it with standard TLMs .",2,0.57214946,94.34740041568944,48
1974,"We show that on the GLUE tasks , the combination of our pretrained CNN with ALBERT outperforms the original ALBERT and achieves a similar performance to that of SOTA .",3,0.9510627,21.23284729204441,30
1974,"Furthermore , on open-domain QA ( Quasar-T and Search QA ) , the combination of the CNN with ALBERT or RoBERTa achieved stronger performance than SOTA and the original TLMs .",3,0.9672034,66.4791871605128,32
1974,We hope that this work provides a hint for developing a novel strong network architecture along with its training scheme .,3,0.9221808,52.03455991493493,21
1974,Our source code and models are available at https://github.com/nict-wisdom/bertac .,3,0.5769334,13.938413106995972,10
1975,"We introduce a FEVER-like dataset COVID-Fact of 4,086 claims concerning the COVID-19 pandemic .",2,0.70230484,37.828033008298895,18
1975,"The dataset contains claims , evidence for the claims , and contradictory claims refuted by the evidence .",2,0.74024624,70.81162039221769,18
1975,"Unlike previous approaches , we automatically detect true claims and their source articles and then generate counter-claims using automatic methods rather than employing human annotators .",2,0.46003902,46.679812310985724,26
1975,"Along with our constructed resource , we formally present the task of identifying relevant evidence for the claims and verifying whether the evidence refutes or supports a given claim .",2,0.41766664,40.21496297007933,30
1975,"In addition to scientific claims , our data contains simplified general claims from media sources , making it better suited for detecting general misinformation regarding COVID-19 .",3,0.95574856,92.58063426489575,28
1975,Our experiments indicate that COVID-Fact will provide a challenging testbed for the development of new systems and our approach will reduce the costs of building domain-specific datasets for detecting misinformation .,3,0.98748744,39.51916355201026,33
1976,We address the task of explaining relationships between two scientific documents using natural language text .,1,0.3792156,51.4859878710544,16
1976,"This task requires modeling the complex content of long technical documents , deducing a relationship between these documents , and expressing the details of that relationship in text .",0,0.8860789,57.15704148465738,29
1976,"In addition to the theoretical interest of this task , successful solutions can help improve researcher efficiency in search and review .",0,0.48978436,121.05813046754606,22
1976,In this paper we establish a dataset of 622 K examples from 154 K documents .,2,0.49723512,58.50045759921469,16
1976,We pretrain a large language model to serve as the foundation for autoregressive approaches to the task .,2,0.7165051,23.460424139694428,18
1976,"We explore the impact of taking different views on the two documents , including the use of dense representations extracted with scientific IE systems .",2,0.4440612,141.3362543977719,25
1976,"We provide extensive automatic and human evaluations which show the promise of such models , but make clear challenges for future work .",3,0.9060227,42.78558587495,23
1977,Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution .,0,0.8835716,27.243301189050595,27
1977,"We present IrEne , an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models .",1,0.4285816,38.49297435796389,29
1977,IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning ( ML ) primitives .,2,0.5107886,48.65884370322284,28
1977,IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage .,3,0.38595012,83.24231320390157,23
1977,IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model .,2,0.5393764,65.26676899901135,21
1977,Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7 % compared to the ground truth .,3,0.89848757,100.57879096736067,27
1977,"In contrast , existing energy models see an error of over 50 % .",0,0.616558,244.06012013234817,14
1977,We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices .,3,0.7800544,63.92719671257227,25
1977,We release the code and data at https://github.com/StonyBrookNLP/irene .,3,0.56969154,12.370638287176257,9
1978,The element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session .,0,0.94164133,78.28043346677795,22
1978,"In contrast to a single text , a session may consist of an initial post and an associated sequence of comments .",0,0.7506866,43.10118625044978,22
1978,"Yet , emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets .",0,0.918911,33.06578807788781,25
1978,"For example , a session containing certain demographic-identity terms ( e.g. , “ gay ” or “ black ” ) is more likely to be classified as an instance of cyberbullying .",3,0.8163427,20.08320492993438,32
1978,"In this paper , we first show evidence of such bias in models trained on sessions collected from different social media platforms ( e.g. , Instagram ) .",1,0.6635528,43.48610175577819,28
1978,"We then propose a context-aware and model-agnostic debiasing strategy that leverages a reinforcement learning technique , without requiring any extra resources or annotations apart from a pre-defined set of sensitive triggers commonly used for identifying cyberbullying instances .",2,0.48509964,36.18900593264583,39
1978,Empirical evaluations show that the proposed strategy can simultaneously alleviate the impacts of the unintended biases and improve the detection performance .,3,0.9407547,23.81637100036314,22
1979,Creating effective visualization is an important part of data analytics .,0,0.94760853,27.075152064855125,11
1979,"While there are many libraries for creating visualization , writing such code remains difficult given the myriad of parameters that users need to provide .",0,0.9252087,49.80609277726952,25
1979,"In this paper , we propose the new task of synthesizing visualization programs from a combination of natural language utterances and code context .",1,0.91188115,30.578930086862325,24
1979,"To tackle the learning problem , we introduce PlotCoder , a new hierarchical encoder-decoder architecture that models both the code context and the input utterance .",2,0.5888855,27.978367639471582,26
1979,"We use PlotCoder to first determine the template of the visualization code , followed by predicting the data to be plotted .",2,0.85649174,101.97145456082569,22
1979,We use Jupyter notebooks containing visualization programs crawled from GitHub to train PlotCoder .,2,0.87321156,107.62328500849105,14
1979,"On a comprehensive set of test samples from those notebooks , we show that PlotCoder correctly predicts the plot type of about 70 % samples , and synthesizes the correct programs for 35 % samples , performing 3-4.5 % better than the baselines .",3,0.9057492,63.78682505537541,46
1980,NLP community is currently investing a lot more research and resources into development of deep learning models than training data .,0,0.89026225,38.810087524254186,21
1980,"While we have made a lot of progress , it is now clear that our models learn all kinds of spurious patterns , social biases , and annotation artifacts .",0,0.8820261,49.97903858331726,30
1980,Algorithmic solutions have so far had limited success .,0,0.9108889,30.302048131453986,9
1980,An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals .,0,0.61936426,98.93559376910495,20
1980,"This position paper maps out the arguments for and against data curation , and argues that fundamentally the point is moot : curation already is and will be happening , and it is changing the world .",1,0.63403034,59.17697620648646,37
1980,The question is only how much thought we want to invest into that process .,0,0.81034446,44.89605366745077,15
1981,"Heavily overparameterized language models such as BERT , XLNet and T5 have achieved impressive success in many NLP tasks .",0,0.82828516,14.069356011212673,20
1981,"However , their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning .",0,0.7927611,22.470082616892324,21
1981,"Many works have studied model compression on large NLP models , but only focusing on reducing inference time while still requiring an expensive training process .",0,0.8496356,60.2623587570444,26
1981,"Other works use extremely large batch sizes to shorten the pre-training time , at the expense of higher computational resource demands .",0,0.5300556,43.39337699406472,22
1981,"In this paper , inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks , we propose EarlyBERT , a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models .",1,0.71543115,33.844360561726155,41
1981,"By slimming the self-attention and fully-connected sub-layers inside a transformer , we are the first to identify structured winning tickets in the early stage of BERT training .",3,0.6244423,65.81176176203627,30
1981,"We apply those tickets towards efficient BERT training , and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks .",2,0.6910037,41.45755661570881,23
1981,"Our results show that EarlyBERT achieves comparable performance to standard BERT , with 35 45 % less training time .",3,0.9876159,69.00421404416278,20
1981,Code is available at https://github.com/VITA-Group/EarlyBERT .,3,0.5223549,14.06574379064877,6
1982,Adapter-based tuning has recently arisen as an alternative to fine-tuning .,0,0.93917143,12.846397124858813,13
1982,It works by adding light-weight adapter modules to a pretrained language model ( PrLM ) and only updating the parameters of adapter modules when learning on a downstream task .,2,0.5130174,40.890467557947055,32
1982,"As such , it adds only a few trainable parameters per new task , allowing a high degree of parameter sharing .",3,0.6097491,87.71119726835188,22
1982,Prior studies have shown that adapter-based tuning often achieves comparable results to fine-tuning .,0,0.84539884,22.87809453085769,15
1982,"However , existing work only focuses on the parameter-efficient aspect of adapter-based tuning while lacking further investigation on its effectiveness .",0,0.77104443,57.47302943472994,24
1982,"In this paper , we study the latter .",1,0.8914307,45.548070504477906,9
1982,We first show that adapter-based tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial PrLM .,3,0.8767577,64.55968477589737,28
1982,We then empirically compare the two tuning methods on several downstream NLP tasks and settings .,2,0.6875715,31.40089738222598,16
1982,We demonstrate that 1 ) adapter-based tuning outperforms fine-tuning on low-resource and cross-lingual tasks ;,3,0.97209305,31.966334249425312,15
1982,2 ) it is more robust to overfitting and less sensitive to changes in learning rates .,3,0.7920778,33.31609365297425,17
1983,Data augmentation is an effective way to improve the performance of many neural text generation models .,0,0.8752995,8.84795633793593,17
1983,"However , current data augmentation methods need to define or choose proper data mapping functions that map the original samples into the augmented samples .",0,0.9287981,63.48590235503095,25
1983,"In this work , we derive an objective to formulate the problem of data augmentation on text generation tasks without any use of augmented data constructed by specific mapping functions .",1,0.87043864,49.07061970264133,31
1983,Our proposed objective can be efficiently optimized and applied to popular loss functions on text generation tasks with a convergence rate guarantee .,3,0.8970573,99.32868964876538,23
1983,Experiments on five datasets of two text generation tasks show that our approach can approximate or even surpass popular data augmentation methods .,3,0.8486946,17.217076785960593,23
1984,"With the need of fast retrieval speed and small memory footprint , document hashing has been playing a crucial role in large-scale information retrieval .",0,0.95658624,29.851319066709472,25
1984,"To generate high-quality hashing code , both semantics and neighborhood information are crucial .",0,0.6926766,142.1361894858807,14
1984,"However , most existing methods leverage only one of them or simply combine them via some intuitive criteria , lacking a theoretical principle to guide the integration process .",0,0.9080241,115.90124308676806,29
1984,"In this paper , we encode the neighborhood information with a graph-induced Gaussian distribution , and propose to integrate the two types of information with a graph-driven generative model .",1,0.61937,27.311860861355207,31
1984,"To deal with the complicated correlations among documents , we further propose a tree-structured approximation method for learning .",2,0.6884829,46.08151580916359,19
1984,"Under the approximation , we prove that the training objective can be decomposed into terms involving only singleton or pairwise documents , enabling the model to be trained as efficiently as uncorrelated ones .",3,0.81514925,52.90088289896576,34
1984,"Extensive experimental results on three benchmark datasets show that our method achieves superior performance over state-of-the-art methods , demonstrating the effectiveness of the proposed model for simultaneously preserving semantic and neighborhood information .",3,0.92199,12.839428038135908,39
1985,The open-ended nature of visual captioning makes it a challenging area for evaluation .,0,0.86070037,16.990554561349942,14
1985,"The majority of proposed models rely on specialized training to improve human-correlation , resulting in limited adoption , generalizability , and explainabilty .",0,0.7987608,114.15524480692295,23
1985,"We introduce “ typicality ” , a new formulation of evaluation rooted in information theory , which is uniquely suited for problems lacking a definite ground truth .",1,0.31890512,115.67437733742986,28
1985,"Typicality serves as our framework to develop a novel semantic comparison , SPARCS , as well as referenceless fluency evaluation metrics .",2,0.36961082,162.52033726880634,22
1985,"Over the course of our analysis , two separate dimensions of fluency naturally emerge : style , captured by metric SPURTS , and grammar , captured in the form of grammatical outlier penalties .",3,0.6493399,92.93332365465413,34
1985,"Through extensive experiments and ablation studies on benchmark datasets , we show how these decomposed dimensions of semantics and fluency provide greater system-level insight into captioner differences .",3,0.73388237,76.51754971242677,28
1985,"Our proposed metrics along with their combination , SMURF , achieve state-of-the-art correlation with human judgment when compared with other rule-based evaluation metrics .",3,0.89682907,39.15010023489378,31
1986,The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains .,0,0.89801294,34.5592175400089,21
1986,"Recently , large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing , improving zero-shot generalization to unseen databases .",0,0.90462816,41.42346996800849,28
1986,"In this work , we examine the challenges that still prevent these techniques from practical deployment .",1,0.9143516,62.481143494274136,17
1986,"First , we present KaggleDBQA , a new cross-domain evaluation dataset of real Web databases , with domain-specific data types , original formatting , and unrestricted questions .",2,0.6431876,105.94731398582486,28
1986,"Second , we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings .",2,0.6071094,19.20062713277459,23
1986,"Finally , we augment our in-domain evaluation task with database documentation , a naturally occurring source of implicit domain knowledge .",2,0.5463492,80.9498496287666,22
1986,"We show that KaggleDBQA presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2 % , doubling their performance .",3,0.9616055,50.306040159384786,42
1987,"We introduce the largest transcribed Arabic speech corpus , QASR , collected from the broadcast domain .",2,0.6285845,88.5947152379221,17
1987,"This multi-dialect speech dataset contains 2,000 hours of speech sampled at 16 kHz crawled from Aljazeera news channel .",2,0.74998796,89.6081436802457,19
1987,"The dataset is released with lightly supervised transcriptions , aligned with the audio segments .",2,0.56373703,240.9506066696471,15
1987,"Unlike previous datasets , QASR contains linguistically motivated segmentation , punctuation , speaker information among others .",0,0.4680001,288.2534978586179,17
1987,"QASR is suitable for training and evaluating speech recognition systems , acoustics-and / or linguistics-based Arabic dialect identification , punctuation restoration , speaker identification , speaker linking , and potentially other NLP modules for spoken data .",3,0.7159601,105.90786551159047,41
1987,"In addition to QASR transcription , we release a dataset of 130M words to aid in designing and training a better language model .",2,0.4650307,65.19690725583283,24
1987,We show that end-to-end automatic speech recognition trained on QASR reports a competitive word error rate compared to the previous MGB-2 corpus .,3,0.96414375,38.13916812577981,26
1987,We report baseline results for downstream natural language processing tasks such as named entity recognition using speech transcript .,3,0.85556144,37.89131743431397,19
1987,We also report the first baseline for Arabic punctuation restoration .,3,0.7889144,158.90134816880314,11
1987,We make the corpus available for the research community .,3,0.3849602,22.449536113223623,10
1988,The performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration .,3,0.65786797,10.779890928001048,14
1988,"In this paper , we investigate the performance of modern hyperparameter optimization methods ( HPO ) on fine-tuning pre-trained language models .",1,0.9180563,20.35216246803856,22
1988,"First , we study and report three HPO algorithms ’ performances on fine-tuning two state-of-the-art language models on the GLUE dataset .",2,0.5423641,35.09088593446505,28
1988,"We find that using the same time budget , HPO often fails to outperform grid search due to two reasons : insufficient time budget and overfitting .",3,0.9719924,85.01641800758986,27
1988,We propose two general strategies and an experimental procedure to systematically troubleshoot HPO ’s failure cases .,3,0.40636945,86.01503726381624,17
1988,"By applying the procedure , we observe that HPO can succeed with more appropriate settings in the search space and time budget ;",3,0.84982824,161.6019053802834,23
1988,"however , in certain cases overfitting remains .",0,0.80484134,371.81076508639023,8
1988,"Finally , we make suggestions for future work .",3,0.74899745,26.935429120638105,9
1988,Our implementation can be found in https://github.com/microsoft/FLAML/tree/main/flaml/nlp/ .,3,0.7467494,12.87410566646742,8
1989,Evaluation in NLP is usually done by comparing the scores of competing systems independently averaged over a common set of test instances .,0,0.8753443,33.31488631214326,23
1989,"In this work , we question the use of averages for aggregating evaluation scores into a final number used to decide which system is best , since the average , as well as alternatives such as the median , ignores the pairing arising from the fact that systems are evaluated on the same test instances .",1,0.56955636,55.03169532824952,56
1989,"We illustrate the importance of taking the instancelevel pairing of evaluation scores into account and demonstrate , both theoretically and empirically , the advantages of aggregation methods based on pairwise comparisons , such as the Bradley –Terry ( BT ) model , a mechanism based on the estimated probability that a given system scores better than another on the test set .",3,0.6022694,73.50379629861847,63
1989,"By re-evaluating 296 real NLP evaluation setups across four tasks and 18 evaluation metrics , we show that the choice of aggregation mechanism matters and yields different conclusions as to which systems are state of the art in about 30 % of the setups .",3,0.85902673,50.54021257826544,45
1989,"To facilitate the adoption of pairwise evaluation , we release a practical tool for performing the full analysis of evaluation scores with the mean , median , BT , and two variants of BT ( Elo and TrueSkill ) , alongside functionality for appropriate statistical testing .",2,0.4151007,158.39759548645767,47
1990,The cross-database context-dependent Text-to-SQL ( XDTS ) problem has attracted considerable attention in recent years due to its wide range of potential applications .,0,0.9616431,23.398190748704337,25
1990,"However , we identify two biases in existing datasets for XDTS : ( 1 ) a high proportion of context-independent questions and ( 2 ) a high proportion of easy SQL queries .",3,0.92476344,53.709422791945926,33
1990,These biases conceal the major challenges in XDTS to some extent .,0,0.59266144,298.41094486921327,12
1990,"In this work , we present Chase , a large-scale and pragmatic Chinese dataset for XDTS .",1,0.8151682,108.28671793091443,17
1990,"It consists of 5,459 coherent question sequences ( 17,940 questions with their SQL queries annotated ) over 280 databases , in which only 35 % of questions are context-independent , and 28 % of SQL queries are easy .",3,0.42502865,82.69236510345026,40
1990,We experiment on Chase with three state-of-the-art XDTS approaches .,2,0.8081514,71.95854181940328,14
1990,"The best approach only achieves an exact match accuracy of 40 % over all questions and 16 % over all question sequences , indicating that Chase highlights the challenging problems of XDTS .",3,0.96037537,125.81380858269634,33
1990,We believe that XDTS can provide fertile soil for addressing the problems .,3,0.9653872,143.5571847267897,13
1991,"Despite pre-trained language models have proven useful for learning high-quality semantic representations , these models are still vulnerable to simple perturbations .",0,0.92045873,14.414045533335882,22
1991,"Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics , neglecting the utilization of different or even opposite semantics .",0,0.8579381,40.300108077469055,32
1991,"Different from the image processing field , the text is discrete and few word substitutions can cause significant semantic changes .",0,0.89627445,78.19876725010177,21
1991,"To study the impact of semantics caused by small perturbations , we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes .",3,0.62435585,35.24695426799356,38
1991,"To address this problem , we propose Contrastive Learning with semantIc Negative Examples ( CLINE ) , which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking .",2,0.39548123,91.72487274769476,32
1991,"By comparing with similar and opposite semantic examples , the model can effectively perceive the semantic changes caused by small perturbations .",3,0.6494865,82.49422654600998,22
1991,"Empirical results show that our approach yields substantial improvements on a range of sentiment analysis , reasoning , and reading comprehension tasks .",3,0.9746124,22.377000255832634,23
1991,And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level .,3,0.6138679,104.48729598961752,20
1992,"Topic modeling has been widely used for discovering the latent semantic structure of documents , but most existing methods learn topics with a flat structure .",0,0.9470223,45.58257335776143,26
1992,"Although probabilistic models can generate topic hierarchies by introducing nonparametric priors like Chinese restaurant process , such methods have data scalability issues .",0,0.804754,105.44900210714817,23
1992,"In this study , we develop a tree-structured topic model by leveraging nonparametric neural variational inference .",1,0.7412891,27.76233268172638,17
1992,"Particularly , the latent components of the stick-breaking process are first learned for each document , then the affiliations of latent components are modeled by the dependency matrices between network layers .",2,0.7231698,83.21092193912028,34
1992,"Utilizing this network structure , we can efficiently extract a tree-structured topic hierarchy with reasonable structure , low redundancy , and adaptable widths .",3,0.6368344,91.47878453274181,24
1992,Experiments on real-world datasets validate the effectiveness of our method .,3,0.8219338,7.196228825736598,11
1993,Prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs .,0,0.89964795,76.55908251577176,20
1993,"However , additional evidence information intermediate to the cause and effect remains unexploited .",0,0.8464983,131.23190162769276,14
1993,"By incorporating such information , the logical law behind the causality can be unveiled , and the interpretability and stability of the causal reasoning system can be improved .",3,0.55772835,48.16057815541296,29
1993,"To facilitate this , we present an Event graph knowledge enhanced explainable CAusal Reasoning framework ( ExCAR ) .",2,0.52354896,299.2980373838749,19
1993,ExCAR first acquires additional evidence information from a large-scale causal event graph as logical rules for causal reasoning .,0,0.41233674,145.64790681532222,19
1993,"To learn the conditional probabilistic of logical rules , we propose the Conditional Markov Neural Logic Network ( CMNLN ) that combines the representation learning and structure learning of logical rules in an end-to-end differentiable manner .",2,0.61141384,29.717320870626455,39
1993,Experimental results demonstrate that ExCAR outperforms previous state-of-the-art methods .,3,0.96963745,9.909604878392122,14
1993,Adversarial evaluation shows the improved stability of ExCAR over baseline systems .,3,0.94879156,137.12647603698665,12
1993,Human evaluation shows that ExCAR can achieve a promising explainable performance .,3,0.8917441,123.56215731400839,12
1994,"Emotion category is usually divided into different ones by human beings , but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories .",0,0.93238956,56.40904341561165,29
1994,"The existing studies working on emotion detection usually focus on how to improve the performance of model prediction , in which emotions are represented with one-hot vectors .",0,0.91058135,42.43507875962953,29
1994,"However , emotion relations are ignored in one-hot representations .",0,0.7109343,221.57341403610243,11
1994,"In this article , we first propose a general framework to learn the distributed representations for emotion categories in emotion space from a given emotion classification dataset .",1,0.82997787,40.06678571547408,28
1994,"Furthermore , based on the soft labels predicted by the pre-trained neural network model , we derive a simple and effective algorithm .",3,0.49357963,32.33396824827718,23
1994,Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space .,3,0.9267853,54.331546666127046,23
1995,Every natural text is written in some style .,0,0.92178345,91.60751182551587,9
1995,"Style is formed by a complex combination of different stylistic factors , including formality markers , emotions , metaphors , etc .",0,0.8677929,117.51396958991597,22
1995,One cannot form a complete understanding of a text without considering these factors .,0,0.81611925,35.884222295405436,14
1995,The factors combine and co-vary in complex ways to form styles .,0,0.7532536,80.59254925410913,12
1995,"Studying the nature of the covarying combinations sheds light on stylistic language in general , sometimes called cross-style language understanding .",0,0.8310782,88.17734671227788,21
1995,This paper provides the benchmark corpus ( XSLUE ) that combines existing datasets and collects a new one for sentence-level cross-style language understanding and evaluation .,1,0.6592001,96.62559808032374,28
1995,"The benchmark contains text in 15 different styles under the proposed four theoretical groupings : figurative , personal , affective , and interpersonal groups .",2,0.71987957,175.3029027141129,25
1995,"For valid evaluation , we collect an additional diagnostic set by annotating all 15 styles on the same text .",2,0.764882,102.26381002457356,20
1995,"Using XSLUE , we propose three interesting cross-style applications in classification , correlation , and generation .",2,0.4265401,285.157279494147,17
1995,"First , our proposed cross-style classifier trained with multiple styles together helps improve overall classification performance against individually-trained style classifiers .",3,0.71872216,60.54751107080293,23
1995,"Second , our study shows that some styles are highly dependent on each other in human-written text .",3,0.9856895,36.02800183737572,18
1995,"Finally , we find that combinations of some contradictive styles likely generate stylistically less appropriate text .",3,0.9839677,163.70785970669488,17
1995,We believe our benchmark and case studies help explore interesting future directions for cross-style research .,3,0.9679404,91.21221631743504,16
1995,The preprocessed datasets and code are publicly available .,3,0.41984826,19.891036121882433,9
1996,"We introduce DynaSent ( ‘ Dynamic Sentiment ’ ) , a new English-language benchmark task for ternary ( positive / negative / neutral ) sentiment analysis .",2,0.5815606,75.20373671685519,29
1996,"DynaSent combines naturally occurring sentences with sentences created using the open-source Dynabench Platform , which facilities human-and-model-in-the-loop dataset creation .",2,0.42719927,94.85325098095291,27
1996,"DynaSent has a total of 121,634 sentences , each validated by five crowdworkers , and its development and test splits are designed to produce chance performance for even the best models we have been able to develop ;",3,0.4504964,148.24390568675025,38
1996,"when future models solve this task , we will use them to create DynaSent version 2 , continuing the dynamic evolution of this benchmark .",3,0.68448496,93.28192987456735,25
1996,"Here , we report on the dataset creation effort , focusing on the steps we took to increase quality and reduce artifacts .",1,0.74041903,41.79410012248156,23
1996,"We also present evidence that DynaSent ’s Neutral category is more coherent than the comparable category in other benchmarks , and we motivate training models from scratch for each round over successive fine-tuning .",3,0.7935369,107.85637035831195,34
1997,"In this digital age , online users expect personalized content .",0,0.9650896,192.48841915198332,11
1997,"To cater to diverse group of audiences across online platforms it is necessary to generate multiple variants of same content with differing degree of characteristics ( sentiment , style , formality , etc. ) .",0,0.89767736,69.15758420245463,35
1997,"Though text-style transfer is a well explored related area , it focuses on flipping the style attribute polarity instead of regulating a fine-grained attribute transfer .",0,0.7046182,80.09451070222448,28
1997,"In this paper we propose a hierarchical architecture for finer control over the at-tribute , preserving content using attribute dis-entanglement .",1,0.8359826,79.11934229861014,21
1997,"We demonstrate the effective-ness of the generative process for two different attributes with varied complexity , namely sentiment and formality .",3,0.77740794,97.23959664502692,21
1997,"With extensive experiments and human evaluation on five real-world datasets , we show that the framework can generate natural looking sentences with finer degree of control of intensity of a given attribute .",3,0.82038265,37.2077675203398,33
1998,"Aspect-based Sentiment Analysis ( ABSA ) aims to identify the aspect terms , their corresponding sentiment polarities , and the opinion terms .",0,0.8852971,51.4062975285595,24
1998,There exist seven subtasks in ABSA .,3,0.49286646,141.38903404805032,7
1998,"Most studies only focus on the subsets of these subtasks , which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework .",0,0.815063,59.141235075387144,29
1998,"In this paper , we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes , which converts all ABSA subtasks into a unified generative formulation .",2,0.52575815,157.67581997589377,32
1998,"Based on the unified formulation , we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an end-to-end framework .",2,0.67476934,25.096957173496925,27
1998,"Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks , which could benefit multiple tasks .",3,0.9209678,31.080101254337688,38
1999,"Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data , which is costly to obtain .",0,0.87064284,26.603906689869973,22
1999,"We use weak supervision from existing linguistic annotation models to identify potential slot candidates , then automatically identify domain-relevant slots by using clustering algorithms .",2,0.8600818,102.79774179167202,25
1999,"Furthermore , we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention .",2,0.69322616,21.180562473369836,29
1999,This tagger is trained solely on the outputs of our method and thus does not rely on any labeled data .,2,0.5498608,22.775115755365377,21
1999,Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains .,3,0.848041,26.90809851692047,23
1999,"Moreover , we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model , compared to using no slot annotation at all .",3,0.97350967,17.252073492451693,34
2000,Intent classification is a major task in spoken language understanding ( SLU ) .,0,0.9531505,41.456459478283165,14
2000,"Since most models are built with pre-collected in-domain ( IND ) training utterances , their ability to detect unsupported out-of-domain ( OOD ) utterances has a critical effect in practical use .",0,0.88166326,43.9986392219009,36
2000,"Recent works have shown that using extra data and labels can improve the OOD detection performance , yet it could be costly to collect such data .",0,0.8614546,39.98915008944225,27
2000,This paper proposes to train a model with only IND data while supporting both IND intent classification and OOD detection .,1,0.8455872,110.42506212220307,21
2000,"Our method designs a novel domain-regularized module ( DRM ) to reduce the overconfident phenomenon of a vanilla classifier , achieving a better generalization in both cases .",2,0.67409784,84.79034940859388,28
2000,"Besides , DRM can be used as a drop-in replacement for the last layer in any neural network-based intent classifier , providing a low-cost strategy for a significant improvement .",3,0.85081714,36.808072486296055,33
2000,The evaluation on four datasets shows that our method built on BERT and RoBERTa models achieves state-of-the-art performance against existing approaches and the strong baselines we created for the comparisons .,3,0.92005813,14.10652476624581,37
2001,Recent research considers few-shot intent detection as a meta-learning problem : the model is learning to learn from a consecutive set of small tasks named episodes .,0,0.89920783,64.53176931077336,27
2001,"In this work , we propose ProtAugment , a meta-learning algorithm for short texts classification ( the intent detection task ) .",1,0.778824,93.65024019600833,22
2001,"ProtAugment is a novel extension of Prototypical Networks , that limits overfitting on the bias introduced by the few-shots classification objective at each episode .",2,0.34282336,70.97635153147418,25
2001,"It relies on diverse paraphrasing : a conditional language model is first fine-tuned for paraphrasing , and diversity is later introduced at the decoding stage at each meta-learning episode .",2,0.35695353,59.87482118072171,30
2001,"The diverse paraphrasing is unsupervised as it is applied to unlabelled data , and then fueled to the Prototypical Network training objective as a consistency loss .",2,0.5245027,96.14898304883515,27
2001,"ProtAugment is the state-of-the-art method for intent detection meta-learning , at no extra labeling efforts and without the need to fine-tune a conditional language model on a given application domain .",3,0.540483,28.341616282900212,37
2002,"Most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data , and evaluated in a small set from the same distribution .",0,0.8314027,26.52885613464024,32
2002,"However , these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice .",0,0.79015195,63.48263301598995,24
2002,"In this paper , we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models , and introduce three important aspects related to language understanding in real-world dialog systems , namely , language variety , speech characteristics , and noise perturbation .",1,0.83408415,37.43379299737874,47
2002,We propose a model-agnostic toolkit LAUG to approximate natural language perturbations for testing the robustness issues in task-oriented dialog .,2,0.32940564,58.77242436124098,23
2002,"Four data augmentation approaches covering the three aspects are assembled in LAUG , which reveals critical robustness issues in state-of-the-art models .",2,0.44924614,44.58089239253916,28
2002,The augmented dataset through LAUG can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog .,3,0.9491862,47.57670788407537,25
2003,Dialogue state tracking ( DST ) plays a key role in task-oriented dialogue systems to monitor the user ’s goal .,0,0.92956525,36.167830068031556,23
2003,"In general , there are two strategies to track a dialogue state : predicting it from scratch and updating it from previous state .",0,0.82658446,62.52728059633108,24
2003,"The scratch-based strategy obtains each slot value by inquiring all the dialogue history , and the previous-based strategy relies on the current turn dialogue to update the previous dialogue state .",2,0.6658494,79.42200013809878,35
2003,"However , it is hard for the scratch-based strategy to correctly track short-dependency dialogue state because of noise ;",3,0.6129512,140.17753376313172,23
2003,"meanwhile , the previous-based strategy is not very useful for long-dependency dialogue state tracking .",3,0.7789711,129.61904233764747,18
2003,"Obviously , it plays different roles for the context information of different granularity to track different kinds of dialogue states .",3,0.5108542,76.90539475535716,21
2003,"Thus , in this paper , we will study and discuss how the context information of different granularity affects dialogue state tracking .",1,0.8770666,63.273926158642105,23
2003,"First , we explore how greatly different granularities affect dialogue state tracking .",2,0.45295134,78.99576965545896,13
2003,"Then , we further discuss how to combine multiple granularities for dialogue state tracking .",1,0.43648455,44.71685176829477,15
2003,"Finally , we apply the findings about context granularity to few-shot learning scenario .",3,0.8798315,58.30573919743268,14
2003,"Besides , we have publicly released all codes .",2,0.54603875,64.38349989957672,9
2004,Mixed initiative in open-domain dialogue requires a system to pro-actively introduce new topics .,0,0.80026865,91.68486136462607,14
2004,The one-turn topic transition task explores how a system connects two topics in a cooperative and coherent manner .,0,0.54428905,63.09700773807935,19
2004,The goal of the task is to generate a “ bridging ” utterance connecting the new topic to the topic of the previous conversation turn .,2,0.39900836,38.32655352910804,26
2004,We are especially interested in commonsense explanations of how a new topic relates to what has been mentioned before .,1,0.6328355,23.656075996607292,20
2004,"We first collect a new dataset of human one-turn topic transitions , which we callOTTers .",2,0.85513496,145.2122726121521,16
2004,"We then explore different strategies used by humans when asked to complete such a task , and notice that the use of a bridging utterance to connect the two topics is the approach used the most .",3,0.6923527,44.973274817884544,37
2004,We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data .,3,0.7683777,21.887684970897187,36
2005,"Recently , there has been significant progress in studying neural networks to translate text descriptions into SQL queries .",0,0.94766927,29.89946951591202,19
2005,"Despite achieving good performance on some public benchmarks , existing text-to-SQL models typically rely on the lexical matching between words in natural language ( NL ) questions and tokens in table schemas , which may render the models vulnerable to attacks that break the schema linking mechanism .",0,0.8554148,37.514274995939125,52
2005,"In this work , we investigate the robustness of text-to-SQL models to synonym substitution .",1,0.92910784,15.26815127740292,19
2005,"In particular , we introduce Spider-Syn , a human-curated dataset based on the Spider benchmark for text-to-SQL translation .",2,0.60984266,37.47779187493076,24
2005,"NL questions in Spider-Syn are modified from Spider , by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases .",2,0.5935373,89.2470908776986,26
2005,"We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas , even if the synonyms are not adversarially selected to conduct worst-case attacks .",3,0.96527463,93.19909988999679,33
2005,"Finally , we present two categories of approaches to improve the model robustness .",3,0.4849135,28.341048686481678,14
2005,"The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input , while the second category is based on adversarial training .",2,0.43534586,64.35645848777843,28
2005,"We demonstrate that both categories of approaches significantly outperform their counterparts without the defense , and the first category of approaches are more effective .",3,0.9552548,37.394044708573716,25
2006,"In order to better understand the reason behind model behaviors ( i.e. , making predictions ) , most recent works have exploited generative models to provide complementary explanations .",0,0.9295876,54.75170834519694,29
2006,"However , existing approaches in NLP mainly focus on “ WHY A ” rather than contrastive “ WHY A NOT B ” , which is shown to be able to better distinguish confusing candidates and improve data efficiency in other research fields .",0,0.81934875,52.65215711750184,43
2006,"In this paper , we focus on generating contrastive explanations with counterfactual examples in NLI and propose a novel Knowledge-Aware Contrastive Explanation generation framework ( KACE ) .",1,0.8984988,25.30282433391304,30
2006,"Specifically , we first identify rationales ( i.e. , key phrases ) from input sentences , and use them as key perturbations for generating counterfactual examples .",2,0.8590822,41.69551829744838,27
2006,"After obtaining qualified counterfactual examples , we take them along with original examples and external knowledge as input , and employ a knowledge-aware generative pre-trained language model to generate contrastive explanations .",2,0.83199185,41.37455279926115,34
2006,Experimental results show that contrastive explanations are beneficial to fit the scenarios by clarifying the difference between the predicted answer and other possible wrong ones .,3,0.9403123,53.029776000394165,26
2006,"Moreover , we train an NLI model enhanced with contrastive explanations and achieves an accuracy of 91.9 % on SNLI , gaining improvements of 5.7 % against ETPA ( “ Explain-Then-Predict-Attention ” ) and 0.6 % against NILE ( “ WHY A ” ) .",3,0.78731817,58.413099358296805,47
2007,"Although BERT and its variants have reshaped the NLP landscape , it still remains unclear how best to derive sentence embeddings from such pre-trained Transformers .",0,0.87420154,17.549819064378717,26
2007,"In this work , we propose a contrastive learning method that utilizes self-guidance for improving the quality of BERT sentence representations .",1,0.7987294,17.300867507740872,22
2007,"Our method fine-tunes BERT in a self-supervised fashion , does not rely on data augmentation , and enables the usual [ CLS ] token embeddings to function as sentence vectors .",3,0.5785588,56.395407814449904,32
2007,"Moreover , we redesign the contrastive learning objective ( NT-Xent ) and apply it to sentence representation learning .",2,0.69976705,75.25245028048883,21
2007,We demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks .,3,0.8452541,20.159789553793445,21
2007,We also show it is efficient at inference and robust to domain shifts .,3,0.91882885,60.9677409119113,14
2008,This work aims to tackle the challenging heterogeneous graph encoding problem in the text-to-SQL task .,1,0.88448447,19.133638494394383,20
2008,"Previous methods are typically node-centric and merely utilize different weight matrices to parameterize edge types , which 1 ) ignore the rich semantics embedded in the topological structure of edges , and 2 ) fail to distinguish local and non-local relations for each node .",0,0.7692613,75.16523300564295,45
2008,"To this end , we propose a Line Graph Enhanced Text-to-SQL ( LGESQL ) model to mine the underlying relational features without constructing meta-paths .",2,0.5587779,60.321582623333136,26
2008,"By virtue of the line graph , messages propagate more efficiently through not only connections between nodes , but also the topology of directed edges .",0,0.63276154,70.6879774959775,26
2008,"Furthermore , both local and non-local relations are integrated distinctively during the graph iteration .",3,0.76898146,88.0205696998067,15
2008,We also design an auxiliary task called graph pruning to improve the discriminative capability of the encoder .,2,0.6163358,19.36668617151392,18
2008,"Our framework achieves state-of-the-art results ( 62.8 % with Glove , 72.0 % with Electra ) on the cross-domain text-to-SQL benchmark Spider at the time of writing .",3,0.8549153,19.756486820000934,36
2009,"Multimodal pre-training models , such as LXMERT , have achieved excellent results in downstream tasks .",0,0.7556683,41.0908808043203,16
2009,"However , current pre-trained models require large amounts of training data and have huge model sizes , which make them impossible to apply in low-resource situations .",0,0.9058076,19.81133433817603,27
2009,How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem .,0,0.82450604,28.468121675806902,29
2009,"In this paper , we propose a new Multi-stage Pre-training ( MSP ) method , which uses information at different granularities from word , phrase to sentence in both texts and images to pre-train a model in stages .",1,0.8419389,33.95197976499764,39
2009,We also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus .,2,0.7880452,56.280644571408324,28
2009,We take a Simplified LXMERT ( LXMERT-S ) which is with 45.9 % parameters of the original LXMERT model and only 11.44 % of the original pre-training data as the testbed of our MSP method .,2,0.75646734,43.97443474726131,37
2009,"Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks , and even outperforms the original model in Image-Text Retrieval task .",3,0.96179336,20.85420186340311,32
2010,"Document-level contextual information has shown benefits to text-based machine translation , but whether and how context helps end-to-end ( E2E ) speech translation ( ST ) is still under-studied .",0,0.9547618,19.71070858848641,34
2010,"We fill this gap through extensive experiments using a simple concatenation-based context-aware ST model , paired with adaptive feature selection on speech encodings for computational efficiency .",2,0.6018925,60.28529391521886,31
2010,"We investigate several decoding approaches , and introduce in-model ensemble decoding which jointly performs document-and sentence-level translation using the same model .",2,0.50332433,115.32229018498222,26
2010,Our results on the MuST-C benchmark with Transformer demonstrate the effectiveness of context to E2E ST .,3,0.989804,88.12060256037584,19
2010,"Compared to sentence-level ST , context-aware ST obtains better translation quality ( + 0.18-2.61 BLEU ) , improves pronoun and homophone translation , shows better robustness to ( artificial ) audio segmentation errors , and reduces latency and flicker to deliver higher quality for simultaneous translation .",3,0.9351492,68.78188026080085,53
2011,Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned / digital-born documents .,0,0.6894312,45.46128809209553,37
2011,"We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text , layout , and image in a single multi-modal framework .",2,0.4843786,53.75372186196614,25
2011,"Specifically , with a two-stream multi-modal Transformer encoder , LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks , which make it better capture the cross-modality interaction in the pre-training stage .",2,0.45259118,33.34013837521763,45
2011,"Meanwhile , it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks .",3,0.57683605,25.079736355096905,31
2011,"Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks , including FUNSD ( 0.7895 to 0.8420 ) , CORD ( 0.9493 to 0.9601 ) , SROIE ( 0.9524 to 0.9781 ) , Kleister-NDA ( 0.8340 to 0.8520 ) , RVL-CDIP ( 0.9443 to 0.9564 ) , and DocVQA ( 0.7295 to 0.8672 ) .",3,0.5561552,17.869626303266024,82
2012,"Existed pre-training methods either focus on single-modal tasks or multi-modal tasks , and cannot effectively adapt to each other .",0,0.76349646,16.593858858043816,20
2012,"They can only utilize single-modal data ( i.e. , text or image ) or limited multi-modal data ( i.e. , image-text pairs ) .",0,0.7726271,23.28171670257055,24
2012,"In this work , we propose a UNIfied-MOdal pre-training architecture , namely UNIMO , which can effectively adapt to both single-modal and multi-modal understanding and generation tasks .",1,0.7824768,55.78430074981399,30
2012,"Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding , and cross-modal contrastive learning ( CMCL ) is leveraged to align the textual and visual information into a unified semantic space , over a corpus of image-text pairs augmented with related images and texts .",2,0.44510654,34.4857987661999,58
2012,"With the help of rich non-paired single-modal data , our model is able to learn more generalizable representations , by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space .",3,0.858345,40.84839283374984,36
2012,The experimental results show that UNIMO greatly improves the performance of several single-modal and multi-modal downstream tasks .,3,0.98242813,14.047659752796813,18
2012,Our code and pre-trained models are public at https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO .,3,0.57601994,13.672975523151527,10
2013,Multimodal fusion has been proved to improve emotion recognition performance in previous works .,0,0.8636158,25.817630849965617,14
2013,"However , in real-world applications , we often encounter the problem of missing modality , and which modalities will be missing is uncertain .",0,0.9286842,34.99440456075274,24
2013,It makes the fixed multimodal fusion fail in such cases .,0,0.5303868,98.47659979951874,11
2013,"In this work , we propose a unified model , Missing Modality Imagination Network ( MMIN ) , to deal with the uncertain missing modality problem .",1,0.807567,77.25972031030135,27
2013,"MMIN learns robust joint multimodal representations , which can predict the representation of any missing modality given available modalities under different missing modality conditions .",0,0.4076131,71.44299918804414,25
2013,Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition .,3,0.931806,50.06733011812513,30
2013,The code will be available at https://github.com/AIM3-RUC/MMIN .,3,0.6304298,17.99380483895928,8
2014,"Encoder pre-training is promising in end-to-end Speech Translation ( ST ) , given the fact that speech-to-translation data is scarce .",0,0.83480775,34.32839429175687,24
2014,But ST encoders are not simple instances of Automatic Speech Recognition ( ASR ) or Machine Translation ( MT ) encoders .,0,0.88900393,31.897169055309668,22
2014,"For example , we find that ASR encoders lack the global context representation , which is necessary for translation , whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences .",3,0.8776208,64.24837573223809,36
2014,"In this work , we propose a Stacked Acoustic-and-Textual Encoding ( SATE ) method for speech translation .",1,0.80837697,40.562515692339886,22
2014,"Our encoder begins with processing the acoustic sequence as usual , but later behaves more like an MT encoder for a global representation of the input sequence .",0,0.3932631,78.11342415557378,28
2014,"In this way , it is straightforward to incorporate the pre-trained models into the system .",3,0.5665291,23.142051919803734,16
2014,"Also , we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder , and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge .",2,0.7301378,23.129475449452034,34
2014,Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2 .,3,0.94462574,13.149096816645075,30
2014,"To our knowledge , we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available .",3,0.9357297,18.55103441002301,37
2015,"In this paper , we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step , where a constituent tree may consist of nodes with more than two children .",1,0.8434598,32.10316306528786,41
2015,"Previous graph-based methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes , in order to transform the tree into a binary tree for prediction .",0,0.596535,65.95275271435597,34
2015,The limitation is that the hidden nodes break the sibling relations of the n-ary node ’s children .,3,0.465707,90.19896701611637,18
2015,"Consequently , the dependencies of such sibling constituents might not be accurately modeled and is being ignored .",0,0.6988866,166.20098408573907,18
2015,"To solve this limitation , we propose a novel graph-based framework , which is called “ recursive semi-Markov model ” .",2,0.58746237,39.215016735135755,23
2015,"The main idea is to utilize 1-order semi-Markov model to predict the immediate children sequence of a constituent candidate , which then recursively serves as a child candidate of its parent .",2,0.6540902,65.19333219843749,34
2015,"In this manner , the dependencies of sibling constituents can be described by 1-order transition features , which solves the above limitation .",3,0.53943914,163.20240959933267,25
2015,"Through experiments , the proposed framework obtains the F1 of 95.92 % and 92.50 % on the datasets of PTB and CTB 5.1 respectively .",3,0.8366829,35.42697905171583,25
2015,"Specially , the recursive semi-Markov model shows advantages in modeling nodes with more than two children , whose average F1 can be improved by 0.3-1.1 points in PTB and 2.3-6.8 points in CTB 5.1 .",3,0.931362,37.25624372854787,37
2016,Pretrained contextualized embeddings are powerful word representations for structured prediction tasks .,0,0.7493489,31.578552455475375,12
2016,Recent work found that better word representations can be obtained by concatenating different types of embeddings .,0,0.86341816,16.95253524533861,17
2016,"However , the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings , and the ever-increasing number of embedding types makes it a more difficult problem .",0,0.8452906,19.80733403061674,39
2016,"In this paper , we propose Automated Concatenation of Embeddings ( ACE ) to automate the process of finding better concatenations of embeddings for structured prediction tasks , based on a formulation inspired by recent progress on neural architecture search .",1,0.81684774,31.277343385322514,41
2016,"Specifically , a controller alternately samples a concatenation of embeddings , according to its current belief of the effectiveness of individual embedding types in consideration for a task , and updates the belief based on a reward .",2,0.63370967,66.32745539423678,38
2016,"We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model , which is fed with the sampled concatenation as input and trained on a task dataset .",2,0.79837525,44.0550281050407,42
2016,Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations .,3,0.9317854,7.76171007640456,33
2017,"In structured prediction problems , cross-lingual transfer learning is an efficient way to train quality models for low-resource languages , and further improvement can be obtained by learning from multiple source languages .",0,0.67317456,24.719578631099953,33
2017,"However , not all source models are created equal and some may hurt performance on the target language .",0,0.7474882,34.33077607173369,19
2017,Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models .,0,0.91933435,41.14419051377205,22
2017,"In this paper , we propose a multi-view framework , by leveraging a small number of labeled target sentences , to effectively combine multiple source models into an aggregated source view at different granularity levels ( language , sentence , or sub-structure ) , and transfer it to a target view based on a task-specific model .",1,0.6447337,43.21324894136225,57
2017,"By encouraging the two views to interact with each other , our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training .",3,0.7601185,23.89082129621232,33
2017,"Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches , including these with access to additional source language data .",3,0.91789585,43.37659937137562,32
2018,Incorporating syntax into neural approaches in NLP has a multitude of practical and scientific benefits .,0,0.73919255,31.007766909826803,16
2018,"For instance , a language model that is syntax-aware is likely to be able to produce better samples ;",3,0.55011195,50.72409977080817,21
2018,even a discriminative model like BERT with a syntax module could be used for core NLP tasks like unsupervised syntactic parsing .,3,0.6236698,29.7081824177821,22
2018,"Rapid progress in recent years was arguably spurred on by the empirical success of the Parsing-Reading-Predict architecture of ( Shen et al. , 2018a ) , later simplified by the Order Neuron LSTM of ( Shen et al. , 2019 ) .",0,0.85037696,52.923590367112496,46
2018,"Most notably , this is the first time neural approaches were able to successfully perform unsupervised syntactic parsing ( evaluated by various metrics like F-1 score ) .",3,0.89474994,51.423348752870666,30
2018,"However , even heuristic ( much less fully mathematical ) understanding of why and when these architectures work is lagging severely behind .",0,0.8725392,139.4694675839278,23
2018,"Concretely , we ground this question in the sandbox of probabilistic context-free-grammars ( PCFGs ) , and identify a key aspect of the representational power of these approaches : the amount and directionality of context that the predictor has access to when forced to make parsing decision .",3,0.32768634,57.63393137977104,48
2018,"We show that with limited context ( either bounded , or unidirectional ) , there are PCFGs , for which these approaches cannot represent the max-likelihood parse ;",3,0.86638623,194.8000426058584,28
2018,"conversely , if the context is unlimited , they can represent the max-likelihood parse of any PCFG .",3,0.55143905,173.0036483675997,18
2019,Neural lexicalized PCFGs ( L-PCFGs ) have been shown effective in grammar induction .,0,0.92730176,55.29349431837369,14
2019,"However , to reduce computational complexity , they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored .",0,0.5537037,77.96290369962996,27
2019,"In this paper , we propose an approach to parameterize L-PCFGs without making implausible independence assumptions .",1,0.9059825,63.91028096214136,17
2019,Our approach directly models bilexical dependencies and meanwhile reduces both learning and representation complexities of L-PCFGs .,3,0.664881,174.01094081877437,17
2019,Experimental results on the English WSJ dataset confirm the effectiveness of our approach in improving both running speed and unsupervised parsing performance .,3,0.958132,15.931748585714823,23
2020,"On social media platforms , hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds .",0,0.8849972,19.849824799904116,27
2020,Automatic methods to detect offensive language have largely relied on datasets with categorical labels .,0,0.92694145,45.59773636584238,15
2020,"However , comments can vary in their degree of offensiveness .",0,0.7603728,87.89076378246976,11
2020,"We create the first dataset of English language Reddit comments that has fine-grained , real-valued scores between-1 ( maximally supportive ) and 1 ( maximally offensive ) .",2,0.7533742,56.87367154406013,32
2020,"The dataset was annotated using Best–Worst Scaling , a form of comparative annotation that has been shown to alleviate known biases of using rating scales .",2,0.8616239,61.565722735860334,26
2020,We show that the method produces highly reliable offensiveness scores .,3,0.9498241,66.41945998456121,11
2020,"Finally , we evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset .",3,0.46017146,26.440447838975157,21
2021,Automatic dialogue coherence evaluation has attracted increasing attention and is crucial for developing promising dialogue systems .,0,0.95380694,28.38307477266834,17
2021,"However , existing metrics have two major limitations : ( a ) they are mostly trained in a simplified two-level setting ( coherent vs .",0,0.88375956,92.42052330624466,27
2021,"incoherent ) , while humans give Likert-type multi-level coherence scores , dubbed as “ quantifiable ” ;",2,0.4841398,205.8888193789982,19
2021,( b ) their predicted coherence scores cannot align with the actual human rating standards due to the absence of human guidance during training .,3,0.9326736,105.10473134917852,25
2021,"To address these limitations , we propose Quantifiable Dialogue Coherence Evaluation ( QuantiDCE ) , a novel framework aiming to train a quantifiable dialogue coherence metric that can reflect the actual human rating standards .",1,0.4890477,48.217127586883095,35
2021,"Specifically , QuantiDCE includes two training stages , Multi-Level Ranking ( MLR ) pre-training and Knowledge Distillation ( KD ) fine-tuning .",2,0.5625476,45.762844199410324,22
2021,"During MLR pre-training , a new MLR loss is proposed for enabling the model to learn the coarse judgement of coherence degrees .",2,0.57605237,86.55975459520113,23
2021,"Then , during KD fine-tuning , the pretrained model is further finetuned to learn the actual human rating standards with only very few human-annotated data .",2,0.58685076,47.05735774386409,27
2021,"To advocate the generalizability even with limited fine-tuning data , a novel KD regularization is introduced to retain the knowledge learned at the pre-training stage .",2,0.57639396,39.55160774054728,26
2021,Experimental results show that the model trained by QuantiDCE presents stronger correlations with human judgements than the other state-of-the-art metrics .,3,0.9797212,15.119456693463464,26
2022,"Accurate assessment of the ability of embedding models to capture idiomaticity may require evaluation at token rather than type level , to account for degrees of idiomaticity and possible ambiguity between literal and idiomatic usages .",0,0.49681312,50.18337248027582,36
2022,"However , most existing resources with annotation of idiomaticity include ratings only at type level .",0,0.8397543,257.2652050514276,16
2022,"This paper presents the Noun Compound Type and Token Idiomaticity ( NCTTI ) dataset , with human annotations for 280 noun compounds in English and 180 in Portuguese at both type and token level .",1,0.6932264,78.61466187662384,35
2022,"We compiled 8,725 and 5,091 token level annotations for English and Portuguese , respectively , which are strongly correlated with the corresponding scores obtained at type level .",3,0.7292501,65.2192324824777,28
2022,The NCTTI dataset is used to explore how vector space models reflect the variability of idiomaticity across sentences .,2,0.71598804,94.23054812612456,19
2022,Several experiments using state-of-the-art contextualised models suggest that their representations are not capturing the noun compounds idiomaticity as human annotators .,3,0.78778744,44.40629391089653,26
2022,"This new multilingual resource also contains suggestions for paraphrases of the noun compounds both at type and token levels , with uses for lexical substitution or disambiguation in context .",3,0.6776382,73.27108732991309,30
2023,"Statutory reasoning is the task of determining whether a legal statute , stated in natural language , applies to the text description of a case .",0,0.9488662,39.29873804746125,26
2023,"Prior work introduced a resource that approached statutory reasoning as a monolithic textual entailment problem , with neural baselines performing nearly at-chance .",0,0.89243907,173.07196735594596,23
2023,"To address this challenge , we decompose statutory reasoning into four types of language-understanding challenge problems , through the introduction of concepts and structure found in Prolog programs .",1,0.4491476,99.3934568102756,31
2023,"Augmenting an existing benchmark , we provide annotations for the four tasks , and baselines for three of them .",2,0.577446,42.51290156958743,20
2023,"Models for statutory reasoning are shown to benefit from the additional structure , improving on prior baselines .",3,0.8396726,90.25218635125364,18
2023,"Further , the decomposition into subtasks facilitates finer-grained model diagnostics and clearer incremental progress .",3,0.74964166,65.29739985929223,16
2024,Ordinal Classification ( OC ) is an important classification task where the classes are ordinal .,0,0.9406779,66.31610213108863,16
2024,"For example , an OC task for sentiment analysis could have the following classes : highly positive , positive , neutral , negative , highly negative .",3,0.36396345,77.991794499895,27
2024,"Clearly , evaluation measures for an OC task should penalise misclassifications by considering the ordinal nature of the classes .",3,0.646228,92.51646839220683,20
2024,"Ordinal Quantification ( OQ ) is a related task where the gold data is a distribution over ordinal classes , and the system is required to estimate this distribution .",0,0.9310309,47.6829757833464,30
2024,Evaluation measures for an OQ task should also take the ordinal nature of the classes into account .,3,0.57203346,76.07425188002401,18
2024,"However , for both OC and OQ , there are only a small number of known evaluation measures that meet this basic requirement .",0,0.7979607,65.69666106906298,24
2024,"In the present study , we utilise data from the SemEval and NTCIR communities to clarify the properties of nine evaluation measures in the context of OC tasks , and six measures in the context of OQ tasks .",1,0.6002776,56.8581342047808,39
2025,Entity Matching ( EM ) aims at recognizing entity records that denote the same real-world object .,0,0.91627675,86.5888170077644,17
2025,Neural EM models learn vector representation of entity descriptions and match entities end-to-end .,0,0.49821758,71.9069885979482,15
2025,"Though robust , these methods require many annotated resources for training , and lack of interpretability .",0,0.7752483,94.79085472726366,17
2025,"In this paper , we propose a novel EM framework that consists of Heterogeneous Information Fusion ( HIF ) and Key Attribute Tree ( KAT ) Induction to decouple feature representation from matching decision .",1,0.8579077,47.92183322169297,35
2025,"Using self-supervised learning and mask mechanism in pre-trained language modeling , HIF learns the embeddings of noisy attribute values by inter-attribute attention with unlabeled data .",2,0.7624347,47.47806797542288,28
2025,"Using a set of comparison features and a limited amount of annotated data , KAT Induction learns an efficient decision tree that can be interpreted by generating entity matching rules whose structure is advocated by domain experts .",2,0.48903552,68.10698831742756,38
2025,Experiments on 6 public datasets and 3 industrial datasets show that our method is highly efficient and outperforms SOTA EM models in most cases .,3,0.8865328,26.069269526880657,25
2025,We will release the codes upon acceptance .,2,0.5066547,67.92792444423563,8
2026,Named entity recognition ( NER ) is a well-studied task in natural language processing .,0,0.9676165,9.631566499488985,17
2026,Traditional NER research only deals with flat entities and ignores nested entities .,0,0.8872425,97.12573852365941,13
2026,The span-based methods treat entity recognition as a span classification task .,0,0.7242584,31.615028505498366,12
2026,"Although these methods have the innate ability to handle nested NER , they suffer from high computational cost , ignorance of boundary information , under-utilization of the spans that partially match with entities , and difficulties in long entity recognition .",0,0.7788993,73.4052335935571,41
2026,"To tackle these issues , we propose a two-stage entity identifier .",2,0.39335257,61.98471350718319,13
2026,"First we generate span proposals by filtering and boundary regression on the seed spans to locate the entities , and then label the boundary-adjusted span proposals with the corresponding categories .",2,0.89286095,112.29747217614893,33
2026,Our method effectively utilizes the boundary information of entities and partially matched spans during training .,3,0.5898468,159.4377184593135,16
2026,"Through boundary regression , entities of any length can be covered theoretically , which improves the ability to recognize long entities .",3,0.48053262,147.23053537082387,22
2026,"In addition , many low-quality seed spans are filtered out in the first stage , which reduces the time complexity of inference .",3,0.6198162,67.11823110195992,23
2026,Experiments on nested NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models .,3,0.9320503,6.606433848124086,20
2027,Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event .,0,0.9417676,21.584860342961885,21
2027,Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks .,0,0.88854986,76.19565234404322,17
2027,"In this paper , we propose Text2Event , a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner .",1,0.85268277,17.74949101206515,29
2027,"Specifically , we design a sequence-to-structure network for unified event extraction , a constrained decoding algorithm for event knowledge injection during inference , and a curriculum learning algorithm for efficient model learning .",2,0.83221495,64.17091335421689,35
2027,"Experimental results show that , by uniformly modeling all tasks in a single model and universally predicting different labels , our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings .",3,0.96116227,39.46683959608232,39
2028,"In this paper , we aim to explore an uncharted territory , which is Chinese multimodal named entity recognition ( NER ) with both textual and acoustic contents .",1,0.94901377,39.337087573045174,29
2028,"To achieve this , we construct a large-scale human-annotated Chinese multimodal NER dataset , named CNERTA .",2,0.78371114,33.43776644906284,17
2028,"Our corpus totally contains 42,987 annotated sentences accompanying by 71 hours of speech data .",2,0.6470142,171.00962100639438,15
2028,"Based on this dataset , we propose a family of strong and representative baseline models , which can leverage textual features or multimodal features .",2,0.541129,49.601249151203014,25
2028,"Upon these baselines , to capture the natural monotonic alignment between the textual modality and the acoustic modality , we further propose a simple multimodal multitask model by introducing a speech-to-text alignment auxiliary task .",2,0.618495,27.07823783550317,39
2028,"Through extensive experiments , we observe that : ( 1 ) Progressive performance boosts as we move from unimodal to multimodal , verifying the necessity of integrating speech clues into Chinese NER .",3,0.81729335,82.72817601486464,33
2028,"( 2 ) Our proposed model yields state-of-the-art ( SoTA ) results on CNERTA , demonstrating its effectiveness .",3,0.91792285,53.08665068194803,23
2028,"For further research , the annotated dataset is publicly available at http://github.com/DianboWork/CNERTA .",3,0.6990397,33.07213490299111,13
2029,Disease is one of the fundamental entities in biomedical research .,0,0.97078687,20.386341860637494,11
2029,Recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications .,0,0.9160435,73.08768114751602,24
2029,Previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart .,0,0.8997682,19.33039991401642,19
2029,"Although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance , it suffers from the boundary inconsistency problem due to the separate decoding procedures .",0,0.6113817,19.828544396063965,35
2029,"Moreover , it ignores the rich information ( e.g. , the text surface form ) of each candidate concept in the vocabulary , which is quite essential for entity normalization .",0,0.43470442,85.73298824299891,31
2029,"In this work , we propose a neural transition-based joint model to alleviate these two issues .",1,0.79092735,28.435472996657072,19
2029,"We transform the end-to-end disease recognition and normalization task as an action sequence prediction task , which not only jointly learns the model with shared representations of the input , but also jointly searches the output by state transitions in one search space .",2,0.7912005,46.63450453374286,45
2029,"Moreover , we introduce attention mechanisms to take advantage of the text surface form of each candidate concept for better normalization performance .",2,0.60428125,80.89089046893238,23
2029,Experimental results conducted on two publicly available datasets show the effectiveness of the proposed method .,3,0.88828576,7.403653754753894,16
2030,Event Detection ( ED ) aims to identify event trigger words from a given text and classify it into an event type .,0,0.9165315,44.57914928488254,23
2030,"Most current methods to ED rely heavily on training instances , and almost ignore the correlation of event types .",0,0.7888529,181.4937950493248,20
2030,"Hence , they tend to suffer from data scarcity and fail to handle new unseen event types .",0,0.9295666,66.22657753906975,18
2030,"To address these problems , we formulate ED as a process of event ontology population : linking event instances to pre-defined event types in event ontology , and propose a novel ED framework entitled OntoED with ontology embedding .",2,0.52747655,60.58733769052886,39
2030,"We enrich event ontology with linkages among event types , and further induce more event-event correlations .",3,0.49436304,82.76747542323706,17
2030,"Based on the event ontology , OntoED can leverage and propagate correlation knowledge , particularly from data-rich to data-poor event types .",3,0.44182384,104.1480024819658,23
2030,"Furthermore , OntoED can be applied to new unseen event types , by establishing linkages to existing ones .",3,0.61881447,110.28330123669662,19
2030,"Experiments indicate that OntoED is more predominant and robust than previous approaches to ED , especially in data-scarce scenarios .",3,0.94437635,59.36820602847254,20
2031,Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data .,0,0.62612706,23.34140861955626,18
2031,"The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data , which we empirically show is sub-optimal .",3,0.4049144,16.96212106076358,26
2031,"In this work , we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data .",1,0.7443769,28.309632396510573,24
2031,"To this end , we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data .",2,0.8570826,36.46415375160243,21
2031,"Intuitively , monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains .",3,0.8979757,66.10466749483922,22
2031,"Accordingly , we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training , in which monolingual sentences with higher uncertainty would be sampled with higher probability .",2,0.8228788,31.559938940015165,33
2031,Experimental results on large-scale WMT English ⇒German and English ⇒Chinese datasets demonstrate the effectiveness of the proposed approach .,3,0.9011499,14.15305465036713,21
2031,Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side .,3,0.9777764,49.17247448999831,35
2032,Context-aware neural machine translation ( NMT ) remains challenging due to the lack of large-scale document-level parallel corpora .,0,0.96507883,8.879054515859357,23
2032,"To break the corpus bottleneck , in this paper we aim to improve context-aware NMT by taking the advantage of the availability of both large-scale sentence-level parallel dataset and source-side monolingual documents .",1,0.8447647,25.406441801923094,37
2032,"To this end , we propose two pre-training tasks .",2,0.60075694,20.309898914237706,10
2032,One learns to translate a sentence from source language to target language on the sentence-level parallel dataset while the other learns to translate a document from deliberately noised to original on the monolingual documents .,2,0.59633094,55.34614589183832,37
2032,"Importantly , the two pre-training tasks are jointly and simultaneously learned via the same model , thereafter fine-tuned on scale-limited parallel documents from both sentence-level and document-level perspectives .",2,0.5469622,38.62632357414609,32
2032,Experimental results on four translation tasks show that our approach significantly improves translation performance .,3,0.9493561,8.016748816163808,15
2032,One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents .,3,0.6978647,18.597604100481206,21
2033,"Although teacher forcing has become the main training paradigm for neural machine translation , it usually makes predictions only conditioned on past information , and hence lacks global planning for the future .",0,0.8965378,81.6368933624409,33
2033,"To address this problem , we introduce another decoder , called seer decoder , into the encoder-decoder framework during training , which involves future information in target predictions .",2,0.5714465,81.5659204635635,29
2033,"Meanwhile , we force the conventional decoder to simulate the behaviors of the seer decoder via knowledge distillation .",2,0.72551465,58.146177861745585,19
2033,"In this way , at test the conventional decoder can perform like the seer decoder without the attendance of it .",3,0.72699714,159.25285487337612,21
2033,"Experiment results on the Chinese-English , English-German and English-Romanian translation tasks show our method can outperform competitive baselines significantly and achieves greater improvements on the bigger data sets .",3,0.9562406,13.930193992219783,32
2033,"Besides , the experiments also prove knowledge distillation the best way to transfer knowledge from the seer decoder to the conventional decoder compared to adversarial learning and L2 regularization .",3,0.9670496,45.6325814705527,30
2034,"Five years after the first published proofs of concept , direct approaches to speech translation ( ST ) are now competing with traditional cascade solutions .",0,0.9428521,92.60822955580248,26
2034,"Starting from this question , we present a systematic comparison between state-of-the-art systems representative of the two paradigms .",1,0.7446819,14.79636753568047,25
2034,"Focusing on three language directions ( English-German / Italian / Spanish ) , we conduct automatic and manual evaluations , exploiting high-quality professional post-edits and annotations .",2,0.8657184,67.88291648535112,29
2034,"Our multi-faceted analysis on one of the few publicly available ST benchmarks attests for the first time that : i ) the gap between the two paradigms is now closed , and ii ) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other .",3,0.9371988,30.07635196427495,58
2035,"Unsupervised machine translation , which utilizes unpaired monolingual corpora as training data , has achieved comparable performance against supervised machine translation .",0,0.9228425,23.293653968532936,22
2035,"However , it still suffers from data-scarce domains .",0,0.95000803,60.981406134764015,9
2035,"To address this issue , this paper presents a novel meta-learning algorithm for unsupervised neural machine translation ( UNMT ) that trains the model to adapt to another domain by utilizing only a small amount of training data .",1,0.7775979,16.439761508929944,39
2035,We assume that domain-general knowledge is a significant factor in handling data-scarce domains .,2,0.5426335,40.80833643655084,14
2035,"Hence , we extend the meta-learning algorithm , which utilizes knowledge learned from high-resource domains , to boost the performance of low-resource UNMT .",2,0.507929,39.0080041997206,25
2035,Our model surpasses a transfer learning-based approach by up to 2-3 BLEU scores .,3,0.85905194,13.95410067591705,18
2035,Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baselines .,3,0.95923996,25.824316482968165,19
2036,"Large-scale models for learning fixed-dimensional cross-lingual sentence representations like LASER ( Artetxe and Schwenk , 2019 b ) lead to significant improvement in performance on downstream tasks .",0,0.6728809,37.26967661161191,30
2036,"However , further increases and modifications based on such large-scale models are usually impractical due to memory limitations .",0,0.6664062,66.6928373356115,19
2036,"In this work , we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations .",1,0.66928333,22.867755016928125,22
2036,We explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture .,3,0.7421005,35.611871195358844,23
2036,"To ameliorate this , we propose a novel cross-lingual language model , which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task .",2,0.48960087,14.501633982968245,33
2036,"We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space , which compensates for the learning bottleneck of the lightweight transformer for generative tasks .",2,0.61156875,46.911105675847715,44
2036,Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model .,3,0.97407883,34.55218167247002,27
2037,"Transformers are not suited for processing long documents , due to their quadratically increasing memory and time consumption .",0,0.8418023,43.27480069931573,19
2037,Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes .,3,0.66727084,95.27877243193285,29
2037,"In this paper , we propose ERNIE-Doc , a document-level language pretraining model based on Recurrence Transformers .",1,0.83018255,56.42601856180291,21
2037,"Two well-designed techniques , namely the retrospective feed mechanism and the enhanced recurrence mechanism , enable ERNIE-Doc , which has a much longer effective context length , to capture the contextual information of a complete document .",0,0.43341008,95.69703036450007,41
2037,We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective .,2,0.7812655,88.65421652503618,22
2037,Various experiments were conducted on both English and Chinese document-level tasks .,2,0.8208996,17.905659133080057,13
2037,ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103 .,3,0.84470576,40.37579634201796,23
2037,"Moreover , it outperformed competitive pretraining models by a large margin on most language understanding tasks , such as text classification and question answering .",3,0.9232515,18.188538538052306,25
2038,"Recently , knowledge distillation ( KD ) has shown great success in BERT compression .",0,0.9505731,37.33119028341109,15
2038,"Instead of only learning from the teacher ’s soft label as in conventional KD , researchers find that the rich information contained in the hidden layers of BERT is conducive to the student ’s performance .",0,0.57701004,51.30062289013047,36
2038,"To better exploit the hidden knowledge , a common practice is to force the student to deeply mimic the teacher ’s hidden states of all the tokens in a layer-wise manner .",0,0.8206758,63.39675213576875,34
2038,"In this paper , however , we observe that although distilling the teacher ’s hidden state knowledge ( HSK ) is helpful , the performance gain ( marginal utility ) diminishes quickly as more HSK is distilled .",3,0.76781565,91.24693063416328,38
2038,"To understand this effect , we conduct a series of analysis .",2,0.48706588,46.44525190230663,12
2038,"Specifically , we divide the HSK of BERT into three dimensions , namely depth , length and width .",2,0.8473073,66.23564141636164,19
2038,We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions .,2,0.5876918,61.047769593968766,23
2038,"In this way , we show that 1 ) the student ’s performance can be improved by extracting and distilling the crucial HSK , and 2 ) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation .",3,0.86086416,41.961959317480144,43
2038,"Based on the second finding , we further propose an efficient KD paradigm to compress BERT , which does not require loading the teacher during the training of student .",3,0.802993,80.45274875734705,30
2038,"For two kinds of student models and computing devices , the proposed KD paradigm gives rise to training speedup of 2.7x 3.4x .",3,0.81521106,64.2898702874885,23
2039,Lifelong learning ( LL ) aims to train a neural network on a stream of tasks while retaining knowledge from previous tasks .,0,0.9172861,27.63851555100202,23
2039,"However , many prior attempts in NLP still suffer from the catastrophic forgetting issue , where the model completely forgets what it just learned in the previous tasks .",0,0.9329566,33.31481482622924,29
2039,"In this paper , we introduce Rational LAMOL , a novel end-to-end LL framework for language models .",1,0.8248119,41.074023919477696,20
2039,"In order to alleviate catastrophic forgetting , Rational LAMOL enhances LAMOL , a recent LL model , by applying critical freezing guided by human rationales .",0,0.4800929,312.1551165515108,26
2039,"When the human rationales are not available , we propose exploiting unsupervised generated rationales as substitutions .",2,0.51850355,38.10069650812844,17
2039,"In the experiment , we tested Rational LAMOL on permutations of three datasets from the ERASER benchmark .",2,0.8285964,88.53339636171083,18
2039,The results show that our proposed framework outperformed vanilla LAMOL on most permutations .,3,0.985441,68.91766563478033,14
2039,"Furthermore , unsupervised rationale generation was able to consistently improve the overall LL performance from the baseline without relying on human-annotated rationales .",3,0.96432054,34.68567840532517,23
2040,"Natural language processing ( NLP ) often faces the problem of data diversity such as different domains , themes , styles , and so on .",0,0.9598073,70.68898870283945,26
2040,"Therefore , a single language model ( LM ) is insufficient to learn all knowledge from diverse samples .",0,0.9233116,59.3530909647452,19
2040,"To solve this problem , we firstly propose an autoencoding topic model with a mixture prior ( mATM ) to perform clustering for the data , where the clusters defined in semantic space describes the data diversity .",2,0.6826337,65.62658951323128,38
2040,"Having obtained the clustering assignment for each sample , we develop the ensemble LM ( EnsLM ) with the technique of weight modulation .",2,0.80588675,268.7867524829339,24
2040,"Specifically , EnsLM contains a backbone that is adjusted by a few modulated weights to fit for different sample clusters .",3,0.50063264,216.198167831783,21
2040,"As a result , the backbone learns the shared knowledge among all clusters while modulated weights extract the cluster-specific features .",3,0.6152825,95.39992689887966,21
2040,EnsLM can be trained jointly with mATM with a flexible LM backbone .,3,0.5648043,119.30121302271893,13
2040,We evaluate the effectiveness of both mATM and EnsLM on various tasks .,2,0.44599018,75.78117152005814,13
2041,Pre-trained language models like BERT are performant in a wide range of natural language tasks .,0,0.59554857,6.561869418095852,16
2041,"However , they are resource exhaustive and computationally expensive for industrial scenarios .",0,0.8580266,173.80818544687185,13
2041,"Thus , early exits are adopted at each layer of BERT to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference .",3,0.51390237,150.30330083763866,30
2041,"In this work , to improve efficiency without performance drop , we propose a novel training scheme called Learned Early Exit for BERT ( LeeBERT ) .",1,0.45720676,153.74142325593814,27
2041,"First , we ask each exit to learn from each other , rather than learning only from the last layer .",2,0.88037425,89.02955313813003,21
2041,"Second , the weights of different loss terms are learned , thus balancing off different objectives .",2,0.5629681,201.22569662843705,17
2041,"We formulate the optimization of LeeBERT as a bi-level optimization problem , and we propose a novel cross-level optimization ( CLO ) algorithm to improve the optimization results .",2,0.57593673,39.0602692904745,29
2041,Experiments on the GLUE benchmark show that our proposed methods improve the performance of the state-of-the-art ( SOTA ) early exit methods for pre-trained models .,3,0.91400677,11.663766763578055,30
2042,We pioneer the first extractive summarization-based collaborative filtering model called ESCOFILT .,0,0.49324954,140.19270768584852,14
2042,Our proposed model specifically produces extractive summaries for each item and user .,3,0.4681845,79.54153627552384,13
2042,"Unlike other types of explanations , summary-level explanations closely resemble real-life explanations .",0,0.8314256,59.43270055192838,15
2042,The strength of ESCOFILT lies in the fact that it unifies representation and explanation .,0,0.7150598,93.38754134382131,15
2042,"In other words , extractive summaries both represent and explain the items and users .",0,0.68746173,92.83446728414874,15
2042,"Our model uniquely integrates BERT , K-Means embedding clustering , and multilayer perceptron to learn sentence embeddings , representation-explanations , and user-item interactions , respectively .",2,0.5732856,89.47958050387906,29
2042,We argue that our approach enhances both rating prediction accuracy and user / item explainability .,3,0.941613,190.19733411419534,16
2042,Our experiments illustrate that ESCOFILT ’s prediction accuracy is better than the other state-of-the-art recommender models .,3,0.9744921,32.68362224128508,23
2042,"Furthermore , we propose a comprehensive set of criteria that assesses the real-life explainability of explanations .",3,0.4252391,35.48153440291796,17
2042,Our explainability study demonstrates the superiority of and preference for summary-level explanations over other explanation types .,3,0.9787315,40.34356097798059,19
2043,Chinese spelling correction ( CSC ) is a task to detect and correct spelling errors in texts .,0,0.9691599,26.69415347731184,18
2043,"CSC is essentially a linguistic problem , thus the ability of language understanding is crucial to this task .",0,0.8970678,71.22696900151603,19
2043,"In this paper , we propose a Pre-trained masked Language model with Misspelled knowledgE ( PLOME ) for CSC , which jointly learns how to understand language and correct spelling errors .",1,0.86935276,117.26264145992333,32
2043,"To this end , PLOME masks the chosen tokens with similar characters according to a confusion set rather than the fixed token “ [ MASK ] ” as in BERT .",2,0.67818904,191.13750812482928,31
2043,"Besides character prediction , PLOME also introduces pronunciation prediction to learn the misspelled knowledge on phonic level .",3,0.40492877,617.7333314820222,18
2043,"Moreover , phonological and visual similarity knowledge is important to this task .",0,0.5135276,86.20971012959912,13
2043,PLOME utilizes GRU networks to model such knowledge based on characters ’ phonics and strokes .,2,0.45312387,277.0787911707548,16
2043,Experiments are conducted on widely used benchmarks .,2,0.71459335,33.41723640618306,8
2043,Our method achieves superior performance against state-of-the-art approaches by a remarkable margin .,3,0.8942159,8.75417830486489,18
2043,We release the source code and pre-trained model for further use by the community ( https://github.com/liushulinle/PLOME) .,2,0.5700497,25.451002487065058,17
2044,"Medical report generation task , which targets to produce long and coherent descriptions of medical images , has attracted growing research interests recently .",0,0.9651029,100.65262804258406,24
2044,"Different from the general image captioning tasks , medical report generation is more challenging for data-driven neural models .",0,0.84567237,44.04509287930385,19
2044,This is mainly due to 1 ) the serious data bias and 2 ) the limited medical data .,0,0.6408011,48.432949596516615,19
2044,"To alleviate the data bias and make best use of available data , we propose a Competence-based Multimodal Curriculum Learning framework ( CMCL ) .",2,0.48208007,27.600335818531555,27
2044,"Specifically , CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner .",2,0.6099742,58.712033776906466,20
2044,"Firstly , CMCL estimates the difficulty of each training instance and evaluates the competence of current model ;",2,0.60196364,198.633983430288,18
2044,"Secondly , CMCL selects the most suitable batch of training instances considering current model competence .",2,0.588065,227.67884857423718,16
2044,"By iterating above two steps , CMCL can gradually improve the model ’s performance .",3,0.5323672,111.33105072174565,15
2044,The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMCL can be incorporated into existing models to improve their performance .,3,0.9235982,33.03749842791633,27
2045,Deep learning models for automatic readability assessment generally discard linguistic features traditionally used in machine learning models for the task .,0,0.86314994,67.00953602867078,21
2045,We propose to incorporate linguistic features into neural network models by learning syntactic dense embeddings based on linguistic features .,2,0.5221345,18.94514375040931,20
2045,"To cope with the relationships between the features , we form a correlation graph among features and use it to learn their embeddings so that similar features will be represented by similar embeddings .",2,0.67004144,23.082935866387373,34
2045,Experiments with six data sets of two proficiency levels demonstrate that our proposed methodology can complement BERT-only model to achieve significantly better performances for automatic readability assessment .,3,0.95943797,52.78561640912845,30
2046,Pre-trained language models have been applied to various NLP tasks with considerable performance gains .,0,0.837575,6.644259775016344,15
2046,"However , the large model sizes , together with the long inference time , limit the deployment of such models in real-time applications .",0,0.85318553,32.2440829134951,24
2046,One line of model compression approaches considers knowledge distillation to distill large teacher models into small student models .,0,0.7624447,50.957921966576485,19
2046,"Most of these studies focus on single-domain only , which ignores the transferable knowledge from other domains .",0,0.85185903,23.103753606478573,18
2046,We notice that training a teacher with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation .,3,0.9820488,85.62088502845486,22
2046,Hence we propose a Meta-Knowledge Distillation ( Meta-KD ) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students .,1,0.51377994,24.95495717615545,29
2046,"Specifically , we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains , and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher .",2,0.7915711,32.106530997769845,43
2046,Experiments on public multi-domain NLP tasks show the effectiveness and superiority of the proposed Meta-KD framework .,3,0.86920583,18.4959978918178,19
2046,"Further , we also demonstrate the capability of Meta-KD in the settings where the training data is scarce .",3,0.9214429,26.232297402644054,20
2047,Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data .,0,0.8877778,30.456402770697277,17
2047,"Among existing work , a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context .",0,0.79445714,46.259513469092695,25
2047,"However , such scores from language models can be easily affected by irrelevant factors , such as word frequencies , sentence structures , etc .",0,0.64352643,94.14198263271135,25
2047,These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers .,3,0.54127896,29.695372064932926,26
2047,"In this paper , we present a novel SEmantic-based Question Answering method ( SEQA ) for unsupervised commonsense question answering .",1,0.90853757,16.295449960103465,23
2047,"Instead of directly scoring each answer choice , our method first generates a set of plausible answers with generative models ( e.g. , GPT-2 ) , and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice .",2,0.804149,25.45915917488361,52
2047,"We devise a simple , yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments .",2,0.5471379,76.44887663544858,21
2047,"We evaluate the proposed method on four benchmark datasets , and our method achieves the best results in unsupervised settings .",3,0.63296455,15.706218113417973,21
2047,"Moreover , when attacked by TextFooler with synonym replacement , SEQA demonstrates much less performance drops than baselines , thereby indicating stronger robustness .",3,0.9579289,159.5092748166107,24
2048,"CommonsenseQA ( CQA ) ( Talmor et al. , 2019 ) dataset was recently released to advance the research on common-sense question answering ( QA ) task .",0,0.91941047,35.658718774420144,28
2048,"Whereas the prior work has mostly focused on proposing QA models for this dataset , our aim is to retrieve as well as generate explanation for a given ( question , correct answer choice , incorrect answer choices ) tuple from this dataset .",2,0.39001676,59.56374598028457,44
2048,"Our explanation definition is based on certain desiderata , and translates an explanation into a set of positive and negative common-sense properties ( aka facts ) which not only explain the correct answer choice but also refute the incorrect ones .",2,0.55928415,60.20873376589325,41
2048,"We human-annotate a first-of-its-kind dataset ( called ECQA ) of positive and negative properties , as well as free-flow explanations , for 11 K QA pairs taken from the CQA dataset .",2,0.8668355,45.24456394277655,36
2048,We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure .,2,0.49894533,23.696589273150746,28
2048,We also propose a free-flow explanation generation model .,2,0.3759922,77.02690949379782,9
2048,"Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100 % in F1 score , property generation model achieves a respectable F1 score of 36.4 , and free-flow generation model achieves a similarity score of 61.9 , where last two scores are based on a human correlated semantic similarity metric .",3,0.8981148,55.32574933163687,57
2049,"In several question answering benchmarks , pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers .",0,0.5737038,40.931629240476695,24
2049,"We explore the more realistic few-shot setting , where only a few hundred training examples are available , and observe that standard models perform poorly , highlighting the discrepancy between current pretraining objectives and question answering .",3,0.62148637,44.38773827733252,37
2049,We propose a new pretraining scheme tailored for question answering : recurring span selection .,1,0.31276152,144.06861903954697,15
2049,"Given a passage with multiple sets of recurring spans , we mask in each set all recurring spans but one , and ask the model to select the correct span in the passage for each masked span .",2,0.8326471,46.84128824052011,38
2049,"Masked spans are replaced with a special token , viewed as a question representation , that is later used during fine-tuning to select the answer span .",2,0.583957,59.60284024163491,27
2049,"The resulting model obtains surprisingly good results on multiple benchmarks ( e.g. , 72.7 F1 on SQuAD with only 128 training examples ) , while maintaining competitive performance in the high-resource setting .",3,0.83782023,23.6501152169585,34
2050,"To date , most of recent work under the retrieval-reader framework for open-domain QA focuses on either extractive or generative reader exclusively .",0,0.9121658,70.88276648789694,25
2050,"In this paper , we study a hybrid approach for leveraging the strengths of both models .",1,0.88282996,30.203045749005625,17
2050,"We apply novel techniques to enhance both extractive and generative readers built upon recent pretrained neural language models , and find that proper training methods can provide large improvement over previous state-of-the-art models .",3,0.7371639,24.73069647297251,40
2050,We demonstrate that a simple hybrid approach by combining answers from both readers can efficiently take advantages of extractive and generative answer inference strategies and outperforms single models as well as homogeneous ensembles .,3,0.9412427,64.43303876624562,34
2050,Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively .,3,0.8617855,10.688654214269993,25
2051,Neural models have shown impressive performance gains in answering queries from natural language text .,0,0.93309665,29.996690102825763,15
2051,"However , existing works are unable to support database queries , such as “ List / Count all female athletes who were born in 20th century ” , which require reasoning over sets of relevant facts with operations such as join , filtering and aggregation .",0,0.8862797,89.11135440093906,46
2051,"We show that while state-of-the-art transformer models perform very well for small databases , they exhibit limitations in processing noisy data , numerical operations , and queries that aggregate facts .",3,0.90752566,53.792503900515314,37
2051,We propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale .,2,0.3285926,100.22594830138125,22
2051,"We evaluate the architecture using WikiNLDB , a novel dataset for exploring such queries .",2,0.72905356,132.8897306487039,15
2051,Our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded .,0,0.38594386,62.62416595938245,22
2051,"In direct comparison on small databases , our approach increases overall answer accuracy from 85 % to 90 % .",3,0.93617505,75.47939398253446,20
2051,"On larger databases , our approach retains its accuracy whereas transformer baselines could not encode the context .",3,0.91563153,181.43114874216747,18
2052,"In Machine Translation , assessing the quality of a large amount of automatic translations can be challenging .",0,0.93045706,36.85469232670086,18
2052,Automatic metrics are not reliable when it comes to high performing systems .,0,0.8167799,34.015242694879525,13
2052,"In addition , resorting to human evaluators can be expensive , especially when evaluating multiple systems .",0,0.7459922,58.18312096859548,17
2052,"To overcome the latter challenge , we propose a novel application of online learning that , given an ensemble of Machine Translation systems , dynamically converges to the best systems , by taking advantage of the human feedback available .",2,0.39896065,81.70952544895775,40
2052,"Our experiments on WMT ’19 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered , despite the lack of human feedback for many translations .",3,0.95153165,60.33979271254167,37
2053,"In this work , we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance .",1,0.8238331,16.778381588649733,29
2053,We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks .,2,0.8239062,23.22901614408539,25
2053,"We first aim to establish , via fair and controlled comparisons , if a gap between the multilingual and the corresponding monolingual representation of that language exists , and subsequently investigate the reason for any performance difference .",1,0.79630274,78.88198096549124,38
2053,"To disentangle conflating factors , we train new monolingual models on the same data , with monolingually and multilingually trained tokenizers .",2,0.8051632,52.501695821009726,22
2053,"We find that while the pretraining data size is an important factor , a designated monolingual tokenizer plays an equally important role in the downstream performance .",3,0.97969306,34.741046911506366,27
2053,Our results show that languages that are adequately represented in the multilingual model ’s vocabulary exhibit negligible performance decreases over their monolingual counterparts .,3,0.9897709,36.284610898170286,24
2053,We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language .,3,0.97909045,19.753048590347486,29
2054,"Cross-lingual transfer has improved greatly through multi-lingual language model pretraining , reducing the need for parallel data and increasing absolute performance .",0,0.73423517,24.888272663691097,22
2054,"However , this progress has also brought to light the differences in performance across languages .",0,0.86396706,37.7812898658763,16
2054,"Specifically , certain language families and typologies seem to consistently perform worse in these models .",3,0.9148869,89.05320237629645,16
2054,"In this paper , we address what effects morphological typology has on zero-shot cross-lingual transfer for two tasks : Part-of-speech tagging and sentiment analysis .",1,0.9132274,20.52532263292883,26
2054,"We perform experiments on 19 languages from four language typologies ( fusional , isolating , agglutinative , and introflexive ) and find that transfer to another morphological type generally implies a higher loss than transfer to another language with the same morphological typology .",3,0.5715521,29.545071603010086,44
2054,"Furthermore , POS tagging is more sensitive to morphological typology than sentiment analysis and , on this task , models perform much better on fusional languages than on the other typologies .",3,0.8604339,50.965029811727156,32
2055,"Generating code-switched text is a problem of growing interest , especially given the scarcity of corpora containing large volumes of real code-switched text .",0,0.96220976,25.799158941832577,24
2055,"In this work , we adapt a state-of-the-art neural machine translation model to generate Hindi-English code-switched sentences starting from monolingual Hindi sentences .",2,0.43457383,11.18574487058307,29
2055,"We outline a carefully designed curriculum of pretraining steps , including the use of synthetic code-switched text , that enable the model to generate high-quality code-switched text .",3,0.43334207,29.842808217740437,28
2055,"Using text generated from our model as data augmentation , we show significant reductions in perplexity on a language modeling task , compared to using text from other generative models of CS text .",3,0.88240695,41.566832403757346,34
2055,We also show improvements using our text for a downstream code-switched natural language inference task .,3,0.9196339,59.54324311609586,16
2055,"Our generated text is further subjected to a rigorous evaluation using a human evaluation study and a range of objective metrics , where we show performance comparable ( and sometimes even superior ) to code-switched text obtained via crowd workers who are native Hindi speakers .",3,0.58165103,35.81271860745973,46
2056,It is generally believed that a translation memory ( TM ) should be beneficial for machine translation tasks .,0,0.94859356,44.48067888673241,19
2056,"Unfortunately , existing wisdom demonstrates the superiority of TM-based neural machine translation ( NMT ) only on the TM-specialized translation tasks rather than general tasks , with a non-negligible computational overhead .",0,0.9076164,45.21986812384135,36
2056,"In this paper , we propose a fast and accurate approach to TM-based NMT within the Transformer framework : the model architecture is simple and employs a single bilingual sentence as its TM , leading to efficient training and inference ;",1,0.8003951,59.37590656676939,43
2056,and its parameters are effectively optimized through a novel training criterion .,2,0.5119756,97.09522294525718,12
2056,"Extensive experiments on six TM-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple TMs , in terms of BLEU and running time .",3,0.89724016,27.225555291341003,31
2056,"In particular , the proposed approach also advances the strong baselines on two general tasks ( WMT news Zh->En and En->De ) .",3,0.816292,123.98716248779631,25
2057,"Online misogyny , a category of online abusive language , has serious and harmful social consequences .",0,0.97021735,76.98387478899778,17
2057,"Automatic detection of misogynistic language online , while imperative , poses complicated challenges to both data gathering , data annotation , and bias mitigation , as this type of data is linguistically complex and diverse .",0,0.87946343,120.12481213849793,36
2057,"This paper makes three contributions in this area : Firstly , we describe the detailed design of our iterative annotation process and codebook .",1,0.71281713,49.168172105469495,24
2057,"Secondly , we present a comprehensive taxonomy of labels for annotating misogyny in natural written language , and finally , we introduce a high-quality dataset of annotated posts sampled from social media posts .",2,0.6010913,40.576802063939034,34
2058,"Recently , considerable literature has grown up around the theme of few-shot named entity recognition ( NER ) , but little published benchmark data specifically focused on the practical and challenging task .",0,0.9557148,60.712475044307624,33
2058,Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study .,0,0.8466607,47.54737214836779,18
2058,"These strategies conventionally aim to recognize coarse-grained entity types with few examples , while in practice , most unseen entity types are fine-grained .",0,0.8645291,54.89200754669023,24
2058,"In this paper , we present Few-NERD , a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types .",1,0.77356124,24.003331075012646,30
2058,"Few-NERD consists of 188,238 sentences from Wikipedia , 4,601,160 words are included and each is annotated as context or a part of the two-level entity type .",2,0.6112947,70.59352719382176,29
2058,"To the best of our knowledge , this is the first few-shot NER dataset and the largest human-crafted NER dataset .",3,0.876826,10.522583228770015,21
2058,We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models .,2,0.80272233,45.97457480066215,16
2058,Extensive empirical results and analysis show that Few-NERD is challenging and the problem requires further research .,3,0.84364706,44.36252648114054,17
2058,The Few-NERD dataset and the baselines will be publicly available to facilitate the research on this problem .,3,0.7577279,34.74972020185321,18
2059,"Metaphor involves not only a linguistic phenomenon , but also a cognitive phenomenon structuring human thought , which makes understanding it challenging .",0,0.94206846,48.4960621161571,23
2059,"As a means of cognition , metaphor is rendered by more than texts alone , and multimodal information in which vision / audio content is integrated with the text can play an important role in expressing and understanding metaphor .",0,0.85962546,63.04512900596667,40
2059,"However , previous metaphor processing and understanding has focused on texts , partly due to the unavailability of large-scale datasets with ground truth labels of multimodal metaphor .",0,0.946512,70.68089945296143,28
2059,"In this paper , we introduce MultiMET , a novel multimodal metaphor dataset to facilitate understanding metaphorical information from multimodal text and image .",1,0.8890351,32.216619175134106,24
2059,"It contains 10,437 text-image pairs from a range of sources with multimodal annotations of the occurrence of metaphors , domain relations , sentiments metaphors convey , and author intents .",2,0.43156338,139.1486217947703,30
2059,MultiMET opens the door to automatic metaphor understanding by investigating multimodal cues and their interplay .,0,0.49883905,76.2703167163481,16
2059,"Moreover , we propose a range of strong baselines and show the importance of combining multimodal cues for metaphor understanding .",3,0.78825307,33.28482810681649,21
2059,MultiMET will be released publicly for research .,3,0.44192892,87.13691867912566,8
2060,"Undermining the impact of hateful content with informed and non-aggressive responses , called counter narratives , has emerged as a possible solution for having healthier online communities .",0,0.9210253,85.1667477082932,28
2060,"Thus , some NLP studies have started addressing the task of counter narrative generation .",0,0.92705137,93.13015329295906,15
2060,"Although such studies have made an effort to build hate speech / counter narrative ( HS / CN ) datasets for neural generation , they fall short in reaching either high-quality and / or high-quantity .",0,0.9155262,107.72360863780905,36
2060,"In this paper , we propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and / or post-edit .",1,0.8356224,27.17760253865301,46
2060,Our experiments comprised several loops including diverse dynamic variations .,2,0.62387353,1342.2321755614312,10
2060,"Results show that the methodology is scalable and facilitates diverse , novel , and cost-effective data collection .",3,0.9887367,64.64205491528811,18
2060,"To our knowledge , the resulting dataset is the only expert-based multi-target HS / CN dataset available to the community .",3,0.85249996,70.19756939088346,21
2061,Recent work has investigated the interesting question using pre-trained language models ( PLMs ) as knowledge bases for answering open questions .,0,0.9381927,30.92132500357871,22
2061,"However , existing work is limited in using small benchmarks with high test-train overlaps .",0,0.8130471,108.99227706781497,15
2061,"We construct a new dataset of closed-book QA using SQuAD , and investigate the performance of BART .",2,0.6412111,35.41892205473086,20
2061,"Experiments show that it is challenging for BART to remember training facts in high precision , and also challenging to answer closed-book questions even if relevant knowledge is retained .",3,0.9158103,72.26684633440871,32
2061,"Some promising directions are found , including decoupling the knowledge memorizing process and the QA finetune process , forcing the model to recall relevant knowledge when question answering .",3,0.93456537,58.384946219708745,29
2062,"This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection ( AS2 ) modules , which are core components of retrieval-based Question Answering ( QA ) systems .",1,0.82469004,79.44472620544057,39
2062,Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers .,3,0.9796132,109.78008919034535,23
2062,"For this purpose , we build a three-way multi-classifier , which decides if an answer supports , refutes , or is neutral with respect to another one .",2,0.6585632,55.36150764290445,30
2062,"More specifically , our neural architecture integrates a state-of-the-art AS2 module with the multi-classifier , and a joint layer connecting all components .",2,0.511085,40.61634068129922,29
2062,"We tested our models on WikiQA , TREC-QA , and a real-world dataset .",2,0.74572253,27.494607037147723,16
2062,The results show that our models obtain the new state of the art in AS2 .,3,0.9887531,25.50086948947639,16
2063,"In open-domain question answering , questions are highly likely to be ambiguous because users may not know the scope of relevant topics when formulating them .",0,0.83488137,26.54741376580794,26
2063,"Therefore , a system needs to find possible interpretations of the question , and predict one or multiple plausible answers .",0,0.8966318,57.0075006702865,21
2063,"When multiple plausible answers are found , the system should rewrite the question for each answer to resolve the ambiguity .",3,0.5633207,47.174161250797624,21
2063,"In this paper , we present a model that aggregates and combines evidence from multiple passages to adaptively predict a single answer or a set of question-answer pairs for ambiguous questions .",1,0.89227146,27.12359012060687,34
2063,"In addition , we propose a novel round-trip prediction approach to iteratively generate additional interpretations that our model fails to find in the first pass , and then verify and filter out the incorrect question-answer pairs to arrive at the final disambiguated output .",2,0.5767001,37.245364156035976,46
2063,"Our model , named Refuel , achieves a new state-of-the-art performance on the AmbigQA dataset , and shows competitive performance on NQ-Open and TriviaQA .",3,0.78053373,19.579507625735417,33
2063,"The proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions , which improves our Refuel as well as several baseline models .",3,0.71522903,48.816305417582605,27
2063,We release source code for our models and experiments at https://github.com/amzn/refuel-open-domain-qa .,3,0.44334626,17.27628851363038,12
2064,"Hybrid data combining both tabular and textual content ( e.g. , financial reports ) are quite pervasive in the real world .",0,0.9080849,58.04381953737845,22
2064,"However , Question Answering ( QA ) over such hybrid data is largely neglected in existing research .",0,0.9514411,45.17706524254119,18
2064,"In this work , we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data , named TAT-QA , where numerical reasoning is usually required to infer the answer , such as addition , subtraction , multiplication , division , counting , comparison / sorting , and the compositions .",2,0.65064096,65.04112362896016,61
2064,"We further propose a novel QA model termed TAGOP , which is capable of reasoning over both tables and text .",3,0.41546196,41.61007372042505,21
2064,"It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics , and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer .",2,0.6664144,48.63727030640991,43
2064,"TAGOP achieves 58.0 % inF1 , which is an 11.1 % absolute increase over the previous best baseline model , according to our experiments on TAT-QA .",3,0.9304293,39.84862575655659,29
2064,"But this result still lags far behind performance of expert human , i.e.90.8 % in F1 .",3,0.9403716,65.73529813519616,17
2064,It is demonstrated that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid form data .,3,0.9698065,42.28487046708243,30
2065,Conversational KBQA is about answering a sequence of questions related to a KB .,0,0.8898973,56.77171244304983,14
2065,Follow-up questions in conversational KBQA often have missing information referring to entities from the conversation history .,0,0.7465192,51.001739076093465,19
2065,"In this paper , we propose to model these implied entities , which we refer to as the focal entities of the conversation .",1,0.7567924,39.62124231365846,24
2065,"We propose a novel graph-based model to capture the transitions of focal entities and apply a graph neural network to derive a probability distribution of focal entities for each question , which is then combined with a standard KBQA module to perform answer ranking .",2,0.735085,31.94632680093852,46
2065,Our experiments on two datasets demonstrate the effectiveness of our proposed method .,3,0.8353226,7.981461149969025,13
2066,This paper introduces the task of factual error correction : performing edits to a claim so that the generated rewrite is better supported by evidence .,1,0.79118526,59.69858163277428,26
2066,This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence .,3,0.4065202,28.15113950181308,28
2066,"We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence , but not the correction .",3,0.93419474,64.99000160383723,31
2066,We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections .,2,0.773965,106.34473754915082,21
2066,"Our approach , based on the T5 transformer and using retrieved evidence , achieved better results than existing work which used a pointer copy network and gold evidence , producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score .",3,0.88864386,146.35216358465428,49
2066,"The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task .",2,0.51091266,35.32633080625789,30
2067,We present algorithms for aligning components of Abstract Meaning Representation ( AMR ) graphs to spans in English sentences .,1,0.55761653,42.3936483736309,20
2067,"We leverage unsupervised learning in combination with heuristics , taking the best of both worlds from previous AMR aligners .",2,0.8339961,48.93544477886216,20
2067,"Our unsupervised models , however , are more sensitive to graph substructures , without requiring a separate syntactic parse .",3,0.8345114,65.72555055279226,20
2067,"Our approach covers a wider variety of AMR substructures than previously considered , achieves higher coverage of nodes and edges , and does so with higher accuracy .",3,0.81645226,49.38586074999436,28
2067,"We will release our LEAMR datasets and aligner for use in research on AMR parsing , generation , and evaluation .",3,0.5819203,86.93919856491256,21
2068,Natural language is compositional ; the meaning of a sentence is a function of the meaning of its parts .,0,0.9041512,20.76348619033108,20
2068,"This property allows humans to create and interpret novel sentences , generalizing robustly outside their prior experience .",0,0.7877546,121.82587295654376,18
2068,"Neural networks have been shown to struggle with this kind of generalization , in particular performing poorly on tasks designed to assess compositional generalization ( i.e .",0,0.86653805,26.49598665889301,27
2068,where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve ) .,3,0.42616564,93.07772230672833,20
2068,Their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution .,0,0.6985582,16.19986947767464,31
2068,We implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization .,2,0.7865508,34.32398311515029,18
2068,We construct pairs of tasks for meta-learning by sub-sampling existing training data .,2,0.8520163,42.17004715646736,13
2068,"Each pair of tasks is constructed to contain relevant examples , as determined by a similarity metric , in an effort to inhibit models from memorizing their input .",2,0.67712784,66.09852115404752,29
2068,Experimental results on the COGS and SCAN datasets show that our similarity-driven meta-learning can improve generalization performance .,3,0.9562785,22.527935285139417,20
2069,"Large pre-trained models such as BERT are known to improve different downstream NLP tasks , even when such a model is trained on a generic domain .",0,0.79153675,16.651637885151185,27
2069,"Moreover , recent studies have shown that when large domain-specific corpora are available , continued pre-training on domain-specific data can further improve the performance of in-domain tasks .",0,0.8547892,16.705041240978407,29
2069,"However , this practice requires significant domain-specific data and computational resources which may not always be available .",0,0.818981,22.863780783120287,18
2069,"In this paper , we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data .",1,0.8735661,20.73650919451028,21
2069,"We demonstrate that by explicitly incorporating multi-granularity information of unseen and domain-specific words via the adaptation of ( word based ) n-grams , the performance of a generic pretrained model can be greatly improved .",3,0.916788,41.51941883508103,35
2069,"Specifically , we introduce a Transformer-based Domain-aware N-gram Adaptor , T-DNA , to effectively learn and incorporate the semantic representation of different combinations of words in the new domain .",2,0.6581317,31.304066125882642,32
2069,Experimental results illustrate the effectiveness of T-DNA on eight low-resource downstream tasks from four domains .,3,0.95870036,31.72918014610524,16
2069,We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs .,3,0.9723984,24.96868101529176,25
2069,"Moreover , further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities .",3,0.9807612,50.68401334817555,20
2069,Our code is available at https://github.com/shizhediao/T-DNA .,3,0.59707534,15.574605080642257,7
2070,Pre-trained Language Models ( PLMs ) have shown superior performance on various downstream Natural Language Processing ( NLP ) tasks .,0,0.94736445,13.45560826307312,21
2070,"However , conventional pre-training objectives do not explicitly model relational facts in text , which are crucial for textual understanding .",0,0.862552,45.130106337448964,21
2070,"To address this issue , we propose a novel contrastive learning framework ERICA to obtain a deep understanding of the entities and their relations in text .",1,0.5959155,20.56317537593252,27
2070,"Specifically , we define two novel pre-training tasks to better understand entities and relations : ( 1 ) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation ;",2,0.76983386,50.01359480917317,38
2070,"( 2 ) the relation discrimination task to distinguish whether two relations are close or not semantically , which involves complex relational reasoning .",2,0.6004307,115.47834718701084,24
2070,"Experimental results demonstrate that ERICA can improve typical PLMs ( BERT and RoBERTa ) on several language understanding tasks , including relation extraction , entity typing and question answering , especially under low-resource settings .",3,0.9806691,35.45646097624928,35
2071,The Emotion Cause Extraction ( ECE ) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text .,0,0.90686244,40.105014606009995,26
2071,We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves .,3,0.9580423,67.51033857841057,34
2071,Existing models for ECE tend to explore such relative position information and suffer from the dataset bias .,0,0.86218506,99.04656713000759,18
2071,"To investigate the degree of reliance of existing ECE models on clause relative positions , we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses .",1,0.56456375,63.659054215973406,40
2071,We test the performance of existing models on such adversarial examples and observe a significant performance drop .,3,0.4709416,26.212803844456833,18
2071,"To address the dataset bias , we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause .",2,0.6905459,38.824173215652515,40
2071,"Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset , and is more robust against adversarial attacks compared to existing models .",3,0.96268743,10.719168593289066,39
2072,"Previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business , or on creating a textual summary .",0,0.9084007,77.79608957974108,28
2072,"These approaches provide only a partial view of the data : aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating , while textual summaries do not quantify the significance of each element , and are not well-suited for representing conflicting views .",0,0.71901375,51.20706759937356,48
2072,"Recently , Key Point Analysis ( KPA ) has been proposed as a summarization framework that provides both textual and quantitative summary of the main points in the data .",0,0.954281,28.948332686162814,30
2072,We adapt KPA to review data by introducing Collective Key Point Mining for better key point extraction ;,2,0.59054345,621.4710498463727,18
2072,integrating sentiment analysis into KPA ; identifying good key point candidates for review summaries ;,3,0.6333801,614.0494218774462,15
2072,and leveraging the massive amount of available reviews and their metadata .,0,0.58229434,93.8886037095965,12
2072,We show empirically that these novel extensions of KPA substantially improve its performance .,3,0.91807675,62.8871696697154,14
2072,"We demonstrate that promising results can be achieved without any domain-specific annotation , while human supervision can lead to further improvement .",3,0.97082573,32.97465452172898,22
2073,"Structured sentiment analysis attempts to extract full opinion tuples from a text , but over time this task has been subdivided into smaller and smaller sub-tasks , e.g. , target extraction or targeted polarity classification .",0,0.93966514,53.65110061774827,36
2073,We argue that this division has become counterproductive and propose a new unified framework to remedy the situation .,3,0.5064266,50.34821636919116,19
2073,"We cast the structured sentiment problem as dependency graph parsing , where the nodes are spans of sentiment holders , targets and expressions , and the arcs are the relations between them .",2,0.7751082,91.1890807770608,33
2073,"We perform experiments on five datasets in four languages ( English , Norwegian , Basque , and Catalan ) and show that this approach leads to strong improvements over state-of-the-art baselines .",3,0.54524463,11.23090086873479,38
2073,Our analysis shows that refining the sentiment graphs with syntactic dependency information further improves results .,3,0.9881954,63.94280584366616,16
2074,Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others .,0,0.4778492,14.045010767134901,17
2074,"In this work , we propose to improve cross-lingual fine-tuning with consistency regularization .",1,0.7678798,19.061865558795542,14
2074,"Specifically , we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations , i.e. , subword sampling , Gaussian noise , code-switch substitution , and machine translation .",2,0.8477634,90.23639366939307,34
2074,"In addition , we employ model consistency to regularize the models trained with two augmented versions of the same training set .",2,0.76348495,41.819965974670716,22
2074,"Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks , including text classification , question answering , and sequence labeling .",3,0.9244333,14.765690500487866,29
2075,The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences .,0,0.7533816,23.966727113679557,18
2075,"In this paper , we introduce denoising word alignment as a new cross-lingual pre-training task .",1,0.7995556,17.6909433849632,16
2075,"Specifically , the model first self-label word alignments for parallel sentences .",2,0.67682576,100.56330117557788,12
2075,Then we randomly mask tokens in a bitext pair .,2,0.88833433,221.8965293627951,10
2075,"Given a masked token , the model uses a pointer network to predict the aligned token in the other language .",2,0.665019,49.037895844723366,21
2075,We alternately perform the above two steps in an expectation-maximization manner .,2,0.77747416,29.577724921468516,14
2075,"Experimental results show that our method improves cross-lingual transferability on various datasets , especially on the token-level tasks , such as question answering , and structured prediction .",3,0.973332,25.13365135616881,30
2075,"Moreover , the model can serve as a pretrained word aligner , which achieves reasonably low error rate on the alignment benchmarks .",3,0.84763,50.371149135245915,23
2075,-Align .,3,0.3862783,216.7557850097593,3
2076,Knowledge distillation ( KD ) is commonly used to construct synthetic data for training non-autoregressive translation ( NAT ) models .,0,0.91757876,27.02439712110547,21
2076,"However , there exists a discrepancy on low-frequency words between the distilled and the original data , leading to more errors on predicting low-frequency words .",3,0.7682237,47.0385690872303,26
2076,"To alleviate the problem , we directly expose the raw data into NAT by leveraging pretraining .",2,0.6623702,75.05052006076748,17
2076,"By analyzing directed alignments , we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source .",3,0.9549674,72.3346600031638,30
2076,"Accordingly , we propose reverse KD to rejuvenate more alignments for low-frequency target words .",3,0.72151196,173.4553999481747,15
2076,"To make the most of authentic and synthetic data , we combine these complementary approaches as a new training strategy for further boosting NAT performance .",2,0.5897455,68.55179611787723,26
2076,We conduct experiments on five translation benchmarks over two advanced architectures .,2,0.85570014,115.07784433320168,12
2076,Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words .,3,0.9896862,21.465520995676155,21
2076,"Encouragingly , our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets , respectively .",3,0.911527,11.503789975813799,25
2076,"Our code , data , and trained models are available at https://github.com/longyuewangdcu/RLFW-NAT .",3,0.5177251,38.803333388511604,13
2077,Document-level MT models are still far from satisfactory .,0,0.87939453,30.12492285226394,11
2077,Existing work extend translation unit from single sentence to multiple sentences .,0,0.8472511,144.78429280032483,12
2077,"However , study shows that when we further enlarge the translation unit to a whole document , supervised training of Transformer can fail .",3,0.88111854,134.38946693326108,24
2077,"In this paper , we find such failure is not caused by overfitting , but by sticking around local minima during training .",1,0.53351593,88.52605108014868,23
2077,Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure .,3,0.9866682,34.675532955003476,20
2077,"As a solution , we propose G-Transformer , introducing locality assumption as an inductive bias into Transformer , reducing the hypothesis space of the attention from target to source .",2,0.60237503,79.45124221061145,30
2077,"Experiments show that G-Transformer converges faster and more stably than Transformer , achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets .",3,0.94185597,15.726002337697516,33
2078,The Neural Machine Translation ( NMT ) model is essentially a joint language model conditioned on both the source sentence and partial translation .,0,0.8646086,25.215351445696403,24
2078,"Therefore , the NMT model naturally involves the mechanism of the Language Model ( LM ) that predicts the next token only based on partial translation .",0,0.62703794,65.2718731443715,27
2078,"Despite its success , NMT still suffers from the hallucination problem , generating fluent but inadequate translations .",0,0.880485,75.81250728342928,18
2078,"The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent , namely overconfidence of the LM .",3,0.53678364,68.22797110402801,28
2078,"Accordingly , we define the Margin between the NMT and the LM , calculated by subtracting the predicted probability of the LM from that of the NMT model for each token .",2,0.89073724,28.573383657323568,32
2078,The Margin is negatively correlated to the overconfidence degree of the LM .,3,0.89694,73.91258773761993,13
2078,"Based on the property , we propose a Margin-based Token-level Objective ( MTO ) and a Margin-based Sentence-level Objective ( MSO ) to maximize the Margin for preventing the LM from being overconfident .",2,0.634385,23.625374594756707,40
2078,"Experiments on WMT14 English-to-German , WMT19 Chinese-to-English , and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach , with 1.36 , 1.50 , and 0.63 BLEU improvements , respectively , compared to the Transformer baseline .",3,0.842922,8.810232329651814,44
2078,The human evaluation further verifies that our approaches improve translation adequacy as well as fluency .,3,0.9772669,41.5456792865027,16
2079,"Emotional support is a crucial ability for many conversation scenarios , including social interactions , mental health support , and customer service chats .",0,0.7559432,95.16789093660098,24
2079,Following reasonable procedures and using various support skills can help to effectively provide support .,3,0.68349826,198.59647940987907,15
2079,"However , due to the lack of a well-designed task and corpora of effective emotional support conversations , research on building emotional support into dialog systems remains lacking .",0,0.92598045,50.44580691118218,31
2079,"In this paper , we define the Emotional Support Conversation ( ESC ) task and propose an ESC Framework , which is grounded on the Helping Skills Theory .",1,0.89655167,57.74881055187984,29
2079,We construct an Emotion Support Conversation dataset ( ESConv ) with rich annotation ( especially support strategy ) in a help-seeker and supporter mode .,2,0.8438679,221.71905369373175,25
2079,"To ensure a corpus of high-quality conversations that provide examples of effective emotional support , we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection .",2,0.5059069,105.36752647357105,35
2079,"Finally , we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support .",2,0.47135952,15.541228843932352,23
2079,Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems .,3,0.99031794,50.19829461490718,25
2080,Existing slot filling models can only recognize pre-defined in-domain slot types from a limited slot set .,0,0.7323613,49.973676701842386,19
2080,"In the practical application , a reliable dialogue system should know what it does not know .",0,0.54026145,60.2190988818883,17
2080,"In this paper , we introduce a new task , Novel Slot Detection ( NSD ) , in the task-oriented dialogue system .",1,0.89815485,50.84628288354589,25
2080,NSD aims to discover unknown or out-of-domain slot types to strengthen the capability of a dialogue system based on in-domain training data .,0,0.75578076,28.75645206267482,27
2080,"Besides , we construct two public NSD datasets , propose several strong NSD baselines , and establish a benchmark for future work .",2,0.51749295,59.39510563902885,23
2080,"Finally , we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide new guidance for future directions .",3,0.7887884,39.91057060602681,21
2081,Generating some appealing questions in open-domain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction .,3,0.6190899,45.64061136253681,26
2081,"To avoid dull or deviated questions , some researchers tried to utilize answer , the “ future ” information , to guide question generation .",0,0.86898726,225.56248623426038,25
2081,"However , they separate a post-question-answer ( PQA ) triple into two parts : post-question ( PQ ) and question-answer ( QA ) pairs , which may hurt the overall coherence .",0,0.78480315,29.78302324502878,36
2081,"Besides , the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations .",3,0.5058943,29.854820889756265,22
2081,"To tackle these problems , we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation ( CQG ) .",1,0.4237599,33.848548696379744,24
2081,Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs .,2,0.75966316,48.53635054945773,28
2081,"Experimental results on a large-scale CQG dataset show that our method significantly improves the quality of questions in terms of fluency , coherence and diversity over competitive baselines .",3,0.9530869,14.543123953368081,29
2082,"Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue , which leads to poor generation diversity as widely reported in the literature .",0,0.8003885,45.94340078102701,29
2082,"Although existing approaches such as label smoothing can alleviate this issue , they fail to adapt to diverse dialog contexts .",0,0.7845354,33.34599723545029,21
2082,"In this paper , we propose an Adaptive Label Smoothing ( AdaLabel ) approach that can adaptively estimate a target label distribution at each time step for different contexts .",1,0.8618969,33.09792894388794,30
2082,The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module .,2,0.624837,46.67467084160571,26
2082,The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model .,3,0.46380398,44.636601417426334,22
2082,Our model can be trained in an endto-end manner .,3,0.6541554,19.77193797689224,10
2082,Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses .,3,0.86948586,12.602134029896419,19
2083,Out-of-scope intent detection is of practical importance in task-oriented dialogue systems .,0,0.7180679,13.417028924604379,16
2083,"Since the distribution of outlier utterances is arbitrary and unknown in the training stage , existing methods commonly rely on strong assumptions on data distribution such as mixture of Gaussians to make inference , resulting in either complex multi-step training procedures or hand-crafted rules such as confidence threshold selection for outlier detection .",0,0.8282555,55.92428584001767,54
2083,"In this paper , we propose a simple yet effective method to train an out-of-scope intent classifier in a fully end-to-end manner by simulating the test scenario in training , which requires no assumption on data distribution and no additional post-processing or threshold setting .",1,0.79408926,25.53990785717685,48
2083,"Specifically , we construct a set of pseudo outliers in the training stage , by generating synthetic outliers using inliner features via self-supervision and sampling out-of-scope sentences from easily available open-domain datasets .",2,0.88217676,44.63691004215076,35
2083,The pseudo outliers are used to train a discriminative classifier that can be directly applied to and generalize well on the test task .,2,0.67934227,31.83121938373707,24
2083,We evaluate our method extensively on four benchmark dialogue datasets and observe significant improvements over state-of-the-art approaches .,3,0.6865359,11.384110321150358,24
2083,Our code has been released at https://github.com/liam0949/DCLOOS .,3,0.6129334,18.2822302851433,8
2084,Document-level event extraction aims to recognize event information from a whole piece of article .,0,0.7699032,60.73954929569161,17
2084,Existing methods are not effective due to two challenges of this task : a ) the target event arguments are scattered across sentences ;,0,0.7972361,101.03727244558675,24
2084,b) the correlation among events in a document is non-trivial to model .,3,0.5208126,19.105275924361003,14
2084,"In this paper , we propose Heterogeneous Graph-based Interaction Model with a Tracker ( GIT ) to solve the aforementioned two challenges .",1,0.83714384,39.442212877004806,25
2084,"For the first challenge , GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions .",2,0.608289,74.03060918573034,23
2084,"For the second , GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events .",2,0.4885284,67.79796843264913,23
2084,"Experiments on a large-scale dataset ( Zheng et al , 2019 ) show GIT outperforms the previous methods by 2.8 F1 .",3,0.69125986,22.43202458560033,22
2084,Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document .,3,0.9602017,142.24840273406147,19
2085,This paper presents a novel method for nested named entity recognition .,1,0.8810666,18.406761273468398,12
2085,"As a layered method , our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path .",2,0.5734571,65.2135727185379,26
2085,Our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level .,2,0.6180062,51.674018441772134,28
2085,"In addition , we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme .",3,0.9584413,74.22473385543779,22
2085,"We provide extensive experimental results on ACE2004 , ACE2005 , and GENIA datasets to show the effectiveness and efficiency of our proposed method .",3,0.6041806,17.934280294443983,24
2086,"Modern models for event causality identification ( ECI ) are mainly based on supervised learning , which are prone to the data lacking problem .",0,0.94049007,96.25797822206974,25
2086,"Unfortunately , the existing NLP-related augmentation methods cannot directly produce available data required for this task .",0,0.85099185,65.5309334064076,19
2086,"To solve the data lacking problem , we introduce a new approach to augment training data for event causality identification , by iteratively generating new examples and classifying event causality in a dual learning framework .",2,0.5745926,49.83101212523845,36
2086,"On the one hand , our approach is knowledge guided , which can leverage existing knowledge bases to generate well-formed new sentences .",2,0.46278238,37.91042914994409,25
2086,"On the other hand , our approach employs a dual mechanism , which is a learnable augmentation framework , and can interactively adjust the generation process to generate task-related sentences .",2,0.4567076,43.750328722578196,33
2086,Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1 ) our method can augment suitable task-related training data for ECI ;,3,0.96086466,168.085553186584,27
2086,2 ) our method outperforms previous methods on EventStoryLine and Causal-TimeBank ( + 2.5 and + 2.1 points on F1 value respectively ) .,3,0.9126922,64.10500621387975,24
2087,Distantly supervision automatically generates plenty of training samples for relation extraction .,3,0.5841443,100.18514270661375,12
2087,"However , it also incurs two major problems : noisy labels and imbalanced training data .",0,0.89160633,74.68716367228869,16
2087,Previous works focus more on reducing wrongly labeled relations ( false positives ) while few explore the missing relations that are caused by incompleteness of knowledge base ( false negatives ) .,0,0.8628065,78.82362585088622,32
2087,"Furthermore , the quantity of negative labels overwhelmingly surpasses the positive ones in previous problem formulations .",3,0.74939066,102.49970538856259,17
2087,"In this paper , we first provide a thorough analysis of the above challenges caused by negative data .",1,0.86121327,38.25355758234375,19
2087,"Next , we formulate the problem of relation extraction into as a positive unlabeled learning task to alleviate false negative problem .",2,0.6712737,68.64603508804741,22
2087,"Thirdly , we propose a pipeline approach , dubbed ReRe , that first performs sentence classification with relational labels and then extracts the subjects / objects .",2,0.6326587,140.02982429541512,27
2087,Experimental results show that the proposed method consistently outperforms existing approaches and remains excellent performance even learned with a large quantity of false positive samples .,3,0.9592324,25.544864937651283,26
2087,Source code is available online at https://github.com/redreamality/RERE-relation-extraction .,3,0.45426932,31.881065990420186,8
2088,This paper studies a new problem setting of entity alignment for knowledge graphs ( KGs ) .,1,0.7598526,68.82686068594857,17
2088,"Since KGs possess different sets of entities , there could be entities that cannot find alignment across them , leading to the problem of dangling entities .",0,0.7461922,77.14662974210623,27
2088,"As the first attempt to this problem , we construct a new dataset and design a multi-task learning framework for both entity alignment and dangling entity detection .",2,0.48754087,31.370957948396992,28
2088,The framework can opt to abstain from predicting alignment for the detected dangling entities .,3,0.78699046,199.86435133809033,15
2088,"We propose three techniques for dangling entity detection that are based on the distribution of nearest-neighbor distances , i.e. , nearest neighbor classification , marginal ranking and background ranking .",2,0.4876534,69.81854605551136,31
2088,"After detecting and removing dangling entities , an incorporated entity alignment model in our framework can provide more robust alignment for remaining entities .",3,0.80408716,97.43040019687858,24
2088,Comprehensive experiments and analyses demonstrate the effectiveness of our framework .,3,0.8900801,15.561632589335003,11
2088,"We further discover that the dangling entity detection module can , in turn , improve alignment learning and the final performance .",3,0.96436834,166.9505591781935,22
2088,The contributed resource is publicly available to foster further research .,3,0.79292357,70.06868364796391,11
2089,"We present the first study investigating this question , taking BERT as the example PLM and focusing on its semantic representations of English derivatives .",1,0.51874983,68.03132895060996,25
2089,"We show that PLMs can be interpreted as serial dual-route models , i.e. , the meanings of complex words are either stored or else need to be computed from the subwords , which implies that maximally meaningful input tokens should allow for the best generalization on new words .",3,0.89560276,80.93255872217865,49
2089,"This hypothesis is confirmed by a series of semantic probing tasks on which DelBERT ( Derivation leveraging BERT ) , a model with derivational input segmentation , substantially outperforms BERT with WordPiece segmentation .",3,0.7730281,85.91977081363031,34
2089,Our results suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used .,3,0.990737,23.71158834083747,25
2090,Analogies play a central role in human commonsense reasoning .,0,0.96082914,31.994622033565218,10
2090,"The ability to recognize analogies such as “ eye is to seeing what ear is to hearing ” , sometimes referred to as analogical proportions , shape how we structure knowledge and understand language .",0,0.9286622,82.69256225764718,35
2090,"Surprisingly , however , the task of identifying such analogies has not yet received much attention in the language model era .",0,0.9454794,48.19999030722165,22
2090,"In this paper , we analyze the capabilities of transformer-based language models on this unsupervised task , using benchmarks obtained from educational settings , as well as more commonly used datasets .",1,0.8399367,34.136685244175816,34
2090,"We find that off-the-shelf language models can identify analogies to a certain extent , but struggle with abstract and complex relations , and results are highly sensitive to model architecture and hyperparameters .",3,0.97384,38.66384178081464,35
2090,"Overall the best results were obtained with GPT-2 and RoBERTa , while configurations using BERT were not able to outperform word embedding models .",3,0.9814232,17.68297766100443,26
2090,"Our results raise important questions for future work about how , and to what extent , pre-trained language models capture knowledge about abstract semantic relations .",3,0.98389995,24.401106438142012,26
2091,This paper presents a multilingual study of word meaning representations in context .,1,0.88686186,34.82155884360517,13
2091,"We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations , such as homonymy and synonymy .",2,0.66519356,22.21707355939334,24
2091,"To do so , we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words , conveying the same or different senses .",2,0.8132696,31.890066890310777,42
2091,A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context .,3,0.8276945,57.210403633384374,22
2091,"However , as they rely heavily on context , these models fail at representing words with different senses when occurring in similar sentences .",0,0.8456965,78.93085650463178,24
2091,"Experiments are performed in Galician , Portuguese , English , and Spanish , and both the dataset ( with more than 3,000 evaluation items ) and new models are freely released with this study .",2,0.6564248,80.3955317257827,35
2092,"We propose to measure fine-grained domain relevance– the degree that a term is relevant to a broad ( e.g. , computer science ) or narrow ( e.g. , deep learning ) domain .",2,0.49406216,28.377079797487458,34
2092,Such measurement is crucial for many downstream tasks in natural language processing .,0,0.9135096,13.168959418687127,13
2092,"To handle long-tail terms , we build a core-anchored semantic graph , which uses core terms with rich description information to bridge the vast remaining fringe terms semantically .",2,0.8194744,137.72495773438405,29
2092,"To support a fine-grained domain without relying on a matching corpus for supervision , we develop hierarchical core-fringe learning , which learns core and fringe terms jointly in a semi-supervised manner contextualized in the hierarchy of the domain .",2,0.7712704,60.644683155096665,39
2092,"To reduce expensive human efforts , we employ automatic annotation and hierarchical positive-unlabeled learning .",2,0.6231032,146.83868206608392,16
2092,"Our approach applies to big or small domains , covers head or tail terms , and requires little human effort .",3,0.62892276,252.46422369082075,21
2092,Extensive experiments demonstrate that our methods outperform strong baselines and even surpass professional human performance .,3,0.9191238,15.88597128914679,16
2093,Open-domain dialog systems have a user-centric goal : to provide humans with an engaging conversation experience .,0,0.8754147,25.11556702278043,17
2093,"User engagement is one of the most important metrics for evaluating open-domain dialog systems , and could also be used as real-time feedback to benefit dialog policy learning .",0,0.48294562,22.41481529765907,29
2093,Existing work on detecting user disengagement typically requires hand-labeling many dialog samples .,0,0.81420064,101.29057381946839,15
2093,"We propose HERALD , an efficient annotation framework that reframes the training data annotation process as a denoising problem .",1,0.43286085,37.98584943063837,20
2093,"Specifically , instead of manually labeling training samples , we first use a set of labeling heuristics to label training samples automatically .",2,0.8126609,25.847908550701856,23
2093,We then denoise the weakly labeled data using the Shapley algorithm .,2,0.8157024,44.29230951460808,12
2093,"Finally , we use the denoised data to train a user engagement detector .",2,0.71087116,42.577333364349805,14
2093,Our experiments show that HERALD improves annotation efficiency significantly and achieves 86 % user disengagement detection accuracy in two dialog corpora .,3,0.9631946,115.13640913993886,22
2094,"Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances , programs , and system responses .",0,0.6235076,88.71946872993165,23
2094,Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed .,0,0.8795356,85.69918659038862,21
2094,We propose a model that abstracts over values to focus prediction on type-and function-level context .,1,0.39343828,84.91266102733138,20
2094,"This approach provides a compact encoding of dialogue histories and predicted programs , improving generalization and computational efficiency .",3,0.68681985,151.81325735858317,19
2094,"Our model incorporates several other components , including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs , that are particularly advantageous in the low-data regime .",3,0.5089031,175.79074824961887,34
2094,"Trained on the SMCalFlow and TreeDST datasets , our model outperforms prior work by 7.3 % and 10.6 % respectively in terms of absolute accuracy .",3,0.8841622,32.36203354060479,26
2094,"Trained on only a thousand examples from each dataset , it outperforms strong baselines by 12.4 % and 6.4 % .",3,0.8689191,26.428019481894193,21
2094,These results indicate that simple representations are key to effective generalization in conversational semantic parsing .,3,0.9895415,24.075208616204314,16
2095,"Recently , various neural models for multi-party conversation ( MPC ) have achieved impressive improvements on a variety of tasks such as addressee recognition , speaker identification and response prediction .",0,0.9537331,29.174396500639215,31
2095,"However , these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process .",0,0.8192748,70.88455788599097,37
2095,"To this end , we present MPC-BERT , a pre-trained model for MPC understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks .",1,0.5655641,44.05015473334478,34
2095,"Particularly , these tasks can be generally categorized into ( 1 ) interlocutor structure modeling including reply-to utterance recognition , identical speaker searching and pointer consistency distinction , and ( 2 ) utterance semantics modeling including masked shared utterance restoration and shared node detection .",0,0.66044354,133.87714999360014,47
2095,"We evaluate MPC-BERT on three downstream tasks including addressee recognition , speaker identification and response selection .",2,0.64195603,60.47834609181317,19
2095,Experimental results show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks .,3,0.9659531,8.430137218778047,33
2096,"While Transformer-based text classifiers pre-trained on large volumes of text have yielded significant improvements on a wide range of computational linguistics tasks , their implementations have been unsuitable for live incremental processing thus far , operating only on the level of complete sentence inputs .",0,0.86120707,36.416291422745815,47
2096,"We address the challenge of introducing methods for word-by-word left-to-right incremental processing to Transformers such as BERT , models without an intrinsic sense of linear order .",1,0.6300822,49.24905420240567,32
2096,We modify the training method and live decoding of non-incremental models to detect speech disfluencies with minimum latency and without pre-segmentation of dialogue acts .,2,0.6466873,85.85408047644025,25
2096,"We experiment with several decoding methods to predict the rightward context of the word currently being processed using a GPT-2 language model and apply a BERT-based disfluency detector to sequences , including predicted words .",2,0.8558922,69.76503291388254,39
2096,We show our method of incrementalising Transformers maintains most of their high non-incremental performance while operating strictly incrementally .,3,0.9540434,141.13435040393287,19
2096,"We also evaluate our models ’ incremental performance to establish the trade-off between incremental performance and final performance , using different prediction strategies .",2,0.6153674,44.617864445562134,25
2096,We apply our system to incremental speech recognition results as they arrive into a live system and achieve state-of-the-art results in this setting .,3,0.49986252,26.600158320370646,30
2097,"We propose NeuralWOZ , a novel dialogue collection framework that uses model-based dialogue simulation .",1,0.35844147,67.45647161305372,17
2097,"NeuralWOZ has two pipelined models , Collector and Labeler .",2,0.4073406,80.86840624523829,10
2097,"Collector generates dialogues from ( 1 ) user ’s goal instructions , which are the user context and task constraints in natural language , and ( 2 ) system ’s API call results , which is a list of possible query responses for user requests from the given knowledge base .",2,0.71317536,54.152834737936956,51
2097,"Labeler annotates the generated dialogue by formulating the annotation as a multiple-choice problem , in which the candidate labels are extracted from goal instructions and API call results .",2,0.6221719,52.6929709985709,30
2097,We demonstrate the effectiveness of the proposed method in the zero-shot domain transfer learning for dialogue state tracking .,3,0.8223892,22.53598797042893,19
2097,"In the evaluation , the synthetic dialogue corpus generated from NeuralWOZ achieves a new state-of-the-art with improvements of 4.4 % point joint goal accuracy on average across domains , and improvements of 5.7 % point of zero-shot coverage against the MultiWOZ 2.1 dataset .",3,0.9276787,30.337353488916115,50
2098,"The human mind is a dynamical system , yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes .",0,0.9585195,21.790125307817302,32
2098,"This study proposes the continuous-time deconvolutional regressive neural network ( CDRNN ) , a deep neural extension of continuous-time deconvolutional regression ( Shain & Schuler , 2021 ) that jointly captures time-varying , non-linear , and delayed influences of predictors ( e.g .",1,0.6231746,36.49084457800462,49
2098,word surprisal ) on the response ( e.g .,2,0.36013868,102.43135113063607,9
2098,reading time ) .,3,0.62757564,704.2115712471077,4
2098,"Despite this flexibility , CDRNN is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study .",3,0.8360482,70.46320886288858,22
2098,"Behavioral and fMRI experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than CDR and other baselines , supporting a potential role for CDRNN in studying human language processing .",3,0.9338041,48.37205273139904,35
2099,Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations .,0,0.92378163,16.723910410840777,23
2099,Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data .,1,0.8561368,28.6590964609464,28
2099,We explore two general ideas .,2,0.37217698,214.0002722660878,6
2099,The “ Generative Parsing ” idea jointly models the incremental parse and word sequence as part of the same sequence modeling task .,0,0.51886034,81.59268369261238,23
2099,The “ Structural Scaffold ” idea guides the language model ’s representation via additional structure loss that separately predicts the incremental constituency parse .,3,0.4201957,187.131757432027,24
2099,"We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset , and evaluate models ’ syntactic generalization performances on SG Test Suites and sized BLiMP .",2,0.7669226,99.9103058911486,46
2099,Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training .,3,0.9729973,40.9144865127112,32
2100,"While the use of character models has been popular in NLP applications , it has not been explored much in the context of psycholinguistic modeling .",0,0.8992614,14.274247516701314,26
2100,This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities .,1,0.85023475,44.46880284973642,24
2100,"Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading , eye-tracking , and fMRI data than those from large-scale language models trained on much more data .",3,0.9735411,56.80797190163246,41
2100,"This may suggest that the proposed processing model provides a more humanlike account of sentence processing , which assumes a larger role of morphology , phonotactics , and orthographic complexity than was previously thought .",3,0.9798707,69.5825388042905,35
2101,"Most previous studies integrate cognitive language processing signals ( e.g. , eye-tracking or EEG data ) into neural models of natural language processing ( NLP ) just by directly concatenating word embeddings with cognitive features , ignoring the gap between the two modalities ( i.e. , textual vs .",0,0.8161922,33.78256720375075,49
2101,cognitive ) and noise in cognitive features .,3,0.4599596,454.40743794612087,8
2101,"In this paper , we propose a CogAlign approach to these issues , which learns to align textual neural representations to cognitive features .",1,0.875967,51.94636600832559,24
2101,"In CogAlign , we use a shared encoder equipped with a modality discriminator to alternatively encode textual and cognitive inputs to capture their differences and commonalities .",2,0.79061145,56.32958915032959,27
2101,"Additionally , a text-aware attention mechanism is proposed to detect task-related information and to avoid using noise in cognitive features .",2,0.5849734,43.779273585415005,25
2101,"Experimental results on three NLP tasks , namely named entity recognition , sentiment analysis and relation extraction , show that CogAlign achieves significant improvements with multiple cognitive features over state-of-the-art models on public datasets .",3,0.91728944,19.615415001085367,41
2101,"Moreover , our model is able to transfer cognitive information to other datasets that do not have any cognitive processing signals .",3,0.9252739,28.855913313753184,22
2102,"Despite their impressive performance in NLP , self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure , such as Dyck-k , the language consisting of well-nested parentheses of k types .",0,0.88890886,73.98430935462818,39
2102,"This suggested that natural language can be approximated well with models that are too weak for formal languages , or that the role of hierarchy and recursion in natural language might be limited .",3,0.9679204,45.92955731401159,34
2102,"We qualify this implication by proving that self-attention networks can process Dyck-( k , D ) , the subset of Dyck-k with depth bounded by D , which arguably better captures the bounded hierarchical structure of natural language .",3,0.57873327,96.25288352563167,40
2102,"Specifically , we construct a hard-attention network with D+1 layers and O( log k ) memory size ( per token per layer ) that recognizes Dyck-( k , D ) , and a soft-attention network with two layers and O( log k ) memory size that generates Dyck-( k , D ) .",2,0.85706055,35.28039095151999,57
2102,"Experiments show that self-attention networks trained on Dyck-( k , D ) generalize to longer inputs with near-perfect accuracy , and also verify the theoretical memory advantage of self-attention networks over recurrent networks .",3,0.9329003,50.19435722953408,35
2103,We present a novel approach to the problem of text style transfer .,1,0.65842855,17.04544718099937,13
2103,"Unlike previous approaches requiring style-labeled training data , our method makes use of readily-available unlabeled text by relying on the implicit connection in style between adjacent sentences , and uses labeled data only at inference time .",2,0.59126955,48.98447122196671,40
2103,"We adapt T5 ( Raffel et al. , 2020 ) , a strong pretrained text-to-text model , to extract a style vector from text and use it to condition the decoder to perform style transfer .",2,0.83926314,38.488514380398485,40
2103,"As our label-free training results in a style vector space encoding many facets of style , we recast transfers as “ targeted restyling ” vector operations that adjust specific attributes of the input while preserving others .",2,0.4782753,142.5661304543157,39
2103,"We demonstrate that training on unlabeled Amazon reviews data results in a model that is competitive on sentiment transfer , even compared to models trained fully on labeled data .",3,0.95009995,46.27459274351736,30
2103,"Furthermore , applying our novel method to a diverse corpus of unlabeled web text results in a single model capable of transferring along multiple dimensions of style ( dialect , emotiveness , formality , politeness , sentiment ) despite no additional training and using only a handful of exemplars at inference time .",3,0.9180424,40.72891436014054,53
2104,We describe an efficient hierarchical method to compute attention in the Transformer architecture .,1,0.48528227,42.221555696734555,14
2104,"The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix ( H-Matrix ) developed by the numerical analysis community , and has linear run time and memory complexity .",3,0.43796206,74.8915444331182,32
2104,We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks .,3,0.6617402,39.3334112973705,33
2104,Our method is superior to alternative sub-quadratic proposals by over + 6 points on average on the Long Range Arena benchmark .,3,0.896563,59.74058449255088,22
2104,It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models .,3,0.46639916,37.28217211722275,29
2105,"The recent GPT-3 model ( Brown et al. , 2020 ) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context .",0,0.86592215,59.15217797416899,34
2105,"Inspired by their findings , we study few-shot learning in a more practical scenario , where we use smaller language models for which fine-tuning is computationally efficient .",3,0.32105,35.05100127589798,28
2105,We present LM-BFF — better few-shot fine-tuning of language models — a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples .,3,0.36838275,18.844168069048536,32
2105,Our approach includes ( 1 ) prompt-based fine-tuning together with a novel pipeline for automating prompt generation ;,2,0.608203,77.85732217426582,19
2105,and ( 2 ) a refined strategy for dynamically and selectively incorporating demonstrations into each context .,3,0.4102057,165.6096388278954,17
2105,"Finally , we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks , including classification and regression .",3,0.5994927,27.703360045619874,23
2105,"Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting , achieving up to 30 % absolute improvement , and 11 % on average across all tasks .",3,0.95656127,46.09574576874629,36
2105,"Our approach makes minimal assumptions on task resources and domain expertise , and hence constitutes a strong task-agnostic method for few-shot learning .",3,0.8146567,48.2583463537589,25
2106,The Universal Trigger ( UniTrigger ) is a recently-proposed powerful adversarial textual attack method .,0,0.76783043,70.06246941207351,17
2106,"Utilizing a learning-based mechanism , UniTrigger generates a fixed phrase that , when added to any benign inputs , can drop the prediction accuracy of a textual neural network ( NN ) model to near zero on a target class .",2,0.41407394,101.11954632716348,43
2106,"To defend against this attack that can cause significant harm , in this paper , we borrow the “ honeypot ” concept from the cybersecurity community and propose DARCY , a honeypot-based defense framework against UniTrigger .",1,0.77386576,51.95561841136509,39
2106,DARCY greedily searches and injects multiple trapdoors into an NN model to “ bait and catch ” potential attacks .,0,0.37781274,170.72901483256453,20
2106,"Through comprehensive experiments across four public datasets , we show that DARCY detects UniTrigger ’s adversarial attacks with up to 99 % TPR and less than 2 % FPR in most cases , while maintaining the prediction accuracy ( in F1 ) for clean inputs within a 1 % margin .",3,0.87090975,92.36223769206576,51
2106,We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers ’ varying levels of knowledge and skills .,3,0.9348494,64.92802089302242,28
2106,We release the source code of DARCY at : https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP .,3,0.4723261,52.0993844230166,11
2107,"Detecting rumors on social media is a very critical task with significant implications to the economy , public health , etc .",0,0.92874885,47.59019682633407,22
2107,Previous works generally capture effective features from texts and the propagation structure .,0,0.87081194,252.49022804374275,13
2107,"However , the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data .",0,0.6779463,128.04828962180952,29
2107,Most approaches neglect it and may seriously limit the learning of features .,0,0.8585682,125.46238313056844,13
2107,"Towards this issue , this paper makes the first attempt to explore propagation uncertainty for rumor detection .",1,0.81476414,58.926735376503764,18
2107,"Specifically , we propose a novel Edge-enhanced Bayesian Graph Convolutional Network ( EBGCN ) to capture robust structural features .",2,0.44899195,20.54189911442429,20
2107,The model adaptively rethinks the reliability of latent relations by adopting a Bayesian approach .,2,0.6725652,56.289689266886256,15
2107,"Besides , we design a new edge-wise consistency training framework to optimize the model by enforcing consistency on relations .",2,0.737721,55.96104473493445,20
2107,Experiments on three public benchmark datasets demonstrate that the proposed model achieves better performance than baseline methods on both rumor detection and early rumor detection tasks .,3,0.905319,13.441016268044644,27
2108,Multi-label text classification is one of the fundamental tasks in natural language processing .,0,0.9482174,6.19301518749654,14
2108,"Previous studies have difficulties to distinguish similar labels well because they learn the same document representations for different labels , that is they do not explicitly extract label-specific semantic components from documents .",0,0.91108954,59.23479444928199,35
2108,"Moreover , they do not fully explore the high-order interactions among these semantic components , which is very helpful to predict tail labels .",0,0.54215306,78.08698298964642,24
2108,"In this paper , we propose a novel label-specific dual graph neural network ( LDGN ) , which incorporates category information to learn label-specific components from documents , and employs dual Graph Convolution Network ( GCN ) to model complete and adaptive interactions among these components based on the statistical label co-occurrence and dynamic reconstruction graph in a joint way .",1,0.66914,63.295049635202965,64
2108,"Experimental results on three benchmark datasets demonstrate that LDGN significantly outperforms the state-of-the-art models , and also achieves better performance with respect to tail labels .",3,0.92761254,13.517866445822175,32
2109,Topic models have been widely used to learn text representations and gain insight into document corpora .,0,0.9455684,20.758254275877512,17
2109,"To perform topic discovery , most existing neural models either take document bag-of-words ( BoW ) or sequence of tokens as input followed by variational inference and BoW reconstruction to learn topic-word distribution .",0,0.8147479,72.02527625789399,34
2109,"However , leveraging topic-word distribution for learning better features during document encoding has not been explored much .",0,0.8828429,158.75646612005377,18
2109,"To this end , we develop a framework TAN-NTM , which processes document as a sequence of tokens through a LSTM whose contextual outputs are attended in a topic-aware manner .",2,0.7443833,54.374116075002824,33
2109,We propose a novel attention mechanism which factors in topic-word distribution to enable the model to attend on relevant words that convey topic related cues .,3,0.33596924,72.62286018324349,26
2109,The output of topic attention module is then used to carry out variational inference .,2,0.6634431,52.24202865907766,15
2109,"We perform extensive ablations and experiments resulting in ~9-15 percentage improvement over score of existing SOTA topic models in NPMI coherence on several benchmark datasets-20 Newsgroups , Yelp Review Polarity and AGNews .",3,0.46849284,161.71645383974143,37
2109,"Further , we show that our method learns better latent document-topic features compared to existing topic models through improvement on two downstream tasks : document classification and topic guided keyphrase generation .",3,0.9408906,55.59963050458942,32
2110,This paper proposes an approach to cross-language sentence selection in a low-resource setting .,1,0.8882316,17.557122017539037,14
2110,It uses data augmentation and negative sampling techniques on noisy parallel sentence data to directly learn a cross-lingual embedding-based query relevance model .,2,0.6848864,42.03528634309852,25
2110,Results show that this approach performs as well as or better than multiple state-of-the-art machine translation + monolingual retrieval systems trained on the same parallel data .,3,0.9851247,13.184544807732026,33
2110,"Moreover , when a rationale training secondary objective is applied to encourage the model to match word alignment hints from a phrase-based statistical machine translation model , consistent improvements are seen across three language pairs ( English-Somali , English-Swahili and English-Tagalog ) over a variety of state-of-the-art baselines .",3,0.9077739,26.674870003446973,62
2111,"Question answering ( QA ) systems for large document collections typically use pipelines that ( i ) retrieve possibly relevant documents , ( ii ) re-rank them , ( iii ) rank paragraphs or other snippets of the top-ranked documents , and ( iv ) select spans of the top-ranked snippets as exact answers .",0,0.92175376,60.0317976105745,55
2111,"Pipelines are conceptually simple , but errors propagate from one component to the next , without later components being able to revise earlier decisions .",0,0.8795384,90.02490079166705,25
2111,"We present an architecture for joint document and snippet ranking , the two middle stages , which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents .",2,0.38279203,90.44350993851434,34
2111,The architecture is general and can be used with any neural text relevance ranker .,3,0.60570157,59.96802550904042,15
2111,"We experiment with two main instantiations of the architecture , based on POSIT-DRMM ( PDRMM ) and a BERT-based ranker .",2,0.83947855,52.88266089845324,25
2111,"Experiments on biomedical data from BIOASQ show that our joint models vastly outperform the pipelines in snippet retrieval , the main goal for QA , with fewer trainable parameters , also remaining competitive in document retrieval .",3,0.9326392,108.03508295393888,37
2111,"Furthermore , our joint PDRMM-based model is competitive with BERT-based models , despite using orders of magnitude fewer parameters .",3,0.9599828,32.393938286887746,24
2111,These claims are also supported by human evaluation on two test batches of BIOASQ .,3,0.7185621,48.907673251360364,15
2111,"To test our key findings on another dataset , we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval .",2,0.6429413,50.750809398230366,28
2111,"Our joint PDRMM-based model again outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions dataset , even though it performs worse than the pipeline in document retrieval .",3,0.9383532,72.00429493003284,33
2111,We make our code and the modified Natural Questions dataset publicly available .,2,0.51091516,37.644368381153825,13
2112,"Aiming for a better integration of data-driven and linguistically-inspired approaches , we explore whether RST Nuclearity , assigning a binary assessment of importance between text segments , can be replaced by automatically generated , real-valued scores , in what we call a Weighted-RST framework .",1,0.5726229,87.31984785593859,49
2112,"In particular , we find that weighted discourse trees from auxiliary tasks can benefit key NLP downstream applications , compared to nuclearity-centered approaches .",3,0.9748309,176.70882587127895,25
2112,We further show that real-valued importance distributions partially and interestingly align with the assessment and uncertainty of human annotators .,3,0.96865493,100.75260306984374,20
2113,Atomic clauses are fundamental text units for understanding complex sentences .,0,0.9455087,155.34013057655346,11
2113,"Identifying the atomic sentences within complex sentences is important for applications such as summarization , argument mining , discourse analysis , discourse parsing , and question answering .",0,0.9311774,51.66667621599256,28
2113,Previous work mainly relies on rule-based methods dependent on parsing .,0,0.8736922,58.91282828161032,11
2113,"We propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source , and a novel problem formulation as a graph edit task .",2,0.515441,57.28367015199964,33
2113,"Our neural model learns to Accept , Break , Copy or Drop elements of a graph that combines word adjacency and grammatical dependencies .",2,0.70885116,126.581769710448,24
2113,"The full processing pipeline includes modules for graph construction , graph editing , and sentence generation from the output graph .",2,0.4944933,101.40694487206329,21
2113,"We introduce DeSSE , a new dataset designed to train and evaluate complex sentence decomposition , and MinWiki , a subset of MinWikiSplit .",2,0.58771986,127.98687982044086,24
2113,ABCD achieves comparable performance as two parsing baselines on MinWiki .,3,0.8628583,121.25652055221235,11
2113,"On DeSSE , which has a more even balance of complex sentence types , our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline .",3,0.9163364,47.98656783371819,30
2113,Results include a detailed error analysis .,3,0.91548216,109.4648365915586,7
2114,"Many Question-Answering ( QA ) datasets contain unanswerable questions , but their treatment in QA systems remains primitive .",0,0.9660215,37.10645667813091,21
2114,Our analysis of the Natural Questions ( Kwiatkowski et al .,0,0.55979437,69.83196405177488,11
2114,2019 ) dataset reveals that a substantial portion of unanswerable questions ( ~ 21 % ) can be explained based on the presence of unverifiable presuppositions .,3,0.7394919,46.970857631645224,27
2114,"Through a user preference study , we demonstrate that the oracle behavior of our proposed system — which provides responses based on presupposition failure — is preferred over the oracle behavior of existing QA systems .",3,0.8429533,31.705896593874495,36
2114,"Then , we present a novel framework for implementing such a system in three steps : presupposition generation , presupposition verification , and explanation generation , reporting progress on each .",2,0.3718351,67.0197936100788,31
2114,"Finally , we show that a simple modification of adding presuppositions and their verifiability to the input of a competitive end-to-end QA system yields modest gains in QA performance and unanswerability detection , demonstrating the promise of our approach .",3,0.9432866,27.65328005295623,42
2115,Text-level discourse rhetorical structure ( DRS ) parsing is known to be challenging due to the notorious lack of training data .,0,0.9670046,41.724814730300686,23
2115,"Although recent top-down DRS parsers can better leverage global document context and have achieved certain success , the performance is still far from perfect .",0,0.84496653,64.2370414073493,25
2115,"To our knowledge , all previous DRS parsers make local decisions for either bottom-up node composition or top-down split point ranking at each time step , and largely ignore DRS parsing from the global view point .",0,0.7543609,88.25773368835664,39
2115,"Obviously , it is not sufficient to build an entire DRS tree only through these local decisions .",0,0.6289891,109.32201472794965,18
2115,"In this work , we present our insight on evaluating the pros and cons of the entire DRS tree for global optimization .",1,0.8932987,67.39676624922042,23
2115,"Specifically , based on recent well-performing top-down frameworks , we introduce a novel method to transform both gold standard and predicted constituency trees into tree diagrams with two color channels .",2,0.7176572,80.88741908301056,33
2115,"After that , we learn an adversarial bot between gold and fake tree diagrams to estimate the generated DRS trees from a global perspective .",2,0.83001786,168.9603284123466,25
2115,We perform experiments on both RST-DT and CDTB corpora and use the original Parseval for performance evaluation .,2,0.7202609,60.58193544642314,20
2115,The experimental results show that our parser can substantially improve the performance when compared with previous state-of-the-art parsers .,3,0.98204285,6.163303604122637,25
2116,Discourse relations among arguments reveal logical structures of a debate conversation .,0,0.8065694,244.32304076727988,12
2116,"However , no prior work has explicitly studied how the sequence of discourse relations influence a claim ’s impact .",0,0.9341431,100.73050590431723,20
2116,This paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument .,3,0.7069064,46.06593931660085,27
2116,We further propose DisCOC to inject and fuse the sentence-level structural discourse information with contextualized features derived from large-scale language models .,3,0.60803354,51.24104351464701,24
2116,Experimental results and extensive analysis show that the attention and gate mechanisms that explicitly model contexts and texts can indeed help the argument impact classification task defined by Durmus et al .,3,0.94473827,139.40331157567027,32
2116,"( 2019 ) , and discourse structures among the context path of the claim to be classified can further boost the performance .",3,0.6749951,153.5609676423768,23
2117,This paper proposes a sophisticated neural architecture to incorporate bilingual dictionaries into Neural Machine Translation ( NMT ) models .,1,0.7604142,27.650675918564374,20
2117,"Pointer , Disambiguator , and Copier , our method PDC achieves the following merits inherently compared with previous efforts : ( 1 ) Pointer leverages the semantic information from bilingual dictionaries , for the first time , to better locate source words whose translation in dictionaries can potentially be used ;",3,0.6580937,107.43879833654951,51
2117,"( 2 ) Disambiguator synthesizes contextual information from the source view and the target view , both of which contribute to distinguishing the proper translation of a specific source word from multiple candidates in dictionaries ;",2,0.37231454,66.63746191028903,36
2117,"( 3 ) Copier systematically connects Pointer and Disambiguator based on a hierarchical copy mechanism seamlessly integrated with Transformer , thereby building an end-to-end architecture that could avoid error propagation problems in alternative pipe-line methods .",3,0.68270403,77.97703171116513,38
2117,The experimental results on Chinese-English and English-Japanese benchmarks demonstrate the PDC ’s overall superiority and effectiveness of each component .,3,0.9736651,40.54670692200574,22
2118,Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages .,0,0.90347564,12.48969039803783,22
2118,"However , much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages , which is loose and implicit for aligning the contextual representations between languages .",0,0.75251985,72.8079814439055,35
2118,"In this paper , we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages .",2,0.46542794,24.36699217794509,21
2118,It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language .,3,0.7319616,64.51940049580348,20
2118,"More importantly , when fine-tuning on downstream tasks , the cross-attention module can be plugged in or out on-demand , thus naturally benefiting a wider range of cross-lingual tasks , from language understanding to generation .",3,0.7664642,40.72045736673721,37
2118,"As a result , the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark , covering text classification , sequence labeling , question answering , and sentence retrieval .",3,0.93426734,20.44070093565146,42
2118,"For cross-lingual generation tasks , it also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets , with gains of up to 1 2 BLEU .",3,0.82109034,9.69163480848534,46
2119,The sentence is a fundamental unit of text processing .,0,0.9361643,48.1681341518987,10
2119,"Yet sentences in the wild are commonly encountered not in isolation , but unsegmented within larger paragraphs and documents .",0,0.90417206,91.46460897399102,20
2119,"Therefore , the first step in many NLP pipelines is sentence segmentation .",0,0.8687836,36.817770943599044,13
2119,"Despite its importance , this step is the subject of relatively little research .",0,0.9023258,52.44368480411707,14
2119,"There are no standard test sets or even methods for evaluation , leaving researchers and engineers without a clear footing for evaluating and selecting models for the task .",0,0.86609983,41.28828935036751,29
2119,"Existing tools have relatively small language coverage , and efforts to extend them to other languages are often ad hoc .",0,0.90668327,33.38680725355484,21
2119,"We introduce a modern context-based modeling approach that provides a solution to the problem of segmenting punctuated text in many languages , and show how it can be trained on noisily-annotated data .",1,0.46958584,29.081359408146533,36
2119,We also establish a new 23-language multilingual evaluation set .,3,0.46012753,93.76163358721729,12
2119,"Our approach exceeds high baselines set by existing methods on prior English corpora ( WSJ and Brown corpora ) , and also performs well on average on our new evaluation set .",3,0.86833125,89.20199256712682,32
2119,"We release our tool , ersatz , as open source .",3,0.49282444,116.96844992288635,11
2120,"A good translation should not only translate the original content semantically , but also incarnate personal traits of the original text .",0,0.7423824,67.48211258955783,22
2120,"For a real-world neural machine translation ( NMT ) system , these user traits ( e.g. , topic preference , stylistic characteristics and expression habits ) can be preserved in user behavior ( e.g. , historical inputs ) .",0,0.7110214,70.88117792746182,39
2120,"However , current NMT systems marginally consider the user behavior due to : 1 ) the difficulty of modeling user portraits in zero-shot scenarios , and 2 ) the lack of user-behavior annotated parallel dataset .",0,0.8401391,97.6944554500437,36
2120,"To fill this gap , we introduce a novel framework called user-driven NMT .",2,0.39618495,31.147041743367886,14
2120,"Specifically , a cache-based module and a user-driven contrastive learning method are proposed to offer NMT the ability to capture potential user traits from their historical inputs under a zero-shot learning fashion .",2,0.6131094,44.8547657222288,34
2120,"Furthermore , we contribute the first Chinese-English parallel corpus annotated with user behavior called UDT-Corpus .",3,0.49712878,59.61773462718881,18
2120,Experimental results confirm that the proposed user-driven NMT can generate user-specific translations .,3,0.9846736,28.898441058818953,13
2121,Lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases .,0,0.8422832,29.105685488774554,24
2121,"Although current approaches can enforce terms to appear in the translation , they often struggle to make the constraint word form agree with the rest of the generated output .",0,0.8251004,67.23960563869535,30
2121,Our manual analysis shows that 46 % of the errors in the output of a baseline constrained model for English to Czech translation are related to agreement .,3,0.97317487,68.14564565606537,28
2121,We investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints .,1,0.71579254,42.18887267499237,18
2121,"In particular , we focus on methods based on training the model with constraints provided as part of the input sequence .",2,0.5862176,39.10648757470822,22
2121,Our experiments on English-Czech language pair show that this approach improves translation of constrained terms in both automatic and manual evaluation by reducing errors in agreement .,3,0.96375316,47.189684935604745,29
2121,"Our approach thus eliminates inflection errors , without introducing new errors or decreasing overall quality of the translation .",3,0.8990934,121.13331177239849,19
2122,Technical logbooks are a challenging and under-explored text type in automated event identification .,0,0.8779067,56.19577046928418,14
2122,"These texts are typically short and written in non-standard yet technical language , posing challenges to off-the-shelf NLP pipelines .",0,0.90993124,30.278339144066404,21
2122,"The granularity of issue types described in these datasets additionally leads to class imbalance , making it challenging for models to accurately predict which issue each logbook entry describes .",0,0.71042114,68.94547292143311,30
2122,"In this paper we focus on the problem of technical issue classification by considering logbook datasets from the automotive , aviation , and facilities maintenance domains .",1,0.7770076,50.083578986363484,27
2122,"We adapt a feedback strategy from computer vision for handling extreme class imbalance , which resamples the training data based on its error in the prediction process .",2,0.8227263,102.18080003011671,28
2122,Our experiments show that with statistical significance this feedback strategy provides the best results for four different neural network models trained across a suite of seven different technical logbook datasets from distinct technical domains .,3,0.9720205,89.23883533168669,35
2122,The feedback strategy is also generic and could be applied to any learning problem with substantial class imbalances .,3,0.7890684,63.480968139513855,19
2123,An automated system that could assist a judge in predicting the outcome of a case would help expedite the judicial process .,0,0.76947945,21.635816627626415,22
2123,"For such a system to be practically useful , predictions by the system should be explainable .",0,0.6101228,69.15168158820408,17
2123,"To promote research in developing such a system , we introduce ILDC ( Indian Legal Documents Corpus ) .",1,0.51739544,186.86461109986968,19
2123,ILDC is a large corpus of 35 k Indian Supreme Court cases annotated with original court decisions .,0,0.48912773,54.32975909181278,18
2123,A portion of the corpus ( a separate test set ) is annotated with gold standard explanations by legal experts .,2,0.7202632,70.01788340574285,21
2123,"Based on ILDC , we propose the task of Court Judgment Prediction and Explanation ( CJPE ) .",0,0.35490513,64.89722290719259,18
2123,The task requires an automated system to predict an explainable outcome of a case .,0,0.88161445,50.794448418183656,15
2123,We experiment with a battery of baseline models for case predictions and propose a hierarchical occlusion based model for explainability .,2,0.7379602,65.24778757111618,21
2123,"Our best prediction model has an accuracy of 78 % versus 94 % for human legal experts , pointing towards the complexity of the prediction task .",3,0.9619726,56.31876559371039,27
2123,"The analysis of explanations by the proposed algorithm reveals a significant difference in the point of view of the algorithm and legal experts for explaining the judgments , pointing towards scope for future research .",3,0.97984445,53.87717787076336,35
2124,We present an annotation approach to capturing emotional and cognitive empathy in student-written peer reviews on business models in German .,1,0.6723037,113.32641614795556,21
2124,We propose an annotation scheme that allows us to model emotional and cognitive empathy scores based on three types of review components .,3,0.3134548,52.19693441927503,23
2124,"Also , we conducted an annotation study with three annotators based on 92 student essays to evaluate our annotation scheme .",2,0.861972,42.681520862398436,21
2124,The obtained inter-rater agreement of α =0.79 for the components and the multi-π =0.41 for the empathy scores indicate that the proposed annotation scheme successfully guides annotators to a substantial to moderate agreement .,3,0.9624856,58.56079810205166,36
2124,"Moreover , we trained predictive models to detect the annotated empathy structures and embedded them in an adaptive writing support system for students to receive individual empathy feedback independent of an instructor , time , and location .",2,0.65128815,111.92715981594077,38
2124,"We evaluated our tool in a peer learning exercise with 58 students and found promising results for perceived empathy skill learning , perceived feedback accuracy , and intention to use .",3,0.9124239,114.38583980954571,31
2124,"Finally , we present our freely available corpus of 500 empathy-annotated , student-written peer reviews on business models and our annotation guidelines to encourage future research on the design and development of empathy support systems .",3,0.8337351,57.5494038295194,39
2125,The current state-of-the-art generative models for open-domain question answering ( ODQA ) have focused on generating direct answers from unstructured textual information .,0,0.96609336,9.71516047622201,28
2125,"However , a large amount of world ’s knowledge is stored in structured databases , and need to be accessed using query languages such as SQL .",0,0.94801927,45.75004772334114,27
2125,"Furthermore , query languages can answer questions that require complex reasoning , as well as offering full explainability .",0,0.6742252,88.35091599118293,19
2125,"In this paper , we propose a hybrid framework that takes both textual and tabular evidences as input and generates either direct answers or SQL queries depending on which form could better answer the question .",1,0.8048145,28.192292473357767,36
2125,The generated SQL queries can then be executed on the associated databases to obtain the final answers .,2,0.41645864,33.474514482498066,18
2125,"To the best of our knowledge , this is the first paper that applies Text2SQL to ODQA tasks .",3,0.88324815,17.10568766645599,19
2125,"Empirically , we demonstrate that on several ODQA datasets , the hybrid methods consistently outperforms the baseline models that only takes homogeneous input by a large margin .",3,0.90670836,46.559326087567044,28
2125,Specifically we achieve the state-of-the-art performance on OpenSQuAD dataset using a T5-base model .,3,0.65204555,8.739105312695468,20
2125,"In a detailed analysis , we demonstrate that the being able to generate structural SQL queries can always bring gains , especially for those questions that requires complex reasoning .",3,0.9347441,62.35078488232334,30
2126,"We propose Generation-Augmented Retrieval ( GAR ) for answering open-domain questions , which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision .",2,0.3473337,62.046547209430955,32
2126,We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations ( BM25 ) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR .,3,0.94762427,60.835695164423186,41
2126,We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy .,3,0.9563695,81.3493374636103,21
2126,"Moreover , as sparse and dense representations are often complementary , GAR can be easily combined with DPR to achieve even better performance .",3,0.69829756,86.10014479500519,24
2126,"GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader , and consistently outperforms other retrieval methods when the same generative reader is used .",3,0.865749,19.178954400135652,40
2127,"While sophisticated neural-based models have achieved remarkable success in Visual Question Answering ( VQA ) , these models tend to answer questions only according to superficial correlations between question and answer .",0,0.94442165,18.18755418247799,34
2127,Several recent approaches have been developed to address this language priors problem .,0,0.88577163,34.11413188869586,13
2127,"However , most of them predict the correct answer according to one best output without checking the authenticity of answers .",0,0.8225307,89.94723604741917,21
2127,"Besides , they only explore the interaction between image and question , ignoring the semantics of candidate answers .",0,0.49373552,94.67354349295653,19
2127,"In this paper , we propose a select-and-rerank ( SAR ) progressive framework based on Visual Entailment .",1,0.8658495,74.81027445170679,21
2127,"Specifically , we first select the candidate answers relevant to the question or the image , then we rerank the candidate answers by a visual entailment task , which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer .",2,0.9015181,43.668397867397466,46
2127,"Experimental results show the effectiveness of our proposed framework , which establishes a new state-of-the-art accuracy on VQA-CP v2 with a 7.55 % improvement .",3,0.95284086,16.17446742744412,33
2128,Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided .,0,0.8803928,80.56480791472919,25
2128,"This setting gives rise to the spurious solution problem : there may exist many spurious solutions that coincidentally derive the correct answer , but training on such solutions can hurt model performance ( e.g. , producing wrong solutions or answers ) .",0,0.74637073,71.20754443401208,42
2128,"For example , for discrete reasoning tasks as on DROP , there may exist many equations to derive a numeric answer , and typically only one of them is correct .",0,0.65679246,88.20993856650097,31
2128,"Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence , but do not explicitly exploit the semantic correlations between a question and its solution .",0,0.800201,61.92297095368931,30
2128,"In this paper , to alleviate the spurious solution problem , we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions .",1,0.7151756,51.80818513730732,33
2128,Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions .,3,0.91994,12.180254771648537,33
2129,Privacy plays a crucial role in preserving democratic ideals and personal autonomy .,0,0.9208623,30.937540762224618,13
2129,"The dominant legal approach to privacy in many jurisdictions is the “ Notice and Choice ” paradigm , where privacy policies are the primary instrument used to convey information to users .",0,0.950081,40.41074544702598,32
2129,"However , privacy policies are long and complex documents that are difficult for users to read and comprehend .",0,0.932373,22.399023678533403,19
2129,"We discuss how language technologies can play an important role in addressing this information gap , reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies : consumers , enterprises , and regulators .",3,0.4651128,65.78076414879813,41
2129,"Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy , limit privacy harms , and rally research efforts from the community towards addressing an issue with large social impact .",1,0.88974214,35.49921061013459,45
2129,We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies .,3,0.8719508,49.11098734690753,28
2130,Open pit mines left many regions worldwide inhospitable or uninhabitable .,0,0.926804,151.93202372258224,11
2130,"Many sites are left behind in a hazardous or contaminated state , show remnants of waste , or have other restrictions imposed upon them , e.g. , for the protection of human or nature .",0,0.8504396,83.74566790849723,35
2130,Such information has to be permanently managed in order to reuse those areas in the future .,0,0.5236094,54.72605053998926,17
2130,"In this work we present and evaluate an automated workflow for supporting the post-mining management of former lignite open pit mines in the eastern part of Germany , where prior to any planned land reuse , aforementioned information has to be acquired to ensure the safety and validity of such an endeavor .",1,0.8836633,72.11239177073277,53
2130,"Usually , this information is found in expert reports , either in the form of paper documents , or in the best case as digitized unstructured text — all of them in German language .",0,0.8673293,69.63923254234953,35
2130,"However , due to the size and complexity of these documents , any inquiry is tedious and time-consuming , thereby slowing down or even obstructing the reuse of related areas .",0,0.8736032,45.96543407467407,33
2130,"Since no training data is available , we employ active learning in order to perform multi-label sentence classification for two categories of restrictions and seven categories of topics .",2,0.7592707,45.310791498836366,29
2130,"The final system integrates optical character recognition ( OCR ) , active-learning-based text classification , and geographic information system visualization in order to effectively extract , query , and visualize this information for any area of interest .",2,0.41922602,83.25049036929266,41
2130,"Whereas the restriction categories were reasonably accurate ( >0.85 F1 ) , the seven topic-oriented categories seemed to be complex even for human annotators and achieved mediocre evaluation scores ( < 0.70 F1 ) .",3,0.97118765,60.204858072836046,35
2131,"Questions of fairness , robustness , and transparency are paramount to address before deploying NLP systems .",0,0.5940378,55.505830831241894,17
2131,"To address this , we argue for the need for reliability testing and contextualize it among existing work on improving accountability .",1,0.7662478,70.21621622194925,22
2131,"We show how adversarial attacks can be reframed for this goal , via a framework for developing reliability tests .",3,0.41401133,89.46481887428703,20
2131,"We argue that reliability testing — with an emphasis on interdisciplinary collaboration — will enable rigorous and targeted testing , and aid in the enactment and enforcement of industry standards .",3,0.81967646,70.44184283596405,31
2132,Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care .,0,0.9102451,35.19946433440153,16
2132,"The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection , intervention , and treatment of mental health disorders .",0,0.9023756,65.14345728786475,30
2132,One promising data source to help monitor human behavior is daily smartphone usage .,0,0.9186911,68.20351019875258,14
2132,"However , care must be taken to summarize behaviors without identifying the user through personal ( e.g. , personally identifiable information ) or protected ( e.g. , race , gender ) attributes .",0,0.58984286,49.6659906425099,33
2132,"In this paper , we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors .",1,0.9207816,68.49033747396281,28
2132,"Using computational models , we find that language and multimodal representations of mobile typed text ( spanning typed characters , words , keystroke timings , and app usage ) are predictive of daily mood .",3,0.814579,118.04148707687989,35
2132,"However , we find that models trained to predict mood often also capture private user identities in their intermediate representations .",3,0.97388,82.04215809189971,21
2132,"To tackle this problem , we evaluate approaches that obfuscate user identity while remaining predictive .",1,0.44675443,119.14464835600292,16
2132,"By combining multimodal representations with privacy-preserving learning , we are able to push forward the performance-privacy frontier .",3,0.6231819,43.07808144350208,22
2133,"This position paper investigates the problem of automated text anonymisation , which is a prerequisite for secure sharing of documents containing sensitive information about individuals .",1,0.8143268,40.66303320283083,26
2133,We summarise the key concepts behind text anonymisation and provide a review of current approaches .,1,0.6979571,30.85775345308855,16
2133,"Anonymisation methods have so far been developed in two fields with little mutual interaction , namely natural language processing and privacy-preserving data publishing .",0,0.9261401,49.0072618766055,26
2133,"Based on a case study , we outline the benefits and limitations of these approaches and discuss a number of open challenges , such as ( 1 ) how to account for multiple types of semantic inferences , ( 2 ) how to strike a balance between disclosure risk and data utility and ( 3 ) how to evaluate the quality of the resulting anonymisation .",3,0.46592808,23.84637112018788,66
2133,We lay out a case for moving beyond sequence labelling models and incorporate explicit measures of disclosure risk into the text anonymisation process .,3,0.81212443,86.94259801028355,24
2134,"Although parsing to Abstract Meaning Representation ( AMR ) has become very popular and AMR has been shown effective on the many sentence-level downstream tasks , little work has studied how to generate AMRs that can represent multi-sentence information .",0,0.9206049,22.082294941251764,42
2134,We introduce the first end-to-end AMR coreference resolution model in order to build multi-sentence AMRs .,1,0.42000857,15.548904451702686,17
2134,"Compared with the previous pipeline and rule-based approaches , our model alleviates error propagation and it is more robust for both in-domain and out-domain situations .",3,0.9059055,19.876344875360488,28
2134,"Besides , the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method ( Liu et al. , 2015 ) on text summarization .",3,0.88573587,29.741803097305684,32
2135,"Transformer language models have shown remarkable ability in detecting when a word is anomalous in context , but likelihood scores offer no information about the cause of the anomaly .",0,0.9012778,36.419234848328756,30
2135,"In this work , we use Gaussian models for density estimation at intermediate layers of three language models ( BERT , RoBERTa , and XLNet ) , and evaluate our method on BLiMP , a grammaticality judgement benchmark .",2,0.7837371,46.79368163974337,39
2135,"In lower layers , surprisal is highly correlated to low token frequency , but this correlation diminishes in upper layers .",3,0.921936,59.99282248458848,21
2135,"Next , we gather datasets of morphosyntactic , semantic , and commonsense anomalies from psycholinguistic studies ;",2,0.8703524,73.58231368051578,17
2135,"we find that the best performing model RoBERTa exhibits surprisal in earlier layers when the anomaly is morphosyntactic than when it is semantic , while commonsense anomalies do not exhibit surprisal at any intermediate layer .",3,0.9798371,60.08271513845886,36
2135,These results suggest that language models employ separate mechanisms to detect different types of linguistic anomalies .,3,0.9904578,32.49428798594865,17
2136,"Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner , without the exploitation of psycholinguistic knowledge that may unveil the connections between one ’s language use and his psychological traits .",0,0.911766,47.92358134825075,50
2136,"In this paper , we propose a psycholinguistic knowledge-based tripartite graph network , TrigNet , which consists of a tripartite graph network and a BERT-based graph initializer .",1,0.805025,16.7381949968157,32
2136,"The graph network injects structural psycholinguistic knowledge in LIWC , a computerized instrument for psycholinguistic analysis , by constructing a heterogeneous tripartite graph .",2,0.44919765,67.43051884473836,24
2136,The initializer is employed to provide initial embeddings for the graph nodes .,2,0.67665803,50.27613627775828,13
2136,"To reduce the computational cost in graph learning , we further propose a novel flow graph attention network ( GAT ) that only transmits messages between neighboring parties in the tripartite graph .",2,0.68636495,53.08325875616968,33
2136,"Benefiting from the tripartite graph , TrigNet can aggregate post information from a psychological perspective , which is a novel way of exploiting domain knowledge .",3,0.6332372,83.12898743342637,26
2136,Extensive experiments on two datasets show that TrigNet outperforms the existing state-of-art model by 3.47 and 2.10 points in average F1 .,3,0.92740446,13.087020547767809,26
2136,"Moreover , the flow GAT reduces the FLOPS and Memory measures by 38 % and 32 % , respectively , in comparison to the original GAT in our setting .",3,0.9658453,77.2586151090421,30
2137,Correct natural language understanding requires computers to distinguish the literal and metaphorical senses of a word .,0,0.93594384,25.274522851365674,17
2137,Recent neu-ral models achieve progress on verb metaphor detection by viewing it as sequence labeling .,0,0.8310859,216.41156902372188,16
2137,"In this paper , we argue that it is appropriate to view this task as relation classification between a verb and its various contexts .",1,0.8246278,36.73591029585745,25
2137,"We propose the Metaphor-relation BERT ( Mr-BERT ) model , which explicitly models the relation between a verb and its grammatical , sentential and semantic contexts .",0,0.3820352,36.917034928717904,28
2137,"We evaluate our method on the VUA , MOH-X and TroFi datasets .",2,0.46976033,137.38453950005547,14
2137,Our method gets competitive results compared with state-of-the-art approaches .,3,0.9112633,9.157630352048606,15
2138,Pretraining and multitask learning are widely used to improve the speech translation performance .,0,0.79508954,23.70578878163944,14
2138,"In this study , we are interested in training a speech translation model along with an auxiliary text translation task .",1,0.7797588,29.11492321087683,21
2138,We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework .,2,0.65007174,19.88267704070076,23
2138,Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules .,3,0.9884616,43.47643995151487,26
2138,We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task .,3,0.9717092,56.26451802401049,28
2138,"The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible , and those layers are critical for the translation quality .",3,0.9752518,45.43816394248009,29
2138,"Inspired by these findings , we propose three methods to improve translation quality .",3,0.47208077,24.934225050011378,14
2138,"First , a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks .",2,0.74393797,39.63820230283846,18
2138,"Second , a novel attention-based regularization is proposed for the encoders and pulls the representations from different modalities closer .",2,0.6693598,28.786565977419038,22
2138,"Third , an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task .",2,0.60840833,26.21766024858059,21
2138,"Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state-of-the-art results on the MuST-C English-German , English-French and English-Spanish language pairs .",3,0.9576627,7.745532918737502,46
2139,Large pre-trained language models ( PTLMs ) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems .,0,0.93551105,32.92453455602673,32
2139,"We propose a method based on logistic regression classifiers to probe English , French , and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates .",2,0.5296474,75.77322217576108,35
2139,The templates are prompted by a name of a social group followed by a cause-effect relation .,2,0.6037122,53.535794935046475,17
2139,We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities .,2,0.855659,97.17836452684566,26
2139,"We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study , then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs .",1,0.43307737,56.863583992240194,45
2140,"Technology for language generation has advanced rapidly , spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner .",0,0.94987434,25.474812281368067,33
2140,"While techniques can effectively generate fluent text , they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations .",0,0.8403967,37.547552855566195,26
2140,Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques .,0,0.88334286,82.64975140068015,20
2140,"To better understand these challenges , we present a survey on societal biases in language generation , focusing on how data and techniques contribute to biases and progress towards reducing biases .",1,0.8940464,45.69852714696998,32
2140,"Motivated by a lack of studies on biases from decoding techniques , we also conduct experiments to quantify the effects of these techniques .",2,0.50376993,48.67694489813715,24
2140,"By further discussing general trends and open challenges , we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications .",3,0.59766793,78.44712652992065,30
2141,We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated .,3,0.93561083,45.277390339343846,20
2141,"Inspired by old and well-established ideas in machine learning , we explore a variety of non-linear “ reservoir ” layers interspersed with regular transformer layers , and show improvements in wall-clock compute time until convergence , as well as overall performance , on various machine translation and ( masked ) language modelling tasks .",2,0.46125466,82.6190169479972,58
2142,Active Learning ( AL ) has been successfully applied to Deep Learning in order to drastically reduce the amount of data required to achieve high performance .,0,0.9271807,20.646157810598773,27
2142,Previous works have shown that lightweight architectures for Named Entity Recognition ( NER ) can achieve optimal performance with only 25 % of the original training data .,0,0.93550134,24.86831334470305,28
2142,"However , these methods do not exploit the sequential nature of language and the heterogeneity of uncertainty within each instance , requiring the labelling of whole sentences .",0,0.7773954,50.28674568401567,28
2142,"Additionally , this standard method requires that the annotator has access to the full sentence when labelling .",0,0.66883516,46.45262737955765,18
2142,"In this work , we overcome these limitations by allowing the AL algorithm to query subsequences within sentences , and propagate their labels to other sentences .",2,0.38555625,83.68650803967776,27
2142,"We achieve highly efficient results on OntoNotes 5.0 , only requiring 13 % of the original training data , and CoNLL 2003 , requiring only 27 % .",3,0.90968287,62.00806763123833,28
2142,This is an improvement of 39 % and 37 % compared to querying full sentences .,3,0.9476951,50.23519418138383,16
2143,"In this paper , we detail the relationship between convolutions and self-attention in natural language tasks .",1,0.88357055,16.88349676836316,17
2143,"We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions , and we consider multiple new ways of integrating convolutions into Transformer self-attention .",3,0.79177946,39.3195814506179,32
2143,"Specifically , we propose composite attention , which unites previous relative position encoding methods under a convolutional framework .",2,0.6780173,160.89982021803013,19
2143,"We conduct experiments by training BERT with composite attention , finding that convolutions consistently improve performance on multiple downstream tasks , replacing absolute position embeddings .",3,0.5381645,94.98903674327512,26
2143,"To inform future work , we present results comparing lightweight convolutions , dynamic convolutions , and depthwise-separable convolutions in language model pre-training , considering multiple injection points for convolutions in self-attention layers .",3,0.7367566,84.96754204386083,34
2144,"The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques , among which quantization is a popular solution .",0,0.9333182,21.55746890749214,26
2144,"In this paper , we propose BinaryBERT , which pushes BERT quantization to the limit by weight binarization .",1,0.8389832,47.45086332542615,19
2144,We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape .,3,0.969779,79.10281957954858,25
2144,"Therefore , we propose ternary weight splitting , which initializes BinaryBERT by equivalently splitting from a half-sized ternary network .",2,0.51623374,83.89479071555965,20
2144,"The binary model thus inherits the good performance of the ternary one , and can be further enhanced by fine-tuning the new architecture after splitting .",3,0.90150243,46.654221858647155,26
2144,"Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller , achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks .",3,0.9563883,16.302957775778264,40
2144,Code will be released .,3,0.4366207,48.462427311322244,5
2145,"In the era of pre-trained language models , Transformers are the de facto choice of model architectures .",0,0.8502337,21.53257116510285,18
2145,"While recent research has shown promise in entirely convolutional , or CNN , architectures , they have not been explored using the pre-train-fine-tune paradigm .",0,0.9149807,57.99883344569359,28
2145,This paper investigates this research question and presents several interesting findings .,1,0.731451,43.60835524021502,12
2145,"Across an extensive set of experiments on 8 datasets / tasks , we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios , albeit with caveats .",3,0.89676195,30.135101949240646,35
2145,"Overall , the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently .",3,0.9888854,29.455494091780654,26
2145,We believe our research paves the way for a healthy amount of optimism in alternative architectures .,3,0.97507894,58.65478173092166,17
2146,"Distance based knowledge graph embedding methods show promising results on link prediction task , on which two topics have been widely studied : one is the ability to handle complex relations , such as N-to-1 , 1-to-N and N-to-N , the other is to encode various relation patterns , such as symmetry / antisymmetry .",0,0.6707968,39.36178003844066,63
2146,"However , the existing methods fail to solve these two problems at the same time , which leads to unsatisfactory results .",0,0.8854834,16.059283263635198,22
2146,"To mitigate this problem , we propose PairRE , a model with paired vectors for each relation representation .",2,0.5865073,85.7694615608411,19
2146,The paired vectors enable an adaptive adjustment of the margin in loss function to fit for different complex relations .,3,0.57540053,143.71478244267854,20
2146,"Besides , PairRE is capable of encoding three important relation patterns , symmetry / antisymmetry , inverse and composition .",3,0.66433537,219.60876239889365,20
2146,"Given simple constraints on relation representations , PairRE can encode subrelation further .",3,0.5005115,641.3212750388499,13
2146,Experiments on link prediction benchmarks demonstrate the proposed key capabilities of PairRE .,3,0.856915,128.39391714835335,13
2146,We set a new state-of-the-art on two knowledge graph datasets of the challenging Open Graph Benchmark .,2,0.7936832,16.670152619605272,22
2147,Hierarchical text classification is an important yet challenging task due to the complex structure of the label hierarchy .,0,0.9394277,12.814421331438629,19
2147,"Existing methods ignore the semantic relationship between text and labels , so they cannot make full use of the hierarchical information .",0,0.8182641,33.26915874646415,22
2147,"To this end , we formulate the text-label semantics relationship as a semantic matching problem and thus propose a hierarchy-aware label semantics matching network ( HiMatch ) .",2,0.482324,58.97508454692651,30
2147,"First , we project text semantics and label semantics into a joint embedding space .",2,0.8670023,43.50906233384098,15
2147,We then introduce a joint embedding loss and a matching learning loss to model the matching relationship between the text semantics and the label semantics .,2,0.808571,44.30940963199553,26
2147,Our model captures the text-label semantics matching relationship among coarse-grained labels and fine-grained labels in a hierarchy-aware manner .,2,0.5062673,28.53091933232602,22
2147,The experimental results on various benchmark datasets verify that our model achieves state-of-the-art results .,3,0.95140725,5.17845532552615,20
2148,Fine-tuning large pre-trained models with task-specific data has achieved great success in NLP .,0,0.8939477,12.723040886474019,14
2148,"However , it has been demonstrated that the majority of information within the self-attention networks is redundant and not utilized effectively during the fine-tuning stage .",0,0.88217044,16.80335789389604,26
2148,This leads to inferior results when generalizing the obtained models to out-of-domain distributions .,3,0.86838186,27.82312899113583,15
2148,"To this end , we propose a simple yet effective data augmentation technique , HiddenCut , to better regularize the model and encourage it to learn more generalizable features .",2,0.42385712,28.376829470538944,30
2148,"Specifically , contiguous spans within the hidden space are dynamically and strategically dropped during training .",3,0.46569145,156.8466047082504,16
2148,"Experiments show that our HiddenCut method outperforms the state-of-the-art augmentation methods on the GLUE benchmark , and consistently exhibits superior generalization performances on out-of-distribution and challenging counterexamples .",3,0.94113106,15.089685496902892,34
2148,We have publicly released our code at https://github.com/GT-SALT/HiddenCut .,3,0.47948828,14.30004016878271,9
2149,Generating open-domain conversational responses in the desired style usually suffers from the lack of parallel data in the style .,0,0.80613136,25.918789431229083,20
2149,"Meanwhile , using monolingual stylistic data to increase style intensity often leads to the expense of decreasing content relevance .",0,0.66246146,106.81748146815288,20
2149,"In this paper , we propose to disentangle the content and style in latent space by diluting sentence-level information in style representations .",1,0.8727615,28.969957260309478,25
2149,Combining the desired style representation and a response content representation will then obtain a stylistic response .,3,0.42562222,130.81337216572035,17
2149,"Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores , compared with baselines .",3,0.9315788,47.0227251465556,20
2149,Human evaluation results show that our approach significantly improves style intensity and maintains content relevance .,3,0.97627103,51.54614699867637,16
2150,Understanding privacy policies is crucial for users as it empowers them to learn about the information that matters to them .,0,0.90481424,17.686093522841965,21
2150,"Sentences written in a privacy policy document explain privacy practices , and the constituent text spans convey further specific information about that practice .",0,0.8213095,147.52604351686256,24
2150,We refer to predicting the privacy practice explained in a sentence as intent classification and identifying the text spans sharing specific information as slot filling .,2,0.5552708,239.08053022883854,26
2150,"In this work , we propose PolicyIE , an English corpus consisting of 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of websites and mobile applications .",1,0.4237196,100.68973494871918,29
2150,PolicyIE corpus is a challenging real-world benchmark with limited labeled examples reflecting the cost of collecting large-scale annotations from domain experts .,0,0.66010743,55.26724002461344,22
2150,"We present two alternative neural approaches as baselines , ( 1 ) intent classification and slot filling as a joint sequence tagging and ( 2 ) modeling them as a sequence-to-sequence ( Seq2Seq ) learning task .",2,0.7631734,39.504298270275235,37
2150,"The experiment results show that both approaches perform comparably in intent classification , while the Seq2Seq method outperforms the sequence tagging approach in slot filling by a large margin .",3,0.9777297,21.075861196519693,30
2150,We perform a detailed error analysis to reveal the challenges of the proposed corpus .,3,0.46778876,46.284446013725955,15
2151,"For task-oriented dialog systems to be maximally useful , it must be able to process conversations in a way that is ( 1 ) generalizable with a small number of training examples for new task domains , and ( 2 ) robust to user input in various styles , modalities , or domains .",0,0.7177593,39.625436755152656,56
2151,"In pursuit of these goals , we introduce the RADDLE benchmark , a collection of corpora and tools for evaluating the performance of models across a diverse set of domains .",1,0.5757328,26.49595507323438,31
2151,"By including tasks with limited training data , RADDLE is designed to favor and encourage models with a strong generalization ability .",3,0.33469787,86.82812646928745,22
2151,"RADDLE also includes a diagnostic checklist that facilitates detailed robustness analysis in aspects such as language variations , speech errors , unseen entities , and out-of-domain utterances .",3,0.4602371,79.22448190833019,29
2151,"We evaluate recent state-of-the-art systems based on pre-training and fine-tuning , and find that grounded pre-training on heterogeneous dialog corpora performs better than training a separate model per domain .",3,0.75862855,16.360371388682434,36
2151,Adversarial training is also proposed to improve model robustness against noisy inputs .,2,0.46311748,22.62177493818438,13
2151,"Overall , existing models are less than satisfactory in robustness evaluation , which suggests opportunities for future improvement .",3,0.9520152,59.69767071165961,19
2152,"Although neural models have achieved competitive results in dialogue systems , they have shown limited ability in representing core semantics , such as ignoring important entities .",0,0.90933675,69.99935600109663,27
2152,"To this end , we exploit Abstract Meaning Representation ( AMR ) to help dialogue modeling .",2,0.58771133,49.2750811044568,17
2152,"Compared with the textual input , AMR explicitly provides core semantic knowledge and reduces data sparsity .",3,0.6267182,104.69052383089215,17
2152,We develop an algorithm to construct dialogue-level AMR graphs from sentence-level AMRs and explore two ways to incorporate AMRs into dialogue systems .,2,0.36627454,23.56152955214082,27
2152,Experimental results on both dialogue understanding and response generation tasks show the superiority of our model .,3,0.9589613,12.857492383825521,17
2152,"To our knowledge , we are the first to leverage a formal semantic representation into neural dialogue modeling .",3,0.8270673,35.04477600539574,19
2153,"Recently , many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge ( e.g. , documents ) when conversing with humans .",0,0.93757194,51.991144610465106,32
2153,"However , it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents , which hinders the effective and adequate training of knowledge selection and response matching .",0,0.8309456,38.96475421024997,32
2153,"To overcome the challenge , we consider decomposing the training of the knowledge-grounded response selection into three tasks including : 1 ) query-passage matching task ;",2,0.7783593,105.50357219506085,30
2153,2 ) query-dialogue history matching task ;,2,0.6987216,622.9788784254704,8
2153,"3 ) multi-turn response matching task , and joint learning all these tasks in a unified pre-trained language model .",2,0.74678576,49.88922559148064,20
2153,"The former two tasks could help the model in knowledge selection and comprehension , while the last task is designed for matching the proper response with the given query and background knowledge ( dialogue history ) .",3,0.46852398,81.39737413980592,37
2153,"By this means , the model can be learned to select relevant knowledge and distinguish proper response , with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues .",3,0.5111519,39.07544261518992,34
2153,Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training .,3,0.9471476,16.046163391421928,31
2154,"Syntactic information , especially dependency trees , has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities .",0,0.857767,65.37662730927478,33
2154,"However , most existing studies suffer from the noise in the dependency trees , especially when they are automatically generated , so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task .",0,0.8652017,79.48621804294167,44
2154,"In this paper , we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks ( A-GCN ) .",1,0.863937,22.622136302316907,24
2154,"In this approach , an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an off-the-shelf dependency parser , to distinguish the importance of different word dependencies .",2,0.7387811,33.79437700571104,37
2154,"Consider that dependency types among words also contain important contextual guidance , which is potentially helpful for relation extraction , we also include the type information in A-GCN modeling .",2,0.50442517,147.25047493810368,31
2154,"Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN , which outperforms previous studies and achieves state-of-the-art performance on both datasets .",3,0.89131933,8.439570940406917,34
2155,Retrieval is a core component for open-domain NLP tasks .,0,0.85890454,12.495257098269608,10
2155,"In open-domain tasks , multiple entities can share a name , making disambiguation an inherent yet under-explored problem .",0,0.8945351,39.87839291349986,19
2155,"We propose an evaluation benchmark for assessing the entity disambiguation capabilities of these retrievers , which we call Ambiguous Entity Retrieval ( AmbER ) sets .",1,0.41964287,52.38808600815308,26
2155,We define an AmbER set as a collection of entities that share a name along with queries about those entities .,2,0.7202686,86.20247542638256,21
2155,"By covering the set of entities for polysemous names , AmbER sets act as a challenging test of entity disambiguation .",3,0.50509876,131.64976131806773,21
2155,"We create AmbER sets for three popular open-domain tasks : fact checking , slot filling , and question answering , and evaluate a diverse set of retrievers .",2,0.8758486,86.28324256328358,28
2155,"We find that the retrievers exhibit popularity bias , significantly under-performing on rarer entities that share a name , e.g. , they are twice as likely to retrieve erroneous documents on queries for the less popular entity under the same name .",3,0.963982,59.24663043371727,42
2155,These experiments on AmbER sets show their utility as an evaluation tool and highlight the weaknesses of popular retrieval systems .,3,0.9480592,108.93974647596096,21
2156,Leaderboards are widely used in NLP and push the field forward .,0,0.8735197,32.27190860791913,12
2156,"While leaderboards are a straightforward ranking of NLP models , this simplicity can mask nuances in evaluation items ( examples ) and subjects ( NLP models ) .",0,0.6076182,122.13991088499854,28
2156,"Rather than replace leaderboards , we advocate a re-imagining so that they better highlight if and where progress is made .",3,0.7029311,72.57460300964847,21
2156,"Building on educational testing , we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses .",2,0.71999615,174.2566367891134,23
2156,"Using this model , we analyze the ranking reliability of leaderboards .",2,0.63843006,108.26384598375527,12
2156,"Afterwards , we show the model can guide what to annotate , identify annotation errors , detect overfitting , and identify informative examples .",3,0.5997613,112.47994987780262,24
2156,We conclude with recommendations for future benchmark tasks .,3,0.8652041,44.111363052661645,9
2157,Manual fact-checking does not scale well to serve the needs of the internet .,0,0.60097384,33.747476347991665,16
2157,This issue is further compounded in non-English contexts .,3,0.6494832,34.73756826304062,9
2157,"In this paper , we discuss claim matching as a possible solution to scale fact-checking .",1,0.899976,63.07531874116937,17
2157,We define claim matching as the task of identifying pairs of textual messages containing claims that can be served with one fact-check .,2,0.5692637,70.05679021616311,24
2157,We construct a novel dataset of WhatsApp tipline and public group messages alongside fact-checked claims that are first annotated for containing “ claim-like statements ” and then matched with potentially similar items and annotated for claim matching .,2,0.88558984,63.582150773739954,39
2157,"Our dataset contains content in high-resource ( English , Hindi ) and lower-resource ( Bengali , Malayalam , Tamil ) languages .",2,0.8117447,22.98494726100858,22
2157,We train our own embedding model using knowledge distillation and a high-quality “ teacher ” model in order to address the imbalance in embedding quality between the low-and high-resource languages in our dataset .,2,0.78985536,21.773024324630235,36
2157,"We provide evaluations on the performance of our solution and compare with baselines and existing state-of-the-art multilingual embedding models , namely LASER and LaBSE .",3,0.5758686,19.42778715009252,31
2157,We demonstrate that our performance exceeds LASER and LaBSE in all settings .,3,0.95415616,59.47698368404995,13
2157,"We release our annotated datasets , codebooks , and trained embedding model to allow for further research .",2,0.46647352,75.28238279113354,18
2158,"While pre-training techniques are working very well in natural language processing , how to pre-train a decoder and effectively use it for neural machine translation ( NMT ) still remains a tricky issue .",0,0.9408086,19.880041553203213,34
2158,"The main reason is that the cross-attention module between the encoder and decoder cannot be pre-trained , and the combined encoder-decoder model cannot work well in the fine-tuning stage because the inputs of the decoder cross-attention come from unknown encoder outputs .",0,0.56920385,13.666441002872258,42
2158,"In this paper , we propose a better pre-training method for NMT by defining a semantic interface ( SemFace ) between the pre-trained encoder and the pre-trained decoder .",1,0.8573552,17.658352593066443,29
2158,"Specifically , we propose two types of semantic interfaces , including CL-SemFace which regards cross-lingual embeddings as an interface , and VQ-SemFace which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space .",2,0.63254184,31.020174556529604,47
2158,We conduct massive experiments on six supervised translation pairs and three unsupervised pairs .,2,0.9023005,45.617591768138205,14
2158,"Experimental results demonstrate that our proposed SemFace can effectively connect the pre-trained encoder and decoder , and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models .",3,0.9562834,14.008201140763008,40
2159,"The discrepancy between maximum likelihood estimation ( MLE ) and task measures such as BLEU score has been studied before for autoregressive neural machine translation ( NMT ) and resulted in alternative training algorithms ( Ranzato et al. , 2016 ; Norouzi et al. , 2016 ; Shen et al. , 2016 ; Wu et al. , 2018 ) .",0,0.6620771,24.301994703289633,60
2159,"However , MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability .",0,0.8407007,36.33345226142951,20
2159,"Despite this mismatch between the training objective and task measure , we notice that the samples drawn from an MLE-based trained NMT support the desired distribution – there are samples with much higher BLEU score comparing to the beam decoding output .",3,0.9512426,69.97886472842923,44
2159,"To benefit from this observation , we train an energy-based model to mimic the behavior of the task measure ( i.e. , the energy-based model assigns lower energy to samples with higher BLEU score ) , which is resulted in a re-ranking algorithm based on the samples drawn from NMT : energy-based re-ranking ( EBR ) .",2,0.6878695,36.78308769547678,63
2159,We use both marginal energy models ( over target sentence ) and joint energy models ( over both source and target sentences ) .,2,0.8841631,72.15222160138889,24
2159,"Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT : +3.7 BLEU points on IWSLT’14 German-English , + 3.37 BELU points on Sinhala-English , + 1.4 BLEU points on WMT’16 English-German tasks .",3,0.9139799,19.25325669474256,43
2160,"In recent years , we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning .",0,0.943169,12.049922834609243,27
2160,"However , due to typological differences across languages , the cross-lingual transfer is challenging .",0,0.9063247,21.654291513774076,15
2160,"Nevertheless , language syntax , e.g. , syntactic dependencies , can bridge the typological gap .",0,0.72023016,90.97331616753209,16
2160,"Previous works have shown that pre-trained multilingual encoders , such as mBERT ( CITATION ) , capture language syntax , helping cross-lingual transfer .",0,0.9147196,36.04907864456264,24
2160,This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer .,3,0.9663384,63.31382526987648,26
2160,"We perform rigorous experiments on four NLP tasks , including text classification , question answering , named entity recognition , and task-oriented semantic parsing .",2,0.7947924,22.39593183311637,27
2160,"The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks , such as PAWS-X and MLQA , by 1.4 and 1.6 points on average across all languages .",3,0.9794647,22.086912679372578,34
2160,"In the generalized transfer setting , the performance boosted significantly , with 3.9 and 3.1 points on average in PAWS-X and MLQA .",3,0.97372097,60.98195862307444,24
2161,"Pretrained multilingual models ( PMMs ) enable zero-shot learning via cross-lingual transfer , performing best for languages seen during pretraining .",0,0.736791,43.60566248945294,21
2161,"While methods exist to improve performance for unseen languages , they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world ’s languages .",0,0.9023212,35.5549367973577,33
2161,"In this paper , we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for close to 1600 languages : the New Testament .",1,0.90716195,81.04513160465986,31
2161,"This is challenging for two reasons : ( 1 ) the small corpus size , and ( 2 ) the narrow domain .",0,0.87284786,37.91857375884203,23
2161,"While performance drops for all approaches , we surprisingly still see gains of up to 17.69 % accuracy for part-of-speech tagging and 6.29 F1 for NER on average over all languages as compared to XLM-R .",3,0.95052624,34.119475979190675,38
2161,"Another unexpected finding is that continued pretraining , the simplest approach , performs best .",3,0.9720889,145.67138291999115,15
2161,"Finally , we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language .",3,0.6452979,19.382128218591173,28
2162,We study the problem of building entity tagging systems by using a few rules as weak supervision .,1,0.53467524,63.93792758082731,18
2162,"Previous methods mostly focus on disambiguating entity types based on contexts and expert-provided rules , while assuming entity spans are given .",0,0.7434701,70.77523041623817,22
2162,"In this work , we propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger in a fully automated manner .",1,0.8373555,36.53261152004691,26
2162,"Specifically , we introduce compound rules that are composed from simple rules to increase the precision of boundary detection and generate more diverse pseudo labels .",2,0.73610544,53.59767189831504,26
2162,We further design a dynamic label selection strategy to ensure pseudo label quality and therefore avoid overfitting the neural tagger .,2,0.62650585,74.56923335612171,21
2162,"Experiments on three datasets demonstrate that our method outperforms other weakly supervised methods and even rivals a state-of-the-art distantly supervised tagger with a lexicon of over 2,000 terms when starting from only 20 simple rules .",3,0.90142155,19.661276605232494,42
2162,Our method can serve as a tool for rapidly building taggers in emerging domains and tasks .,3,0.9663747,56.10620852048881,17
2162,Case studies show that learned rules can potentially explain the predicted entities .,0,0.59481096,174.8163276637614,13
2163,Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks .,0,0.6294761,11.718457815037418,16
2163,"However , fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task .",3,0.49880636,44.76858911732053,20
2163,"In this paper , we propose prefix-tuning , a lightweight alternative to fine-tuning for natural language generation tasks , which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors , which we call the prefix .",1,0.773087,37.91372836448306,42
2163,"Prefix-tuning draws inspiration from prompting for language models , allowing subsequent tokens to attend to this prefix as if it were “ virtual tokens ” .",0,0.48241267,128.4285127980826,26
2163,We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization .,2,0.69685346,22.927795869960725,20
2163,"We show that by learning only 0.1 % of the parameters , prefix-tuning obtains comparable performance in the full data setting , outperforms fine-tuning in low-data settings , and extrapolates better to examples with topics that are unseen during training .",3,0.90002286,46.00320338493825,41
2164,"Recently , the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation ( KG ) by concatenating multiple keyphrases in a predefined order as a target sequence during training .",0,0.9543256,16.18683543080202,36
2164,"However , the keyphrases are inherently an unordered set rather than an ordered sequence .",0,0.8518556,42.286372637075495,15
2164,"Imposing a predefined order will introduce wrong bias during training , which can highly penalize shifts in the order between keyphrases .",3,0.74914414,93.94001325141481,22
2164,"In this work , we propose a new training paradigm One2 Set without predefining an order to concatenate the keyphrases .",1,0.7070351,78.62579612343761,21
2164,"To fit this paradigm , we propose a novel model that utilizes a fixed set of learned control codes as conditions to generate a set of keyphrases in parallel .",2,0.5161744,35.83571140518886,30
2164,"To solve the problem that there is no correspondence between each prediction and target during training , we propose a K-step label assignment mechanism via bipartite matching , which greatly increases the diversity and reduces the repetition rate of generated keyphrases .",2,0.6739157,51.91080852446131,42
2164,The experimental results on multiple benchmarks demonstrate that our approach significantly outperforms the state-of-the-art methods .,3,0.9540279,4.38407730617052,21
2165,"Recent years have witnessed various types of generative models for natural language generation ( NLG ) , especially RNNs or transformer based sequence-to-sequence models , as well as variational autoencoder ( VAE ) and generative adversarial network ( GAN ) based models .",0,0.9309011,13.818018972014663,45
2165,"However , flow-based generative models , which achieve strong performance in image generation due to their invertibility and exact density estimation properties , have been less explored for NLG .",0,0.8896259,66.25051896965284,32
2165,"In this paper , we propose a flow-based language generation model by adapting previous flow generative models to language generation via continuous input embeddings , adapted affine coupling structures , and a novel architecture for autoregressive text generation .",1,0.82895714,51.782314087978875,40
2165,"We also apply our framework to Sequence-to-Sequence generation , including text-and video-based Question Generation ( QG ) and Neural Machine Translation ( NMT ) , and data augmentation for Question Answering ( QA ) .",2,0.6549628,24.34547151817499,41
2165,"We use our language flow model to provide extra input features for QG and NMT , which achieves improvements over the strong QG baselines on SQuAD and TVQA and NMT baseline on WMT16 .",2,0.55030864,38.97824551112674,34
2165,We also augment QA data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on SQuAD and TVQA .,3,0.72590786,34.593997731267,37
2166,Wikipedia abstract generation aims to distill a Wikipedia abstract from web sources and has met significant success by adopting multi-document summarization techniques .,0,0.9533028,69.60530369188591,23
2166,"However , previous works generally view the abstract as plain text , ignoring the fact that it is a description of a certain entity and can be decomposed into different topics .",0,0.92397213,35.53291200931934,32
2166,"In this paper , we propose a two-stage model TWAG that guides the abstract generation with topical information .",1,0.86846,51.57433466205972,20
2166,"First , we detect the topic of each input paragraph with a classifier trained on existing Wikipedia articles to divide input documents into different topics .",2,0.87380934,40.953699973173784,26
2166,"Then , we predict the topic distribution of each abstract sentence , and decode the sentence from topic-aware representations with a Pointer-Generator network .",2,0.86717683,44.64558433384343,26
2166,"We evaluate our model on the WikiCatSum dataset , and the results show that TWAG outperforms various existing baselines and is capable of generating comprehensive abstracts .",3,0.8695516,39.06840011839586,27
2167,"Event forecasting is a challenging , yet important task , as humans seek to constantly plan for the future .",0,0.96029997,79.07215984275797,20
2167,"Existing automated forecasting studies rely mostly on structured data , such as time-series or event-based knowledge graphs , to help predict future events .",0,0.9257584,37.530673114398944,26
2167,"In this work , we aim to formulate a task , construct a dataset , and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data .",1,0.92373645,48.55519336956754,32
2167,"To simulate the forecasting scenario on temporal news documents , we formulate the problem as a restricted-domain , multiple-choice , question-answering ( QA ) task .",2,0.7955666,59.977376797472246,31
2167,"Unlike existing QA tasks , our task limits accessible information , and thus a model has to make a forecasting judgement .",0,0.69064975,180.86080101153627,22
2167,"To showcase the usefulness of this task formulation , we introduce ForecastQA , a question-answering dataset consisting of 10,392 event forecasting questions , which have been collected and verified via crowdsourcing efforts .",2,0.60189676,38.43735340671749,35
2167,"We present our experiments on ForecastQA using BERTbased models and find that our best model achieves 61.0 % accuracy on the dataset , which still lags behind human performance by about 19 % .",3,0.8864684,31.306872516753913,34
2167,We hope ForecastQA will support future research efforts in bridging this gap .,3,0.89305,22.552795600454743,13
2168,Syntactic structure is an important component of natural language text .,0,0.94094527,19.305312474502873,11
2168,"Recent top-performing models in Answer Sentence Selection ( AS2 ) use self-attention and transfer learning , but not syntactic structure .",0,0.76110196,55.60283854548282,21
2168,Tree structures have shown strong performance in tasks with sentence pair input like semantic relatedness .,0,0.81846136,110.85228478625763,16
2168,We investigate whether tree structures can boost performance in AS2 .,1,0.7800558,153.1053156745563,11
2168,"We introduce the Tree Aggregation Transformer : a novel recursive , tree-structured self-attention model for AS2 .",2,0.41862634,59.642187678695265,17
2168,The recursive nature of our model is able to represent all levels of syntactic parse trees with only one additional self-attention layer .,3,0.74067557,31.650407215295083,23
2168,"Without transfer learning , we establish a new state of the art on the popular TrecQA and WikiQA benchmark datasets .",3,0.45056763,21.58605430174444,21
2168,"Additionally , we evaluate our method on four Community Question Answering datasets , and find that tree-structured representations have limitations with noisy user-generated text .",3,0.7562412,34.20764802236612,26
2168,We conduct probing experiments to evaluate how our models leverage tree structures across datasets .,2,0.76121837,80.30756140580233,15
2168,Our findings show that the ability of tree-structured models to successfully absorb syntactic information is strongly correlated with a higher performance in AS2 .,3,0.9905178,30.636959811300347,24
2169,Knowledge Graph ( KG ) and attention mechanism have been demonstrated effective in introducing and selecting useful information for weakly supervised methods .,0,0.70862794,37.11343751408665,23
2169,"However , only qualitative analysis and ablation study are provided as evidence .",0,0.59273607,67.85699378952164,13
2169,"In this paper , we contribute a dataset and propose a paradigm to quantitatively evaluate the effect of attention and KG on bag-level relation extraction ( RE ) .",1,0.881227,42.977762092219066,29
2169,We find that ( 1 ) higher attention accuracy may lead to worse performance as it may harm the model ’s ability to extract entity mention features ;,3,0.9797805,77.91132102559848,28
2169,"( 2 ) the performance of attention is largely influenced by various noise distribution patterns , which is closely related to real-world datasets ;",3,0.769656,103.18955045304529,24
2169,"( 3 ) KG-enhanced attention indeed improves RE performance , while not through enhanced attention but by incorporating entity prior ;",3,0.9449129,373.12861052649896,23
2169,and ( 4 ) attention mechanism may exacerbate the issue of insufficient training data .,3,0.8943922,75.13133452082423,15
2169,"Based on these findings , we show that a straightforward variant of RE model can achieve significant improvements ( 6 % AUC on average ) on two real-world datasets as compared with three state-of-the-art baselines .",3,0.96762556,19.664994234269734,42
2169,Our codes and datasets are available at https://github.com/zig-kwin-hu/how-KG-ATT-help .,3,0.5642052,27.082163340647462,9
2170,Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event .,0,0.79620725,35.226170068213,19
2170,"However , most prior works focus on capturing direct relations between arguments and the event trigger .",0,0.90190274,45.54854832525919,17
2170,The lack of reasoning ability brings many challenges to the extraction of implicit arguments .,0,0.88711476,68.93344144194833,15
2170,"In this work , we present a Frame-aware Event Argument Extraction ( FEAE ) learning framework to tackle this issue through reasoning in event frame-level scope .",1,0.82270664,56.55785039939927,29
2170,The proposed method leverages related arguments of the expected one as clues to guide the reasoning process .,3,0.4976021,61.804238057027945,18
2170,"To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage , we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model .",2,0.7292117,56.28112763412855,53
2170,Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset .,3,0.9716789,17.65071716040176,18
2171,"Open relation extraction aims to cluster relation instances referring to the same underlying relation , which is a critical step for general relation extraction .",0,0.8214307,36.17456532263362,25
2171,"Current OpenRE models are commonly trained on the datasets generated from distant supervision , which often results in instability and makes the model easily collapsed .",0,0.84351313,90.25735077053481,26
2171,"In this paper , we revisit the procedure of OpenRE from a causal view .",1,0.8974548,90.10323409734205,15
2171,"By formulating OpenRE using a structural causal model , we identify that the above-mentioned problems stem from the spurious correlations from entities and context to the relation type .",3,0.6696678,88.17818764099621,29
2171,"To address this issue , we conduct Element Intervention , which intervene on the context and entities respectively to obtain the underlying causal effects of them .",1,0.41583514,137.79064577184766,27
2171,We also provide two specific implementations of the interventions based on entity ranking and context contrasting .,3,0.60917884,139.25867563805406,17
2171,Experimental results on unsupervised relation extraction datasets show our method to outperform previous state-of-the-art methods and is robust across different datasets .,3,0.94587713,8.503530855558882,28
2172,Automatic extraction of product attribute values is an important enabling technology in e-Commerce platforms .,0,0.92353296,40.33375114326687,15
2172,"This task is usually modeled using sequence labeling architectures , with several extensions to handle multi-attribute extraction .",0,0.78917354,103.33470765019453,18
2172,"One line of previous work constructs attribute-specific models , through separate decoders or entirely separate models .",0,0.867925,149.81801865139835,17
2172,"However , this approach constrains knowledge sharing across different attributes .",0,0.772764,61.2194675809471,11
2172,"Other contributions use a single multi-attribute model , with different techniques to embed attribute information .",0,0.44454846,132.5636025621809,16
2172,But sharing the entire network parameters across all attributes can limit the model ’s capacity to capture attribute-specific characteristics .,3,0.6273216,66.37931295683985,21
2172,"In this paper we present AdaTag , which uses adaptive decoding to handle extraction .",1,0.84889877,121.02592419406082,15
2172,"We parameterize the decoder with pretrained attribute embeddings , through a hypernetwork and a Mixture-of-Experts ( MoE ) module .",2,0.7924667,64.84658504145189,23
2172,"This allows for separate , but semantically correlated , decoders to be generated on the fly for different attributes .",3,0.4051514,71.26009127430945,20
2172,"This approach facilitates knowledge sharing , while maintaining the specificity of each attribute .",3,0.609568,102.0431512038247,14
2172,Our experiments on a real-world e-Commerce dataset show marked improvements over previous methods .,3,0.89301133,19.67589332746307,14
2173,Integrating extracted knowledge from the Web to knowledge graphs ( KGs ) can facilitate tasks like question answering .,0,0.8586262,64.37343094100109,19
2173,We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target KG .,1,0.652987,98.12214811435314,22
2173,"To address the challenge that free-text relations are ambiguous , previous methods exploit neighbor entities and relations for additional context .",0,0.90490884,103.5858160459101,21
2173,"However , the predictions are made independently , which can be mutually inconsistent .",0,0.88722765,88.75729722933579,14
2173,"We propose a two-stage Collective Relation Integration ( CoRI ) model , where the first stage independently makes candidate predictions , and the second stage employs a collective model that accesses all candidate predictions to make globally coherent predictions .",2,0.65555865,58.709962103586115,40
2173,We further improve the collective model with augmented data from the portion of the target KG that is otherwise unused .,3,0.59284294,56.563702950157385,21
2173,"Experiment results on two datasets show that CoRI can significantly outperform the baselines , improving AUC from .677 to .748 and from .716 to .780 , respectively .",3,0.96015203,18.627699910989904,28
2174,Streaming cross document entity coreference ( CDC ) systems disambiguate mentions of named entities in a scalable manner via incremental clustering .,0,0.9143686,115.81588853160031,22
2174,"Unlike other approaches for named entity disambiguation ( e.g. , entity linking ) , streaming CDC allows for the disambiguation of entities that are unknown at inference time .",0,0.5744318,36.80273723016903,29
2174,"Thus , it is well-suited for processing streams of data where new entities are frequently introduced .",0,0.5549565,37.04599341101878,19
2174,"Despite these benefits , this task is currently difficult to study , as existing approaches are either evaluated on datasets that are no longer available , or omit other crucial details needed to ensure fair comparison .",0,0.8049566,49.83847372518413,37
2174,"In this work , we address this issue by compiling a large benchmark adapted from existing free datasets , and performing a comprehensive evaluation of a number of novel and existing baseline models .",1,0.7847226,45.266521108311736,34
2174,"We investigate : how to best encode mentions , which clustering algorithms are most effective for grouping mentions , how models transfer to different domains , and how bounding the number of mentions tracked during inference impacts performance .",1,0.63911176,121.71880004335553,39
2174,"Our results show that the relative performance of neural and feature-based mention encoders varies across different domains , and in most cases the best performance is achieved using a combination of both approaches .",3,0.9872665,19.89035797130551,36
2174,We also find that performance is minimally impacted by limiting the number of tracked mentions .,3,0.9778131,29.055280211191157,16
2175,Temporal Knowledge Graphs ( TKGs ) have been developed and used in many different areas .,0,0.93577665,20.94032071654094,16
2175,Reasoning on TKGs that predicts potential facts ( events ) in the future brings great challenges to existing models .,0,0.89375764,126.79870416712413,20
2175,"When facing a prediction task , human beings usually search useful historical information ( i.e. , clues ) in their memories and then reason for future meticulously .",0,0.94534266,194.85002270601046,28
2175,"Inspired by this mechanism , we propose CluSTeR to predict future facts in a two-stage manner , Clue Searching and Temporal Reasoning , accordingly .",2,0.4359093,102.14840397119436,26
2175,"Specifically , at the clue searching stage , CluSTeR learns a beam search policy via reinforcement learning ( RL ) to induce multiple clues from historical facts .",2,0.7196511,145.29642675545185,28
2175,"At the temporal reasoning stage , it adopts a graph convolution network based sequence method to deduce answers from clues .",2,0.6604842,69.64660479191927,21
2175,Experiments on four datasets demonstrate the substantial advantages of CluSTeR compared with the state-of-the-art methods .,3,0.9232434,12.474789578212064,21
2175,"Moreover , the clues found by CluSTeR further provide interpretability for the results .",3,0.9433032,101.94515248930148,14
2176,"Generating high-quality arguments , while being challenging , may benefit a wide range of downstream applications , such as writing assistants and argument search engines .",0,0.83480805,61.6294306272829,26
2176,"Motivated by the effectiveness of utilizing knowledge graphs for supporting general text generation tasks , this paper investigates the usage of argumentation-related knowledge graphs to control the generation of arguments .",1,0.67225045,33.28293945870272,33
2176,"In particular , we construct and populate three knowledge graphs , employing several compositions of them to encode various knowledge into texts of debate portals and relevant paragraphs from Wikipedia .",2,0.7244335,199.06208713117047,31
2176,"Then , the texts with the encoded knowledge are used to fine-tune a pre-trained text generation model , GPT-2 .",2,0.7662657,22.798362627443126,24
2176,"We evaluate the newly created arguments manually and automatically , based on several dimensions important in argumentative contexts , including argumentativeness and plausibility .",2,0.80905986,106.87194437577257,24
2176,The results demonstrate the positive impact of encoding the graphs ’ knowledge into debate portal texts for generating arguments with superior quality than those generated without knowledge .,3,0.9904744,135.37795918754483,28
2177,"Aspect Sentiment Triplet Extraction ( ASTE ) is the most recent subtask of ABSA which outputs triplets of an aspect target , its associated sentiment , and the corresponding opinion term .",0,0.80104405,100.5682403857011,32
2177,Recent models perform the triplet extraction in an end-to-end manner but heavily rely on the interactions between each target word and opinion word .,0,0.8235029,28.329233100223657,26
2177,"Thereby , they cannot perform well on targets and opinions which contain multiple words .",0,0.70491534,180.98951858996494,15
2177,Our proposed span-level approach explicitly considers the interaction between the whole spans of targets and opinions when predicting their sentiment relation .,3,0.5892415,99.81483144059668,22
2177,"Thus , it can make predictions with the semantics of whole spans , ensuring better sentiment consistency .",3,0.8120734,218.96861595181863,18
2177,"To ease the high computational cost caused by span enumeration , we propose a dual-channel span pruning strategy by incorporating supervision from the Aspect Term Extraction ( ATE ) and Opinion Term Extraction ( OTE ) tasks .",2,0.70944244,35.35266078120647,38
2177,This strategy not only improves computational efficiency but also distinguishes the opinion and target spans more properly .,3,0.74554694,92.89739189853071,18
2177,Our framework simultaneously achieves strong performance for the ASTE as well as ATE and OTE tasks .,3,0.89128965,56.27390896213842,17
2177,"In particular , our analysis shows that our span-level approach achieves more significant improvements over the baselines on triplets with multi-word targets or opinions .",3,0.9825028,55.23196398617083,25
2178,Modern neural machine translation ( NMT ) models have achieved competitive performance in standard benchmarks such as WMT .,0,0.9573049,16.020704424358403,19
2178,"However , there still exist significant issues such as robustness , domain generalization , etc .",0,0.8744671,62.176892352957644,16
2178,"In this paper , we study NMT models from the perspective of compositional generalization by building a benchmark dataset , CoGnition , consisting of 216 k clean and consistent sentence pairs .",1,0.61777896,58.2901997588296,32
2178,"We quantitatively analyze effects of various factors using compound translation error rate , then demonstrate that the NMT model fails badly on compositional generalization , although it performs remarkably well under traditional metrics .",3,0.750597,84.17291955698403,34
2179,"Word alignment , which aims to align translationally equivalent words between source and target sentences , plays an important role in many natural language processing tasks .",0,0.9477641,28.21315048413606,27
2179,"Current unsupervised neural alignment methods focus on inducing alignments from neural machine translation models , which does not leverage the full context in the target sequence .",0,0.9043477,34.729941303233524,27
2179,"In this paper , we propose Mask-Align , a self-supervised word alignment model that takes advantage of the full context on the target side .",1,0.8225725,26.07956425845313,27
2179,Our model masks out each target token and predicts it conditioned on both source and the remaining target tokens .,2,0.728896,62.69982101358671,20
2179,This two-step process is based on the assumption that the source token contributing most to recovering the masked target token should be aligned .,0,0.434727,34.597973424261625,25
2179,"We also introduce an attention variant called leaky attention , which alleviates the problem of unexpected high cross-attention weights on special tokens such as periods .",2,0.55197054,76.73659868789444,26
2179,Experiments on four language pairs show that our model outperforms previous unsupervised neural aligners and obtains new state-of-the-art results .,3,0.9432602,6.44114413072601,26
2180,"Computer-aided translation ( CAT ) , the use of software to assist a human translator in the translation process , has been proven to be useful in enhancing the productivity of human translators .",0,0.964896,23.79405452313018,34
2180,"Autocompletion , which suggests translation results according to the text pieces provided by human translators , is a core function of CAT .",0,0.66736794,92.49944145734463,23
2180,There are two limitations in previous research in this line .,0,0.7291283,58.99958347079239,11
2180,"First , most research works on this topic focus on sentence-level autocompletion ( i.e. , generating the whole translation as a sentence based on human input ) , but word-level autocompletion is under-explored so far .",0,0.88144463,29.46732978095462,38
2180,"Second , almost no public benchmarks are available for the autocompletion task of CAT .",0,0.80493146,76.64572424772577,15
2180,This might be among the reasons why research progress in CAT is much slower compared to automatic MT .,3,0.547237,82.24085788420233,19
2180,"In this paper , we propose the task of general word-level autocompletion ( GWLAN ) from a real-world CAT scenario , and construct the first public benchmark to facilitate research in this topic .",1,0.8846438,53.88385785133984,34
2180,"In addition , we propose an effective method for GWLAN and compare it with several strong baselines .",3,0.3978932,37.60819809823215,18
2180,Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets .,3,0.9531057,12.005059638659544,21
2181,Distant supervision tackles the data bottleneck in NER by automatically generating training instances via dictionary matching .,0,0.6155425,127.30359980997858,17
2181,"Unfortunately , the learning of DS-NER is severely dictionary-biased , which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models .",0,0.6678737,62.860576878748,32
2181,"In this paper , we fundamentally explain the dictionary bias via a Structural Causal Model ( SCM ) , categorize the bias into intra-dictionary and inter-dictionary biases , and identify their causes .",1,0.79747933,41.12784126069422,33
2181,"Based on the SCM , we learn de-biased DS-NER via causal interventions .",3,0.48575854,279.65277282869727,15
2181,"For intra-dictionary bias , we conduct backdoor adjustment to remove the spurious correlations introduced by the dictionary confounder .",2,0.8333809,110.75300816787117,19
2181,"For inter-dictionary bias , we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries .",2,0.49428025,50.63690761105317,25
2181,Experiments on four datasets and three DS-NER models show that our method can significantly improve the performance of DS-NER .,3,0.9369938,12.755308676438041,24
2182,Research on overlapped and discontinuous named entity recognition ( NER ) has received increasing attention .,0,0.96575224,33.97119411943264,16
2182,The majority of previous work focuses on either overlapped or discontinuous entities .,0,0.8562976,47.70725372500286,13
2182,"In this paper , we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly .",1,0.9082254,28.856993460347528,20
2182,The model includes two major steps .,2,0.58716905,96.07464730376529,7
2182,"First , entity fragments are recognized by traversing over all possible text spans , thus , overlapped entities can be recognized .",2,0.42596918,136.27374573813756,22
2182,"Second , we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession .",2,0.85493326,132.5286513484484,21
2182,"In this way , we can recognize not only discontinuous entities , and meanwhile doubly check the overlapped entities .",3,0.6315283,141.77783681114778,20
2182,"As a whole , our model can be regarded as a relation extraction paradigm essentially .",3,0.7766453,51.456474438662326,16
2182,"Experimental results on multiple benchmark datasets ( i.e. , CLEF , GENIA and ACE05 ) show that our model is highly competitive for overlapped and discontinuous NER .",3,0.9201961,42.241018538854206,28
2183,"We consider the problem of collectively detecting multiple events , particularly in cross-sentence settings .",1,0.49059343,79.51885834961735,15
2183,The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level .,0,0.74380195,33.802515765010384,21
2183,"In this paper , we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network ( MLBiNet ) to capture the document-level association of events and semantic information simultaneously .",1,0.68641424,26.167377270456058,33
2183,"Specifically , a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence .",2,0.6940184,64.23505044866582,23
2183,"Secondly , an information aggregation module is employed to aggregate sentence-level semantic and event tag information .",2,0.7057312,52.73872010635319,19
2183,"Finally , we stack multiple bidirectional decoders and feed cross-sentence information , forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences .",2,0.74080825,34.69493344551784,25
2183,We show that our approach provides significant improvement in performance compared to the current state-of-the-art results .,3,0.97008204,6.696014533260506,23
2184,We study the problem of event coreference resolution ( ECR ) that seeks to group coreferent event mentions into the same clusters .,1,0.6540699,43.924222564260354,23
2184,Deep learning methods have recently been applied for this task to deliver state-of-the-art performance .,0,0.9251292,11.514958252831784,21
2184,"However , existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR , e.g. , context words and entity mentions , to support the encoding of document-level context .",0,0.87045574,44.73831817292771,40
2184,"In addition , consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ECR .",0,0.70644647,140.98650666464448,29
2184,This work addresses such limitations by introducing a novel deep learning model for ECR .,1,0.6147791,33.875586330771036,15
2184,At the core of our model are document structures to explicitly capture relevant objects for ECR .,2,0.5677324,92.57493962067737,17
2184,"Our document structures introduce diverse knowledge sources ( discourse , syntax , semantics ) to compute edges / interactions between structure nodes for document-level representation learning .",2,0.6111651,168.96951325015044,28
2184,We also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents .,3,0.47485134,111.67665010962389,20
2184,Extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets .,3,0.9064571,4.7653800229732655,20
2185,"Relational triple extraction is critical to understanding massive text corpora and constructing large-scale knowledge graph , which has attracted increasing research interest .",0,0.9492566,37.9812580462938,23
2185,"However , existing studies still face some challenging issues , including information loss , error propagation and ignoring the interaction between entity and relation .",0,0.90453297,74.16359974928774,25
2185,"To intuitively explore the above issues and address them , in this paper , we provide a revealing insight into relational triple extraction from a stereoscopic perspective , which rationalizes the occurrence of these issues and exposes the shortcomings of existing methods .",1,0.8279742,58.37879387916834,43
2185,"Further , a novel model is proposed for relational triple extraction , which maps relational triples to a three-dimension ( 3-D ) space and leverages three decoders to extract them , aimed at simultaneously handling the above issues .",2,0.58840585,44.161220604342745,43
2185,"A series of experiments are conducted on five public datasets , demonstrating that the proposed model outperforms the recent advanced baselines .",3,0.5069116,18.215236171540795,22
2186,Identifying causal relations of events is an important task in natural language processing area .,0,0.9393837,22.32030681013296,15
2186,"However , the task is very challenging , because event causality is usually expressed in diverse forms that often lack explicit causal clues .",0,0.917791,69.15534180920703,24
2186,"Existing methods cannot handle well the problem , especially in the condition of lacking training data .",0,0.8372263,65.35714645905495,17
2186,"Nonetheless , humans can make a correct judgement based on their background knowledge , including descriptive knowledge and relational knowledge .",0,0.887048,69.26315782513049,21
2186,"Inspired by it , we propose a novel Latent Structure Induction Network ( LSIN ) to incorporate the external structural knowledge into this task .",2,0.4506064,45.41930707674346,25
2186,"Specifically , to make use of the descriptive knowledge , we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge .",2,0.77798504,32.385860704839075,26
2186,"To leverage the relational knowledge , we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning .",2,0.6458583,47.79072299511397,27
2186,Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods .,3,0.93429863,3.9513627190799365,22
2187,Recent studies show that neural natural language processing ( NLP ) models are vulnerable to backdoor attacks .,0,0.9554905,18.99098668012846,18
2187,"Injected with backdoors , models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated , presenting serious security threats to real-world applications .",0,0.6778649,80.85699297665064,29
2187,"Since existing textual backdoor attacks pay little attention to the invisibility of backdoors , they can be easily detected and blocked .",0,0.7733775,47.81710784022355,22
2187,"In this work , we present invisible backdoors that are activated by a learnable combination of word substitution .",1,0.6887191,85.99034967745445,19
2187,"We show that NLP models can be injected with backdoors that lead to a nearly 100 % attack success rate , whereas being highly invisible to existing defense strategies and even human inspections .",3,0.92659694,97.80081878751064,34
2187,"The results raise a serious alarm to the security of NLP models , which requires further research to be resolved .",3,0.9881711,54.80535972569508,21
2187,All the data and code of this paper are released at https://github.com/thunlp/BkdAtk-LWS .,3,0.5359648,11.877613260897851,13
2188,The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings .,0,0.76448095,25.56186881822949,20
2188,Diff pruning enables parameter-efficient transfer learning that scales well with new tasks .,3,0.63047683,64.28738721571867,15
2188,The approach learns a task-specific “ diff ” vector that extends the original pretrained parameters .,2,0.6722491,75.61296807075938,16
2188,This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity .,2,0.5816193,117.99764798364824,22
2188,"As the number of tasks increases , diff pruning remains parameter-efficient , as it requires storing only a small diff vector for each task .",3,0.8052193,88.31928277674334,27
2188,"Since it does not require access to all tasks during training , it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers .",0,0.4714741,64.7907034927724,32
2188,Diff pruning can match the performance of finetuned baselines on the GLUE benchmark while only modifying 0.5 % of the pretrained model ’s parameters per task and scales favorably in comparison to popular pruning approaches .,3,0.89911103,37.09486908982474,36
2189,"Human language understanding operates at multiple levels of granularity ( e.g. , words , phrases , and sentences ) with increasing levels of abstraction that can be hierarchically combined .",0,0.9195074,40.79195530520614,30
2189,"However , existing deep models with stacked layers do not explicitly model any sort of hierarchical process .",0,0.8893161,102.54947289692531,18
2189,"In this paper , we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process , and we extend the bidirectional language model pre-training objective to this architecture , attempting to predict each word given its left and right abstraction nodes .",1,0.698226,97.26264399999884,49
2189,"To scale up our approach , we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time .",2,0.5101781,41.821820557968984,27
2189,Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach .,3,0.9076053,8.791287105651636,15
2190,Zero-shot sequence labeling aims to build a sequence labeler without human-annotated datasets .,0,0.8206716,25.540620302173,13
2190,One straightforward approach is utilizing existing systems ( source models ) to generate pseudo-labeled datasets and train a target sequence labeler accordingly .,0,0.575075,92.5216300251045,23
2190,"However , due to the gap between the source and the target languages / domains , this approach may fail to recover the true labels .",0,0.7021112,40.272647323697676,26
2190,"In this paper , we propose a novel unified framework for zero-shot sequence labeling with minimum risk training and design a new decomposable risk function that models the relations between the predicted labels from the source models and the true labels .",1,0.84071755,28.35824379151483,42
2190,"By making the risk function trainable , we draw a connection between minimum risk training and latent variable model learning .",3,0.45410505,80.08973683963922,21
2190,We propose a unified learning algorithm based on the expectation maximization ( EM ) algorithm .,2,0.5410074,58.38706210654762,16
2190,We extensively evaluate our proposed approaches on cross-lingual / domain sequence labeling tasks over twenty-one datasets .,2,0.6481381,62.65161471860465,18
2190,The results show that our approaches outperform state-of-the-art baseline systems .,3,0.9841526,6.333518654513509,17
2191,Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks .,0,0.93262786,18.733093256862496,17
2191,A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model .,0,0.74557877,51.0923488990111,28
2191,"In this paper , we present an alternative approach based on adversarial reprogramming , which extends earlier work on automatic prompt generation .",1,0.89968413,31.080531041576897,23
2191,"Adversarial reprogramming attempts to learn task-specific word embeddings that , when concatenated to the input text , instruct the language model to solve the specified task .",0,0.52137965,24.323877076810792,28
2191,"Using up to 25 K trainable parameters per task , this approach outperforms all existing methods with up to 25 M trainable parameters on the public leaderboard of the GLUE benchmark .",3,0.7736127,26.35592822279249,32
2191,"Our method , initialized with task-specific human-readable prompts , also works in a few-shot setting , outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples .",3,0.8255883,44.23905524254252,31
2192,"Sequence-to-sequence transduction is the core problem in language processing applications as diverse as semantic parsing , machine translation , and instruction following .",0,0.95274466,42.19515977532938,23
2192,"The neural network models that provide the dominant solution to these problems are brittle , especially in low-resource settings : they fail to generalize correctly or systematically from small datasets .",0,0.90376925,38.04470810444926,31
2192,Past work has shown that many failures of systematic generalization arise from neural models ’ inability to disentangle lexical phenomena from syntactic ones .,0,0.9206514,30.641905303042524,24
2192,"To address this , we augment neural decoders with a lexical translation mechanism that generalizes existing copy mechanisms to incorporate learned , decontextualized , token-level translation rules .",2,0.7086704,66.43387197420338,30
2192,"We describe how to initialize this mechanism using a variety of lexicon learning algorithms , and show that it improves systematic generalization on a diverse set of sequence modeling tasks drawn from cognitive science , formal semantics , and machine translation .",3,0.45253888,44.991495974897354,42
2193,"Personalization of natural language generation plays a vital role in a large spectrum of tasks , such as explainable recommendation , review summarization and dialog systems .",0,0.9364218,37.32779936622472,27
2193,"In these tasks , user and item IDs are important identifiers for personalization .",0,0.806367,133.65760119435382,14
2193,"Transformer , which is demonstrated with strong language modeling capability , however , is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words .",3,0.5595518,65.17048754851865,43
2193,"To address this problem , we present a PErsonalized Transformer for Explainable Recommendation ( PETER ) , on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation , so as to endow the IDs with linguistic meanings and to achieve personalized Transformer .",1,0.4103747,57.76800687646824,55
2193,"Besides generating explanations , PETER can also make recommendations , which makes it a unified model for the whole recommendation-explanation pipeline .",3,0.71307117,118.42299346063335,24
2193,"Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task , in terms of both effectiveness and efficiency , which highlights the importance and the nice utility of our design .",3,0.94184124,26.674526577289104,37
2194,"Following each patient visit , physicians draft long semi-structured clinical summaries called SOAP notes .",2,0.8231179,132.29276654953782,15
2194,"While invaluable to clinicians and researchers , creating digital SOAP notes is burdensome , contributing to physician burnout .",0,0.8075069,268.1428424164219,19
2194,"In this paper , we introduce the first complete pipelines to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients .",1,0.8709627,47.38277301848282,29
2194,"After exploring a spectrum of methods across the extractive-abstractive spectrum , we propose Cluster2Sent , an algorithm that ( i ) extracts important utterances relevant to each summary section ;",2,0.6341057,137.6295031917117,32
2194,( ii ) clusters together related utterances ;,2,0.45828974,1434.310167225841,8
2194,and then ( iii ) generates one summary sentence per cluster .,2,0.6558187,437.73325847779705,12
2194,"Cluster2Sent outperforms its purely abstractive counterpart by 8 ROUGE-1 points , and produces significantly more factual and coherent sentences as assessed by expert human evaluators .",3,0.92337054,59.97520327927117,28
2194,"For reproducibility , we demonstrate similar benefits on the publicly available AMI dataset .",3,0.8849109,53.88218777855413,14
2194,Our results speak to the benefits of structuring summaries into sections and annotating supporting evidence when constructing summarization corpora .,3,0.98945194,36.38634105785608,20
2195,We investigate the problem of Chinese Grammatical Error Correction ( CGEC ) and present a new framework named Tail-to-Tail ( TtT ) non-autoregressive sequence prediction to address the deep issues hidden in CGEC .,1,0.7978561,30.20346340799399,38
2195,"Considering that most tokens are correct and can be conveyed directly from source to target , and the error positions can be estimated and corrected based on the bidirectional context information , thus we employ a BERT-initialized Transformer Encoder as the backbone model to conduct information modeling and conveying .",2,0.6958512,44.751077061646235,52
2195,"Considering that only relying on the same position substitution cannot handle the variable-length correction cases , various operations such substitution , deletion , insertion , and local paraphrasing are required jointly .",3,0.47635832,216.5910962090888,34
2195,"Therefore , a Conditional Random Fields ( CRF ) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the token dependencies .",2,0.58788145,78.42696701455051,27
2195,"Since most tokens are correct and easily to be predicted / conveyed to the target , then the models may suffer from a severe class imbalance issue .",0,0.6211918,88.24363648441478,28
2195,"To alleviate this problem , focal loss penalty strategies are integrated into the loss functions .",2,0.39043438,184.39225964009174,16
2195,"Moreover , besides the typical fix-length error correction datasets , we also construct a variable-length corpus to conduct experiments .",2,0.72770315,77.57612469594072,22
2195,"Experimental results on standard datasets , especially on the variable-length datasets , demonstrate the effectiveness of TtT in terms of sentence-level Accuracy , Precision , Recall , and F1-Measure on tasks of error Detection and Correction .",3,0.93392575,68.59849054006986,43
2196,"An important risk that children face today is online grooming , where a so-called sexual predator establishes an emotional connection with a minor online with the objective of sexual abuse .",0,0.9321792,40.84674697324169,31
2196,"Prior work has sought to automatically identify grooming chats , but only after an incidence has already happened in the context of legal prosecution .",0,0.93404126,109.16303334533382,25
2196,"In this work , we instead investigate this problem from the point of view of prevention .",1,0.8167333,33.31222555097022,17
2196,"We define and study the task of early sexual predator detection ( eSPD ) in chats , where the goal is to analyze a running chat from its beginning and predict grooming attempts as early and as accurately as possible .",1,0.7126292,59.54429364530405,41
2196,"We survey existing datasets and their limitations regarding eSPD , and create a new dataset called PANC for more realistic evaluations .",2,0.38876137,101.1534489518539,22
2196,We present strong baselines built on BERT that also reach state-of-the-art results for conventional SPD .,3,0.80868304,32.78546338217824,22
2196,"Finally , we consider coping with limited computational resources , as real-life applications require eSPD on mobile devices .",3,0.44939888,110.05651589765624,19
2197,Medical report generation is one of the most challenging tasks in medical image analysis .,0,0.96804327,19.49812520108144,15
2197,"Although existing approaches have achieved promising results , they either require a predefined template database in order to retrieve sentences or ignore the hierarchical nature of medical report generation .",0,0.8253365,48.141831084110834,30
2197,"To address these issues , we propose MedWriter that incorporates a novel hierarchical retrieval mechanism to automatically extract both report and sentence-level templates for clinically accurate report generation .",1,0.58406764,61.370106721061674,31
2197,MedWriter first employs the Visual-Language Retrieval ( VLR ) module to retrieve the most relevant reports for the given images .,2,0.43843117,44.85016744785129,22
2197,"To guarantee the logical coherence between generated sentences , the Language-Language Retrieval ( LLR ) module is introduced to retrieve relevant sentences based on the previous generated description .",2,0.56304634,37.661011925925024,30
2197,"At last , a language decoder fuses image features and features from retrieved reports and sentences to generate meaningful medical reports .",2,0.6880146,129.1312531448029,22
2197,"We verified the effectiveness of our model by automatic evaluation and human evaluation on two datasets , i.e. , Open-I and MIMIC-CXR .",2,0.54053444,29.223447679729798,25
2198,Hierarchical Text Classification ( HTC ) is a challenging task that categorizes a textual description within a taxonomic hierarchy .,0,0.95887834,43.48926407860551,20
2198,Most of the existing methods focus on modeling the text .,0,0.858464,19.81776864024168,11
2198,"Recently , researchers attempt to model the class representations with some resources ( e.g. , external dictionaries ) .",0,0.92296416,80.11315058582753,19
2198,"However , the concept shared among classes which is a kind of domain-specific and fine-grained information has been ignored in previous work .",0,0.8739711,34.02517873142513,24
2198,"In this paper , we propose a novel concept-based label embedding method that can explicitly represent the concept and model the sharing mechanism among classes for the hierarchical text classification .",1,0.85076946,34.60123185355818,32
2198,Experimental results on two widely used datasets prove that the proposed model outperforms several state-of-the-art methods .,3,0.9139512,5.026141108621082,22
2198,We release our complementary resources ( concepts and definitions of classes ) for these two datasets to benefit the research on HTC .,3,0.5931626,146.0473067072619,23
2199,"Text-to-image retrieval is an essential task in cross-modal information retrieval , i.e. , retrieving relevant images from a large and unlabelled dataset given textual queries .",0,0.94063,29.304192894506055,27
2199,"In this paper , we propose VisualSparta , a novel ( Visual-text Sparse Transformer Matching ) model that shows significant improvement in terms of both accuracy and efficiency .",1,0.8640557,42.4821300876127,31
2199,VisualSparta is capable of outperforming previous state-of-the-art scalable methods in MSCOCO and Flickr30K .,3,0.9165995,28.221156201473836,19
2199,"We also show that it achieves substantial retrieving speed advantages , i.e. , for a 1 million image index , VisualSparta using CPU gets ~ 391X speedup compared to CPU vector search and ~ 5.4X speedup compared to vector search with GPU acceleration .",3,0.9346203,65.6525993123509,44
2199,Experiments show that this speed advantage even gets bigger for larger datasets because VisualSparta can be efficiently implemented as an inverted index .,3,0.91900885,99.00307869405437,23
2199,"To the best of our knowledge , VisualSparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets , with significant accuracy improvement compared to previous state-of-the-art methods .",3,0.95545244,12.44368237868865,47
2200,"The effectiveness of Neural Information Retrieval ( Neu-IR ) often depends on a large scale of in-domain relevance training signals , which are not always available in real-world ranking scenarios .",0,0.9230771,31.509257186906993,34
2200,"To democratize the benefits of Neu-IR , this paper presents MetaAdaptRank , a domain adaptive learning method that generalizes Neu-IR models from label-rich source domains to few-shot target domains .",1,0.5896904,50.46531883067031,33
2200,"Drawing on source-domain massive relevance supervision , MetaAdaptRank contrastively synthesizes a large number of weak supervision signals for target domains and meta-learns to reweight these synthetic “ weak ” data based on their benefits to the target-domain ranking accuracy of Neu-IR models .",2,0.5284049,130.17265377473484,45
2200,"Experiments on three TREC benchmarks in the web , news , and biomedical domains show that MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models .",3,0.9195452,78.16219619985186,28
2200,Further analyses indicate that MetaAdaptRank thrives from both its contrastive weak data synthesis and meta-reweighted data selection .,3,0.9872071,162.16981512284758,18
2200,The code and data of this paper can be obtained from https://github.com/thunlp/MetaAdaptRank .,3,0.57711285,10.379597298400066,13
2201,Semi-Supervised Text Classification ( SSTC ) mainly works under the spirit of self-training .,0,0.8589915,30.97993801603808,14
2201,They initialize the deep classifier by training over labeled texts ;,2,0.3552727,439.3147143354221,11
2201,and then alternatively predict unlabeled texts as their pseudo-labels and train the deep classifier over the mixture of labeled and pseudo-labeled texts .,2,0.7354132,33.592032211411876,23
2201,"Naturally , their performance is largely affected by the accuracy of pseudo-labels for unlabeled texts .",0,0.8380298,28.189953465315504,16
2201,"Unfortunately , they often suffer from low accuracy because of the margin bias problem caused by the large difference between representation distributions of labels in SSTC .",0,0.8502365,57.79252795897917,27
2201,"To alleviate this problem , we apply the angular margin loss , and perform Gaussian linear transformation to achieve balanced label angle variances , i.e. , the variance of label angles of texts within the same label .",2,0.7605994,85.62692767015868,38
2201,"More accuracy of predicted pseudo-labels can be achieved by constraining all label angle variances balanced , where they are estimated over both labeled and pseudo-labeled texts during self-training loops .",3,0.7972499,91.34374803970019,30
2201,"With this insight , we propose a novel SSTC method , namely Semi-Supervised Text Classification with Balanced Deep representation Distributions ( S2TC-BDD ) .",1,0.50073874,71.01081325168116,26
2201,"To evaluate S2TC-BDD , we compare it against the state-of-the-art SSTC methods .",2,0.6841667,27.625451590354437,20
2201,"Empirical results demonstrate the effectiveness of S2TC-BDD , especially when the labeled texts are scarce .",3,0.9781487,46.36801271664806,18
2202,"Recently , the retrieval models based on dense representations have been gradually applied in the first stage of the document retrieval tasks , showing better performance than traditional sparse vector space models .",0,0.9334961,52.42885769161268,33
2202,"To obtain high efficiency , the basic structure of these models is Bi-encoder in most cases .",0,0.6504732,77.79497670571276,17
2202,"However , this simple structure may cause serious information loss during the encoding of documents since the queries are agnostic .",0,0.6652239,100.29546117352817,21
2202,"To address this problem , we design a method to mimic the queries to each of the documents by an iterative clustering process and represent the documents by multiple pseudo queries ( i.e. , the cluster centroids ) .",2,0.72403044,47.79974805062297,39
2202,"To boost the retrieval process using approximate nearest neighbor search library , we also optimize the matching function with a two-step score calculation procedure .",2,0.8489962,112.93940809127608,26
2202,Experimental results on several popular ranking and QA datasets show that our model can achieve state-of-the-art results while still remaining high efficiency .,3,0.9336137,10.724835964681093,29
2203,Learning high-quality sentence representations benefits a wide range of natural language processing tasks .,0,0.87612283,16.522160168847222,14
2203,"Though BERT-based pre-trained language models achieve high performance on many downstream tasks , the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity ( STS ) tasks .",0,0.633682,32.58618376598344,41
2203,"In this paper , we present ConSERT , a Contrastive Framework for Self-Supervised SEntence Representation Transfer , that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way .",1,0.8527159,31.620682218595544,32
2203,"By making use of unlabeled texts , ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks .",3,0.54100585,48.41708618413966,27
2203,"Experiments on STS datasets demonstrate that ConSERT achieves an 8 % relative improvement over the previous state-of-the-art , even comparable to the supervised SBERT-NLI .",3,0.9327609,29.280796914454687,32
2203,"And when further incorporating NLI supervision , we achieve new state-of-the-art performance on STS tasks .",3,0.9180819,25.0567916522829,22
2203,"Moreover , ConSERT obtains comparable results with only 1000 samples available , showing its robustness in data scarcity scenarios .",3,0.9644056,61.82239458673133,20
2204,"Due to the great potential in facilitating software development , code generation has attracted increasing attention recently .",0,0.9634571,43.01914792153951,18
2204,"Generally , dominant models are Seq2 Tree models , which convert the input natural language description into a sequence of tree-construction actions corresponding to the pre-order traversal of an Abstract Syntax Tree ( AST ) .",0,0.8609355,41.665825208456575,36
2204,"However , such a traversal order may not be suitable for handling all multi-branch nodes .",0,0.5913016,39.15929541416068,16
2204,"In this paper , we propose to equip the Seq2 Tree model with a context-based Branch Selector , which is able to dynamically determine optimal expansion orders of branches for multi-branch nodes .",1,0.86116624,52.447448507357606,35
2204,"Particularly , since the selection of expansion orders is a non-differentiable multi-step operation , we optimize the selector through reinforcement learning , and formulate the reward function as the difference of model losses obtained through different expansion orders .",2,0.7291462,61.74603166214022,39
2204,Experimental results and in-depth analysis on several commonly-used datasets demonstrate the effectiveness and generality of our approach .,3,0.8994365,7.969962431505912,21
2204,We have released our code at https://github.com/DeepLearnXMU/CG-RL .,3,0.5330158,19.902098456353887,8
2205,"Despite recent successes of large pre-trained language models in solving reasoning tasks , their inference capabilities remain opaque .",0,0.928587,54.75236104031265,19
2205,"We posit that such models can be made more interpretable by explicitly generating interim inference rules , and using them to guide the generation of task-specific textual outputs .",3,0.41101152,38.61538455438939,30
2205,"In this paper we present Coins , a recursive inference framework that i ) iteratively reads context sentences , ii ) dynamically generates contextualized inference rules , encodes them , and iii ) uses them to guide task-specific output generation .",1,0.79321903,60.71403836249429,42
2205,"We apply to a Narrative Story Completion task that asks a model to complete a story with missing sentences , to produce a coherent story with plausible logical connections , causal relationships , and temporal dependencies .",2,0.79060465,57.27798891738553,37
2205,"By modularizing inference and sentence generation steps in a recurrent model , we aim to make reasoning steps and their effects on next sentence generation transparent .",1,0.433009,89.82204102735315,27
2205,"Our automatic and manual evaluations show that the model generates better story sentences than SOTA baselines , especially in terms of coherence .",3,0.9684921,34.328656197619644,23
2205,We further demonstrate improved performance over strong pre-trained LMs in generating commonsense inference rules .,3,0.96156496,55.021698355479266,15
2205,The recursive nature of holds the potential for controlled generation of longer sequences .,0,0.6319675,149.17530899325104,14
2206,"Procedural text understanding aims at tracking the states ( e.g. , create , move , destroy ) and locations of the entities mentioned in a given paragraph .",0,0.9411625,51.91636587095539,28
2206,"To effectively track the states and locations , it is essential to capture the rich semantic relations between entities , actions , and locations in the paragraph .",0,0.66126835,50.922688988637994,28
2206,"Although recent works have achieved substantial progress , most of them focus on leveraging the inherent constraints or incorporating external knowledge for state prediction .",0,0.8861698,47.15007590653075,25
2206,The rich semantic relations in the given paragraph are largely overlooked .,0,0.6404929,68.39853003068832,12
2206,"In this paper , we propose a novel approach ( REAL ) to procedural text understanding , where we build a general framework to systematically model the entity-entity , entity-action , and entity-location relations using a graph neural network .",1,0.76507545,41.150930219312855,43
2206,"We further develop algorithms for graph construction , representation learning , and state and location tracking .",3,0.4677715,179.67072493514559,17
2206,"We evaluate the proposed approach on two benchmark datasets , ProPara , and Recipes .",2,0.57619816,81.32176217323222,15
2206,"The experimental results show that our method outperforms strong baselines by a large margin , i.e. , 5.0 % on ProPara and 3.2 % on Recipes , illustrating the utility of semantic relations and the effectiveness of the graph-based reasoning model .",3,0.9453068,22.86319751749649,44
2207,Semantic parsing is challenging due to the structure gap and the semantic gap between utterances and logical forms .,0,0.9262177,22.716861209504206,19
2207,"In this paper , we propose an unsupervised semantic parsing method-Synchronous Semantic Decoding ( SSD ) , which can simultaneously resolve the semantic gap and the structure gap by jointly leveraging paraphrasing and grammar-constrained decoding .",1,0.89495707,39.300031066533876,36
2207,"Specifically , we reformulate semantic parsing as a constrained paraphrasing problem : given an utterance , our model synchronously generates its canonical utterancel and meaning representation .",2,0.74990785,101.25459737356654,27
2207,"During synchronously decoding : the utterance paraphrasing is constrained by the structure of the logical form , therefore the canonical utterance can be paraphrased controlledly ;",3,0.6232825,94.69824040696324,26
2207,"the semantic decoding is guided by the semantics of the canonical utterance , therefore its logical form can be generated unsupervisedly .",3,0.43332824,51.72386473416904,22
2207,Experimental results show that SSD is a promising approach and can achieve state-of-the-art unsupervised semantic parsing performance on multiple datasets .,3,0.9701273,10.139067161269258,27
2208,"Despite the well-developed cut-edge representation learning for language , most language representation models usually focus on specific levels of linguistic units .",0,0.9200984,85.81834867719849,24
2208,"This work introduces universal language representation learning , i.e. , embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space .",0,0.32061926,73.81534234099804,29
2208,We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models .,2,0.41401973,40.87703557753896,26
2208,"Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation , which will bring great convenience when handling multiple layers of linguistic objects in a unified way .",3,0.78222156,79.68039928894028,33
2208,"Especially , our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset .",3,0.9468344,27.828867604369933,33
2209,Pre-trained language models ( PrLMs ) have demonstrated superior performance due to their strong ability to learn universal language representations from self-supervised pre-training .,0,0.9303488,12.91532022186526,25
2209,"However , even with the help of the powerful PrLMs , it is still challenging to effectively capture task-related knowledge from dialogue texts which are enriched by correlations among speaker-aware utterances .",0,0.802115,45.89870928230785,36
2209,"In this work , we present SPIDER , Structural Pre-traIned DialoguE Reader , to capture dialogue exclusive features .",1,0.56284153,285.1775402783225,19
2209,"To simulate the dialogue-like features , we propose two training objectives in addition to the original LM objectives : 1 ) utterance order restoration , which predicts the order of the permuted utterances in dialogue context ;",2,0.7288617,72.25564785892414,38
2209,"2 ) sentence backbone regularization , which regularizes the model to improve the factual correctness of summarized subject-verb-object triplets .",2,0.6775935,94.55610691306866,22
2209,Experimental results on widely used dialogue benchmarks verify the effectiveness of the newly introduced self-supervised tasks .,3,0.91411066,21.57758994223793,17
2210,Pre-trained language models ( PLMs ) have achieved great success in natural language processing .,0,0.9258294,6.689886156775598,15
2210,"Most of PLMs follow the default setting of architecture hyper-parameters ( e.g. , the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks ) in BERT .",0,0.5547507,60.77695187739772,30
2210,"Few studies have been conducted to explore the design of architecture hyper-parameters in BERT , especially for the more efficient PLMs with tiny sizes , which are essential for practical deployment on resource-constrained devices .",0,0.84839016,41.40750335206939,36
2210,"In this paper , we adopt the one-shot Neural Architecture Search ( NAS ) to automatically search architecture hyper-parameters .",1,0.53979236,74.74566412688408,20
2210,"Specifically , we carefully design the techniques of one-shot learning and the search space to provide an adaptive and efficient development way of tiny PLMs for various latency constraints .",2,0.744895,87.9173384742379,31
2210,We name our method AutoTinyBERT and evaluate its effectiveness on the GLUE and SQuAD benchmarks .,2,0.4610359,18.85672513354754,16
2210,"The extensive experiments show that our method outperforms both the SOTA search-based baseline ( NAS-BERT ) and the SOTA distillation-based methods ( such as DistilBERT , TinyBERT , MiniLM , and MobileBERT ) .",3,0.9189264,27.737647964770023,40
2210,"In addition , based on the obtained architectures , we propose a more efficient development method that is even faster than the development of a single PLM .",3,0.80394304,48.57159998492903,28
2210,The source code and models will be publicly available upon publication .,3,0.44218832,17.717159854851346,12
2211,"Due to recent pretrained multilingual representation models , it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages .",0,0.9233318,18.020878287738505,34
2211,"In practice , however , we still face the problem of scarce labeled data , leading to subpar results .",0,0.7276235,80.7933241748363,20
2211,"In this paper , we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way .",1,0.9072616,18.222081790317144,31
2211,"To this end , we propose two methods of training a generative model to induce synthesized examples , and then leverage the resulting data using an adversarial training regimen for more robustness .",2,0.6274195,34.89752283600823,33
2211,"In a series of detailed experiments , we show that this fruitful combination leads to substantial gains in cross-lingual inference .",3,0.86029285,21.197764158590665,21
2212,"As high-quality labeled data is scarce , unsupervised sentence representation learning has attracted much attention .",0,0.9498171,18.9765168058933,16
2212,"In this paper , we propose a new framework with a two-branch Siamese Network which maximizes the similarity between two augmented views of each sentence .",1,0.753583,34.52431592225491,27
2212,"Specifically , given one augmented view of the input sentence , the online network branch is trained by predicting the representation yielded by the target network of the same sentence under another augmented view .",2,0.7261435,92.05270362689254,35
2212,"Meanwhile , the target network branch is bootstrapped with a moving average of the online network .",2,0.54470295,97.42357104625883,17
2212,The proposed method significantly outperforms other state-of-the-art unsupervised methods on semantic textual similarity ( STS ) and classification tasks .,3,0.8986382,12.346803662175596,26
2212,It can be adopted as a post-training procedure to boost the performance of the supervised methods .,3,0.7316226,17.207141760889925,17
2212,We further extend our method for learning multilingual sentence representations and demonstrate its effectiveness on cross-lingual STS tasks .,3,0.7872026,16.414832527755525,19
2212,Our code is available at https://github.com/yanzhangnlp/BSL .,3,0.54547995,10.243452270428342,7
2213,"Abductive reasoning aims at inferring the most plausible explanation for observed events , which would play critical roles in various NLP applications , such as reading comprehension and question answering .",0,0.92358726,28.504273564077078,31
2213,"To facilitate this task , a narrative text based abductive reasoning task 𝛼 NLI is proposed , together with explorations about building reasoning framework using pretrained language models .",2,0.48021644,99.17336385296541,29
2213,"However , abundant event commonsense knowledge is not well exploited for this task .",0,0.93772453,69.6854049797947,14
2213,"To fill this gap , we propose a variational autoencoder based model ege-RoBERTa , which employs a latent variable to capture the necessary commonsense knowledge from event graph for guiding the abductive reasoning task .",2,0.5592373,36.19377760385012,35
2213,"Experimental results show that through learning the external event graph knowledge , our approach outperforms the baseline methods on the 𝛼NLI task .",3,0.96308047,31.075396213785446,23
2214,"The uniform information density ( UID ) hypothesis , which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal , has gained traction in psycholinguistics as an explanation for certain syntactic , morphological , and prosodic choices .",0,0.958552,59.283198501691416,43
2214,"In this work , we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling .",1,0.9164721,33.77750942443346,22
2214,"Specifically , we augment the canonical MLE objective for training language models with a regularizer that encodes UID .",2,0.8027311,63.90766018335733,19
2214,"In experiments on ten languages spanning five language families , we find that using UID regularization consistently improves perplexity in language models , having a larger effect when training data is limited .",3,0.9278044,48.23769806537915,33
2214,"Moreover , via an analysis of generated sequences , we find that UID-regularized language models have other desirable properties , e.g. , they generate text that is more lexically diverse .",3,0.9424962,54.32398226106831,33
2214,"Our results not only suggest that UID is a reasonable inductive bias for language modeling , but also provide an alternative validation of the UID hypothesis using modern-day NLP tools .",3,0.99060494,36.50230440261648,33
2215,"In computational psycholinguistics , various language models have been evaluated against human reading behavior ( e.g. , eye movement ) to build human-like computational models .",0,0.8884085,45.140695277638066,26
2215,"However , most previous efforts have focused almost exclusively on English , despite the recent trend towards linguistic universal within the general community .",0,0.9364002,88.8087766499471,24
2215,"In order to fill the gap , this paper investigates whether the established results in computational psycholinguistics can be generalized across languages .",1,0.9196246,28.243335148139188,23
2215,"Specifically , we re-examine an established generalization — the lower perplexity a language model has , the more human-like the language model is — in Japanese with typologically different structures from English .",2,0.5675102,45.41439106458555,34
2215,Our experiments demonstrate that this established generalization exhibits a surprising lack of universality ;,3,0.9719092,152.08068486293112,14
2215,"namely , lower perplexity is not always human-like .",3,0.5555847,125.50199361565166,9
2215,"Moreover , this discrepancy between English and Japanese is further explored from the perspective of ( non-) uniform information density .",3,0.61558515,78.99520463570494,23
2215,"Overall , our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models .",3,0.99133223,20.23642498448043,18
2216,Lately proposed Word Sense Disambiguation ( WSD ) systems have approached the estimated upper bound of the task on standard evaluation benchmarks .,0,0.9493857,58.39274197143417,23
2216,"However , these systems typically implement the disambiguation of words in a document almost independently , underutilizing sense and word dependency in context .",0,0.9117075,89.46149144446714,24
2216,"In this paper , we convert the nearly isolated decisions into interrelated ones by exposing senses in context when learning sense embeddings in a similarity-based Sense Aware Context Exploitation ( SACE ) architecture .",1,0.75061077,91.18055863624815,36
2216,"Meanwhile , we enhance the context embedding learning with selected sentences from the same document , rather than utilizing only the sentence where each ambiguous word appears .",2,0.5874913,83.02306032001273,28
2216,"Experiments on both English and multilingual WSD datasets have shown the effectiveness of our approach , surpassing previous state-of-the-art by large margins ( 3.7 % and 1.2 % respectively ) , especially on few-shot ( 14.3 % ) and zero-shot ( 35.9 % ) scenarios .",3,0.893303,14.380324115181004,52
2217,Frame Identification ( FI ) is a fundamental and challenging task in frame semantic parsing .,0,0.94856995,79.54623953122355,16
2217,The task aims to find the exact frame evoked by a target word in a given sentence .,0,0.6792032,22.583260763724383,18
2217,"It is generally regarded as a classification task in existing work , where frames are treated as discrete labels or represented using onehot embeddings .",0,0.78157175,65.69951185234764,25
2217,"However , the valuable knowledge about frames is neglected .",0,0.8765595,200.87520212124056,10
2217,"In this paper , we propose a Knowledge-Guided Frame Identification framework ( KGFI ) that integrates three types frame knowledge , including frame definitions , frame elements and frame-to-frame relations , to learn better frame representation , which guides the KGFI to jointly map target words and frames into the same embedding space and subsequently identify the best frame by calculating the dot-product similarity scores between the target word embedding and all of the frame embeddings .",1,0.34072945,35.420281650005194,82
2217,The extensive experimental results demonstrate KGFI significantly outperforms the state-of-the-art methods on two benchmark datasets .,3,0.9548527,10.191989882045345,20
2218,The advent of contextual word embeddings — representations of words which incorporate semantic and syntactic information from their context — has led to tremendous improvements on a wide variety of NLP tasks .,0,0.9569632,12.165435974346636,33
2218,"However , recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret .",0,0.924446,34.012136758076615,20
2218,"In this work , we demonstrate that our proposed distillation method , which is a simple extension of CBOW-based training , allows to significantly improve computational efficiency of NLP applications , while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods .",3,0.4817126,41.879253603745184,54
2218,"As a side-effect , our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks .",3,0.708962,32.97246902202051,23
2219,A critical challenge faced by supervised word sense disambiguation ( WSD ) is the lack of large annotated datasets with sufficient coverage of words in their diversity of senses .,0,0.9660076,30.954938459454105,30
2219,This inspired recent research on few-shot WSD using meta-learning .,0,0.8593725,58.62430368254394,10
2219,"While such work has successfully applied meta-learning to learn new word senses from very few examples , its performance still lags behind its fully-supervised counterpart .",0,0.84948945,36.27686914597575,28
2219,"Aiming to further close this gap , we propose a model of semantic memory for WSD in a meta-learning setting .",1,0.7377456,33.52444057160879,21
2219,"Semantic memory encapsulates prior experiences seen throughout the lifetime of the model , which aids better generalization in limited data settings .",0,0.410612,107.46451928212937,22
2219,Our model is based on hierarchical variational inference and incorporates an adaptive memory update rule via a hypernetwork .,2,0.7770964,49.361105168975776,19
2219,"We show our model advances the state of the art in few-shot WSD , supports effective learning in extremely data scarce ( e.g .",3,0.9528893,76.1037491967006,24
2219,one-shot ) scenarios and produces meaning prototypes that capture similar senses of distinct words .,2,0.37604442,242.87514678321168,15
2220,"Transformer-based language models ( LMs ) pretrained on large text collections implicitly store a wealth of lexical semantic knowledge , but it is non-trivial to extract that knowledge effectively from their parameters .",0,0.93642503,32.50956138825316,35
2220,"Inspired by prior work on semantic specialization of static word embedding ( WE ) models , we show that it is possible to expose and enrich lexical knowledge from the LMs , that is , to specialize them to serve as effective and universal “ decontextualized ” word encoders even when fed input words “ in isolation ” ( i.e. , without any context ) .",3,0.44463798,55.13192274394783,66
2220,Their transformation into such word encoders is achieved through a simple and efficient lexical fine-tuning procedure ( termed LexFit ) based on dual-encoder network structures .,2,0.4712114,42.27484056556803,26
2220,"Further , we show that LexFit can yield effective word encoders even with limited lexical supervision and , via cross-lingual transfer , in different languages without any readily available external knowledge .",3,0.9705118,45.10395670676892,32
2220,"Our evaluation over four established , structurally different lexical-level tasks in 8 languages indicates the superiority of LexFit-based WEs over standard static WEs ( e.g. , fastText ) and WEs from vanilla LMs .",3,0.9309376,88.15018902719187,36
2220,"Other extensive experiments and ablation studies further profile the LexFit framework , and indicate best practices and performance variations across LexFit variants , languages , and lexical tasks , also directly questioning the usefulness of traditional WE models in the era of large neural models .",3,0.8461919,157.95741598218908,46
2221,"In this paper we present the first model for directly synthesizing fluent , natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision .",1,0.8350231,48.18609875562332,37
2221,"Instead , we connect the image captioning module and the speech synthesis module with a set of discrete , sub-word speech units that are discovered with a self-supervised visual grounding task .",2,0.66155463,52.341093634005,32
2221,"We conduct experiments on the Flickr8 k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset , demonstrating that our generated captions also capture diverse visual semantics of the images they describe .",2,0.67114943,51.64484049349863,43
2221,"We investigate several different intermediate speech representations , and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text .",3,0.7553868,66.2248722792007,29
2222,Multimodal sentiment analysis is the challenging research area that attends to the fusion of multiple heterogeneous modalities .,0,0.954888,27.56432488095035,18
2222,The main challenge is the occurrence of some missing modalities during the multimodal fusion procedure .,0,0.8120673,35.364336501462624,16
2222,"However , the existing techniques require all modalities as input , thus are sensitive to missing modalities at predicting time .",0,0.87340266,85.26309005789422,21
2222,"In this work , the coupled-translation fusion network ( CTFN ) is firstly proposed to model bi-direction interplay via couple learning , ensuring the robustness in respect to missing modalities .",1,0.38946828,122.91803061511482,33
2222,"Specifically , the cyclic consistency constraint is presented to improve the translation performance , allowing us directly to discard decoder and only embraces encoder of Transformer .",2,0.44354033,175.0387038645431,27
2222,This could contribute to a much lighter model .,3,0.85050154,66.10564465807694,9
2222,"Due to the couple learning , CTFN is able to conduct bi-direction cross-modality intercorrelation parallelly .",3,0.75617516,148.902836780319,16
2222,"Based on CTFN , a hierarchical architecture is further established to exploit multiple bi-direction translations , leading to double multimodal fusing embeddings compared with traditional translation methods .",3,0.5677782,102.96075700192732,28
2222,"Moreover , the convolution block is utilized to further highlight explicit interactions among those translations .",2,0.4967464,185.2124851929595,16
2222,"For evaluation , CTFN was verified on two multimodal benchmarks with extensive ablation studies .",2,0.76999855,98.42632136858295,15
2222,The experiments demonstrate that the proposed framework achieves state-of-the-art or often competitive performance .,3,0.951771,13.077105225481754,20
2222,"Additionally , CTFN still maintains robustness when considering missing modality .",3,0.91372085,167.6578597992548,11
2223,"In this work , we demonstrate that the contextualized word vectors derived from pretrained masked language model-based encoders share a common , perhaps undesirable pattern across layers .",1,0.5693625,49.97142488316206,30
2223,"Namely , we find cases of persistent outlier neurons within BERT and RoBERTa ’s hidden state vectors that consistently bear the smallest or largest values in said vectors .",3,0.9365954,130.58869740948725,29
2223,"In an attempt to investigate the source of this information , we introduce a neuron-level analysis method , which reveals that the outliers are closely related to information captured by positional embeddings .",2,0.4657132,31.400755137958768,33
2223,We also pre-train the RoBERTa-base models from scratch and find that the outliers disappear without using positional embeddings .,3,0.5981665,27.98213676123035,21
2223,"These outliers , we find , are the major cause of anisotropy of encoders ’ raw vector spaces , and clipping them leads to increased similarity across vectors .",3,0.92287654,85.30099043907948,29
2223,"We demonstrate this in practice by showing that clipped vectors can more accurately distinguish word senses , as well as lead to better sentence embeddings when mean pooling .",3,0.87500674,73.17384853023819,29
2223,"In three supervised tasks , we find that clipping does not affect the performance .",3,0.96235025,37.43049092568418,15
2224,We propose an alternate approach to quantifying how well language models learn natural language : we ask how well they match the statistical tendencies of natural language .,1,0.38059902,40.494277560492264,28
2224,"To answer this question , we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained .",1,0.71466297,33.2652088521521,29
2224,We provide a framework –paired with significance tests –for evaluating the fit of language models to these trends .,3,0.49301478,91.67633660495717,20
2224,"We find that neural language models appear to learn only a subset of the tendencies considered , but align much more closely with empirical trends than proposed theoretical distributions ( when present ) .",3,0.977227,103.80535579768619,34
2224,"Further , the fit to different distributions is highly-dependent on both model architecture and generation strategy .",3,0.82716227,48.784274550601346,19
2224,"As concrete examples , text generated under the nucleus sampling scheme adheres more closely to the type–token relationship of natural language than text produced using standard ancestral sampling ;",3,0.90372676,181.24679650811893,29
2224,"text from LSTMs reflects the natural language distributions over length , stopwords , and symbols surprisingly well .",3,0.597917,187.0383554992863,18
2225,"The importance of explaining the outcome of a machine learning model , especially a black-box model , is widely acknowledged .",0,0.9368076,29.04836756964065,23
2225,Recent approaches explain an outcome by identifying the contributions of input features to this outcome .,0,0.8980105,54.758131203553916,16
2225,"In environments involving large black-box models or complex inputs , this leads to computationally demanding algorithms .",0,0.8210058,69.47382810182494,19
2225,"Further , these algorithms often suffer from low stability , with explanations varying significantly across similar examples .",0,0.7710452,126.6965035700624,18
2225,"In this paper , we propose a Learning to Explain ( L2E ) approach that learns the behaviour of an underlying explanation algorithm simultaneously from all training examples .",1,0.8224029,47.77739364467305,29
2225,"Once the explanation algorithm is distilled into an explainer network , it can be used to explain new instances .",0,0.42681175,49.97726314107996,20
2225,"Our experiments on three classification tasks , which compare our approach to six explanation algorithms , show that L2E is between 5 and 7.5 × 10 ˆ4 times faster than these algorithms , while generating more stable explanations , and having comparable faithfulness to the black-box model .",3,0.87349343,48.62910738715853,51
2226,"A stereotype is an over-generalized belief about a particular group of people , e.g. , Asians are good at math or African Americans are athletic .",0,0.87273675,26.230208560611807,26
2226,Such beliefs ( biases ) are known to hurt target groups .,0,0.9256241,201.55287040412531,12
2226,"Since pretrained language models are trained on large real-world data , they are known to capture stereotypical biases .",0,0.83589244,23.683135491384352,19
2226,It is important to quantify to what extent these biases are present in them .,0,0.83959013,19.639767544887967,15
2226,"Although this is a rapidly growing area of research , existing literature lacks in two important aspects : 1 ) they mainly evaluate bias of pretrained language models on a small set of artificial sentences , even though these models are trained on natural data 2 ) current evaluations focus on measuring bias without considering the language modeling ability of a model , which could lead to misleading trust on a model even if it is a poor language model .",0,0.7095331,37.359557887694315,81
2226,We address both these problems .,1,0.4565587,73.62070863299395,6
2226,"We present StereoSet , a large-scale natural English dataset to measure stereotypical biases in four domains : gender , profession , race , and religion .",2,0.41821033,87.8107110501996,26
2226,"We contrast both stereotypical bias and language modeling ability of popular models like BERT , GPT-2 , RoBERTa , and XLnet .",2,0.662746,62.48099452779719,24
2226,We show that these models exhibit strong stereotypical biases .,3,0.9076319,70.5998895261043,10
2226,Our data and code are available at https://stereoset.mit.edu .,3,0.6290547,13.76273987474796,9
2227,"Deep learning models have achieved great success on the task of Natural Language Inference ( NLI ) , though only a few attempts try to explain their behaviors .",0,0.9544543,28.211906101818197,29
2227,Existing explanation methods usually pick prominent features such as words or phrases from the input text .,0,0.8482038,54.78022532175891,17
2227,"However , for NLI , alignments among words or phrases are more enlightening clues to explain the model .",0,0.7268079,100.3551640402915,19
2227,"To this end , this paper presents AREC , a post-hoc approach to generate alignment rationale explanations for co-attention based models in NLI .",1,0.8076184,54.46066639190697,24
2227,"The explanation is based on feature selection , which keeps few but sufficient alignments while maintaining the same prediction of the target model .",3,0.5896132,68.91352508368405,24
2227,Experimental results show that our method is more faithful and human-readable compared with many existing approaches .,3,0.9768908,13.429020442182418,17
2227,"We further study and re-evaluate three typical models through our explanation beyond accuracy , and propose a simple method that greatly improves the model robustness .",3,0.6398681,85.60712736090119,26
2228,This paper presents a novel pre-trained language models ( PLM ) compression approach based on the matrix product operator ( short as MPO ) from quantum many-body physics .,1,0.80487317,83.9350445481473,31
2228,It can decompose an original matrix into central tensors ( containing the core information ) and auxiliary tensors ( with only a small proportion of parameters ) .,2,0.3392569,60.354813699151514,28
2228,"With the decomposed MPO structure , we propose a novel fine-tuning strategy by only updating the parameters from the auxiliary tensors , and design an optimization algorithm for MPO-based approximation over stacked network architectures .",2,0.62478834,61.51460438165646,37
2228,"Our approach can be applied to the original or the compressed PLMs in a general way , which derives a lighter network and significantly reduces the parameters to be fine-tuned .",3,0.8935279,55.617184199009856,31
2228,"Extensive experiments have demonstrated the effectiveness of the proposed approach in model compression , especially the reduction in fine-tuning parameters ( 91 % reduction on average ) .",3,0.92668813,34.54801355557352,28
2228,The code to reproduce the results of this paper can be found at https://github.com/RUCAIBox/MPOP .,3,0.77904135,14.100119215305636,15
2229,"In the recent advances of natural language processing , the scale of the state-of-the-art models and datasets is usually extensive , which challenges the application of sample-based explanation methods in many aspects , such as explanation interpretability , efficiency , and faithfulness .",0,0.89032763,34.05374578333969,51
2229,"In this work , for the first time , we can improve the interpretability of explanations by allowing arbitrary text sequences as the explanation unit .",3,0.76076835,47.01497892055917,26
2229,"On top of this , we implement a hessian-free method with a model faithfulness guarantee .",2,0.6869409,74.17732223520336,18
2229,"Finally , to compare our method with the others , we propose a semantic-based evaluation metric that can better align with humans ’ judgment of explanations than the widely adopted diagnostic or re-training measures .",3,0.52818877,51.556975237444476,37
2229,The empirical results on multiple real data sets demonstrate the proposed method ’s superior performance to popular explanation techniques such as Influence Function or TracIn on semantic evaluation .,3,0.95899683,106.07803829467844,29
2230,We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa .,1,0.5672773,9.478122462771214,21
2230,"Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage , so that they suffer from discrepancy between the two stages .",0,0.6908324,20.21070136144757,28
2230,"Such a problem would lead to the necessity of having human-annotated syntactic information , which limits the application of existing methods to broader scenarios .",0,0.84938604,29.71139826594238,25
2230,"To address this , we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages .",2,0.4791786,15.96051720976037,21
2230,Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text .,2,0.7893911,22.963902593192877,22
2230,We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree .,2,0.53687876,27.529391380955552,19
2230,"We evaluate the model on three downstream tasks , including relation classification , entity typing , and question answering .",2,0.6026567,41.74408840031727,20
2230,Results show that our model achieves state-of-the-art performance on six public benchmark datasets .,3,0.9770082,5.624234641540329,20
2230,We have two major findings .,3,0.9401142,31.87827653134777,6
2230,"First , we demonstrate that infusing automatically produced syntax of text improves pre-trained models .",3,0.6571672,95.14933249540623,15
2230,"Second , global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens .",3,0.7367905,177.59638123781176,20
2231,Unsupervised Domain Adaptation ( UDA ) aims to transfer the knowledge of source domain to the unlabeled target domain .,0,0.902506,25.074540729478137,20
2231,Existing methods typically require to learn to adapt the target model by exploiting the source data and sharing the network architecture across domains .,0,0.8435056,40.386040067017206,24
2231,"However , this pipeline makes the source data risky and is inflexible for deploying the target model .",0,0.83377194,90.7137574411168,18
2231,This paper tackles a novel setting where only a trained source model is available and different network architectures can be adapted for target domain in terms of deployment environments .,1,0.46919355,51.89363278288181,30
2231,We propose a generic framework named Cross-domain Knowledge Distillation ( CdKD ) without needing any source data .,2,0.3999704,37.95694292740344,18
2231,CdKD matches the joint distributions between a trained source model and a set of target data during distilling the knowledge from the source model to the target domain .,3,0.4508324,36.08875673404677,29
2231,"As a type of important knowledge in the source domain , for the first time , the gradient information is exploited to boost the transfer performance .",0,0.4669748,69.63059936834726,27
2231,"Experiments on cross-domain text classification demonstrate that CdKD achieves superior performance , which verifies the effectiveness in this novel setting .",3,0.94167334,47.154696370542766,21
2232,"Today ’s text classifiers inevitably suffer from unintended dataset biases , especially the document-level label bias and word-level keyword bias , which may hurt models ’ generalization .",0,0.9238601,56.90242545895414,30
2232,Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases .,0,0.88928336,53.392775039531216,27
2232,"Unfortunately , they either suffer from the extra cost of data collection / selection / annotation or need an elaborate design of balancing strategies .",0,0.90305334,100.26696176021336,25
2232,"Different from traditional factual inference in which debiasing occurs before or during training , counterfactual inference mitigates the influence brought by unintended confounders after training , which can make unbiased decisions with biased observations .",0,0.72779393,61.09724725993773,35
2232,"Inspired by this , we propose a model-agnostic text classification debiasing framework – Corsair , which can effectively avoid employing data manipulations or designing balancing mechanisms .",1,0.40423104,64.18248084738784,28
2232,"Concretely , Corsair first trains a base model on a training set directly , allowing the dataset biases ‘ poison ’ the trained model .",2,0.4126084,239.75054789168465,25
2232,"In inference , given a factual input document , Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model .",3,0.48836365,217.4456529706772,28
2232,"Extensive experiments demonstrate Corsair ’s effectiveness , generalizability and fairness .",3,0.8196816,92.23139434448409,11
2233,User interest modeling is critical for personalized news recommendation .,0,0.89071757,113.79226203044382,10
2233,Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest .,0,0.8644207,66.74017494250411,23
2233,"However , user interest is usually diverse and multi-grained , which is difficult to be accurately modeled by a single user embedding .",0,0.8960657,52.37572207726931,23
2233,"In this paper , we propose a news recommendation method with hierarchical user interest modeling , named HieRec .",1,0.8781173,132.94924536063283,19
2233,"Instead of a single user embedding , in our method each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news .",2,0.63430804,49.827709417020515,30
2233,We use a three-level hierarchy to represent 1 ) overall user interest ;,2,0.88150704,280.4065410147278,15
2233,2 ) user interest in coarse-grained topics like sports ;,2,0.39010522,263.3108229697696,10
2233,and 3 ) user interest in fine-grained topics like football .,2,0.38700685,114.17843581730547,12
2233,"Moreover , we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting .",2,0.5332349,49.02956048131241,27
2233,Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation .,3,0.88682896,22.711781455313524,22
2234,Personalized news recommendation methods are widely used in online news services .,0,0.9419798,40.47264740748753,12
2234,These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors .,0,0.8350996,141.68801830756647,20
2234,"However , these methods usually have difficulties in making accurate recommendations to cold-start users , and tend to recommend similar news with those users have read .",0,0.8351785,145.3354381716418,27
2234,"In general , popular news usually contain important information and can attract users with different interests .",0,0.9327052,123.22965407097058,17
2234,"Besides , they are usually diverse in content and topic .",0,0.8846391,76.53590457824936,11
2234,"Thus , in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation .",1,0.8517579,59.13035059206434,24
2234,"In our method , the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score .",2,0.7484699,64.07404866286164,30
2234,The former is used to capture the personalized user interest in news .,2,0.45208213,85.18035336938493,13
2234,"The latter is used to measure time-aware popularity of candidate news , which is predicted based on news content , recency , and real-time CTR using a unified framework .",2,0.75213856,76.2520619037018,33
2234,"Besides , we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling .",2,0.6124371,81.77294144367181,23
2234,Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation .,3,0.89975166,16.39895881487237,19
2235,False claims that have been previously fact-checked can still spread on social media .,0,0.77938384,32.0035176489509,15
2235,"To mitigate their continual spread , detecting previously fact-checked claims is indispensable .",0,0.8835897,316.7190666257494,13
2235,"Given a claim , existing works focus on providing evidence for detection by reranking candidate fact-checking articles ( FC-articles ) retrieved by BM25 .",0,0.8039108,152.11078269528298,27
2235,"However , these performances may be limited because they ignore the following characteristics of FC-articles : ( 1 ) claims are often quoted to describe the checked events , providing lexical information besides semantics ;",0,0.5739169,196.96240382042075,36
2235,"( 2 ) sentence templates to introduce or debunk claims are common across articles , providing pattern information .",3,0.6257812,732.7926570941397,19
2235,Models that ignore the two aspects only leverage semantic relevance and may be misled by sentences that describe similar but irrelevant events .,0,0.650146,115.94391630764841,23
2235,"In this paper , we propose a novel reranker , MTM ( Memory-enhanced Transformers for Matching ) to rank FC-articles using key sentences selected with event ( lexical and semantic ) and pattern information .",1,0.8648982,98.298839213828,39
2235,"For event information , we propose a ROUGE-guided Transformer which is finetuned with regression of ROUGE .",2,0.5561991,47.743164510885265,19
2235,"For pattern information , we generate pattern vectors for matching with sentences .",2,0.81911534,189.26566532810978,13
2235,"By fusing event and pattern information , we select key sentences to represent an article and then predict if the article fact-checks the given claim using the claim , key sentences , and patterns .",2,0.79193515,74.29038169744013,37
2235,Experiments on two real-world datasets show that MTM outperforms existing methods .,3,0.815843,12.271466533370772,12
2235,Human evaluation proves that MTM can capture key sentences for explanations .,3,0.8912494,263.93431966634665,12
2236,"Although deep neural networks have achieved prominent performance on many NLP tasks , they are vulnerable to adversarial examples .",0,0.8977407,14.154903915617346,20
2236,"We propose Dirichlet Neighborhood Ensemble ( DNE ) , a randomized method for training a robust model to defense synonym substitution-based attacks .",2,0.38719264,76.51707539055447,25
2236,"During training , DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms , and it augments them with the training data .",2,0.6691966,54.34704144076607,39
2236,"In such a way , the model is robust to adversarial attacks while maintaining the performance on the original clean data .",3,0.59896195,23.4553179203632,22
2236,"DNE is agnostic to the network architectures and scales to large models ( e.g. , BERT ) for NLP applications .",3,0.46953127,54.347235801279595,21
2236,"Through extensive experimentation , we demonstrate that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets .",3,0.840359,27.912114133475978,28
2237,Increasing the input length has been a driver of progress in language modeling with transformers .,0,0.8447795,39.66900394060117,16
2237,"We identify conditions where shorter inputs are not harmful , and achieve perplexity and efficiency improvements through two new methods that decrease input length .",2,0.4956632,118.86993577195564,25
2237,"First , we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and , surprisingly , substantially improves perplexity .",3,0.8238909,52.304779608509804,31
2237,"Second , we show how to improve the efficiency of recurrence methods in transformers , which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once .",2,0.3696809,60.09225623642976,38
2237,Existing methods require computationally expensive relative position embeddings ;,0,0.710618,115.50406510972145,9
2237,"we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings , which efficiently produces superior results .",2,0.5034489,74.14782910008886,26
2237,We show that these recurrent models also benefit from short input lengths .,3,0.93757635,78.83377473329887,13
2237,"Combining these techniques speeds up training by a factor of 1.65 , reduces memory usage , and substantially improves perplexity on WikiText-103 , without adding any parameters .",3,0.86194354,67.0419438364823,30
2238,"Task variance regularization , which can be used to improve the generalization of Multi-task Learning ( MTL ) models , remains unexplored in multi-task text classification .",0,0.91506803,26.632871284556114,27
2238,"Accordingly , to fill this gap , this paper investigates how the task might be effectively regularized , and consequently proposes a multi-task learning method based on adversarial multi-armed bandit .",1,0.8720157,54.83717309895151,31
2238,"The proposed method , named BanditMTL , regularizes the task variance by means of a mirror gradient ascent-descent algorithm .",2,0.6060866,181.04303406738688,20
2238,Adopting BanditMTL in the multi-task text classification context is found to achieve state-of-the-art performance .,3,0.8922501,25.325487128126614,21
2238,The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals .,3,0.95742977,31.797526205859203,18
2239,"In knowledge graph embedding , the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated .",0,0.8488974,43.45262634747496,22
2239,This makes it difficult to fairly compare the results of the two different loss functions .,0,0.57920665,35.07744383482708,16
2239,We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions .,2,0.6579943,32.166654386370226,26
2239,"Under this interpretation , we can derive theoretical findings for fair comparison .",3,0.8138661,184.33467770468658,13
2239,Experimental results on the FB15k-237 and WN18 RR datasets show that the theoretical findings are valid in practical settings .,3,0.9532921,45.352801968581296,22
2240,Logical table-to-text generation aims to automatically generate fluent and logically faithful text from tables .,0,0.90366614,22.843184717849407,18
2240,The task remains challenging where deep learning models often generated linguistically fluent but logically inconsistent text .,0,0.91682726,128.6524155943426,17
2240,The underlying reason may be that deep learning models often capture surface-level spurious correlations rather than the causal relationships between the table x and the sentence y .,0,0.60788906,43.79606077298218,30
2240,"Specifically , in the training stage , a model can get a low empirical loss without understanding x and use spurious statistical cues instead .",3,0.4610033,201.35085680012634,25
2240,"In this paper , we propose a de-confounded variational encoder-decoder ( DCVED ) based on causal intervention , learning the objective p(y |do ( x ) ) .",1,0.75000614,105.72052191483334,29
2240,"Firstly , we propose to use variational inference to estimate the confounders in the latent space and cooperate with the causal intervention based on Pearl ’s do-calculus to alleviate the spurious correlations .",2,0.6859913,36.199973949356824,33
2240,"Secondly , to make the latent confounder meaningful , we propose a back-prediction process to predict the not-used entities but linguistically similar to the exactly selected ones .",2,0.736432,98.6296585559721,29
2240,"Finally , since our variational model can generate multiple candidates , we train a table-text selector to find out the best candidate sentence for the given table .",2,0.57018363,49.162709674651126,30
2240,An extensive set of experiments show that our model outperforms the baselines and achieves new state-of-the-art performance on two logical table-to-text datasets in terms of logical fidelity .,3,0.89192605,9.029264599192432,38
2241,Recent researches have shown that large natural language processing ( NLP ) models are vulnerable to a kind of security threat called the Backdoor Attack .,0,0.9603535,21.0678932483508,26
2241,Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words .,3,0.693158,235.2973983179854,24
2241,"In this work , we point out a potential problem of current backdoor attacking research : its evaluation ignores the stealthiness of backdoor attacks , and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users .",1,0.7323835,53.734193999659205,44
2241,"To address this issue , we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible .",2,0.3974796,70.84329962022704,23
2241,"We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings , making an important step towards achieving stealthy backdoor attacking .",3,0.6369761,55.052193488652854,30
2241,Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance .,3,0.9474755,67.48452598112213,22
2241,Our code is available at https://github.com/lancopku/SOS .,3,0.5913146,11.19395656886951,7
2242,"Crowdsourcing is regarded as one prospective solution for effective supervised learning , aiming to build large-scale annotated training data by crowd workers .",0,0.9430917,37.330433752882676,23
2242,Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models .,0,0.90064526,70.92732827901911,18
2242,"We take a different point in this work , regarding all crowdsourced annotations as gold-standard with respect to the individual annotators .",2,0.54121554,58.709710148303365,22
2242,"In this way , we find that crowdsourcing could be highly similar to domain adaptation , and then the recent advances of cross-domain methods can be almost directly applied to crowdsourcing .",3,0.94213563,39.373643934709136,32
2242,"Here we take named entity recognition ( NER ) as a study case , suggesting an annotator-aware representation learning model that inspired by the domain adaptation methods which attempt to capture effective domain-aware features .",1,0.52214015,61.47495969093614,38
2242,"We investigate both unsupervised and supervised crowdsourcing learning , assuming that no or only small-scale expert annotations are available .",2,0.79530203,52.093546663335935,22
2242,"Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective , leading to a new state-of-the-art performance .",3,0.9275888,9.762297151981809,29
2242,"In addition , under the supervised setting , we can achieve impressive performance gains with only a very small scale of expert annotations .",3,0.8684766,44.16078892337844,24
2243,"Recent studies strive to incorporate various human rationales into neural networks to improve model performance , but few pay attention to the quality of the rationales .",0,0.9040126,32.51325101812646,27
2243,"Most existing methods distribute their models ’ focus to distantly-labeled rationale words entirely and equally , while ignoring the potential important non-rationale words and not distinguishing the importance of different rationale words .",0,0.77924013,114.11050926857193,35
2243,"In this paper , we propose two novel auxiliary loss functions to make better use of distantly-labeled rationales , which encourage models to maintain their focus on important words beyond labeled rationales ( PINs ) and alleviate redundant training on non-helpful rationales ( NoIRs ) .",1,0.759564,50.676690966833135,48
2243,"Experiments on two representative classification tasks show that our proposed methods can push a classification model to effectively learn crucial clues from non-perfect rationales while maintaining the ability to spread its focus to other unlabeled important words , thus significantly outperform existing methods .",3,0.93339014,53.79668505614712,44
2244,QA models based on pretrained language models have achieved remarkable performance on various benchmark datasets .,0,0.77467126,7.639645357591591,16
2244,"However , QA models do not generalize well to unseen data that falls outside the training distribution , due to distributional shifts .",0,0.863854,30.99111534161026,23
2244,Data augmentation ( DA ) techniques which drop / replace words have shown to be effective in regularizing the model from overfitting to the training data .,0,0.87440884,75.0227545817288,27
2244,"Yet , they may adversely affect the QA tasks since they incur semantic changes that may lead to wrong answers for the QA task .",0,0.8368114,47.26420214931174,25
2244,"To tackle this problem , we propose a simple yet effective DA method based on a stochastic noise generator , which learns to perturb the word embedding of the input questions and context without changing their semantics .",2,0.43540546,30.168270710183048,38
2244,"We validate the performance of the QA models trained with our word embedding perturbation on a single source dataset , on five different target domains .",3,0.5029445,29.768803685946292,26
2244,The results show that our method significantly outperforms the baseline DA methods .,3,0.98334867,18.716423448696123,13
2244,"Notably , the model trained with ours outperforms the model trained with more than 240 K artificially generated QA pairs .",3,0.9617875,63.45179448376209,21
2245,"Arguably , the visual perception of conversational agents to the physical world is a key way for them to exhibit the human-like intelligence .",0,0.81752694,29.10817681708016,25
2245,Image-grounded conversation is thus proposed to address this challenge .,3,0.3132125,31.777707753093562,12
2245,Existing works focus on exploring the multimodal dialog models that ground the conversation on a given image .,0,0.8613408,43.513357120487925,18
2245,"In this paper , we take a step further to study image-grounded conversation under a fully open-ended setting where no paired dialog and image are assumed available .",1,0.7568737,68.06825553565122,30
2245,"Specifically , we present Maria , a neural conversation agent powered by the visual world experiences which are retrieved from a large-scale image index .",2,0.43122315,67.2056601894655,25
2245,"Maria consists of three flexible components , i.e. , text-to-image retriever , visual concept detector and visual-knowledge-grounded response generator .",2,0.40883437,55.46918580206644,25
2245,"The retriever aims to retrieve a correlated image to the dialog from an image index , while the visual concept detector extracts rich visual knowledge from the image .",0,0.40558973,80.09263931418239,29
2245,"Then , the response generator is grounded on the extracted visual knowledge and dialog context to generate the target response .",2,0.7189223,58.62159218335433,21
2245,"Extensive experiments demonstrate Maria outperforms previous state-of-the-art methods on automatic metrics and human evaluation , and can generate informative responses that have some visual commonsense of the physical world .",3,0.9149366,31.30180477586188,34
2246,Conversational dialogue systems ( CDSs ) are hard to evaluate due to the complexity of natural language .,0,0.95589775,21.257155198637196,18
2246,Automatic evaluation of dialogues often shows insufficient correlation with human judgements .,0,0.8949793,38.31276712573567,12
2246,Human evaluation is reliable but labor-intensive .,0,0.82452047,62.38349774904658,7
2246,"We introduce a human-machine collaborative framework , HMCEval , that can guarantee reliability of the evaluation outcomes with reduced human effort .",2,0.53866076,88.75488486380895,22
2246,"HMCEval casts dialogue evaluation as a sample assignment problem , where we need to decide to assign a sample to a human or a machine for evaluation .",3,0.3484863,52.769346419416635,28
2246,"HMCEval includes a model confidence estimation module to estimate the confidence of the predicted sample assignment , and a human effort estimation module to estimate the human effort should the sample be assigned to human evaluation , as well as a sample assignment execution module that finds the optimum assignment solution based on the estimated confidence and effort .",2,0.52024424,38.54131473154438,59
2246,We assess the performance of HMCEval on the task of evaluating malevolence in dialogues .,2,0.44491088,64.88657856009598,15
2246,"The experimental results show that HMCEval achieves around 99 % evaluation accuracy with half of the human effort spared , showing that HMCEval provides reliable evaluation outcomes while reducing human effort by a large amount .",3,0.9764618,66.04367068474951,36
2247,Conditional Variational AutoEncoder ( CVAE ) effectively increases the diversity and informativeness of responses in open-ended dialogue generation tasks through enriching the context vector with sampled latent variables .,0,0.6814153,37.87026504466684,29
2247,"However , due to the inherent one-to-many and many-to-one phenomena in human dialogues , the sampled latent variables may not correctly reflect the contexts ’ semantics , leading to irrelevant and incoherent generated responses .",0,0.69733846,41.54216306736145,41
2247,"To resolve this problem , we propose Self-separated Conditional Variational AutoEncoder ( abbreviated as SepaCVAE ) that introduces group information to regularize the latent variables , which enhances CVAE by improving the responses ’ relevance and coherence while maintaining their diversity and informativeness .",2,0.6893415,55.68583465287686,44
2247,"SepaCVAE actively divides the input data into groups , and then widens the absolute difference between data pairs from distinct groups , while narrowing the relative distance between data pairs in the same group .",0,0.3562249,50.197768016601984,35
2247,Empirical results from automatic evaluation and detailed analysis demonstrate that SepaCVAE can significantly boost responses in well-established open-domain dialogue datasets .,3,0.9513139,37.05336924353023,23
2248,"Conversational Question Simplification ( CQS ) aims to simplify self-contained questions into conversational ones by incorporating some conversational characteristics , e.g. , anaphora and ellipsis .",0,0.94307363,30.066371920997224,26
2248,Existing maximum likelihood estimation based methods often get trapped in easily learned tokens as all tokens are treated equally during training .,0,0.7928865,128.93842253141634,22
2248,"In this work , we introduce a Reinforcement Iterative Sequence Editing ( RISE ) framework that optimizes the minimum Levenshtein distance through explicit editing actions .",1,0.6547174,41.056713879209035,26
2248,RISE is able to pay attention to tokens that are related to conversational characteristics .,3,0.7439267,49.1066903422449,15
2248,"To train RISE , we devise an Iterative Reinforce Training ( IRT ) algorithm with a Dynamic Programming based Sampling ( DPS ) process to improve exploration .",2,0.73154134,144.5238378580425,28
2248,Experimental results on two benchmark datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well on unseen data .,3,0.8951195,6.754283344643349,26
2249,"A video-grounded dialogue system is required to understand both dialogue , which contains semantic dependencies from turn to turn , and video , which contains visual cues of spatial and temporal scene variations .",0,0.7634206,47.606300073477385,35
2249,"Building such dialogue systems is a challenging problem , involving various reasoning types on both visual and language inputs .",0,0.8874853,77.68239909860618,20
2249,Existing benchmarks do not have enough annotations to thoroughly analyze dialogue systems and understand their capabilities and limitations in isolation .,0,0.882289,59.54528740620853,21
2249,These benchmarks are also not explicitly designed to minimise biases that models can exploit without actual reasoning .,0,0.67816347,105.70585319385893,18
2249,"To address these limitations , in this paper , we present DVD , a Diagnostic Dataset for Video-grounded Dialogue .",1,0.865403,49.80781463822354,20
2249,The dataset is designed to contain minimal biases and has detailed annotations for the different types of reasoning over the spatio-temporal space of video .,2,0.6164252,35.60996087695889,25
2249,"Dialogues are synthesized over multiple question turns , each of which is injected with a set of cross-turn semantic relationships .",2,0.5662441,42.22001556511776,21
2249,"We use DVD to analyze existing approaches , providing interesting insights into their abilities and limitations .",2,0.47707522,92.89535426217364,17
2249,"In total , DVD is built from 11 k CATER synthetic videos and contains 10 instances of 10-round dialogues for each video , resulting in more than 100k dialogues and 1 M question-answer pairs .",2,0.61064965,83.93696569084251,39
2249,Our code and dataset are publicly available .,3,0.47382334,9.545336708581193,8
2250,"Emotion recognition in conversation ( ERC ) is a crucial component in affective dialogue systems , which helps the system understand users ’ emotions and generate empathetic responses .",0,0.9427267,37.28981723672526,29
2250,"However , most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation .",0,0.87904316,72.81475168109834,25
2250,"In order to explore a more effective way of utilizing both multimodal and long-distance contextual information , we propose a new model based on multimodal fused graph convolutional network , MMGCN , in this work .",2,0.43993184,27.149605462213522,36
2250,"MMGCN can not only make use of multimodal dependencies effectively , but also leverage speaker information to model inter-speaker and intra-speaker dependency .",3,0.6979502,38.24306148954752,23
2250,"We evaluate our proposed model on two public benchmark datasets , IEMOCAP and MELD , and the results prove the effectiveness of MMGCN , which outperforms other SOTA methods by a significant margin under the multimodal conversation setting .",3,0.7376574,42.07772082282707,39
2251,A dialogue is essentially a multi-turn interaction among interlocutors .,0,0.90479124,20.03345491200624,10
2251,Effective evaluation metrics should reflect the dynamics of such interaction .,0,0.47084692,65.30272437111374,11
2251,"Existing automatic metrics are focused very much on the turn-level quality , while ignoring such dynamics .",0,0.85068583,119.50253602090147,18
2251,"To this end , we propose DynaEval , a unified automatic evaluation framework which is not only capable of performing turn-level evaluation , but also holistically considers the quality of the entire dialogue .",1,0.48543698,33.30635674267835,36
2251,"In DynaEval , the graph convolutional network ( GCN ) is adopted to model a dialogue in totality , where the graph nodes denote each individual utterance and the edges represent the dependency between pairs of utterances .",2,0.53143257,30.993849339541402,38
2251,A contrastive loss is then applied to distinguish well-formed dialogues from carefully constructed negative samples .,2,0.59577256,34.10352752773841,18
2251,"Experiments show that DynaEval significantly outperforms the state-of-the-art dialogue coherence model , and correlates strongly with human judgements across multiple dialogue evaluation aspects at both turn and dialogue level .",3,0.96713614,25.690828043612893,35
2252,Finding codes given natural language query is beneficial to the productivity of software developers .,0,0.7018845,93.19154526122921,15
2252,Future progress towards better semantic matching between query and code requires richer supervised training resources .,3,0.6484542,191.4164220307199,16
2252,"To remedy this , we introduce CoSQA dataset .",2,0.5721007,75.64693957845407,9
2252,"It includes 20,604 labels for pairs of natural language queries and codes , each annotated by at least 3 human annotators .",2,0.6089174,40.63196347747624,22
2252,"We further introduce a contrastive learning method dubbed CoCLR to enhance text-code matching , which works as a data augmenter to bring more artificially generated training instances .",2,0.44839823,85.70789118298238,29
2252,"We show that , evaluated on CodeXGLUE with the same CodeBERT model , training on CoSQA improves the accuracy of code question answering by 5.1 % and incorporating CoCLR brings a further improvement of 10.5 % .",3,0.961658,57.41068886113851,37
2253,A few approaches have been developed to improve neural machine translation ( NMT ) models with multiple passes of decoding .,0,0.9179315,28.669573151422032,21
2253,"However , their performance gains are limited because of lacking proper policies to terminate the multi-pass process .",0,0.8196702,90.6900564195814,18
2253,"To address this issue , we introduce a novel architecture of Rewriter-Evaluator .",1,0.44619036,28.462861981682558,14
2253,Translating a source sentence involves multiple rewriting passes .,0,0.7663133,98.41416638738367,9
2253,"In every pass , a rewriter generates a new translation to improve the past translation .",2,0.42528,76.77715207455601,16
2253,Termination of this multi-pass process is determined by a score of translation quality estimated by an evaluator .,2,0.5465892,37.96221923444617,18
2253,We also propose prioritized gradient descent ( PGD ) to jointly and efficiently train the rewriter and the evaluator .,2,0.57446647,50.15873135629575,20
2253,Extensive experiments on three machine translation tasks show that our architecture notably improves the performances of NMT models and significantly outperforms prior methods .,3,0.8899104,17.688252601582228,24
2253,An oracle experiment reveals that it can largely reduce performance gaps to the oracle policy .,3,0.8548669,74.24388399853986,16
2253,Experiments confirm that the evaluator trained with PGD is more accurate than prior methods in determining proper numbers of rewriting .,3,0.9713309,77.14810121132686,21
2254,"Neural chat translation aims to translate bilingual conversational text , which has a broad application in international exchanges and cooperation .",0,0.9169858,89.08263470499128,21
2254,"Despite the impressive performance of sentence-level and context-aware Neural Machine Translation ( NMT ) , there still remain challenges to translate bilingual conversational text due to its inherent characteristics such as role preference , dialogue coherence , and translation consistency .",0,0.9266907,38.242742365905684,42
2254,"In this paper , we aim to promote the translation quality of conversational text by modeling the above properties .",1,0.9418294,29.99046155943572,20
2254,"Specifically , we design three latent variational modules to learn the distributions of bilingual conversational characteristics .",2,0.86782944,56.85230540827597,17
2254,"Through sampling from these learned distributions , the latent variables , tailored for role preference , dialogue coherence , and translation consistency , are incorporated into the NMT model for better translation .",2,0.6164214,154.46183341823505,33
2254,"We evaluate our approach on the benchmark dataset BConTrasT ( English <-> German ) and a self-collected bilingual dialogue corpus , named BMELD ( English <-> Chinese ) .",2,0.74624366,70.67442871955961,29
2254,Extensive experiments show that our approach notably boosts the performance over strong baselines by a large margin and significantly surpasses some state-of-the-art context-aware NMT models in terms of BLEU and TER .,3,0.9085184,9.515835865778774,40
2254,"Additionally , we make the BMELD dataset publicly available for the research community .",2,0.71030855,34.35268637428579,14
2255,Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages .,0,0.94616437,18.5131775858694,22
2255,"However , the current multilingual translation paradigm often makes the model tend to preserve the general knowledge , but ignore the language-specific knowledge .",0,0.9231737,54.10526554267208,25
2255,"Some previous works try to solve this problem by adding various kinds of language-specific modules to the model , but they suffer from the parameter explosion problem and require specialized manual design .",0,0.85752,29.72100538295702,34
2255,"To solve these problems , we propose to divide the model neurons into general and language-specific parts based on their importance across languages .",2,0.35216758,36.65085853909664,25
2255,"The general part is responsible for preserving the general knowledge and participating in the translation of all the languages , while the language-specific part is responsible for preserving the language-specific knowledge and participating in the translation of some specific languages .",0,0.7238387,15.501625081012078,43
2255,"Experimental results on several language pairs , covering IWSLT and Europarl corpus datasets , demonstrate the effectiveness and universality of the proposed method .",3,0.87955225,21.842222316776365,24
2256,"Multi-source sequence generation ( MSG ) is an important kind of sequence generation tasks that takes multiple sources , including automatic post-editing , multi-source translation , multi-document summarization , etc .",0,0.9526916,54.84269068929303,31
2256,"As MSG tasks suffer from the data scarcity problem and recent pretrained models have been proven to be effective for low-resource downstream tasks , transferring pretrained sequence-to-sequence models to MSG tasks is essential .",0,0.86762434,25.081895038506822,36
2256,"Although directly finetuning pretrained models on MSG tasks and concatenating multiple sources into a single long sequence is regarded as a simple method to transfer pretrained models to MSG tasks , we conjecture that the direct finetuning method leads to catastrophic forgetting and solely relying on pretrained self-attention layers to capture cross-source information is not sufficient .",3,0.8336574,23.89104913792543,57
2256,"Therefore , we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks .",1,0.40031892,26.9689659126041,32
2256,Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set .,3,0.961742,9.771189972122308,30
2256,"When adapted to document-level translation , our framework outperforms strong baselines significantly .",3,0.91765225,42.86866077035477,13
2257,Few-shot crosslingual transfer has been shown to outperform its zero-shot counterpart with pretrained encoders like multilingual BERT .,0,0.6634739,13.236567377628324,20
2257,"Despite its growing popularity , little to no attention has been paid to standardizing and analyzing the design of few-shot experiments .",0,0.9483084,30.380659340036665,22
2257,"In this work , we highlight a fundamental risk posed by this shortcoming , illustrating that the model exhibits a high degree of sensitivity to the selection of few shots .",1,0.5867302,61.14449081871645,31
2257,We conduct a large-scale experimental study on 40 sets of sampled few shots for six diverse NLP tasks across up to 40 languages .,2,0.8911995,54.24212356779782,24
2257,"We provide an analysis of success and failure cases of few-shot transfer , which highlights the role of lexical features .",3,0.5375988,37.378900451971106,21
2257,"Additionally , we show that a straightforward full model finetuning approach is quite effective for few-shot transfer , outperforming several state-of-the-art few-shot approaches .",3,0.9332482,15.065255587342985,32
2257,"As a step towards standardizing few-shot crosslingual experimental designs , we make our sampled few shots publicly available .",2,0.5803106,72.34124824723402,19
2258,Coreference resolution is essential for natural language understanding and has been long studied in NLP .,0,0.95452064,21.653935284510652,16
2258,"In recent years , as the format of Question Answering ( QA ) became a standard for machine reading comprehension ( MRC ) , there have been data collection efforts , e.g. , Dasigi et al .",0,0.9194683,33.40911872378062,37
2258,"( 2019 ) , that attempt to evaluate the ability of MRC models to reason about coreference .",0,0.63005495,54.431342451387785,18
2258,"However , as we show , coreference reasoning in MRC is a greater challenge than earlier thought ;",3,0.97277087,146.83875208424047,18
2258,"MRC datasets do not reflect the natural distribution and , consequently , the challenges of coreference reasoning .",0,0.7021955,114.23022429807544,18
2258,"Specifically , success on these datasets does not reflect a model ’s proficiency in coreference reasoning .",3,0.9236499,71.69191414638091,17
2258,We propose a methodology for creating MRC datasets that better reflect the challenges of coreference reasoning and use it to create a sample evaluation set .,1,0.44690463,50.382811629841676,26
2258,The results on our dataset show that state-of-the-art models still struggle with these phenomena .,3,0.98519903,10.498611592177726,21
2258,"Furthermore , we develop an effective way to use naturally occurring coreference phenomena from existing coreference resolution datasets when training MRC models .",3,0.49471703,46.46272903226495,23
2258,This allows us to show an improvement in the coreference reasoning abilities of state-of-the-art models .,3,0.6734161,11.260928000991708,22
2259,One of the main bottlenecks in developing discourse dependency parsers is the lack of annotated training data .,0,0.9355892,12.303187434547638,18
2259,"A potential solution is to utilize abundant unlabeled data by using unsupervised techniques , but there is so far little research in unsupervised discourse dependency parsing .",0,0.8587273,25.462455363793126,27
2259,"Fortunately , unsupervised syntactic dependency parsing has been studied by decades , which could potentially be adapted for discourse parsing .",0,0.9301361,46.86521584893824,21
2259,"In this paper , we propose a simple yet effective method to adapt unsupervised syntactic dependency parsing methodology for unsupervised discourse dependency parsing .",1,0.91945064,15.427283975509452,24
2259,We apply the method to adapt two state-of-the-art unsupervised syntactic dependency parsing methods .,2,0.68069357,13.284977966284863,20
2259,Experimental results demonstrate that our adaptation is effective .,3,0.9698691,19.47287579608096,9
2259,"Moreover , we extend the adapted methods to the semi-supervised and supervised setting and surprisingly , we find that they outperform previous methods specially designed for supervised discourse parsing .",3,0.87818956,35.06302878848088,30
2259,Further analysis shows our adaptations result in superiority not only in parsing accuracy but also in time and space efficiency .,3,0.9860318,35.80410438039771,21
2260,We introduce a generic seq2seq parsing framework that casts constituency parsing problems ( syntactic and discourse parsing ) into a series of conditional splitting decisions .,2,0.6094241,107.23038887083695,26
2260,"Our parsing model estimates the conditional probability distribution of possible splitting points in a given text span and supports efficient top-down decoding , which is linear in number of nodes .",2,0.47164658,63.978031846311005,31
2260,The conditional splitting formulation together with efficient beam search inference facilitate structural consistency without relying on expensive structured inference .,3,0.60496986,351.2746692520264,20
2260,"Crucially , for discourse analysis we show that in our formulation , discourse segmentation can be framed as a special case of parsing which allows us to perform discourse parsing without requiring segmentation as a pre-requisite .",3,0.8563684,32.67429608994846,37
2260,Experiments show that our model achieves good results on the standard syntactic parsing tasks under settings with / without pre-trained representations and rivals state-of-the-art ( SoTA ) methods that are more computationally expensive than ours .,3,0.9282354,27.57248832374084,40
2260,"In discourse parsing , our method outperforms SoTA by a good margin .",3,0.88594246,45.774749334801406,13
2261,Named Entity Recognition ( NER ) is the task of identifying spans that represent entities in sentences .,0,0.9339513,18.370560958517768,18
2261,"Whether the entity spans are nested or discontinuous , the NER task can be categorized into the flat NER , nested NER , and discontinuous NER subtasks .",0,0.5999444,24.043556248347077,28
2261,These subtasks have been mainly solved by the token-level sequence labelling or span-level classification .,0,0.7866812,37.109385107212134,17
2261,"However , these solutions can hardly tackle the three kinds of NER subtasks concurrently .",0,0.65511376,98.54804776223324,15
2261,"To that end , we propose to formulate the NER subtasks as an entity span sequence generation task , which can be solved by a unified sequence-to-sequence ( Seq2Seq ) framework .",2,0.5906225,22.997598670316485,34
2261,"Based on our unified framework , we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans .",3,0.6998562,38.84612635440648,35
2261,We exploit three types of entity representations to linearize entities into a sequence .,2,0.79310215,61.171320191199804,14
2261,"Our proposed framework is easy-to-implement and achieves state-of-the-art ( SoTA ) or near SoTA performance on eight English NER datasets , including two flat NER datasets , three nested NER datasets , and three discontinuous NER datasets .",3,0.6835959,17.60177155795273,45
2262,"Unlike English letters , Chinese characters have rich and specific meanings .",0,0.92866135,91.47224169418686,12
2262,"Usually , the meaning of a word can be derived from its constituent characters in some way .",0,0.8933193,19.236683545114584,18
2262,Several previous works on syntactic parsing propose to annotate shallow word-internal structures for better utilizing character-level information .,0,0.87594545,65.2232754717385,20
2262,This work proposes to model the deep internal structures of Chinese words as dependency trees with 11 labels for distinguishing syntactic relationships .,1,0.47790155,78.25218201227418,23
2262,"First , based on newly compiled annotation guidelines , we manually annotate a word-internal structure treebank ( WIST ) consisting of over 30 K multi-char words from Chinese Penn Treebank .",2,0.93541706,96.71179614876446,31
2262,"To guarantee quality , each word is independently annotated by two annotators and inconsistencies are handled by a third senior annotator .",2,0.6347268,52.626554709105534,22
2262,"Second , we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation .",3,0.43011576,121.25865989682742,18
2262,"Third , we propose word-internal structure parsing as a new task , and conduct benchmark experiments using a competitive dependency parser .",2,0.673883,77.76660379622304,22
2262,"Finally , we present two simple ways to encode word-internal structures , leading to promising gains on the sentence-level syntactic parsing task .",3,0.73383975,47.75413884655947,25
2263,Named Entity Recognition ( NER ) for low-resource languages is a both practical and challenging research problem .,0,0.95756716,18.436697701670766,18
2263,"This paper addresses zero-shot transfer for cross-lingual NER , especially when the amount of source-language training data is also limited .",1,0.8583162,30.005888686872012,23
2263,The paper first proposes a simple but effective labeled sequence translation method to translate source-language training data to target languages and avoids problems such as word order change and entity span determination .,1,0.3847868,50.37946033796242,35
2263,"With the source-language data as well as the translated data , a generation-based multilingual data augmentation method is introduced to further increase diversity by generating synthetic labeled data in multiple languages .",2,0.68677735,24.397505561718525,36
2263,These augmented data enable the language model based NER models to generalize better with both the language-specific features from the target-language synthetic data and the language-independent features from multilingual synthetic data .,3,0.6857315,25.592810639794582,37
2263,An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages .,2,0.4953207,24.583394381902696,25
2264,"Lexicon information and pre-trained models , such as BERT , have been combined to explore Chinese sequence labeling tasks due to their respective strengths .",0,0.4202412,48.4543283936483,25
2264,"However , existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT .",0,0.7845346,60.12974764763475,28
2264,"In this paper , we propose Lexicon Enhanced BERT ( LEBERT ) for Chinese sequence labeling , which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer .",1,0.7655751,43.094497013130315,32
2264,"Compared with existing methods , our model facilitates deep lexicon knowledge fusion at the lower layers of BERT .",3,0.8891872,52.15972553352288,19
2264,"Experiments on ten Chinese datasets of three tasks including Named Entity Recognition , Word Segmentation , and Part-of-Speech Tagging , show that LEBERT achieves state-of-the-art results .",3,0.81842697,16.144882040304783,33
2265,"In recent years , math word problem solving has received considerable attention and achieved promising results , but previous methods rarely take numerical values into consideration .",0,0.9338935,46.87081412590643,27
2265,"Most methods treat the numerical values in the problems as number symbols , and ignore the prominent role of the numerical values in solving the problem .",0,0.7858618,47.52933971119171,27
2265,"In this paper , we propose a novel approach called NumS2T , which enhances math word problem solving performance by explicitly incorporating numerical values into a sequence-to-tree network .",1,0.8214481,44.38889182422379,32
2265,"In addition , a numerical properties prediction mechanism is used to capture the category and comparison information of numerals and measure their importance in global expressions .",2,0.7720492,90.70259815946103,27
2265,Experimental results on the Math23 K and APE datasets demonstrate that our model achieves better performance than existing state-of-the-art models .,3,0.9524963,11.244819906851994,27
2266,"Previous math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints , leading to unexplainable and unreasonable predictions .",0,0.8792528,81.5896879530057,25
2266,"Herein , we propose Neural-Symbolic Solver ( NS-Solver ) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks .",1,0.63482076,58.161623438638316,25
2266,"Our NS-Solver consists of a problem reader to encode problems , a programmer to generate symbolic equations , and a symbolic executor to obtain answers .",2,0.6712209,128.72224657425272,28
2266,"Along with target expression supervision , our solver is also optimized via 4 new auxiliary objectives to enforce different symbolic reasoning : a ) self-supervised number prediction task predicting both number quantity and number locations ;",2,0.6119874,274.5993299211377,38
2266,b) commonsense constant prediction task predicting what prior knowledge ( e.g .,2,0.57956135,195.86306212233526,13
2266,how many legs a chicken has ) is required ;,3,0.5311307,979.8459661642014,10
2266,c ) program consistency checker computing the semantic loss between predicted equation and target equation to ensure reasonable equation mapping ;,2,0.61249727,736.2081630239546,21
2266,d ) duality exploiting task exploiting the quasi-duality between symbolic equation generation and problem ’s part-of-speech generation to enhance the understanding ability of a solver .,2,0.5009897,97.46594738283487,27
2266,"Besides , to provide a more realistic and challenging benchmark for developing a universal and scalable solver , we also construct a new largescale MWP benchmark CM17 K consisting of 4 kinds of MWPs ( arithmetic , one-unknown linear , one-unknown non-linear , equation set ) with more than 17 K samples .",2,0.8165662,105.54402760033362,55
2266,Extensive experiments on Math23 K and our CM17 k demonstrate the superiority of our NS-Solver compared to state-of-the-art methods .,3,0.9181413,50.05990586601226,28
2267,"Recently , the performance of Pre-trained Language Models ( PLMs ) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding .",0,0.94412804,27.42464705906472,27
2267,"For medical domains , the background knowledge sources are especially useful , due to the massive medical terms and their complicated relations are difficult to understand in text .",0,0.83597803,96.49156774312125,29
2267,"In this work , we introduce SMedBERT , a medical PLM trained on large-scale medical corpora , incorporating deep structured semantic knowledge from neighbours of linked-entity .",1,0.5976126,114.39827638506361,28
2267,"In SMedBERT , the mention-neighbour hybrid attention is proposed to learn heterogeneous-entity information , which infuses the semantic representations of entity types into the homogeneous neighbouring entity structure .",0,0.3854838,100.5812369420499,31
2267,"Apart from knowledge integration as external features , we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts of text mentions , allowing them to communicate via shared neighbors , thus enrich their semantic representations .",2,0.46178585,101.60839654882913,43
2267,Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks .,3,0.94445527,37.60462063762689,15
2267,"It also improves the performance of other tasks such as question answering , question matching and natural language inference .",0,0.4958027,20.133452905693616,20
2268,"When evaluating an article and the claims it makes , a critical reader must be able to assess where the information presented comes from , and whether the various claims are mutually consistent and support the conclusion .",0,0.73417795,45.42962810197825,38
2268,"This motivates the study of claim provenance , which seeks to trace and explain the origins of claims .",0,0.81574136,45.00708472769795,19
2268,"In this paper , we introduce new techniques to model and reason about the provenance of multiple interacting claims , including how to capture fine-grained information about the context .",1,0.8571697,37.62293293227167,32
2268,Our solution hinges on first identifying the sentences that potentially contain important external information .,2,0.51513815,96.52097310158968,15
2268,"We then develop a query generator with our novel rank-aware cross attention mechanism , which aims at generating metadata for the source article , based on the context and the signals collected from a search engine .",2,0.79173297,69.35708489133945,39
2268,"This establishes relevant search queries , and it allows us to obtain source article candidates for each identified sentence and propose an ILP based algorithm to infer the best sources .",3,0.47577235,133.248807404459,31
2268,"We experiment with a newly created evaluation dataset , Politi-Prov , based on fact-checking articles from www.politifact.com; our experimental results show that our solution leads to a significant improvement over baselines .",3,0.74884856,27.464109390969373,32
2269,"Medical imaging plays a significant role in clinical practice of medical diagnosis , where the text reports of the images are essential in understanding them and facilitating later treatments .",0,0.954636,66.65738795051888,30
2269,"By generating the reports automatically , it is beneficial to help lighten the burden of radiologists and significantly promote clinical automation , which already attracts much attention in applying artificial intelligence to medical domain .",3,0.5526034,87.36970198799158,35
2269,"Previous studies mainly follow the encoder-decoder paradigm and focus on the aspect of text generation , with few studies considering the importance of cross-modal mappings and explicitly exploit such mappings to facilitate radiology report generation .",0,0.87060785,29.88649117281814,36
2269,"In this paper , we propose a cross-modal memory networks ( CMN ) to enhance the encoder-decoder framework for radiology report generation , where a shared memory is designed to record the alignment between images and texts so as to facilitate the interaction and generation across modalities .",1,0.8795192,26.535086963844773,48
2269,"Experimental results illustrate the effectiveness of our proposed model , where state-of-the-art performance is achieved on two widely used benchmark datasets , i.e. , IU X-Ray and MIMIC-CXR .",3,0.82440436,18.03286094580073,37
2269,Further analyses also prove that our model is able to better align information from radiology images and texts so as to help generating more accurate reports in terms of clinical indicators .,3,0.9846489,37.73767245858098,32
2270,"There is content such as hate speech , offensive , toxic or aggressive documents , which are perceived differently by their consumers .",0,0.77728605,159.26470159187474,23
2270,They are commonly identified using classifiers solely based on textual content that generalize pre-agreed meanings of difficult problems .,0,0.8718649,86.79691431124209,19
2270,"Such models provide the same results for each user , which leads to high misclassification rate observable especially for contentious , aggressive documents .",3,0.65486896,255.472472003622,24
2270,Both document controversy and user nonconformity require new solutions .,0,0.6919443,232.89167804388075,10
2270,"Therefore , we propose novel personalized approaches that respect individual beliefs expressed by either user conformity-based measures or various embeddings of their previous text annotations .",1,0.39883393,126.02125419109434,28
2270,"We found that only a few annotations of most controversial documents are enough for all our personalization methods to significantly outperform classic , generalized solutions .",3,0.9819889,142.28150729574034,26
2270,"The more controversial the content , the greater the gain .",3,0.7669096,70.13189300820429,11
2270,The personalized solutions may be used to efficiently filter unwanted aggressive content in the way adjusted to a given person .,3,0.8402168,137.1538759267158,21
2271,"As more and more product reviews are posted in both text and images , Multimodal Review Analysis ( MRA ) becomes an attractive research topic .",0,0.96197504,27.177621977596104,26
2271,"Among the existing review analysis tasks , helpfulness prediction on review text has become predominant due to its importance for e-commerce platforms and online shops , i.e .",0,0.91785115,60.9074470003859,28
2271,helping customers quickly acquire useful product information .,0,0.61253506,123.70075362050571,8
2271,This paper proposes a new task Multimodal Review Helpfulness Prediction ( MRHP ) aiming to analyze the review helpfulness from text and visual modalities .,1,0.8767203,68.37081293151611,25
2271,"Meanwhile , a novel Multi-perspective Coherent Reasoning method ( MCR ) is proposed to solve the MRHP task , which conducts joint reasoning over texts and images from both the product and the review , and aggregates the signals to predict the review helpfulness .",2,0.6644816,50.10850575329996,45
2271,"Concretely , we first propose a product-review coherent reasoning module to measure the intra-and inter-modal coherence between the target product and the review .",2,0.5585669,35.7428827631314,25
2271,"In addition , we also devise an intra-review coherent reasoning module to identify the coherence between the text content and images of the review , which is a piece of strong evidence for review helpfulness prediction .",2,0.52597725,73.35327351294613,37
2271,"To evaluate the effectiveness of MCR , we present two newly collected multimodal review datasets as benchmark evaluation resources for the MRHP task .",2,0.58660144,74.72221569202718,24
2271,Experimental results show that our MCR method can lead to a performance increase of up to 8.5 % as compared to the best performing text-only model .,3,0.9660562,15.305414191413906,27
2271,The source code and datasets can be obtained from https://github.com/jhliu17/MCR .,3,0.64724773,9.700229672784664,11
2272,"In this paper , we propose Shallow Aggressive Decoding ( SAD ) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction ( GEC ) .",1,0.8723194,39.36746750457665,30
2272,SAD optimizes the online inference efficiency for GEC by two innovations : 1 ) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism ;,3,0.44232044,118.15755071406655,39
2272,2 ) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference .,2,0.56321836,37.0024661759676,25
2272,Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield identical predictions to greedy decoding but with significant speedup for online inference .,3,0.9065399,70.27811767502492,27
2272,Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss .,3,0.93094605,85.04427285396655,23
2272,"Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks : 66.4 F0.5 in the CoNLL-14 and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference speedup over the Transformer-big model , but also it is easily adapted to other languages .",3,0.9093263,21.432587788367517,65
2272,Our code is available at https://github.com/AutoTemp/Shallow-Aggressive-Decoding .,3,0.6043179,13.795858372499774,7
2273,The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes .,0,0.9353036,56.670368486063914,18
2273,"Since manual coding is very laborious and prone to errors , many methods have been proposed for the automatic ICD coding task .",0,0.9439941,30.956813095256624,23
2273,"However , existing works either ignore the long-tail of code frequency or the noisy clinical notes .",0,0.9160019,122.82704005331777,17
2273,"To address the above issues , we propose an Interactive Shared Representation Network with Self-Distillation Mechanism .",2,0.40054843,37.674258405268596,17
2273,"Specifically , an interactive shared representation network targets building connections among codes while modeling the co-occurrence , consequently alleviating the long-tail problem .",3,0.5358477,129.85771388916152,23
2273,"Moreover , to cope with the noisy text issue , we encourage the model to focus on the clinical note ’s noteworthy part and extract valuable information through a self-distillation learning mechanism .",3,0.6981237,72.74624510969821,33
2273,Experimental results on two MIMIC datasets demonstrate the effectiveness of our method .,3,0.8989402,7.4950131099074,13
2274,Chinese Spelling Check ( CSC ) is a challenging task due to the complex characteristics of Chinese characters .,0,0.9731942,23.01967307017881,19
2274,Statistics reveal that most Chinese spelling errors belong to phonological or visual errors .,0,0.5967839,70.65808601542064,14
2274,"However , previous methods rarely utilize phonological and morphological knowledge of Chinese characters or heavily rely on external resources to model their similarities .",0,0.893291,58.64966366759237,24
2274,"To address the above issues , we propose a novel end-to-end trainable model called PHMOSpell , which promotes the performance of CSC with multi-modal information .",2,0.36751932,31.96743174525565,28
2274,"Specifically , we derive pinyin and glyph representations for Chinese characters from audio and visual modalities respectively , which are integrated into a pre-trained language model by a well-designed adaptive gating mechanism .",2,0.8101632,24.03891342795104,35
2274,"To verify its effectiveness , we conduct comprehensive experiments and ablation tests .",2,0.67189276,46.62843421021812,13
2274,Experimental results on three shared benchmarks demonstrate that our model consistently outperforms previous state-of-the-art models .,3,0.9363527,5.979047619732814,21
2275,"This paper explores the task of Difficulty-Controllable Question Generation ( DCQG ) , which aims at generating questions with required difficulty levels .",1,0.8339786,30.69389948412338,24
2275,"Previous research on this task mainly defines the difficulty of a question as whether it can be correctly answered by a Question Answering ( QA ) system , lacking interpretability and controllability .",0,0.9440165,28.128798228648552,33
2275,"In our work , we redefine question difficulty as the number of inference steps required to answer it and argue that Question Generation ( QG ) systems should have stronger control over the logic of generated questions .",1,0.35838905,33.48047678021672,38
2275,"To this end , we propose a novel framework that progressively increases question difficulty through step-by-step rewriting under the guidance of an extracted reasoning chain .",1,0.5413852,44.174246719317644,26
2275,"A dataset is automatically constructed to facilitate the research , on which extensive experiments are conducted to test the performance of our method .",2,0.7439025,27.150071520079454,24
2276,Table-to-text generation aims at automatically generating natural text to help people conveniently obtain salient information in tables .,0,0.93243206,30.065490222979722,21
2276,"Although neural models for table-to-text have achieved remarkable progress , some problems are still overlooked .",0,0.8932378,30.28123406880718,20
2276,Previous methods cannot deduce the factual results from the entity ’s ( player or team ) performance and the relations between entities .,0,0.82110673,109.84501894416209,23
2276,"To solve this issue , we first build an entity graph from the input tables and introduce a reasoning module to perform reasoning on the graph .",2,0.68734676,29.169103679289698,27
2276,"Moreover , there are different relations ( e.g. , the numeric size relation and the importance relation ) between records in different dimensions .",0,0.54302657,63.582666187474466,24
2276,And these relations may contribute to the data-to-text generation .,3,0.6185011,27.589539440378633,12
2276,"However , it is hard for a vanilla encoder to capture these .",0,0.8548898,50.47967489268919,13
2276,"Consequently , we propose to utilize two auxiliary tasks , Number Ranking ( NR ) and Importance Ranking ( IR ) , to supervise the encoder to capture the different relations .",2,0.58960605,43.68699651117755,32
2276,"Experimental results on ROTOWIRE and RW-FG show that our method not only has a good generalization but also outperforms previous methods on several metrics : BLEU , Content Selection , Content Ordering .",3,0.95047474,44.97269580854071,35
2277,The multimodality problem has become a major challenge of existing non-autoregressive generation ( NAG ) systems .,0,0.9571795,26.786749992231048,17
2277,A common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation ( hereinafter known as “ teacher AG ” ) .,0,0.8282713,90.6969325380579,28
2277,"The success of such methods may largely depend on a latent assumption , i.e. , the teacher AG is superior to the NAG model .",3,0.5252098,62.87982329872221,25
2277,"However , in this work , we experimentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending generation .",3,0.52166057,29.405394403921832,29
2277,"To provide a feasible solution to the multimodality problem of NAG , we propose incorporating linguistic structure ( Part-of-Speech sequence in particular ) into NAG inference instead of relying on teacher AG .",2,0.48662555,59.80619648972351,37
2277,"More specifically , the proposed POS-constrained Parallel Decoding ( POSPD ) method aims at providing a specific POS sequence to constrain the NAG model during decoding .",2,0.43342227,65.33128486280889,29
2277,Our experiments demonstrate that POSPD consistently improves NAG models on four text generation tasks to a greater extent compared to knowledge distillation .,3,0.9848862,57.21485045374046,23
2277,This observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation .,3,0.98375285,34.6228524961128,16
2278,A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary .,0,0.91270393,21.22009411426739,19
2278,"This potentially weakens the effect when applying pretrained models into natural language generation ( NLG ) tasks , especially for the subword distributions between upstream and downstream tasks with significant discrepancy .",3,0.6777195,64.18587804697898,32
2278,"Towards approaching this problem , we extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step .",2,0.4764363,47.71461346191601,19
2278,"Specifically , a plug-and-play embedding generator is introduced to produce the representation of any input token , according to pre-trained embeddings of its morphologically similar ones .",2,0.68200797,27.144822358502427,29
2278,"Thus , embeddings of mismatch tokens in downstream tasks can also be efficiently initialized .",3,0.6936275,72.69579140995506,15
2278,We conduct experiments on a variety of NLG tasks under the pretrain-finetune fashion .,2,0.83466625,21.497643717213098,14
2278,"Experimental results and extensive analyses show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary , leading to more efficient and better performed downstream NLG models .",3,0.96494526,75.9261402014662,32
2279,"In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation , we propose TGEA , an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models ( PLMs ) .",2,0.38568956,31.181554775952357,42
2279,"We use carefully selected prompt words to guide GPT-2 to generate candidate sentences , from which we select 47 K for error annotation .",2,0.82523584,107.56782370111694,26
2279,Crowdsourced workers manually check each of these sentences and detect 12 k erroneous sentences .,2,0.74133873,204.0160527863925,15
2279,"We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge ( e.g. , common sense ) .",2,0.8656051,47.96607013915053,35
2279,"For each erroneous span in PLM-generated sentences , we also detect another span that is closely associated with it .",3,0.5954905,64.60122636733287,22
2279,"Each error is hence manually labeled with comprehensive annotations , including the span of the error , the associated span , minimal correction to the error , the type of the error , and rationale behind the error .",2,0.60040414,73.80696571658551,39
2279,"Apart from the fully annotated dataset , we also present a detailed description of the data collection procedure , statistics and analysis of the dataset .",2,0.71186215,27.32944799009031,26
2279,"This is the first dataset with comprehensive annotations for PLM-generated texts , which facilitates the diagnostic evaluation of PLM-based text generation .",3,0.95862836,24.71053949131007,26
2279,"Furthermore , we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks , including error detection , error type classification , associated span detection , error rationale generation , to further promote future study on the automatic error detection and correction on texts generated by pretrained language models .",3,0.5217763,76.41778798209369,54
2280,Transformer-based models have achieved state-of-the-art results in a wide range of natural language processing ( NLP ) tasks including document summarization .,0,0.93060404,6.539034321969403,30
2280,Typically these systems are trained by fine-tuning a large pre-trained model to the target task .,0,0.85150784,11.022441579971513,16
2280,One issue with these transformer-based models is that they do not scale well in terms of memory and compute requirements as the input length grows .,0,0.7549085,16.147823127393355,28
2280,"Thus , for long document summarization , it can be challenging to train or fine-tune these models .",0,0.8013275,30.510118887241703,19
2280,"In this work , we exploit large pre-trained transformer-based models and address long-span dependencies in abstractive summarization using two methods : local self-attention ;",2,0.5033646,51.79231520490632,26
2280,and explicit content selection .,0,0.41006866,397.7057969234591,5
2280,These approaches are compared on a range of network configurations .,2,0.52564454,61.63968761441761,11
2280,"Experiments are carried out on standard long-span summarization tasks , including Spotify Podcast , arXiv , and PubMed datasets .",2,0.7351352,68.23262357200151,20
2280,"We demonstrate that by combining these methods , we can achieve state-of-the-art results on all three tasks in the ROUGE scores .",3,0.92994386,12.451929861928503,28
2280,"Moreover , without a large-scale GPU card , our approach can achieve comparable or better results than existing approaches .",3,0.9286903,27.573231172411642,20
2281,"In the field of dialogue summarization , due to the lack of training data , it is often difficult for supervised summary generation methods to learn vital information from dialogue context with limited data .",0,0.9059738,26.238815158511002,35
2281,"Several attempts on unsupervised summarization for text by leveraging semantic information solely or auto-encoder strategy ( i.e. , sentence compression ) , it however cannot be adapted to the dialogue scene due to the limited words in utterances and huge gap between the dialogue and its summary .",0,0.8446237,73.98564994855113,48
2281,"In this study , we propose a novel unsupervised strategy to address this challenge , which roots from the hypothetical foundation that a superior summary approximates a replacement of the original dialogue , and they are roughly equivalent for auxiliary ( self-supervised ) tasks , e.g. , dialogue generation .",1,0.8562812,79.38269939243335,50
2281,The proposed strategy RepSum is applied to generate both extractive and abstractive summary with the guidance of the followed nˆth utterance generation and classification tasks .,2,0.5024324,87.369368699664,26
2281,Extensive experiments on various datasets demonstrate the superiority of the proposed model compared with the state-of-the-art methods .,3,0.90077806,4.8124242865972295,23
2282,"Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture , as Seq2Seq is not good at analyzing long-distance relations in text .",0,0.9358743,19.82130319873541,25
2282,"In this paper , we present BASS , a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph , which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases .",1,0.82803607,42.88028326466738,40
2282,"Further , a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure .",2,0.64402854,13.350456628025572,25
2282,"Specifically , several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary .",2,0.67113775,41.91760276836868,39
2282,Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks .,3,0.972793,10.230506872952972,18
2283,"Given a set of related publications , related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order .",0,0.4333477,45.28737678873497,35
2283,"Most of existing related work generation models follow the inflexible extractive style , which directly extract sentences from multiple original papers to form a related work discussion .",0,0.88111556,106.14275234087292,28
2283,"Hence , in this paper , we propose a Relation-aware Related work Generator ( RRG ) , which generates an abstractive related work from the given multiple scientific papers in the same research area .",1,0.7799665,41.46210362084609,37
2283,"Concretely , we propose a relation-aware multi-document encoder that relates one document to another according to their content dependency in a relation graph .",2,0.50346416,24.102799263490724,26
2283,"The relation graph and the document representation are interacted and polished iteratively , complementing each other in the training process .",2,0.5413565,87.18434026600872,21
2283,We also contribute two public datasets composed of related work sections and their corresponding papers .,2,0.73712474,57.2449778950463,16
2283,Extensive experiments on the two datasets show that the proposed model brings substantial improvements over several strong baselines .,3,0.9220814,7.992723007652263,19
2283,We hope that this work will promote advances in related work generation task .,3,0.84451205,72.21641520535748,14
2284,"Professional summaries are written with document-level information , such as the theme of the document , in mind .",0,0.6533567,29.019072913450227,19
2284,"This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content , while deciding what to generate , at each decoding step .",0,0.6102758,55.37373146251733,28
2284,"With the motivation to narrow this gap , we introduce Focus Attention Mechanism , a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document .",1,0.60175174,40.79306403411612,36
2284,"Further , we propose a Focus Sampling method to enable generation of diverse summaries , an area currently understudied in summarization .",3,0.46623126,47.8125364635021,22
2284,"When evaluated on the BBC extreme summarization task , two state-of-the-art models augmented with Focus Attention generate summaries that are closer to the target and more faithful to their input documents , outperforming their vanilla counterparts on ROUGE and multiple faithfulness measures .",3,0.91871285,35.929937575212136,49
2284,We also empirically demonstrate that Focus Sampling is more effective in generating diverse and faithful summaries than top-k or nucleus sampling-based decoding methods .,3,0.959636,43.648641581486146,27
2285,The availability of large-scale datasets has driven the development of neural models that create generic summaries from single or multiple documents .,0,0.96070325,19.34876519023781,22
2285,"In this work we consider query focused summarization ( QFS ) , a task for which training data in the form of queries , documents , and summaries is not readily available .",1,0.384084,46.72778211496405,33
2285,"We propose to decompose QFS into ( 1 ) query modeling ( i.e. , finding supportive evidence within a set of documents for a query ) and ( 2 ) conditional language modeling ( i.e. , summary generation ) .",2,0.6714754,47.7523285905169,40
2285,"We introduce MaRGE , a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries , so that summaries in generic data can be converted into proxy queries for learning a query model .",2,0.6296458,64.88592881638287,43
2285,Experiments across QFS benchmarks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision .,3,0.92728627,18.34961954315122,26
2286,"This paper studies the bias problem of multi-hop question answering models , of answering correctly without correct reasoning .",1,0.904082,77.61404992163166,19
2286,"One way to robustify these models is by supervising to not only answer right , but also with right reasoning chains .",0,0.84444845,101.44113733707384,22
2286,"An existing direction is to annotate reasoning chains to train models , requiring expensive additional annotations .",0,0.84586966,214.98797659054495,17
2286,"In contrast , we propose a new approach to learn evidentiality , deciding whether the answer prediction is supported by correct evidences , without such annotations .",2,0.4176123,126.80547612933442,27
2286,"Instead , we compare counterfactual changes in answer confidence with and without evidence sentences , to generate “ pseudo-evidentiality ” annotations .",2,0.79682916,75.58805811951306,22
2286,"We validate our proposed model on an original set and challenge set in HotpotQA , showing that our method is accurate and robust in multi-hop reasoning .",3,0.8809266,35.012063471887366,27
2287,Dense passage retrieval has been shown to be an effective approach for information retrieval tasks such as open domain question answering .,0,0.9220746,20.760560709526967,22
2287,"Under this paradigm , a dual-encoder model is learned to encode questions and passages separately into vector representations , and all the passage vectors are then pre-computed and indexed , which can be efficiently retrieved by vector space search during inference time .",2,0.59597397,43.47438761293205,43
2287,"In this paper , we propose a new contrastive learning method called Cross Momentum Contrastive learning ( xMoCo ) , for learning a dual-encoder model for question-passage matching .",1,0.8654681,40.24225006733255,31
2287,"Our method efficiently maintains a large pool of negative samples like the original MoCo , and by jointly optimizing question-to-passage and passage-to-question matching tasks , enables using separate encoders for questions and passages .",3,0.71066105,75.1151431795057,40
2287,"We evaluate our method on various open-domain question answering dataset , and the experimental results show the effectiveness of the proposed method .",3,0.655988,13.09905752815227,23
2288,"One of the main challenges in conversational question answering ( CQA ) is to resolve the conversational dependency , such as anaphora and ellipsis .",0,0.95597184,14.834936989800713,25
2288,"However , existing approaches do not explicitly train QA models on how to resolve the dependency , and thus these models are limited in understanding human dialogues .",0,0.9185417,34.24214787174925,28
2288,"In this paper , we propose a novel framework , ExCorD ( Explicit guidance on how to resolve Conversational Dependency ) to enhance the abilities of QA models in comprehending conversational context .",1,0.91149914,64.98990863496068,33
2288,"ExCorD first generates self-contained questions that can be understood without the conversation history , then trains a QA model with the pairs of original and self-contained questions using a consistency-based regularizer .",2,0.6942883,38.643217000940716,34
2288,"In our experiments , we demonstrate that ExCorD significantly improves the QA models ’ performance by up to 1.2 F1 on QuAC , and 5.2 F1 on CANARD , while addressing the limitations of the existing approaches .",3,0.9279486,34.23592749605798,38
2289,"We present a new human-human dialogue dataset-PhotoChat , the first dataset that casts light on the photo sharing behavior in online messaging .",1,0.3972864,64.80151751036526,25
2289,"PhotoChat contains 12 k dialogues , each of which is paired with a user photo that is shared during the conversation .",2,0.6153076,35.50288403354976,22
2289,"Based on this dataset , we propose two tasks to facilitate research on image-text modeling : a photo-sharing intent prediction task that predicts whether one intends to share a photo in the next conversation turn , and a photo retrieval task that retrieves the most relevant photo according to the dialogue context .",2,0.57357275,28.262337406471868,55
2289,"In addition , for both tasks , we provide baseline models using the state-of-the-art models and report their benchmark performances .",2,0.54974675,27.42551669938875,27
2289,"The best image retrieval model achieves 10.4 % recall@1 ( out of 1000 candidates ) and the best photo intent prediction model achieves 58.1 % F1 score , indicating that the dataset presents interesting yet challenging real-world problems .",3,0.95021003,53.61282955663909,39
2289,We are releasing PhotoChat to facilitate future research work among the community .,3,0.76384246,58.20254492197773,13
2290,A neural multimodal machine translation ( MMT ) system is one that aims to perform better translation by extending conventional text-only translation models with multimodal information .,0,0.91328675,25.291232211003287,27
2290,"Many recent studies report improvements when equipping their models with the multimodal module , despite the controversy of whether such improvements indeed come from the multimodal part .",0,0.8772302,39.35631859199694,28
2290,We revisit the contribution of multimodal information in MMT by devising two interpretable MMT models .,2,0.3995644,24.747494716864043,16
2290,"To our surprise , although our models replicate similar gains as recently developed multimodal-integrated systems achieved , our models learn to ignore the multimodal information .",3,0.92454934,75.67811157799197,28
2290,"Upon further investigation , we discover that the improvements achieved by the multimodal models over text-only counterparts are in fact results of the regularization effect .",3,0.97105265,25.800020096038928,26
2290,"We report empirical findings that highlight the importance of MMT models ’ interpretability , and discuss how our findings will benefit future research .",3,0.8831511,40.19466072453797,24
2291,Answering is a task which requires an AI agent to answer questions grounded in video .,0,0.89882183,29.65930051604597,16
2291,"This task entails three key challenges : ( 1 ) understand the intention of various questions , ( 2 ) capturing various elements of the input video ( e.g. , object , action , causality ) , and ( 3 ) cross-modal grounding between language and vision information .",0,0.75054914,61.167732539117,49
2291,"We propose Motion-Appearance Synergistic Networks ( MASN ) , which embed two cross-modal features grounded on motion and appearance information and selectively utilize them depending on the question ’s intentions .",2,0.4357021,77.35324072138746,33
2291,"MASN consists of a motion module , an appearance module , and a motion-appearance fusion module .",2,0.3858758,40.37560381567711,19
2291,"The motion module computes the action-oriented cross-modal joint representations , while the appearance module focuses on the appearance aspect of the input video .",2,0.3551772,36.36425208254164,24
2291,"Finally , the motion-appearance fusion module takes each output of the motion module and the appearance module as input , and performs question-guided fusion .",2,0.67618966,63.43700093978412,29
2291,"As a result , MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA datasets .",3,0.9220972,16.538724882354227,25
2291,We also conduct qualitative analysis by visualizing the inference results of MASN .,2,0.61246973,100.26036405748297,13
2292,We study the problem of learning a named entity recognition ( NER ) tagger using noisy labels from multiple weak supervision sources .,1,0.55539286,50.69634053767516,23
2292,"Though cheap to obtain , the labels from weak supervision sources are often incomplete , inaccurate , and contradictory , making it difficult to learn an accurate NER model .",0,0.8604624,99.2909006780164,30
2292,"To address this challenge , we propose a conditional hidden Markov model ( CHMM ) , which can effectively infer true labels from multi-source noisy labels in an unsupervised way .",1,0.40594673,42.66438780429582,31
2292,CHMM enhances the classic hidden Markov model with the contextual representation power of pre-trained language models .,2,0.38601294,57.86783685687349,17
2292,"Specifically , CHMM learns token-wise transition and emission probabilities from the BERT embeddings of the input tokens to infer the latent true labels from noisy observations .",2,0.6853081,93.95488606617577,29
2292,We further refine CHMM with an alternate-training approach ( CHMM-ALT ) .,3,0.47360894,127.05859540617948,14
2292,"It fine-tunes a BERT-NER model with the labels inferred by CHMM , and this BERT-NER ’s output is regarded as an additional weak source to train the CHMM in return .",2,0.55628335,46.93809039782918,36
2292,Experiments on four NER benchmarks from various domains show that our method outperforms state-of-the-art weakly supervised NER models by wide margins .,3,0.9106387,9.508705582004497,28
2293,The journey of reducing noise from distant supervision ( DS ) generated training data has been started since the DS was first introduced into the relation extraction ( RE ) task .,0,0.9160698,69.70092445944594,32
2293,"For the past decade , researchers apply the multi-instance learning ( MIL ) framework to find the most reliable feature from a bag of sentences .",0,0.9458037,59.464364450984945,26
2293,"Although the pattern of MIL bags can greatly reduce DS noise , it fails to represent many other useful sentence features in the datasets .",3,0.82973605,207.1364672520468,25
2293,"In many cases , these sentence features can only be acquired by extra sentence-level human annotation with heavy costs .",0,0.6267768,112.96321393810248,22
2293,"Therefore , the performance of distantly supervised RE models is bounded .",3,0.7897984,70.30362433942003,12
2293,"In this paper , we go beyond typical MIL framework and propose a novel contrastive instance learning ( CIL ) framework .",1,0.7699183,54.187589499587816,22
2293,"Specifically , we regard the initial MIL as the relational triple encoder and constraint positive pairs against negative pairs for each instance .",2,0.79740727,146.6852824861484,23
2293,"Experiments demonstrate the effectiveness of our proposed framework , with significant improvements over the previous methods on NYT10 , GDS and KBP .",3,0.9299061,46.153611871947,23
2294,"Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag , while accurate sentence labels are important for downstream applications that need the exact relation type .",0,0.5675806,102.66347181354904,32
2294,"Directly using bag labels for sentence-level training will introduce much noise , thus severely degrading performance .",3,0.5844071,149.52939851801858,19
2294,"In this work , we propose the use of negative training ( NT ) , in which a model is trained using complementary labels regarding that “ the instance does not belong to these complementary labels ” .",1,0.39390838,76.09961235225339,38
2294,"Since the probability of selecting a true label as a complementary label is low , NT provides less noisy information .",3,0.64387465,128.14394244448843,21
2294,"Furthermore , the model trained with NT is able to separate the noisy data from the training data .",3,0.9263057,31.480407790687458,19
2294,"Based on NT , we propose a sentence-level framework , SENT , for distant relation extraction .",2,0.6450968,77.37234945916758,19
2294,"SENT not only filters the noisy data to construct a cleaner dataset , but also performs a re-labeling process to transform the noisy data into useful training data , thus further benefiting the model ’s performance .",3,0.56243926,37.13948788507597,37
2294,Experimental results show the significant improvement of the proposed method over previous methods on sentence-level evaluation and de-noise effect .,3,0.97459996,18.317230225862858,22
2295,Medical named entity recognition ( NER ) and normalization ( NEN ) are fundamental for constructing knowledge graphs and building QA systems .,0,0.9210565,35.00580338678098,23
2295,Existing implementations for medical NER and NEN are suffered from the error propagation between the two tasks .,0,0.82525223,85.07246135179382,18
2295,The mispredicted mentions from NER will directly influence the results of NEN .,3,0.90440786,108.42761852186688,13
2295,"Therefore , the NER module is the bottleneck of the whole system .",0,0.6555034,48.13102010981766,13
2295,"Besides , the learnable features for both tasks are beneficial to improving the model performance .",3,0.7091307,49.497514904788986,16
2295,"To avoid the disadvantages of existing models and exploit the generalized representation across the two tasks , we design an end-to-end progressive multi-task learning model for jointly modeling medical NER and NEN in an effective way .",2,0.680591,39.029456433474664,39
2295,There are three level tasks with progressive difficulty in the framework .,3,0.39182052,161.2871230423928,12
2295,The progressive tasks can reduce the error propagation with the incremental task settings which implies the lower level tasks gain the supervised signals other than errors from the higher level tasks to improve their performances .,3,0.8935351,97.75195744955526,36
2295,"Besides , the context features are exploited to enrich the semantic information of entity mentions extracted by NER .",2,0.4696352,46.478993762679124,19
2295,The performance of NEN profits from the enhanced entity mention features .,3,0.8300132,285.44092540040685,12
2295,The standard entities from knowledge bases are introduced into the NER module for extracting corresponding entity mentions correctly .,2,0.63835317,95.70401229988545,19
2295,The empirical results on two publicly available medical literature datasets demonstrate the superiority of our method over nine typical methods .,3,0.90218383,21.48324093879673,21
2296,Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction .,0,0.94413435,21.002680613097642,17
2296,"Recent methods achieve considerable performance but still suffer from some inherent limitations , such as redundancy of relation prediction , poor generalization of span-based extraction and inefficiency .",0,0.8453972,50.85239309070175,28
2296,"In this paper , we decompose this task into three subtasks , Relation Judgement , Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint relational triple extraction framework based on Potential Relation and Global Correspondence ( PRGC ) .",1,0.659511,42.68306765269462,44
2296,"Specifically , we design a component to predict potential relations , which constrains the following entity extraction to the predicted relation subset rather than all relations ;",2,0.86780983,111.82884021833348,27
2296,then a relation-specific sequence tagging component is applied to handle the overlapping problem between subjects and objects ;,2,0.6487356,205.4080274475009,19
2296,"finally , a global correspondence component is designed to align the subject and object into a triple with low-complexity .",2,0.5035154,98.81984404176441,20
2296,Extensive experiments show that PRGC achieves state-of-the-art performance on public benchmarks with higher efficiency and delivers consistent performance gain on complex scenarios of overlapping triples .,3,0.9258175,26.929425307051712,32
2296,The source code has been submitted as the supplementary material and will be made publicly available after the blind review .,3,0.5573244,26.255642454730726,21
2297,Few-shot Named Entity Recognition ( NER ) exploits only a handful of annotations to iden-tify and classify named entity mentions .,0,0.94583577,28.44870974853706,23
2297,Pro-totypical network shows superior performance on few-shot NER .,3,0.8765904,47.35566813379139,9
2297,"However , existing prototyp-ical methods fail to differentiate rich seman-tics in other-class words , which will aggravate overfitting under few shot scenario .",0,0.84388447,421.6401646250153,24
2297,"To address the issue , we propose a novel model , Mining Undefined Classes from Other-class ( MUCO ) , that can automatically induce different unde-fined classes from the other class to improve few-shot NER .",1,0.50888354,79.0369893272821,36
2297,"With these extra-labeled unde-fined classes , our method will improve the dis-criminative ability of NER classifier and en-hance the understanding of predefined classes with stand-by semantic knowledge .",3,0.8993948,84.13564074982838,30
2297,Experi-mental results demonstrate that our model out-performs five state-of-the-art models in both 1-shot and 5-shots settings on four NER bench-marks .,3,0.97823584,20.284841176709097,31
2297,We will release the code upon accep-tance .,3,0.6150126,31.95062285095652,8
2297,The source code is released on https : //github.com/shuaiwa16/OtherClassNER.git .,3,0.5119541,45.96361491443554,10
2298,"Compared to the general news domain , information extraction ( IE ) from biomedical text requires much broader domain knowledge .",0,0.90869164,109.18989599309856,21
2298,"However , many previous IE methods do not utilize any external knowledge during inference .",0,0.89911336,64.50494244962215,15
2298,"Due to the exponential growth of biomedical publications , models that do not go beyond their fixed set of parameters will likely fall behind .",0,0.87871933,37.49825953219371,25
2298,"Inspired by how humans look up relevant information to comprehend a scientific text , we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI ( Knowledge-Enhanced Collective Inference ) .",1,0.3142873,61.56190646698921,39
2298,"Given an input text , KECI first constructs an initial span graph representing its initial understanding of the text .",2,0.6488629,102.20457991468469,20
2298,It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text .,2,0.62299234,42.98239384881656,24
2298,"To make the final predictions , KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism .",2,0.5648187,73.69889404940203,26
2298,KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks .,2,0.46680495,60.7478332425281,24
2298,"Our experimental results show that the framework is highly effective , achieving new state-of-the-art results in two different benchmark datasets : BioRelEx ( binding interaction detection ) and ADE ( adverse drug event extraction ) .",3,0.96669555,42.56669620708526,42
2298,"For example , KECI achieves absolute improvements of 4.59 % and 4.91 % in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks .",3,0.9404962,24.64887414876171,33
2299,Biomedical Information Extraction from scientific literature presents two unique and non-trivial challenges .,0,0.949683,27.626465917796523,13
2299,"First , compared with general natural language texts , sentences from scientific papers usually possess wider contexts between knowledge elements .",0,0.6740428,304.3778401804603,21
2299,"Moreover , comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge .",0,0.9069633,69.8324635306905,15
2299,"In this paper , we propose a novel biomedical Information Extraction ( IE ) model to tackle these two challenges and extract scientific entities and events from English research papers .",1,0.89729506,43.835663510902926,31
2299,We perform Abstract Meaning Representation ( AMR ) to compress the wide context to uncover a clear semantic structure for each complex sentence .,2,0.8116025,56.18371345873521,24
2299,"Besides , we construct the sentence-level knowledge graph from an external knowledge base and use it to enrich the AMR graph to improve the model ’s understanding of complex scientific concepts .",2,0.7572282,22.054167092854176,34
2299,We use an edge-conditioned graph attention network to encode the knowledge-enriched AMR graph for biomedical IE tasks .,2,0.88694173,74.87486921956983,19
2299,Experiments on the GENIA 2011 dataset show that the AMR and external knowledge have contributed 1.8 % and 3.0 % absolute F-score gains respectively .,3,0.9609887,25.099578118764367,25
2299,"In order to evaluate the impact of our approach on real-world problems that involve topic-specific fine-grained knowledge elements , we have also created a new ontology and annotated corpus for entity and event extraction for the COVID-19 scientific literature , which can serve as a new benchmark for the biomedical IE community .",2,0.48586974,25.81178387943329,56
2300,"Event Detection ( ED ) aims to recognize mentions of events ( i.e. , event triggers ) and their types in text .",0,0.9122947,77.24321761618862,23
2300,"Recently , several ED datasets in various domains have been proposed .",0,0.91268533,82.32046463119497,12
2300,"However , the major limitation of these resources is the lack of enough training data for individual event types which hinders the efficient training of data-hungry deep learning models .",0,0.8766644,22.693307882420527,30
2300,"To overcome this issue , we propose to exploit the powerful pre-trained language model GPT-2 to generate training samples for ED .",2,0.34083185,20.311879494013972,24
2300,"To prevent the noises inevitable in automatically generated data from hampering training process , we propose to exploit a teacher-student architecture in which the teacher is supposed to learn anchor knowledge from the original data .",2,0.50656784,59.991964286517224,37
2300,The student is then trained on combination of the original and GPT-generated data while being led by the anchor knowledge from the teacher .,2,0.6358597,52.193997548230655,26
2300,Optimal transport is introduced to facilitate the anchor knowledge-based guidance between the two networks .,2,0.45676824,90.81464251209083,17
2300,"We evaluate the proposed model on multiple ED benchmark datasets , gaining consistent improvement and establishing state-of-the-art results for ED .",3,0.6307348,31.69402319690245,27
2301,Event extraction ( EE ) has considerably benefited from pre-trained language models ( PLMs ) by fine-tuning .,0,0.92408633,36.22089228932649,18
2301,"However , existing pre-training methods have not involved modeling event characteristics , resulting in the developed EE models cannot take full advantage of large-scale unsupervised data .",0,0.79839927,55.83899059713149,27
2301,"To this end , we propose CLEVE , a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures ( e.g .",2,0.38818112,42.355217127080024,30
2301,AMR ) obtained with automatic parsers .,2,0.62117,123.04598830269828,7
2301,CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively .,2,0.4783947,24.106103759206178,19
2301,"Specifically , the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words ;",2,0.65722215,66.64378547868334,27
2301,the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures .,2,0.6788387,59.944667958103814,17
2301,"The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised “ liberal ” EE , which requires jointly extracting events and discovering event schemata without any annotated data .",3,0.47440314,88.69831888161698,36
2301,"Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements , especially in the challenging unsupervised setting .",3,0.90471977,33.299067836161406,21
2301,The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE .,3,0.6165973,14.474142159162117,12
2302,Document-level event extraction ( DEE ) is indispensable when events are described throughout a document .,0,0.9539956,61.71094581400197,18
2302,We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document .,3,0.8553719,35.614036346766284,29
2302,It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences .,0,0.9180675,25.29197992961226,25
2302,"In this paper , we propose an end-to-end model , which can extract structured events from a document in a parallel manner .",1,0.85641307,16.383690274402905,24
2302,"Specifically , we first introduce a document-level encoder to obtain the document-aware representations .",2,0.71895957,16.80434345604529,17
2302,"Then , a multi-granularity non-autoregressive decoder is used to generate events in parallel .",2,0.7279552,19.806904293562944,14
2302,"Finally , to train the entire model , a matching loss function is proposed , which can bootstrap a global optimization .",2,0.5855266,100.01440791510548,22
2302,The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task .,3,0.9571833,8.566220940609037,30
2302,Code will be available at https://github.com/HangYang-NLP/DE-PPN .,3,0.4588925,12.891262902513361,7
2303,Large pre-trained language models achieve state-of-the-art results when fine-tuned on downstream NLP tasks .,3,0.6000832,4.406696558343514,19
2303,"However , they almost exclusively focus on text-only representation , while neglecting cell-level layout information that is important for form image understanding .",0,0.84109616,51.207470488596286,24
2303,"In this paper , we propose a new pre-training approach , StructuralLM , to jointly leverage cell and layout information from scanned documents .",1,0.85284793,63.972174750676196,24
2303,"Specifically , we pre-train StructuralLM with two new designs to make the most of the interactions of cell and layout information : 1 ) each cell as a semantic unit ;",2,0.76425594,96.30154659144156,31
2303,2 ) classification of cell positions .,2,0.49887013,476.14724208968346,7
2303,"The pre-trained StructuralLM achieves new state-of-the-art results in different types of downstream tasks , including form understanding ( from 78.95 to 85.14 ) , document visual question answering ( from 72.59 to 83.94 ) and document image classification ( from 94.43 to 96.08 ) .",3,0.8809493,22.823858746887144,50
2304,Aspect-based sentiment analysis is a fine-grained sentiment classification task .,0,0.87563276,9.363318019627489,12
2304,"Recently , graph neural networks over dependency trees have been explored to explicitly model connections between aspects and opinion words .",0,0.8737976,82.57927612937755,21
2304,"However , the improvement is limited due to the inaccuracy of the dependency parsing results and the informal expressions and complexity of online reviews .",3,0.53432167,52.78216821212414,25
2304,"To overcome these challenges , in this paper , we propose a dual graph convolutional networks ( DualGCN ) model that considers the complementarity of syntax structures and semantic correlations simultaneously .",1,0.7753991,40.54574022470455,32
2304,"Particularly , to alleviate dependency parsing errors , we design a SynGCN module with rich syntactic knowledge .",2,0.63454044,94.24533211995852,18
2304,"To capture semantic correlations , we design a SemGCN module with self-attention mechanism .",2,0.87160325,73.82276947111691,14
2304,"Furthermore , we propose orthogonal and differential regularizers to capture semantic correlations between words precisely by constraining attention scores in the SemGCN module .",2,0.45963302,83.6000390565509,24
2304,The orthogonal regularizer encourages the SemGCN to learn semantically correlated words with less overlap for each word .,3,0.50918037,68.89193909725428,18
2304,The differential regularizer encourages the SemGCN to learn semantic features that the SynGCN fails to capture .,3,0.4533605,103.89047817284195,17
2304,Experimental results on three public datasets show that our DualGCN model outperforms state-of-the-art methods and verify the effectiveness of our model .,3,0.9408702,8.929589219309532,28
2305,Aspect category detection ( ACD ) in sentiment analysis aims to identify the aspect categories mentioned in a sentence .,0,0.88241524,39.678585876971844,20
2305,"In this paper , we formulate ACD in the few-shot learning scenario .",1,0.6993051,48.64455315775084,13
2305,"However , existing few-shot learning approaches mainly focus on single-label predictions .",0,0.88406426,27.89256262525397,12
2305,These methods can not work well for the ACD task since a sentence may contain multiple aspect categories .,3,0.4755112,78.28028415884259,19
2305,"Therefore , we propose a multi-label few-shot learning method based on the prototypical network .",1,0.5135656,23.327227679135145,15
2305,"To alleviate the noise , we design two effective attention mechanisms .",2,0.70902014,83.46704038492065,12
2305,The support-set attention aims to extract better prototypes by removing irrelevant aspects .,0,0.35463214,190.43755046611528,13
2305,"The query-set attention computes multiple prototype-specific representations for each query instance , which are then used to compute accurate distances with the corresponding prototypes .",2,0.6285564,68.14675047380588,26
2305,"To achieve multi-label inference , we further learn a dynamic threshold per instance by a policy network .",2,0.7434729,161.62602626957008,18
2305,Extensive experimental results on three datasets demonstrate that the proposed method significantly outperforms strong baselines .,3,0.9075779,5.86235260828266,16
2306,Argument pair extraction ( APE ) is a research task for extracting arguments from two passages and identifying potential argument pairs .,0,0.9329045,57.85451071417383,22
2306,"Prior research work treats this task as a sequence labeling problem and a binary classification problem on two passages that are directly concatenated together , which has a limitation of not fully utilizing the unique characteristics and inherent relations of two different passages .",0,0.856274,47.80393069333039,44
2306,This paper proposes a novel attention-guided multi-layer multi-cross encoding scheme to address the challenges .,1,0.8796811,28.342798811949148,17
2306,The new model processes two passages with two individual sequence encoders and updates their representations using each other ’s representations through attention .,2,0.68150824,86.61793048066225,23
2306,Cartesian product .,0,0.40551633,818.4866329982838,3
2306,"Furthermore , an auxiliary attention loss is introduced to guide each argument to align to its paired argument .",2,0.62659603,65.38111652014362,19
2306,An extensive set of experiments show that the new model significantly improves the APE performance over several alternatives .,3,0.9208359,24.005065158194544,19
2307,The goal of argumentation mining is to automatically extract argumentation structures from argumentative texts .,0,0.8734641,19.686897029267,15
2307,"Most existing methods determine argumentative relations by exhaustively enumerating all possible pairs of argument components , which suffer from low efficiency and class imbalance .",0,0.82746464,58.84207939654069,25
2307,"Moreover , due to the complex nature of argumentation , there is , so far , no universal method that can address both tree and non-tree structured argumentation .",0,0.9114046,39.56826434102639,29
2307,"Towards these issues , we propose a neural transition-based model for argumentation mining , which incrementally builds an argumentation graph by generating a sequence of actions , avoiding inefficient enumeration operations .",1,0.3832483,38.839810430623814,34
2307,"Furthermore , our model can handle both tree and non-tree structured argumentation without introducing any structural constraints .",3,0.80274785,32.6595526457969,18
2307,Experimental results show that our model achieves the best performance on two public datasets of different structures .,3,0.95836335,13.535274916562212,18
2308,"This work presents Keep it Simple ( KiS ) , a new approach to unsupervised text simplification which learns to balance a reward across three properties : fluency , salience and simplicity .",1,0.57062685,65.01101598170666,33
2308,"We train the model with a novel algorithm to optimize the reward ( k-SCST ) , in which the model proposes several candidate simplifications , computes each candidate ’s reward , and encourages candidates that outperform the mean reward .",2,0.8430213,102.06422227840677,41
2308,"Finally , we propose a realistic text comprehension task as an evaluation method for text simplification .",3,0.5184833,29.100356566482137,17
2308,"When tested on the English news domain , the KiS model outperforms strong supervised baselines by more than 4 SARI points , and can help people complete a comprehension task an average of 18 % faster while retaining accuracy , when compared to the original text .",3,0.92885447,49.6104860072982,47
2309,"Generating long and coherent text is an important but challenging task , particularly for open-ended language generation tasks such as story generation .",0,0.9115597,23.399869956537895,23
2309,"Despite the success in modeling intra-sentence coherence , existing generation models ( e.g. , BART ) still struggle to maintain a coherent event sequence throughout the generated text .",0,0.8540422,40.1081414301518,29
2309,We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence .,3,0.80586183,20.530079781071624,30
2309,"In this paper , we propose a long text generation model , which can represent the prefix sentences at sentence level and discourse level in the decoding process .",1,0.85373414,53.51064314281005,29
2309,"To this end , we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders .",2,0.66798973,35.025505569166384,27
2309,Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines .,3,0.94376487,6.1984807584263875,20
2310,"Automatic metrics are essential for developing natural language generation ( NLG ) models , particularly for open-ended language generation tasks such as story generation .",0,0.91470313,26.449230621698884,25
2310,"However , existing automatic metrics are observed to correlate poorly with human evaluation .",0,0.9105724,42.639616048654766,14
2310,The lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics .,0,0.9171721,27.186487584526564,23
2310,"Therefore , we propose OpenMEVA , a benchmark for evaluating open-ended story generation metrics .",1,0.6168821,56.54935583870505,15
2310,"OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics , including ( a ) the correlation with human judgments , ( b ) the generalization to different model outputs and datasets , ( c ) the ability to judge story coherence , and ( d ) the robustness to perturbations .",0,0.454161,29.324978601589297,54
2310,"To this end , OpenMEVA includes both manually annotated stories and auto-constructed test examples .",2,0.5854375,82.28447708018665,15
2310,"We evaluate existing metrics on OpenMEVA and observe that they have poor correlation with human judgments , fail to recognize discourse-level incoherence , and lack inferential knowledge ( e.g. , causal order between events ) , the generalization ability and robustness .",3,0.64629,63.68303918525492,44
2310,Our study presents insights for developing NLG models and metrics in further research .,3,0.9742511,67.24502439290656,14
2311,"We study the task of long-form opinion text generation , which faces at least two distinct challenges .",1,0.5579865,44.510404081329995,18
2311,"First , existing neural generation models fall short of coherence , thus requiring efficient content planning .",0,0.8101945,147.21965399948513,17
2311,"Second , diverse types of information are needed to guide the generator to cover both subjective and objective content .",0,0.6730562,48.83086762058043,20
2311,"To this end , we propose DYPLOC , a generation framework that conducts dynamic planning of content while generating the output based on a novel design of mixed language models .",1,0.39042914,61.84270910272149,31
2311,"To enrich the generation with diverse content , we further propose to use large pre-trained models to predict relevant concepts and to generate claims .",3,0.48725474,47.18868361633928,25
2311,"We experiment with two challenging tasks on newly collected datasets : ( 1 ) argument generation with Reddit ChangeMyView , and ( 2 ) writing articles using New York Times ’ Opinion section .",2,0.86335576,101.53705378587226,34
2311,Automatic evaluation shows that our model significantly outperforms competitive comparisons .,3,0.9539595,19.473990074851226,11
2311,Human judges further confirm that our generations are more coherent with richer content .,3,0.9114575,214.5620422894058,14
2312,We investigate the less-explored task of generating open-ended questions that are typically answered by multiple sentences .,1,0.5565026,27.016441041306724,17
2312,We first define a new question type ontology which differentiates the nuanced nature of questions better than widely used question words .,2,0.6510299,71.31420711143763,22
2312,"A new dataset with 4,959 questions is labeled based on the new ontology .",2,0.70973545,28.609757892733885,14
2312,"We then propose a novel question type-aware question generation framework , augmented by a semantic graph representation , to jointly predict question focuses and produce the question .",2,0.5056811,67.32090071926787,28
2312,"Based on this framework , we further use both exemplars and automatically generated templates to improve controllability and diversity .",2,0.4768539,37.05769826010073,20
2312,Experiments on two newly collected large-scale datasets show that our model improves question quality over competitive comparisons based on automatic metrics .,3,0.9121782,36.996591140196585,22
2312,"Human judges also rate our model outputs highly in answerability , coverage of scope , and overall quality .",3,0.88985896,357.18588085995907,19
2312,"Finally , our model variants with templates can produce questions with enhanced controllability and diversity .",3,0.93838537,116.62354545813811,16
2313,"We present BERTGen , a novel , generative , decoder-only model which extends BERT by fusing multimodal and multilingual pre-trained models VL-BERT and M-BERT , respectively .",2,0.4379194,28.197313930537835,29
2313,"BERTGen is auto-regressively trained for language generation tasks , namely image captioning , machine translation and multimodal machine translation , under a multi-task setting .",2,0.48474884,46.31267129515831,25
2313,"With a comprehensive set of evaluations , we show that BERTGen outperforms many strong baselines across the tasks explored .",3,0.91485476,28.46468070791647,20
2313,"We also show BERTGen ’s ability for zero-shot language generation , where it exhibits competitive performance to supervised counterparts .",3,0.8762345,71.37639583349214,20
2313,"Finally , we conduct ablation studies which demonstrate that BERTGen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models .",3,0.8520207,39.25028988788573,25
2314,Neural Machine Translation ( NMT ) models achieve state-of-the-art performance on many translation benchmarks .,0,0.8862089,7.773392039094158,21
2314,"As an active research field in NMT , knowledge distillation is widely applied to enhance the model ’s performance by transferring teacher model ’s knowledge on each training sample .",0,0.8882663,47.476698317105246,30
2314,"However , previous work rarely discusses the different impacts and connections among these samples , which serve as the medium for transferring teacher knowledge .",0,0.8643041,133.47513107322868,25
2314,"In this paper , we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples ’ partitions .",1,0.8914125,71.15595244025677,25
2314,"Based on above protocol , we conduct extensive experiments and find that the teacher ’s knowledge is not the more , the better .",3,0.8195467,72.36046451137051,24
2314,Knowledge over specific samples may even hurt the whole performance of knowledge distillation .,0,0.5200241,91.31609410739647,14
2314,"Finally , to address these issues , we propose two simple yet effective strategies , i.e. , batch-level and global-level selections , to pick suitable samples for distillation .",3,0.39941725,42.570583333297535,33
2314,"We evaluate our approaches on two large-scale machine translation tasks , WMT’14 English-German and WMT’19 Chinese-English .",2,0.72284174,8.76630473970526,19
2314,"Experimental results show that our approaches yield up to + 1.28 and + 0.89 BLEU points improvements over the Transformer baseline , respectively .",3,0.9493465,13.096712321044704,24
2315,"Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context , context from sentences other than those currently being translated .",0,0.92566687,33.63363248417358,28
2315,"However , while many current methods present model architectures that theoretically can use this extra context , it is often not clear how much they do actually utilize it at translation time .",0,0.9122266,58.84836474435108,33
2315,"In this paper , we introduce a new metric , conditional cross-mutual information , to quantify usage of context by these models .",1,0.83605057,74.37814434595684,23
2315,"Using this metric , we measure how much document-level machine translation systems use particular varieties of context .",2,0.7066147,65.57666457135477,20
2315,"We find that target context is referenced more than source context , and that including more context has a diminishing affect on results .",3,0.98277825,62.299757353307044,24
2315,"We then introduce a new , simple training method , context-aware word dropout , to increase the usage of context by context-aware models .",2,0.6154979,82.83886181754374,28
2315,"Experiments show that our method not only increases context usage , but also improves the translation quality according to metrics such as BLEU and COMET , as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets .",3,0.9550889,35.4463689587929,40
2316,Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings .,0,0.9584607,9.417839277234,18
2316,"Such methods critically rely on those embeddings having a similar structure , but it was recently shown that the separate training in different languages causes departures from this assumption .",0,0.86919093,75.24168610105943,30
2316,"In this paper , we propose an alternative approach that does not have this limitation , while requiring a weak seed dictionary ( e.g. , a list of identical words ) as the only form of supervision .",1,0.7559567,48.267528764966144,38
2316,"Rather than aligning two fixed embedding spaces , our method works by fixing the target language embeddings , and learning a new set of embeddings for the source language that are aligned with them .",2,0.55814004,24.98901877995736,35
2316,"To that end , we use an extension of skip-gram that leverages translated context words as anchor points , and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary .",2,0.80992454,57.05267000793888,34
2316,"Our approach outperforms conventional mapping methods on bilingual lexicon induction , and obtains competitive results in the downstream XNLI task .",3,0.84121644,36.256921205849935,21
2317,We show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences .,3,0.94384336,31.04297674536783,25
2317,"We use 32 snapshots of a curated common crawl corpus ( Wenzel et al , 2019 ) totaling 71 billion unique sentences .",2,0.90948945,202.64684546425372,23
2317,"Using one unified approach for 90 languages , we were able to mine 10.8 billion parallel sentences , out of which only 2.9 billions are aligned with English .",3,0.7033869,51.59092501103053,29
2317,We illustrate the capability of our scalable mining system to create high quality training sets from one language to any other by training hundreds of different machine translation models and evaluating them on the many-to-many TED benchmark .,3,0.64672494,36.920124457150905,42
2317,"Further , we evaluate on competitive translation benchmarks such as WMT and WAT .",2,0.47045934,34.875148625674576,14
2317,"Using only mined bitext , we set a new state of the art for a single system on the WMT ’19 test set for English-German / Russian / Chinese .",2,0.66255766,44.83585165450892,33
2317,"In particular , our English / German and English / Russian systems outperform the best single ones by over 4 BLEU points and are on par with best WMT ’19 systems , which train on the WMT training data and augment it with backtranslation .",3,0.94567776,42.44948825668437,46
2317,"We also achieve excellent results for distant languages pairs like Russian / Japanese , outperforming the best submission at the 2020 WAT workshop .",3,0.9318695,182.57045515659374,24
2317,All of the mined bitext will be freely available .,0,0.37898833,96.89218463227601,10
2318,"Despite transformers ’ impressive accuracy , their computational cost is often prohibitive to use with limited computational resources .",0,0.8635441,49.26015153810558,19
2318,Most previous approaches to improve inference efficiency require a separate model for each possible computational budget .,0,0.774271,82.28400624597914,17
2318,"In this paper , we extend PoWER-BERT ( Goyal et al. , 2020 ) and propose Length-Adaptive Transformer that can be used for various inference scenarios after one-shot training .",1,0.6113882,55.71467876023367,35
2318,"We train a transformer with LengthDrop , a structural variant of dropout , which stochastically determines a sequence length at each layer .",2,0.8720225,96.06873774244995,23
2318,We then conduct a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the efficiency metric under any given computational budget .,2,0.86666363,54.2293479304877,27
2318,"Additionally , we significantly extend the applicability of PoWER-BERT beyond sequence-level classification into token-level classification with Drop-and-Restore process that drops word-vectors temporarily in intermediate layers and restores at the last layer if necessary .",3,0.54361033,66.19712061168995,41
2318,"We empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups , including span-based question answering and text classification .",3,0.69226515,25.11092075078857,31
2318,Code is available at https://github.com/clovaai/lengthadaptive-transformer .,3,0.49391326,23.009609695749145,6
2319,"Transformer-based pre-trained language models like BERT , though powerful in many tasks , are expensive in both memory and computation , due to their large number of parameters .",0,0.7357062,22.933202789376192,31
2319,Previous works show that some parameters in these models can be pruned away without severe accuracy drop .,0,0.84441656,44.50531056128688,18
2319,"However , these redundant features contribute to a comprehensive understanding of the training data and removing them weakens the model ’s representation ability .",0,0.6546578,63.28089612646473,24
2319,"In this paper , we propose GhostBERT , which generates more features with very cheap operations from the remaining features .",1,0.8208511,116.25825764731222,21
2319,"In this way , GhostBERT has similar memory and computational cost as the pruned model , but enjoys much larger representation power .",3,0.78951955,142.8678763255245,23
2319,The proposed ghost module can also be applied to unpruned BERT models to enhance their performance with negligible additional parameters and computation .,3,0.86727965,50.84313107962061,23
2319,"Empirical results on the GLUE benchmark on three backbone models ( i.e. , BERT , RoBERTa and ELECTRA ) verify the efficacy of our proposed method .",3,0.8914274,22.649562872620372,27
2320,"The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of ” lottery tickets ” , and training a certain collection of them ( i.e. , a subnetwork ) can match the performance of the full model .",0,0.72044826,47.83355016861584,38
2320,"In this paper , we study such a collection of tickets , which is referred to as ” winning tickets ” , in extremely over-parametrized models , e.g. , pre-trained language models .",1,0.5968834,46.732539466515206,34
2320,"We observe that at certain compression ratios , the generalization performance of the winning tickets can not only match but also exceed that of the full model .",3,0.9589877,62.04675431247133,28
2320,"As the compression ratio increases , generalization performance of the winning tickets first improves then deteriorates after a certain threshold .",3,0.7925939,95.26505281715868,21
2320,We refer to the tickets on the threshold as ” super tickets ” .,2,0.4119383,90.21630179993488,14
2320,"We further show that the phase transition is task and model dependent — as the model size becomes larger and the training data set becomes smaller , the transition becomes more pronounced .",3,0.941111,45.00002459593193,33
2320,"Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large , in terms of task-average score .",3,0.952653,19.457175890004272,38
2320,We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning .,3,0.95305055,88.98910500220848,15
2321,"Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification , style transfer and sentence generation , among others .",0,0.9136886,39.31690042966555,27
2321,The existent dominant approaches in the context of text data either rely on training an adversary ( discriminator ) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute .,0,0.8782009,67.20908921563606,51
2321,"However , the available methods suffer of the impossibility to provide a fine-grained control of the degree ( or force ) of disentanglement .",0,0.8911925,44.89382727956459,25
2321,"In contrast to adversarial methods , which are remarkably simple , although the adversary seems to be performing perfectly well during the training phase , after it is completed a fair amount of information about the undesired attribute still remains .",3,0.62943536,58.025721307231755,41
2321,This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder .,1,0.7397633,24.421523386393808,23
2321,"Our bound aims at controlling the approximation error via the Renyi ’s divergence , leading to both better disentangled representations and in particular , a precise control of the desirable degree of disentanglement than state-of-the-art methods proposed for textual data .",3,0.5162568,57.37885992317531,47
2321,"Furthermore , it does not suffer from the degeneracy of other losses in multi-class scenarios .",3,0.6985927,33.8655082718797,16
2321,We show the superiority of this method on fair classification and on textual style transfer tasks .,3,0.94840467,46.34336662863563,17
2321,"Additionally , we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence .",3,0.85724026,53.86084110081518,25
2322,Beam search is a go-to strategy for decoding neural sequence models .,0,0.8876276,40.898257788621116,13
2322,"The algorithm can naturally be viewed as a subset optimization problem , albeit one where the corresponding set function does not reflect interactions between candidates .",3,0.42301106,104.93061599287147,26
2322,"Empirically , this leads to sets often exhibiting high overlap , e.g. , strings may differ by only a single word .",0,0.55459714,92.73328401151184,22
2322,"Yet in use-cases that call for multiple solutions , a diverse or representative set is often desired .",0,0.8622517,100.00215218545844,18
2322,"To address this issue , we propose a reformulation of beam search , which we call determinantal beam search .",1,0.55004776,37.86803495356064,20
2322,"Determinantal beam search has a natural relationship to determinantal point processes ( DPPs ) , models over sets that inherently encode intra-set interactions .",0,0.90057194,185.70019000043627,24
2322,"By posing iterations in beam search as a series of subdeterminant maximization problems , we can turn the algorithm into a diverse subset selection process .",3,0.43901,93.71747146821761,26
2322,"In a case study , we use the string subsequence kernel to explicitly encourage n-gram coverage in text generated from a sequence model .",2,0.7609273,62.312027486080524,24
2322,"We observe that our algorithm offers competitive performance against other diverse set generation strategies in the context of language generation , while providing a more general approach to optimizing for diversity .",3,0.9537393,50.71763012466181,32
2323,Graph convolutional network ( GCN ) has become popular in various natural language processing ( NLP ) tasks with its superiority in long-term and non-consecutive word interactions .,0,0.9577285,21.93220687604502,28
2323,"However , existing single-hop graph reasoning in GCN may miss some important non-consecutive dependencies .",0,0.54872805,77.75625860461416,17
2323,"In this study , we define the spectral graph convolutional network with the high-order dynamic Chebyshev approximation ( HDGCN ) , which augments the multi-hop graph reasoning by fusing messages aggregated from direct and long-term dependencies into one convolutional layer .",1,0.482923,42.87884178157146,41
2323,"To alleviate the over-smoothing in high-order Chebyshev approximation , a multi-vote-based cross-attention ( MVCAttn ) with linear computation complexity is also proposed .",2,0.5135311,84.17601014545487,25
2323,The empirical results on four transductive and inductive NLP tasks and the ablation study verify the efficacy of the proposed model .,3,0.95654655,18.696634501275607,22
2324,Typing every character in a text message may require more time or effort than strictly necessary .,0,0.82645947,62.34335253256595,17
2324,Skipping spaces or other characters may be able to speed input and reduce a user ’s physical input effort .,3,0.6635542,102.06733707881176,20
2324,This can be particularly important for people with motor impairments .,0,0.61080486,11.020407730575053,11
2324,"In a large crowdsourced study , we found workers frequently abbreviated text by omitting mid-word vowels .",0,0.6950468,42.63575311451124,17
2324,We designed a recognizer optimized for expanding noisy abbreviated input where users often omit spaces and mid-word vowels .,2,0.7449993,121.88409414947462,19
2324,We show using neural language models for selecting conversational-style training text and for rescoring the recognizer ’s n-best sentences improved accuracy .,3,0.94918525,92.22827186306655,23
2324,"On noisy touchscreen data collected from hundreds of users , we found accurate abbreviated input was possible even if a third of characters was omitted .",3,0.8977509,122.56113447834836,26
2324,"Finally , in a study where users had to dwell for a second on each key , sentence abbreviated input was competitive with a conventional keyboard with word predictions .",3,0.9417317,105.53803881985961,30
2324,"After practice , users wrote abbreviated sentences at 9.6 words-per-minute versus word input at 9.9 words-per-minute .",3,0.7825743,33.997859585773156,21
2325,Behavior of deep neural networks can be inconsistent between different versions .,0,0.82750994,68.64292552535095,12
2325,Regressions during model update are a common cause of concern that often over-weigh the benefits in accuracy or efficiency gain .,0,0.87586933,70.51951046675981,22
2325,"This work focuses on quantifying , reducing and analyzing regression errors in the NLP model updates .",1,0.86966693,79.95523100057456,17
2325,"Using negative flip rate as regression measure , we show that regression has a prevalent presence across tasks in the GLUE benchmark .",3,0.9039902,96.0354402666318,23
2325,"We formulate the regression-free model updates into a constrained optimization problem , and further reduce it into a relaxed form which can be approximately optimized through knowledge distillation training method .",2,0.75627446,108.69239864683387,33
2325,We empirically analyze how model ensemble reduces regression .,2,0.43820202,190.13875535789722,9
2325,"Finally , we conduct CheckList behavioral testing to understand the distribution of regressions across linguistic phenomena , and the efficacy of ensemble and distillation methods .",2,0.640323,97.28620710269166,26
2326,Propaganda can be defined as a form of communication that aims to influence the opinions or the actions of people towards a specific goal ;,0,0.92366,24.491669148695554,25
2326,this is achieved by means of well-defined rhetorical and psychological devices .,0,0.7553895,59.80631056109941,14
2326,"Propaganda , in the form we know it today , can be dated back to the beginning of the 17th century .",0,0.8954142,28.469071917742436,22
2326,"However , it is with the advent of the Internet and the social media that propaganda has started to spread on a much larger scale than before , thus becoming major societal and political issue .",0,0.89958173,20.193689206504477,36
2326,"Nowadays , a large fraction of propaganda in social media is multimodal , mixing textual with visual content .",0,0.9482472,75.93392454754422,19
2326,"With this in mind , here we propose a new multi-label multimodal task : detecting the type of propaganda techniques used in memes .",1,0.76351815,39.271360395715625,24
2326,"We further create and release a new corpus of 950 memes , carefully annotated with 22 propaganda techniques , which can appear in the text , in the image , or in both .",2,0.72971493,89.10425858543475,34
2326,Our analysis of the corpus shows that understanding both modalities together is essential for detecting these techniques .,3,0.97068393,45.316571437427505,18
2326,This is further confirmed in our experiments with several state-of-the-art multimodal models .,3,0.9275107,7.7068403939382275,19
2327,"In adversarial data collection ( ADC ) , a human workforce interacts with a model in real time , attempting to produce examples that elicit incorrect predictions .",0,0.8725703,121.68554763953486,28
2327,"Researchers hope that models trained on these more challenging datasets will rely less on superficial patterns , and thus be less brittle .",3,0.5122553,52.89501838675498,23
2327,"However , despite ADC ’s intuitive appeal , it remains unclear when training on adversarial datasets produces more robust models .",0,0.92151004,72.2618154199572,21
2327,"In this paper , we conduct a large-scale controlled study focused on question answering , assigning workers at random to compose questions either ( i ) adversarially ( with a model in the loop ) ;",1,0.7639489,82.81524379784685,36
2327,or ( ii ) in the standard fashion ( without a model ) .,2,0.4434373,185.6815071538821,14
2327,"Across a variety of models and datasets , we find that models trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets .",3,0.94359225,16.085336599611033,36
2327,"Finally , we provide a qualitative analysis of adversarial ( vs standard ) data , identifying key differences and offering guidance for future research .",3,0.81190985,61.819947860308595,25
2328,"Open-domain question answering can be reformulated as a phrase retrieval problem , without the need for processing documents on-demand during inference ( Seo et al. , 2019 ) .",0,0.7847422,36.70349179587728,30
2328,"However , current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches .",0,0.9043157,44.384193160310026,19
2328,"In this work , we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA .",3,0.45363975,32.06809633862387,28
2328,"We present an effective method to learn phrase representations from the supervision of reading comprehension tasks , coupled with novel negative sampling methods .",1,0.33200023,67.51255982417362,24
2328,"We also propose a query-side fine-tuning strategy , which can support transfer learning and reduce the discrepancy between training and inference .",3,0.59600616,23.44043625067344,24
2328,"On five popular open-domain QA datasets , our model DensePhrases improves over previous phrase retrieval models by 15 %-25 % absolute accuracy and matches the performance of state-of-the-art retriever-reader models .",3,0.8698294,23.555014145729984,41
2328,Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs .,3,0.7083682,100.87114576663936,22
2328,"Finally , we directly use our pre-indexed dense phrase representations for two slot filling tasks , showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks .",3,0.8206936,47.46130652797193,31
2329,Recent work on training neural retrievers for open-domain question answering ( OpenQA ) has employed both supervised and unsupervised approaches .,0,0.94470745,17.125862737886596,21
2329,"However , it remains unclear how unsupervised and supervised methods can be used most effectively for neural retrievers .",0,0.9127955,27.277800115105855,19
2329,"In this work , we systematically study retriever pre-training .",1,0.89112854,44.08090591447607,10
2329,"We first propose an approach of unsupervised pre-training with the Inverse Cloze Task and masked salient spans , followed by supervised finetuning using question-context pairs .",2,0.68517447,56.7979230861602,28
2329,This approach leads to absolute gains of 2 + points over the previous best result in the top-20 retrieval accuracy on Natural Questions and TriviaQA datasets .,3,0.76975113,34.749015985781156,29
2329,"We next explore two approaches for end-to-end training of the reader and retriever components in OpenQA models , which differ in the manner the reader ingests the retrieved documents .",2,0.47887295,42.796389969257135,32
2329,Our experiments demonstrate the effectiveness of these approaches as we obtain state-of-the-art results .,3,0.93977404,6.413567849347581,19
2329,"On the Natural Questions dataset , we obtain a top-20 retrieval accuracy of 84 % , an improvement of 5 points over the recent DPR model .",3,0.93077976,37.04821925606798,29
2329,"We also achieve good results on answer extraction , outperforming recent models like REALM and RAG by 3 + points .",3,0.9505489,139.3394459652354,21
2330,Temporal Knowledge Graphs ( Temporal KGs ) extend regular Knowledge Graphs by providing temporal scopes ( start and end times ) on each edge in the KG .,0,0.8335133,29.62243233000312,28
2330,"While Question Answering over KG ( KGQA ) has received some attention from the research community , QA over Temporal KGs ( Temporal KGQA ) is a relatively unexplored area .",0,0.9609718,14.209860821984895,31
2330,Lack of broad coverage datasets has been another factor limiting progress in this area .,0,0.90263534,48.76562186925654,15
2330,"We address this challenge by presenting CRONQUESTIONS , the largest known Temporal KGQA dataset , clearly stratified into buckets of structural complexity .",1,0.56996024,177.80431876887735,23
2330,CRONQUESTIONS expands the only known previous dataset by a factor of 340x .,3,0.5344528,124.21155473005231,13
2330,We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset .,3,0.97782326,14.2216284627138,24
2330,"In response , we also propose CRONKGQA , a transformer-based solution that exploits recent advances in Temporal KG embeddings , and achieves performance superior to all baselines , with an increase of 120 % in accuracy over the next best performing method .",3,0.67814153,56.80155237237939,45
2330,"Through extensive experiments , we give detailed insights into the workings of CRONKGQA , as well as situations where significant further improvements appear possible .",3,0.7785257,102.25327771911512,25
2330,"In addition to the dataset , we have released our code as well .",3,0.5067451,30.010245765640907,14
2331,"Although automated metrics are commonly used to evaluate NLG systems , they often correlate poorly with human judgements .",0,0.9028241,23.085522609677923,19
2331,"Newer metrics such as BERTScore have addressed many weaknesses in prior metrics such as BLEU and ROUGE , which rely on n-gram matching .",0,0.7841812,24.202408435647808,24
2331,"These newer methods , however , are still limited in that they do not consider the generation context , so they cannot properly reward generated text that is correct but deviates from the given reference .",0,0.8018025,62.014986870931835,36
2331,"In this paper , we propose Language Model Augmented Relevance Score ( MARS ) , a new context-aware metric for NLG evaluation .",1,0.8771354,42.50181437863512,25
2331,"MARS leverages off-the-shelf language models , guided by reinforcement learning , to create augmented references that consider both the generation context and available human references , which are then used as additional references to score generated text .",2,0.4971945,58.00964794938576,39
2331,"Compared with seven existing metrics in three common NLG tasks , MARS not only achieves higher correlation with human reference judgements , but also differentiates well-formed candidates from adversarial samples to a larger degree .",3,0.8983425,56.26001093037726,37
2332,"Despite recent advances in natural language generation , it remains challenging to control attributes of generated text .",0,0.95050365,24.346957493180977,18
2332,"We propose DExperts : Decoding-time Experts , a decoding-time method for controlled text generation that combines a pretrained language model with “ expert ” LMs and / or “ anti-expert ” LMs in a product of experts .",2,0.43361616,48.73616888608472,42
2332,"Intuitively , under the ensemble , tokens only get high probability if they are considered likely by the experts , and unlikely by the anti-experts .",3,0.8487671,110.51413725025684,26
2332,"We apply DExperts to language detoxification and sentiment-controlled generation , where we outperform existing controllable generation methods on both automatic and human evaluations .",2,0.6200309,75.47064858394089,25
2332,"Moreover , because DExperts operates only on the output of the pretrained LM , it is effective with ( anti-) experts of smaller size , including when operating on GPT-3 .",3,0.8746716,169.61830431518928,35
2332,Our work highlights the promise of tuning small LMs on text with ( un ) desirable attributes for efficient decoding-time steering .,3,0.97860545,266.7013959612695,24
2333,"While counterfactual examples are useful for analysis and training of NLP models , current generation methods either rely on manual labor to create very few counterfactuals , or only instantiate limited types of perturbations such as paraphrases or word substitutions .",0,0.88435644,30.264496423486893,41
2333,"We present Polyjuice , a general-purpose counterfactual generator that allows for control over perturbation types and locations , trained by finetuning GPT-2 on multiple datasets of paired sentences .",2,0.42927474,47.600818229756754,33
2333,"We show that Polyjuice produces diverse sets of realistic counterfactuals , which in turn are useful in various distinct applications : improving training and evaluation on three different tasks ( with around 70 % less annotation effort than manual generation ) , augmenting state-of-the-art explanation techniques , and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts .",3,0.8991313,77.69284560390005,68
2334,Generating metaphors is a difficult task as it requires understanding nuanced relationships between abstract concepts .,0,0.90951806,24.58323613168718,16
2334,"In this paper , we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs .",1,0.92048186,57.302903127680786,20
2334,"Guided by conceptual metaphor theory , we propose to control the generation process by encoding conceptual mappings between cognitive domains to generate meaningful metaphoric expressions .",2,0.36649933,67.15575081982409,26
2334,"To achieve this , we develop two methods : 1 ) using FrameNet-based embeddings to learn mappings between domains and applying them at the lexical level ( CM-Lex ) , and 2 ) deriving source / target pairs to train a controlled seq-to-seq generation model ( CM-BART ) .",2,0.8920117,50.719552793128614,55
2334,We assess our methods through automatic and human evaluation for basic metaphoricity and conceptual metaphor presence .,2,0.73965806,135.05866865475957,17
2334,"We show that the unsupervised CM-Lex model is competitive with recent deep learning metaphor generation systems , and CM-BART outperforms all other models both in automatic and human evaluations .",3,0.95888895,43.006093296596966,34
2335,Wet laboratory protocols ( WLPs ) are critical for conveying reproducible procedures in biological research .,0,0.9746712,159.26022099856735,16
2335,They are composed of instructions written in natural language describing the step-wise processing of materials by specific actions .,0,0.8937924,62.49961809020662,19
2335,"This process flow description for reagents and materials synthesis in WLPs can be captured by material state transfer graphs ( MSTGs ) , which encode global temporal and causal relationships between actions .",0,0.65803283,175.77113463064688,33
2335,"Here , we propose methods to automatically generate a MSTG for a given protocol by extracting all action relationships across multiple sentences .",1,0.70778006,75.16548389709438,23
2335,We also note that previous corpora and methods focused primarily on local intra-sentence relationships between actions and entities and did not address two critical issues : ( i ) resolution of implicit arguments and ( ii ) establishing long-range dependencies across sentences .,3,0.5667951,51.862846999281096,43
2335,We propose a new model that incrementally learns latent structures and is better suited to resolving inter-sentence relations and implicit arguments .,3,0.39670926,51.349778439015424,22
2335,This model draws upon a new corpus WLP-MSTG which was created by extending annotations in the WLP corpora for inter-sentence relations and implicit arguments .,2,0.6247328,68.93255395695064,27
2335,"Our model achieves an F1 score of 54.53 % for temporal and causal relations in protocols from our corpus , which is a significant improvement over previous models-DyGIE ++:28.17 % ;",3,0.936301,99.29312594572492,33
2335,spERT:27.81 % .,3,0.88864046,412.4884534745651,3
2335,We make our annotated WLP-MSTG corpus available to the research community .,3,0.48510823,40.74892296484772,14
2336,Risk prediction is an essential task in financial markets .,0,0.9535978,43.17589668796429,10
2336,Merger and Acquisition ( M&A ) calls provide key insights into the claims made by company executives about the restructuring of the financial firms .,0,0.9531583,79.20185661075334,25
2336,Extracting vocal and textual cues from M&A calls can help model the risk associated with such financial activities .,0,0.6881168,135.4762448817862,19
2336,"To aid the analysis of M&A calls , we curate a dataset of conference call transcripts and their corresponding audio recordings for the time period ranging from 2016 to 2020 .",2,0.87415534,32.745544553079384,31
2336,"We introduce M3 ANet , a baseline architecture that takes advantage of the multimodal multi-speaker input to forecast the financial risk associated with the M&A calls .",2,0.64614844,63.6112327030967,27
2336,"Empirical results prove that the task is challenging , with the pro-posed architecture performing marginally better than strong BERT-based baselines .",3,0.9539053,29.37891255127643,23
2336,We release the M3A dataset and benchmark models to motivate future research on this challenging problem domain .,3,0.3770116,43.44813037765672,18
2337,"To translate large volumes of text in a globally connected world , more and more translators are integrating machine translation ( MT ) and post-editing ( PE ) into their translation workflows to generate publishable quality translations .",0,0.95993215,36.37594100392604,38
2337,"While this process has been shown to save time and reduce errors , the task of translation is changing from mostly text production from scratch to fixing errors within useful but partly incorrect MT output .",0,0.90415156,89.75105614413954,36
2337,"This is affecting the interface design of translation tools , where better support for text editing tasks is required .",0,0.8224806,86.11024510148417,20
2337,"Here , we present the first study that investigates the usefulness of mid-air hand gestures in combination with the keyboard ( GK ) for text editing in PE of MT .",1,0.80287707,46.297425077676756,31
2337,"Guided by a gesture elicitation study with 14 freelance translators , we develop a prototype supporting mid-air hand gestures for cursor placement , text selection , deletion , and reordering .",2,0.6391652,79.5710879562917,31
2337,These gestures combined with the keyboard facilitate all editing types required for PE .,3,0.70245606,308.46029511109504,14
2337,"An evaluation of the prototype shows that the average editing duration of GK is only slightly slower than the standard mouse and keyboard ( MK ) , even though participants are very familiar with the latter , and relative novices to the former .",3,0.9008208,69.83169766448064,44
2337,"Furthermore , the qualitative analysis shows positive attitudes towards hand gestures for PE , especially when manipulating single words .",3,0.9865459,189.3522338169348,20
2338,Geometry problem solving has attracted much attention in the NLP community recently .,0,0.94437176,17.502993466667053,13
2338,The task is challenging as it requires abstract problem understanding and symbolic reasoning with axiomatic knowledge .,0,0.8730277,36.35723013081738,17
2338,"However , current datasets are either small in scale or not publicly available .",0,0.93264765,41.0731621612009,14
2338,"Thus , we construct a new large-scale benchmark , Geometry3 K , consisting of 3,002 geometry problems with dense annotation in formal language .",2,0.7535446,95.48657962683161,24
2338,"We further propose a novel geometry solving approach with formal language and symbolic reasoning , called Interpretable Geometry Problem Solver ( Inter-GPS ) .",2,0.46309596,62.56071251660696,26
2338,"Inter-GPS first parses the problem text and diagram into formal language automatically via rule-based text parsing and neural object detecting , respectively .",2,0.65501845,193.1260982173165,25
2338,"Unlike implicit learning in existing methods , Inter-GPS incorporates theorem knowledge as conditional rules and performs symbolic reasoning step by step .",3,0.34660512,244.50485197541246,24
2338,"Also , a theorem predictor is designed to infer the theorem application sequence fed to the symbolic solver for the more efficient and reasonable searching path .",2,0.6729596,212.9590130081643,27
2338,Extensive experiments on the Geometry3 K and GEOS datasets demonstrate that Inter-GPS achieves significant improvements over existing methods .,3,0.8909829,39.65703213778088,21
2338,The project with code and data is available at https://lupantech.github.io/inter-gps .,3,0.5552273,22.723832827771158,11
2339,Structured information is an important knowledge source for automatic verification of factual claims .,0,0.9322305,42.987415564763324,14
2339,"Nevertheless , the majority of existing research into this task has focused on textual data , and the few recent inquiries into structured data have been for the closed-domain setting where appropriate evidence for each claim is assumed to have already been retrieved .",0,0.9164842,37.919785204185544,46
2339,"In this paper , we investigate verification over structured data in the open-domain setting , introducing a joint reranking-and-verification model which fuses evidence documents in the verification component .",1,0.83774996,64.00515840996353,33
2339,"Our open-domain model achieves performance comparable to the closed-domain state-of-the-art on the TabFact dataset , and demonstrates performance gains from the inclusion of multiple tables as well as a significant improvement over a heuristic retrieval baseline .",3,0.8949106,20.86077592195677,43
2340,Collecting together microblogs representing opinions about the same topics within the same timeframe is useful to a number of different tasks and practitioners .,0,0.7741778,53.475098598201285,24
2340,A major question is how to evaluate the quality of such thematic clusters .,0,0.87141836,34.579567001338276,14
2340,Here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence .,2,0.51177907,54.21931573411332,24
2340,We provide annotation guidelines and human annotations of thematic coherence by journalist experts .,2,0.40076727,128.94475539792325,14
2340,We subsequently investigate the efficacy of different automated evaluation metrics for the task .,2,0.45652848,28.2083077915393,14
2340,"We consider a range of metrics including surface level metrics , ones for topic model coherence and text generation metrics ( TGMs ) .",2,0.7531433,126.19786691101247,24
2340,"While surface level metrics perform well , outperforming topic coherence metrics , they are not as consistent as TGMs .",3,0.8697336,121.71607219172756,20
2340,TGMs are more reliable than all other metrics considered for capturing thematic coherence in microblog clusters due to being less sensitive to the effect of time windows .,3,0.92101616,80.63517886472322,28
2341,"Monolingual word alignment is important for studying fine-grained editing operations ( i.e. , deletion , addition , and substitution ) in text-to-text generation tasks , such as paraphrase generation , text simplification , neutralizing biased language , etc .",0,0.8893735,46.26595492796842,44
2341,"In this paper , we present a novel neural semi-Markov CRF alignment model , which unifies word and phrase alignments through variable-length spans .",1,0.8740103,34.37414351731092,26
2341,We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings .,2,0.7297419,44.56520687699666,26
2341,"Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline , which was previously only applied to bilingual data .",3,0.9646493,17.81492930744647,34
2341,Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications : automatic text simplification and sentence pair classification tasks .,3,0.9385137,26.96056976713769,29
2342,Organisations disclose their privacy practices by posting privacy policies on their websites .,0,0.9215695,20.774295728204493,13
2342,"Even though internet users often care about their digital privacy , they usually do not read privacy policies , since understanding them requires a significant investment of time and effort .",0,0.92226964,29.338237709206588,31
2342,"Natural language processing has been used to create experimental tools to interpret privacy policies , but there has been a lack of large privacy policy corpora to facilitate the creation of large-scale semi-supervised and unsupervised models to interpret and simplify privacy policies .",0,0.94914186,23.668086706874416,43
2342,"Thus , we present the PrivaSeer Corpus of 1,005,380 English language website privacy policies collected from the web .",2,0.6877328,92.97649557559151,19
2342,"The number of unique websites represented in PrivaSeer is about ten times larger than the next largest public collection of web privacy policies , and it surpasses the aggregate of unique websites represented in all other publicly available privacy policy corpora combined .",3,0.8559432,37.689414497253416,43
2342,"We describe a corpus creation pipeline with stages that include a web crawler , language detection , document classification , duplicate and near-duplicate removal , and content extraction .",2,0.36683005,114.52309879496796,29
2342,We employ an unsupervised topic modelling approach to investigate the contents of policy documents in the corpus and discuss the distribution of topics in privacy policies at web scale .,2,0.79062897,41.75309644455911,30
2342,We further investigate the relationship between privacy policy domain PageRanks and text features of the privacy policies .,3,0.49885097,90.11651112761305,18
2342,"Finally , we use the corpus to pretrain PrivBERT , a transformer-based privacy policy language model , and obtain state of the art results on the data practice classification and question answering tasks .",2,0.4751592,43.64534279901408,36
2343,Estimating the expected output quality of generation systems is central to NLG .,0,0.9207156,70.43724125379131,13
2343,This paper qualifies the notion that automatic metrics are not as good as humans in estimating system-level quality .,1,0.5870631,42.08149306542092,19
2343,"Statistically , humans are unbiased , high variance estimators , while metrics are biased , low variance estimators .",0,0.7985628,103.59144706549972,19
2343,We compare these estimators by their error in pairwise prediction ( which generation system is better ? ) using the bootstrap .,2,0.8082314,125.73812057223894,22
2343,"Measuring this error is complicated : predictions are evaluated against noisy , human predicted labels instead of the ground truth , and metric predictions fluctuate based on the test sets they were calculated on .",0,0.6384788,123.04035583327476,35
2343,"By applying a bias-variance-noise decomposition , we adjust this error to a noise-free , infinite test set setting .",2,0.8164251,109.59990103603442,21
2343,"Our analysis compares the adjusted error of metrics to humans and a derived , perfect segment-level annotator , both of which are unbiased estimators dependent on the number of judgments collected .",2,0.6794978,129.12386440662425,34
2343,"In MT , we identify two settings where metrics outperform humans due to a statistical advantage in variance : when the number of human judgments used is small , and when the quality difference between compared systems is small .",3,0.68970287,64.50137457929813,40
2344,"We present InferWiki , a Knowledge Graph Completion ( KGC ) dataset that improves upon existing benchmarks in inferential ability , assumptions , and patterns .",1,0.4293104,128.1042311332692,26
2344,"First , each testing sample is predictable with supportive data in the training set .",2,0.678144,181.83882330285434,15
2344,"To ensure it , we propose to utilize rule-guided train / test generation , instead of conventional random split .",2,0.47284603,183.40566341172305,20
2344,"Second , InferWiki initiates the evaluation following the open-world assumption and improves the inferential difficulty of the closed-world assumption , by providing manually annotated negative and unknown triples .",2,0.5772157,123.41441786707821,31
2344,"Third , we include various inference patterns ( e.g. , reasoning path length and types ) for comprehensive evaluation .",2,0.6743475,128.78609713729526,20
2344,"In experiments , we curate two settings of InferWiki varying in sizes and structures , and apply the construction process on CoDEx as comparative datasets .",2,0.83269745,228.50795112502155,26
2344,The results and empirical analyses demonstrate the necessity and high-quality of InferWiki .,3,0.9862235,44.83818206968001,13
2344,"Nevertheless , the performance gap among various inferential assumptions and patterns presents the difficulty and inspires future research direction .",3,0.8133548,180.3503713446539,20
2344,Our datasets can be found in https://github.com/TaoMiner/inferwiki .,3,0.7935762,14.971353534939608,8
2345,"While online conversations can cover a vast amount of information in many different formats , abstractive text summarization has primarily focused on modeling solely news articles .",0,0.9521116,42.66883319231989,27
2345,"This research gap is due , in part , to the lack of standardized datasets for summarizing online discussions .",0,0.9125152,43.277008704172836,20
2345,"To address this gap , we design annotation protocols motivated by an issues–viewpoints –assertions framework to crowdsource four new datasets on diverse online conversation forms of news comments , discussion forums , community question answering forums , and email threads .",2,0.6229449,188.48608080766712,41
2345,We benchmark state-of-the-art models on our datasets and analyze characteristics associated with the data .,2,0.8188757,17.34544845944979,20
2345,"To create a comprehensive benchmark , we also evaluate these models on widely-used conversation summarization datasets to establish strong baselines in this domain .",2,0.7396224,36.6534713676652,26
2345,"Furthermore , we incorporate argument mining through graph construction to directly model the issues , viewpoints , and assertions present in a conversation and filter noisy input , showing comparable or improved results according to automatic and human evaluations .",2,0.47935954,151.39455177258412,40
2346,A commonly observed problem with the state-of-the art abstractive summarization models is that the generated summaries can be factually inconsistent with the input documents .,0,0.91015816,13.348486498325244,29
2346,The fact that automatic summarization may produce plausible-sounding yet inaccurate summaries is a major concern that limits its wide application .,0,0.90518194,34.265390398570936,23
2346,In this paper we present an approach to address factual consistency in summarization .,1,0.90514797,24.235506428161884,14
2346,We first propose an efficient automatic evaluation metric to measure factual consistency ;,2,0.5061138,155.04863501775776,13
2346,"next , we propose a novel learning algorithm that maximizes the proposed metric during model training .",2,0.50933355,31.53404280004382,17
2346,"Through extensive experiments , we confirm that our method is effective in improving factual consistency and even overall quality of the summaries , as judged by both automatic metrics and human evaluation .",3,0.9225951,27.29450630503738,33
2347,"Recent years have brought about an interest in the challenging task of summarizing conversation threads ( meetings , online discussions , etc. ) .",0,0.9685109,50.34029438535774,24
2347,Such summaries help analysis of the long text to quickly catch up with the decisions made and thus improve our work or communication efficiency .,0,0.5101374,72.59661593349476,25
2347,"To spur research in thread summarization , we have developed an abstractive Email Thread Summarization ( EmailSum ) dataset , which contains human-annotated short ( < 30 words ) and long ( < 100 words ) summaries of 2,549 email threads ( each containing 3 to 10 emails ) over a wide variety of topics .",0,0.46956155,33.601884682428185,56
2347,"We perform a comprehensive empirical study to explore different summarization techniques ( including extractive and abstractive methods , single-document and hierarchical models , as well as transfer and semisupervised learning ) and conduct human evaluations on both short and long summary generation tasks .",2,0.74890655,52.883619131008864,44
2347,"Our results reveal the key challenges of current abstractive summarization models in this task , such as understanding the sender ’s intent and identifying the roles of sender and receiver .",3,0.9882073,32.38653247281372,31
2347,"Furthermore , we find that widely used automatic evaluation metrics ( ROUGE , BERTScore ) are weakly correlated with human judgments on this email thread summarization task .",3,0.9664193,31.00510560605721,28
2347,"Hence , we emphasize the importance of human evaluation and the development of better metrics by the community .",3,0.6025535,31.711732891127177,19
2348,"Parallel cross-lingual summarization data is scarce , requiring models to better use the limited available cross-lingual resources .",0,0.92003936,38.790207821799314,18
2348,Existing methods to do so often adopt sequence-to-sequence networks with multi-task frameworks .,0,0.868388,34.67456569541266,15
2348,"Such approaches apply multiple decoders , each of which is utilized for a specific task .",0,0.7749265,35.33676776772951,16
2348,"However , these independent decoders share no parameters , hence fail to capture the relationships between the discrete phrases of summaries in different languages , breaking the connections in order to transfer the knowledge of the high-resource languages to low-resource languages .",0,0.51796293,45.42908654183715,43
2348,"To bridge these connections , we propose a novel Multi-Task framework for Cross-Lingual Abstractive Summarization ( MCLAS ) in a low-resource setting .",1,0.4910194,25.164188826578137,23
2348,"Employing one unified decoder to generate the sequential concatenation of monolingual and cross-lingual summaries , MCLAS makes the monolingual summarization task a prerequisite of the CLS task .",3,0.43115643,35.955165805648534,28
2348,"In this way , the shared decoder learns interactions involving alignments and summary patterns across languages , which encourages attaining knowledge transfer .",3,0.50526816,178.49196917996235,23
2348,Experiments on two CLS datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios .,3,0.9403465,11.674959525078059,21
2348,"Moreover , in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using MCLAS , which benefits the CLS task under limited parallel resources .",3,0.9327125,128.0863345202401,31
2349,"Despite the prominence of neural abstractive summarization models , we know little about how they actually form summaries and how to understand where their decisions come from .",0,0.9428523,21.089709279835276,28
2349,We propose a two-step method to interpret summarization model decisions .,2,0.4867238,47.92656359367476,11
2349,"After isolating decisions that do depend on the input , we explore interpreting these decisions using several different attribution methods .",2,0.7385341,81.92382500382074,21
2349,"We compare these techniques based on their ability to select content and reconstruct the model ’s predicted token from perturbations of the input , thus revealing whether highlighted attributions are truly important for the generation of the next token .",2,0.6796174,59.06931056175173,40
2349,"While this machinery can be broadly useful even beyond summarization , we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened , as well as study complex generation phenomena like sentence fusion on a per-instance basis .",3,0.58197623,67.6518715263823,50
2350,Humans create things for a reason .,0,0.90500677,106.76228268923941,7
2350,"Ancient people created spears for hunting , knives for cutting meat , pots for preparing food , etc .",0,0.9163424,37.88108330136706,19
2350,The prototypical function of a physical artifact is a kind of commonsense knowledge that we rely on to understand natural language .,0,0.9408694,25.714071324263816,22
2350,"“ She borrowed the book ” then you would assume that she intends to read the book , or if someone asks “ Can I use your knife ? ” then you would assume that they need to cut something .",0,0.40537477,67.35564307870507,41
2350,"In this paper , we introduce a new NLP task of learning the prototypical uses for human-made physical objects .",1,0.90223086,32.992552766599175,20
2350,"We use frames from FrameNet to represent a set of common functions for objects , and describe a manually annotated data set of physical objects labeled with their prototypical function .",2,0.87718207,50.21300569536276,31
2350,"We also present experimental results for this task , including BERT-based models that use predictions from masked patterns as well as artifact sense definitions from WordNet and frame definitions from FrameNet .",3,0.6698811,79.29515673414642,34
2351,Linguistic probing of pretrained Transformer-based language models ( LMs ) revealed that they encode a range of syntactic and semantic properties of a language .,0,0.8257153,17.985951458191714,27
2351,"However , they are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks , rather than leverage deeper linguistic information .",0,0.8936188,64.81780373614609,27
2351,"In this paper , we target a specific facet of linguistic knowledge , the interplay between verb meaning and argument structure .",1,0.8867405,44.06054280720207,22
2351,"We investigate whether injecting explicit information on verbs ’ semantic-syntactic behaviour improves the performance of pretrained LMs in event extraction tasks , where accurate verb processing is paramount .",1,0.791453,109.71807525002505,29
2351,"Concretely , we impart the verb knowledge from curated lexical resources into dedicated adapter modules ( verb adapters ) , allowing it to complement , in downstream tasks , the language knowledge obtained during LM-pretraining .",2,0.50602084,235.41635883490534,38
2351,We first demonstrate that injecting verb knowledge leads to performance gains in English event extraction .,3,0.6731144,89.59066943080406,16
2351,We then explore the utility of verb adapters for event extraction in other languages : we investigate 1 ) zero-shot language transfer with multilingual Transformers and 2 ) transfer via ( noisy automatic ) translation of English verb-based lexical knowledge .,2,0.517469,111.41761581292006,41
2351,"Our results show that the benefits of verb knowledge injection indeed extend to other languages , even when relying on noisily translated lexical knowledge .",3,0.9887783,69.03834369462643,25
2352,Static word embeddings that represent words by a single vector cannot capture the variability of word meaning in different linguistic and extralinguistic contexts .,0,0.8091387,21.875774535269986,24
2352,"Building on prior work on contextualized and dynamic word embeddings , we introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context .",2,0.67412657,17.246821716609784,30
2352,"Based on a pretrained language model ( PLM ) , dynamic contextualized word embeddings model time and social space jointly , which makes them attractive for a range of NLP tasks involving semantic variability .",0,0.5226387,54.32157327100397,35
2352,We highlight potential application scenarios by means of qualitative and quantitative analyses on four English datasets .,3,0.3995376,38.98946394267835,17
2353,"While there is a large amount of research in the field of Lexical Semantic Change Detection , only few approaches go beyond a standard benchmark evaluation of existing models .",0,0.9131195,33.972020264588885,30
2353,"In this paper , we propose a shift of focus from change detection to change discovery , i.e. , discovering novel word senses over time from the full corpus vocabulary .",1,0.8989547,67.03056416234972,31
2353,"By heavily fine-tuning a type-based and a token-based approach on recently published German data , we demonstrate that both models can successfully be applied to discover new words undergoing meaning change .",3,0.75791633,41.14437689544357,34
2353,"Furthermore , we provide an almost fully automated framework for both evaluation and discovery .",3,0.7456713,40.42398565559715,15
2354,"Humans are increasingly interacting with machines through language , sometimes in contexts where the user may not know they are talking to a machine ( like over the phone or a text chatbot ) .",0,0.9514543,27.869449791030682,35
2354,We aim to understand how system designers and researchers might allow their systems to confirm its non-human identity .,1,0.9486541,47.20271527374286,19
2354,"We collect over 2,500 phrasings related to the intent of “ Are you a robot ? ” .",2,0.78155166,41.01394983079833,18
2354,"This is paired with over 2,500 adversarially selected utterances where only confirming the system is non-human would be insufficient or disfluent .",2,0.47693306,59.27175089218211,22
2354,We compare classifiers to recognize the intent and discuss the precision / recall and model complexity tradeoffs .,2,0.48467973,114.63827216564196,18
2354,Such classifiers could be integrated into dialog systems to avoid undesired deception .,3,0.7432677,35.61542890585355,13
2354,"We then explore how both a generative research model ( Blender ) as well as two deployed systems ( Amazon Alexa , Google Assistant ) handle this intent , finding that systems often fail to confirm their non-human identity .",3,0.5586064,86.81881132165736,40
2354,"Finally , we try to understand what a good response to the intent would be , and conduct a user study to compare the important aspects when responding to this intent .",2,0.45745152,34.92274220421228,32
2355,In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers .,1,0.92147815,32.950032792384654,23
2355,"Developers often include such knowledge , structure as taxonomies , in the documentation of chatbots .",0,0.831591,168.96065067890532,16
2355,"By using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space , we were able to improve accuracy in intent recognition .",3,0.7144201,39.0080228001909,25
2355,"In datasets with intents and example utterances from 200 professional chatbots , we saw decreases in the equal error rate ( EER ) in more than 40 % of the chatbots in comparison to the baseline of the same algorithm without the meta-knowledge .",3,0.9372161,46.44053486878563,44
2355,"The meta-knowledge proved also to be effective in detecting out-of-scope utterances , improving the false acceptance rate ( FAR ) in two thirds of the chatbots , with decreases of 0.05 or more in FAR in almost 40 % of the chatbots .",3,0.9696102,40.0638531543004,45
2355,"When considering only the well-developed workspaces with a high level use of taxonomies , FAR decreased more than 0.05 in 77 % of them , and more than 0.1 in 39 % of the chatbots .",3,0.96009594,57.274930024439655,38
2356,"To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems , recent Transformer-based models aim to integrate fixed background context .",0,0.9101193,42.421675439510295,27
2356,"This often comes in the form of knowledge graphs , and the integration is done by creating pseudo utterances through paraphrasing knowledge triples , added into the accumulated dialogue context .",0,0.64062965,95.4827550532883,31
2356,"However , the context length is fixed in these architectures , which restricts how much background or dialogue context can be kept .",0,0.54246557,102.71493510785403,23
2356,"In this work , we propose a more concise encoding for background context structured in the form of knowledge graphs , by expressing the graph connections through restrictions on the attention weights .",1,0.5447965,76.2199629437906,33
2356,The results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency .,3,0.9839054,76.54732841682628,27
2356,"Further , models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting .",3,0.94193083,54.50409066009795,21
2357,Emotion Recognition in Conversations ( ERC ) has gained increasing attention for developing empathetic machines .,0,0.96871805,42.437871229391924,16
2357,"Recently , many approaches have been devoted to perceiving conversational context by deep learning models .",0,0.93268603,54.711465147297886,16
2357,"However , these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues .",0,0.9163531,46.153127703829796,22
2357,"In this work , we propose novel Contextual Reasoning Networks ( DialogueCRN ) to fully understand the conversational context from a cognitive perspective .",1,0.87194264,36.56404219527955,24
2357,"Inspired by the Cognitive Theory of Emotion , we design multi-turn reasoning modules to extract and integrate emotional clues .",2,0.59928215,36.7577693128978,20
2357,"The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process , which imitates human unique cognitive thinking .",2,0.44845793,218.97582052404664,22
2357,Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model .,3,0.76379085,8.36358016842635,17
2358,"When collecting annotations and labeled data from humans , a standard practice is to use inter-rater reliability ( IRR ) as a measure of data goodness ( Hallgren , 2012 ) .",0,0.891484,60.42545091460335,32
2358,"Metrics such as Krippendorff ’s alpha or Cohen ’s kappa are typically required to be above a threshold of 0.6 ( Landis and Koch , 1977 ) .",0,0.6966095,57.822959586086334,28
2358,"These absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances , especially on subjective topics .",3,0.80640584,123.42754179823663,22
2358,We present a new alternative to interpreting IRR that is more empirical and contextualized .,1,0.4227832,62.485821222319444,15
2358,"It is based upon benchmarking IRR against baseline measures in a replication , one of which is a novel cross-replication reliability ( xRR ) measure based on Cohen ’s ( 1960 ) kappa .",0,0.4531975,88.74951017991296,34
2358,We call this approach the xRR framework .,2,0.45372877,87.87488148605647,8
2358,We opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework .,2,0.8370453,44.98975826547225,21
2358,We argue this framework can be used to measure the quality of crowdsourced datasets .,3,0.8576517,22.158856888116258,15
2359,"Everyday conversations require understanding everyday events , which in turn , requires understanding temporal commonsense concepts interwoven with those events .",0,0.93527,83.79448031106305,21
2359,"Despite recent progress with massive pre-trained language models ( LMs ) such as T5 and GPT-3 , their capability of temporal reasoning in dialogs remains largely under-explored .",0,0.93433654,22.98980856913853,30
2359,"In this paper , we present the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced English challenge set , TimeDial .",1,0.8503927,41.52225994552518,34
2359,We formulate TimeDial as a multiple choice cloze task with over 1.1 K carefully curated dialogs .,2,0.7914367,118.59738347891722,17
2359,"Empirical results demonstrate that even the best performing models struggle on this task compared to humans , with 23 absolute points of gap in accuracy .",3,0.8780041,44.07330805121445,26
2359,"Furthermore , our analysis reveals that the models fail to reason about dialog context correctly ;",3,0.9818378,80.01179129858438,16
2359,"instead , they rely on shallow cues based on existing temporal patterns in context , motivating future research for modeling temporal concepts in text and robust contextual reasoning about them .",3,0.80103046,137.35303277768003,31
2359,The dataset is publicly available at https://github.com/google-research-datasets/timedial .,3,0.4898153,11.327430462085298,8
2360,"Most words are ambiguous —-i.e. , they convey distinct meanings in different contexts —-and even the meanings of unambiguous words are context-dependent .",0,0.920647,32.67996002657272,27
2360,Both phenomena present a challenge for NLP .,0,0.90383077,35.910248899263784,8
2360,"Recently , the advent of contextualized word embeddings has led to success on tasks involving lexical ambiguity , such as Word Sense Disambiguation .",0,0.9444965,17.939313694484596,24
2360,"However , there are few tasks that directly evaluate how well these contextualized embeddings accommodate the more continuous , dynamic nature of word meaning —-particularly in a way that matches human intuitions .",0,0.9232395,81.02175455200826,35
2360,"We introduce RAW-C , a dataset of graded , human relatedness judgments for 112 ambiguous words in context ( with 672 sentence pairs total ) , as well as human estimates of sense dominance .",2,0.83649004,204.88253389949898,37
2360,The average inter-annotator agreement ( assessed using a leave-one-annotator-out method ) was 0.79 .,3,0.92464066,37.98109504876719,18
2360,"We then show that a measure of cosine distance , computed using contextualized embeddings from BERT and ELMo , correlates with human judgments , but that cosine distance also systematically underestimates how similar humans find uses of the same sense of a word to be , and systematically overestimates how similar humans find uses of different-sense homonyms .",3,0.79390585,31.63335020511382,60
2360,"Finally , we propose a synthesis between psycholinguistic theories of the mental lexicon and computational models of lexical semantics .",3,0.61321694,18.8023282142905,20
2361,Pre-trained language models ( LMs ) are currently integral to many natural language processing systems .,0,0.9227613,12.719865303273284,16
2361,"Although multilingual LMs were also introduced to serve many languages , these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training .",0,0.8497526,34.15878929372682,34
2361,"We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models , ARBERT and MARBERT .",2,0.61087227,61.58571797950271,26
2361,"To evaluate our models , we also introduce ARLUE , a new benchmark for multi-dialectal Arabic language understanding evaluation .",2,0.62956214,72.11019110876273,20
2361,"ARLUE is built using 42 datasets targeting six different task clusters , allowing us to offer a series of standardized experiments under rich conditions .",2,0.6029521,187.01918131202092,25
2361,"When fine-tuned on ARLUE , our models collectively achieve new state-of-the-art results across the majority of tasks ( 37 out of 48 classification tasks , on the 42 datasets ) .",3,0.90800756,35.976260044706954,37
2361,"Our best model acquires the highest ARLUE score ( 77.40 ) across all six task clusters , outperforming all other models including XLM-R Large ( 3.4x larger size ) .",3,0.94951844,89.20913870389649,31
2361,Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository .,3,0.5074574,23.11377549379477,17
2362,"If two sentences have the same meaning , it should follow that they are equivalent in their inferential properties , i.e. , each sentence should textually entail the other .",0,0.59514236,29.097352539113075,30
2362,"However , many paraphrase datasets currently in widespread use rely on a sense of paraphrase based on word overlap and syntax .",0,0.9305245,73.29729579886883,22
2362,"We apply the adversarial paradigm to this question , and introduce a new adversarial method of dataset creation for paraphrase identification : the Adversarial Paraphrasing Task ( APT ) , which asks participants to generate semantically equivalent ( in the sense of mutually implicative ) but lexically and syntactically disparate paraphrases .",2,0.5142444,41.50097119698984,52
2362,These sentence pairs can then be used both to test paraphrase identification models ( which get barely random accuracy ) and then improve their performance .,3,0.6646993,116.28470374948031,26
2362,"To accelerate dataset generation , we explore automation of APT using T5 , and show that the resulting dataset also improves accuracy .",3,0.4558665,134.59378700214828,23
2362,We discuss implications for paraphrase detection and release our dataset in the hope of making paraphrase detection models better able to detect sentence-level meaning equivalence .,3,0.8804992,30.024480435824206,28
2363,"A false contract is more likely to be rejected than a contract is , yet a false key is less likely than a key to open doors .",3,0.7014452,31.05464326357713,28
2363,"While correctly interpreting and assessing the effects of such adjective-noun pairs ( e.g. , false key ) on the plausibility of given events ( e.g. , opening doors ) underpins many natural language understanding tasks , doing so often requires a significant degree of world knowledge and common-sense reasoning .",0,0.8918619,50.73084843053966,51
2363,We introduce ADEPT – a large-scale semantic plausibility task consisting of over 16 thousand sentences that are paired with slightly modified versions obtained by adding an adjective to a noun .,2,0.6198687,40.83970655536128,31
2363,"Overall , we find that while the task appears easier for human judges ( 85 % accuracy ) , it proves more difficult for transformer-based models like RoBERTa ( 71 % accuracy ) .",3,0.98132205,27.245301817726837,36
2363,"Our experiments also show that neither the adjective itself nor its taxonomic class suffice in determining the correct plausibility judgement , emphasizing the importance of endowing automatic natural language understanding systems with more context sensitivity and common-sense reasoning .",3,0.98487073,55.11223583668046,39
2364,"We present ReadOnce Transformers , an approach to convert a transformer-based model into one that can build an information-capturing , task-independent , and compressed representation of text .",1,0.46543637,64.6664719227436,34
2364,"The resulting representation is reusable across different examples and tasks , thereby requiring a document shared across many examples or tasks to only be read once .",3,0.6776706,83.28551042280655,27
2364,This leads to faster training and evaluation of models .,3,0.50483775,37.200804418857196,10
2364,"Additionally , we extend standard text-to-text transformer models to Representation + Text-to-text models , and evaluate on multiple downstream tasks : multi-hop QA , abstractive QA , and long-document summarization .",2,0.7335966,28.151629464168504,39
2364,"Our one-time computed representation results in a 2x-5x speedup compared to standard text-to-text models , while the compression also allows existing language models to handle longer documents without the need for designing new pre-trained models .",3,0.9087023,24.952975996234102,44
2365,"Models of narrative schema knowledge have proven useful for a range of event-related tasks , but they typically do not capture the temporal relationships between events .",0,0.9206923,32.53303178059676,27
2365,"We propose a single model that addresses both temporal ordering , sorting given events into the order they occurred , and event infilling , predicting new events which fit into an existing temporally-ordered sequence .",2,0.35260308,82.90983536689765,37
2365,"We use a BART-based conditional generation model that can capture both temporality and common event co-occurrence , meaning it can be flexibly applied to different tasks in this space .",2,0.7440083,35.56864668022012,32
2365,"Our model is trained as a denoising autoencoder : we take temporally-ordered event sequences , shuffle them , delete some events , and then attempt to recover the original event sequence .",2,0.7817287,39.35911490804522,34
2365,This task teaches the model to make inferences given incomplete knowledge about the events in an underlying scenario .,2,0.44150525,55.375078095781674,19
2365,"On the temporal ordering task , we show that our model is able to unscramble event sequences from existing datasets without access to explicitly labeled temporal training data , outperforming both a BERT-based pairwise model and a BERT-based pointer network .",3,0.87747055,25.290930717817957,45
2365,"On event infilling , human evaluation shows that our model is able to generate events that fit better temporally into the input events when compared to GPT-2 story completion models .",3,0.95148957,40.7696898515374,33
2366,The wanton spread of hate speech on the internet brings great harm to society and families .,0,0.94511473,32.77721002443979,17
2366,It is urgent to establish and improve automatic detection and active avoidance mechanisms for hate speech .,0,0.7077246,48.29182800482074,17
2366,"While there exist methods for hate speech detection , they stereotype words and hence suffer from inherently biased training .",0,0.8953928,108.04507735374152,20
2366,"In other words , getting more affective features from other affective resources will significantly affect the performance of hate speech detection .",3,0.8323794,45.005378606531195,22
2366,"In this paper , we propose a hate speech detection framework based on sentiment knowledge sharing .",1,0.8954837,33.02547281461158,17
2366,"While extracting the affective features of the target sentence itself , we make better use of the sentiment features from external resources , and finally fuse features from different feature extraction units to detect hate speech .",2,0.56314284,68.59499062626257,37
2366,Experimental results on two public datasets demonstrate the effectiveness of our model .,3,0.8878306,6.375804843658118,13
2367,We propose a transition-based bubble parser to perform coordination structure identification and dependency-based syntactic analysis simultaneously .,1,0.38631865,100.37062177265649,21
2367,Bubble representations were proposed in the formal linguistics literature decades ago ;,0,0.8851789,127.92635346787117,12
2367,they enhance dependency trees by encoding coordination boundaries and internal relationships within coordination structures explicitly .,0,0.3995095,374.271189553871,16
2367,"In this paper , we introduce a transition system and neural models for parsing these bubble-enhanced structures .",1,0.8623467,95.1948956213454,20
2367,"Experimental results on the English Penn Treebank and the English GENIA corpus show that our parsers beat previous state-of-the-art approaches on the task of coordination structure prediction , especially for the subset of sentences with complex coordination structures .",3,0.9587363,15.97729209957246,45
2368,Recent years have seen the paradigm shift of Named Entity Recognition ( NER ) systems from sequence labeling to span prediction .,0,0.9703368,28.244991695733212,22
2368,"Despite its preliminary effectiveness , the span prediction model ’s architectural bias has not been fully understood .",0,0.80782723,102.75069548879473,18
2368,"In this paper , we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it , which motivates us to make complementary advantages of systems based on different paradigms .",1,0.84573996,34.12910070664058,49
2368,"We then reveal that span prediction , simultaneously , can serve as a system combiner to re-recognize named entities from different systems ’ outputs .",3,0.9234468,191.00658269905475,25
2368,"We experimentally implement 154 systems on 11 datasets , covering three languages , comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners .",3,0.7690001,174.83433411796167,33
2368,"https://github.com/neulab/spanner, as well as an online system demo : http://spanner.sh .",3,0.38220564,22.10711675746071,11
2368,"Our model also has been deployed into the ExplainaBoard platform , which allows users to flexibly perform a system combination of top-scoring systems in an interactive way : http://explainaboard.nlpedia.ai/leaderboard/task-ner/ .",3,0.53761965,71.78601980484055,30
2369,There are two major classes of natural language grammars — the dependency grammar that models one-to-one correspondences between words and the constituency grammar that models the assembly of one or several corresponded words .,0,0.92511255,24.79264926884916,38
2369,"While previous unsupervised parsing methods mostly focus on only inducing one class of grammars , we introduce a novel model , StructFormer , that can induce dependency and constituency structure at the same time .",2,0.5568949,68.69751062338034,35
2369,"To achieve this , we propose a new parsing framework that can jointly generate a constituency tree and dependency graph .",2,0.44194973,29.39511136994398,21
2369,"Then we integrate the induced dependency relations into the transformer , in a differentiable manner , through a novel dependency-constrained self-attention mechanism .",2,0.8051941,31.84397171637891,25
2369,"Experimental results show that our model can achieve strong results on unsupervised constituency parsing , unsupervised dependency parsing , and masked language modeling at the same time .",3,0.9771353,14.719490594240687,28
2370,Cross-lingual language tasks typically require a substantial amount of annotated data or parallel translation data .,0,0.92502934,21.301141077740482,16
2370,We explore whether language representations that capture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data .,1,0.6472625,21.112222386454622,26
2370,"We generate dense embeddings for 29 languages using a denoising autoencoder , and evaluate the embeddings using the World Atlas of Language Structures ( WALS ) and two extrinsic tasks in a zero-shot setting : cross-lingual dependency parsing and cross-lingual natural language inference .",2,0.8477877,16.576539377735557,44
2371,Decipherment of historical ciphers is a challenging problem .,0,0.9499507,30.839020705181838,9
2371,"The language of the target plaintext might be unknown , and ciphertext can have a lot of noise .",0,0.7919324,78.29013909345964,19
2371,"State-of-the-art decipherment methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher , assuming the plaintext language is known .",0,0.8436289,49.747193505522276,32
2371,We propose an end-to-end multilingual model for solving simple substitution ciphers .,1,0.50025326,28.415033238425792,13
2371,We test our model on synthetic and real historical ciphers and show that our proposed method can decipher text without explicit language identification while still being robust to noise .,3,0.78500944,39.20067708867894,30
2372,"While it has been shown that Neural Machine Translation ( NMT ) is highly sensitive to noisy parallel training samples , prior work treats all types of mismatches between source and target as noise .",0,0.9310712,22.95016440150274,35
2372,"As a result , it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training .",0,0.86689675,43.57434938428938,26
2372,"To close this gap , we analyze the impact of different types of fine-grained semantic divergences on Transformer models .",1,0.69560856,16.204871992975285,20
2372,We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions .,3,0.9639208,40.268806807890016,21
2372,"Based on these findings , we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences , improving both translation quality and model calibration on EN-FR tasks .",3,0.9004944,51.33785539351092,42
2373,Reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice .,0,0.6434795,127.9524641488232,21
2373,"These models have a long history in NLP , and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture .",0,0.3796245,31.424174225490344,27
2373,This takes as input both the source sentence as well as a list of hypotheses to output a ranked list .,2,0.5982936,31.5730794096195,21
2373,"The reranker is trained to predict the observed distribution of a desired metric , e.g .",2,0.48291928,37.16661987144884,16
2373,"BLEU , over the n-best list .",2,0.52419597,107.50798219725681,7
2373,"Since such a discriminator contains hundreds of millions of parameters , we improve its generalization using pre-training and data augmentation techniques .",2,0.43608606,29.57354345273288,22
2373,"Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches , yielding improvements of up to 4 BLEU over the beam search output .",3,0.91069543,18.26091064409951,34
2374,Active learning promises to alleviate the massive data needs of supervised machine learning : it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition .,0,0.814726,60.097041681819924,35
2374,"However , we uncover a striking contrast to this promise : across 5 models and 4 datasets on the task of visual question answering , a wide variety of active learning approaches fail to outperform random selection .",3,0.92228204,66.23573616727448,38
2374,"To understand this discrepancy , we profile 8 active learning methods on a per-example basis , and identify the problem as collective outliers – groups of examples that active learning methods prefer to acquire but models fail to learn ( e.g. , questions that ask about text in images or require external knowledge ) .",2,0.624506,92.44273702340911,55
2374,"Through systematic ablation experiments and qualitative visualizations , we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning .",3,0.7971216,82.69149763056795,25
2374,"Notably , we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases .",3,0.9817863,63.981509750568414,24
2374,We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work .,3,0.9238318,21.738209568765097,19
2375,"We run a study assessing non-experts ’ ability to distinguish between human-and machine-authored text ( GPT2 and GPT3 ) in three domains ( stories , news articles , and recipes ) .",2,0.73152834,32.44783776583309,36
2375,"We find that , without training , evaluators distinguished between GPT3-and human-authored text at random chance level .",3,0.9804801,121.57854008526513,21
2375,"We explore three approaches for quickly training evaluators to better identify GPT3-authored text ( detailed instructions , annotated examples , and paired examples ) and find that while evaluators ’ accuracy improved up to 55 % , it did not significantly improve across the three domains .",3,0.8769269,57.6427262907701,49
2375,"Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments , we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models .",1,0.5495816,25.421232140032874,52
2376,This paper presents the first large-scale meta-evaluation of machine translation ( MT ) .,1,0.8337205,19.831934298554913,14
2376,We annotated MT evaluations conducted in 769 research papers published from 2010 to 2020 .,2,0.91813755,118.74490527683764,15
2376,Our study shows that practices for automatic MT evaluation have dramatically changed during the past decade and follow concerning trends .,3,0.9866052,82.5015040848214,21
2376,"An increasing number of MT evaluations exclusively rely on differences between BLEU scores to draw conclusions , without performing any kind of statistical significance testing nor human evaluation , while at least 108 metrics claiming to be better than BLEU have been proposed .",0,0.77234185,62.277243638105794,44
2376,"MT evaluations in recent papers tend to copy and compare automatic metric scores from previous work to claim the superiority of a method or an algorithm without confirming neither exactly the same training , validating , and testing data have been used nor the metric scores are comparable .",0,0.84052604,118.08979083207261,49
2376,"Furthermore , tools for reporting standardized metric scores are still far from being widely adopted by the MT community .",0,0.8262452,37.88218516698678,20
2376,"After showing how the accumulation of these pitfalls leads to dubious evaluation , we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility .",3,0.57252294,81.2196095873561,35
2377,Prior work has proved that Translation Memory ( TM ) can boost the performance of Neural Machine Translation ( NMT ) .,0,0.9480891,21.931500966249473,22
2377,"In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval , we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner .",2,0.4363325,32.659809606064236,39
2377,Our framework has unique advantages .,3,0.69706184,186.64260683297522,6
2377,"First , the cross-lingual memory retriever allows abundant monolingual data to be TM .",0,0.4347087,64.39605760613611,14
2377,"Second , the memory retriever and NMT model can be jointly optimized for the ultimate translation goal .",3,0.44365418,57.665186885870945,18
2377,Experiments show that the proposed method obtains substantial improvements .,3,0.94242066,11.319247688675482,10
2377,"Remarkably , it even outperforms strong TM-augmented NMT baselines using bilingual TM .",3,0.9513431,67.34883447608178,15
2377,"Owning to the ability to leverage monolingual data , our model also demonstrates effectiveness in low-resource and domain adaptation scenarios .",3,0.93121356,29.04568745879508,21
2378,"Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks , the dynamics of this process are not well understood , especially in the low data regime .",0,0.88104045,11.02801425250188,44
2378,"In this paper , we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon .",1,0.8412613,37.85725652674455,28
2378,We empirically show that common pre-trained models have a very low intrinsic dimension ;,3,0.91937464,74.94177103416004,14
2378,"in other words , there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space .",3,0.79338926,30.965597349799847,23
2378,"For example , by optimizing only 200 trainable parameters randomly projected back into the full space , we can tune a RoBERTa model to achieve 90 % of the full parameter performance levels on MRPC .",3,0.7928487,127.39954682589568,36
2378,"Furthermore , we empirically show that pre-training implicitly minimizes intrinsic dimension and , perhaps surprisingly , larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates , at least in part explaining their extreme effectiveness .",3,0.9490682,52.21703651022597,41
2378,"Lastly , we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count .",2,0.5579815,78.84948928430451,32
2379,"Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding ( NLU ) models indicate that they appear to understand human-like syntax , at least to some extent .",0,0.9313722,15.36407605413168,40
2379,"We provide novel evidence that complicates this claim : we find that state-of-the-art Natural Language Inference ( NLI ) models assign the same labels to permuted examples as they do to the original , i.e .",3,0.763336,24.59207039669766,41
2379,they are invariant to random word-order permutations .,3,0.4866967,30.051236109968904,8
2379,This behavior notably differs from that of humans ;,0,0.67328787,178.16730870069478,9
2379,we struggle to understand the meaning of ungrammatical sentences .,0,0.92733526,17.062052441067685,10
2379,"To measure the severity of this issue , we propose a suite of metrics and investigate which properties of particular permutations lead models to be word order invariant .",2,0.4198092,46.105846760853176,29
2379,"For example , in MNLI dataset we find almost all ( 98.7 % ) examples contain at least one permutation which elicits the gold label .",3,0.94951326,50.510061112427294,26
2379,Models are even able to assign gold labels to permutations that they originally failed to predict correctly .,3,0.7807846,53.98952440513536,18
2379,"We provide a comprehensive empirical evaluation of this phenomenon , and further show that this issue exists in pre-Transformer RNN / ConvNet based encoders , as well as across multiple languages ( English and Chinese ) .",3,0.81501776,39.81599488130249,37
2379,Our code and data are available at https://github.com/facebookresearch/unlu .,3,0.5896796,10.434596452597606,9
2380,Signed languages are the primary means of communication for many deaf and hard of hearing individuals .,0,0.9145825,14.761783356920013,17
2380,"Since signed languages exhibit all the fundamental linguistic properties of natural language , we believe that tools and theories of Natural Language Processing ( NLP ) are crucial towards its modeling .",0,0.89921427,48.474075488749556,32
2380,"However , existing research in Sign Language Processing ( SLP ) seldom attempt to explore and leverage the linguistic organization of signed languages .",0,0.9407399,82.26352756531936,24
2380,This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact .,1,0.48576933,35.845204934697925,23
2380,We first discuss the linguistic properties of signed languages to consider during their modeling .,1,0.5143558,123.94613882581535,15
2380,"Then , we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages .",1,0.6827048,58.16198397685786,22
2380,"Finally , we urge ( 1 ) the adoption of an efficient tokenization method ;",3,0.90180326,152.7808549147836,15
2380,( 2 ) the development of linguistically-informed models ;,0,0.4524068,207.61447389195428,11
2380,( 3 ) the collection of real-world signed language data ;,2,0.54049337,325.04284504455137,11
2380,( 4 ) the inclusion of local signed language communities as an active and leading voice in the direction of research .,3,0.7363309,147.0036674034085,22
2381,The choice of token vocabulary affects the performance of machine translation .,0,0.8563176,40.49765680732467,12
2381,This paper aims to figure out what is a good vocabulary and whether we can find the optimal vocabulary without trial training .,1,0.9397966,44.79462975298608,23
2381,"To answer these questions , we first provide an alternative understanding of vocabulary from the perspective of information theory .",1,0.6690532,34.00291793217496,20
2381,It motivates us to formulate the quest of vocabularization – finding the best token dictionary with a proper size – as an optimal transport ( OT ) problem .,0,0.46031857,104.56819070730477,29
2381,"We propose VOLT , a simple and efficient solution without trial training .",3,0.46602404,186.90542518390782,13
2381,"Empirical results show that VOLT beats widely-used vocabularies in diverse scenarios , including WMT-14 English-German translation , TED bilingual translation , and TED multilingual translation .",3,0.97168726,41.41710035725132,32
2381,"For example , VOLT achieves 70 % vocabulary size reduction and 0.5 BLEU gain on English-German translation .",3,0.9102561,49.67006421500868,20
2381,"Also , compared to BPE-search , VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation .",3,0.92082006,111.25739064719166,27
2381,Codes are available at https://github.com/Jingjing-NLP/VOLT .,3,0.5296822,15.808724055367131,6
2382,"A snowclone is a customizable phrasal template that can be realized in multiple , instantly recognized variants .",0,0.8746935,215.28608951608197,18
2382,"For example , “ * is the new * "" ( Orange is the new black , 40 is the new 30 ) .",3,0.54132265,122.79693953488221,24
2382,Snowclones are extensively used in social media .,0,0.91376853,32.46342988812646,8
2382,"In this paper , we study snowclones originating from pop-culture quotes ;",1,0.912724,175.65006481474703,12
2382,our goal is to automatically detect cultural references in text .,1,0.72847384,36.58515346878116,11
2382,"We introduce a new , publicly available data set of pop-culture quotes and their corresponding snowclone usages and train models on them .",2,0.6773373,59.20271641939743,23
2382,"We publish code for Catchphrase , an internet browser plugin to automatically detect and mark references in real-time , and examine its performance via a user study .",2,0.5987977,56.79751683720952,28
2382,"Aside from assisting people to better comprehend cultural references , we hope that detecting snowclones can complement work on paraphrasing and help tackling long-standing questions in social science about the dynamics of information propagation .",3,0.89326626,69.4898968956757,35
2383,Large-scale pretrained language models have led to dramatic improvements in text generation .,0,0.93042856,7.99718148105212,13
2383,Impressive performance can be achieved by finetuning only on a small number of instances ( few-shot setting ) .,3,0.6992373,22.254268152429237,21
2383,"Nonetheless , almost all previous work simply applies random sampling to select the few-shot training instances .",0,0.85707074,51.0429408239925,17
2383,Little to no attention has been paid to the selection strategies and how they would affect model performance .,0,0.88048726,26.629290254299047,19
2383,"In this work , we present a study on training instance selection in few-shot neural text generation .",1,0.91092116,31.416106262162778,18
2383,The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost .,2,0.5983978,41.13838367623242,31
2383,"Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution , we propose a simple selection strategy with K-means clustering .",2,0.67836607,25.14719766993739,30
2383,"We show that even with the naive clustering-based approach , the generation models consistently outperform random sampling on three text generation tasks : data-to-text generation , document summarization and question generation .",3,0.94655406,30.743584938904704,37
2383,The code and training data are made available .,3,0.43746677,24.295593124596337,9
2383,We hope that this work will call for more attention on this largely unexplored area .,3,0.86257464,19.693544461668367,16
2384,The introduction of pretrained language models has reduced many complex task-specific NLP models to simple lightweight layers .,0,0.8781742,30.0456266163106,18
2384,"An exception to this trend is coreference resolution , where a sophisticated task-specific model is appended to a pretrained transformer encoder .",0,0.7411624,31.09006198902503,23
2384,"While highly effective , the model has a very large memory footprint – primarily due to dynamically-constructed span and span-pair representations – which hinders the processing of complete documents and the ability to train on multiple instances in a single batch .",3,0.80265594,33.59071876689772,44
2384,"We introduce a lightweight end-to-end coreference model that removes the dependency on span representations , handcrafted features , and heuristics .",2,0.60201824,46.1427413202974,22
2384,"Our model performs competitively with the current standard model , while being simpler and more efficient .",3,0.8898971,30.897860767552555,17
2385,"In comparison with English , due to the lack of explicit word boundary and tenses information , Chinese Named Entity Recognition ( NER ) is much more challenging .",0,0.9266943,40.6989392795692,29
2385,"In this paper , we propose a boundary enhanced approach for better Chinese NER .",1,0.89732015,87.5369646437273,15
2385,"In particular , our approach enhances the boundary information from two perspectives .",3,0.64003,124.80989795230313,13
2385,"On one hand , we enhance the representation of the internal dependency of phrases by an additional Graph Attention Network ( GAT ) layer .",2,0.61326164,70.1577480466821,25
2385,"On the other hand , taking the entity head-tail prediction ( i.e. , boundaries ) as an auxiliary task , we propose an unified framework to learn the boundary information and recognize the NE jointly .",2,0.5888247,78.94388003814768,36
2385,Experiments on both the OntoNotes and the Weibo corpora show the effectiveness of our approach .,3,0.8485297,15.15126967428307,16
2386,The high-quality translation results produced by machine translation ( MT ) systems still pose a huge challenge for automatic evaluation .,0,0.95591027,37.81475950816141,21
2386,"Current MT evaluation pays the same attention to each sentence component , while the questions of real-world examinations ( e.g. , university examinations ) have different difficulties and weightings .",0,0.86654544,118.50309284722641,30
2386,"In this paper , we propose a novel difficulty-aware MT evaluation metric , expanding the evaluation dimension by taking translation difficulty into consideration .",1,0.90516526,81.37765937506278,26
2386,"A translation that fails to be predicted by most MT systems will be treated as a difficult one and assigned a large weight in the final score function , and conversely .",3,0.7663801,95.19189976589607,32
2386,Experimental results on the WMT19 English-German Metrics shared tasks show that our proposed method outperforms commonly used MT metrics in terms of human correlation .,3,0.9402694,18.764987552322985,27
2386,"In particular , our proposed method performs well even when all the MT systems are very competitive , which is when most existing metrics fail to distinguish between them .",3,0.91439384,58.01554007476053,30
2386,The source code is freely available at https://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation .,3,0.51302946,10.986251113878787,9
2387,Humor recognition has been widely studied as a text classification problem using data-driven approaches .,0,0.95605236,23.248868096770256,15
2387,"However , most existing work does not examine the actual joke mechanism to understand humor .",0,0.9117883,97.87224313932323,16
2387,"We break down any joke into two distinct components : the set-up and the punchline , and further explore the special relationship between them .",2,0.51455015,37.34205931401957,25
2387,"Inspired by the incongruity theory of humor , we model the set-up as the part developing semantic uncertainty , and the punchline disrupting audience expectations .",2,0.60034156,111.49978207495715,27
2387,"With increasingly powerful language models , we were able to feed the set-up along with the punchline into the GPT-2 language model , and calculate the uncertainty and surprisal values of the jokes .",3,0.59828186,43.64799637413433,38
2387,"Task 7 dataset , we found that these two features have better capabilities of telling jokes from non-jokes , compared with existing baselines .",3,0.96032053,81.15964124741035,24
2388,Disentanglement of latent representations into content and style spaces has been a commonly employed method for unsupervised text style transfer .,0,0.9426601,29.239226160540998,21
2388,These techniques aim to learn the disentangled representations and tweak them to modify the style of a sentence .,0,0.67116153,27.09450527055109,19
2388,"In this paper , we propose a counterfactual-based method to modify the latent representation , by posing a ‘ what-if’ scenario .",1,0.8675116,25.817126111864763,26
2388,This simple and disciplined approach also enables a fine-grained control on the transfer strength .,3,0.7241278,33.93780066715593,15
2388,"We conduct experiments with the proposed methodology on multiple attribute transfer tasks like Sentiment , Formality and Excitement to support our hypothesis .",2,0.5353502,69.97973231540506,23
2389,"Shapley Values , a solution to the credit assignment problem in cooperative game theory , are a popular type of explanation in machine learning , having been used to explain the importance of features , embeddings , and even neurons .",0,0.90814817,85.66233468604203,41
2389,"In NLP , however , leave-one-out and attention-based explanations still predominate .",0,0.8367797,62.627211909447134,16
2389,We formally prove that — save for the degenerate case — attention weights and leave-one-out values cannot be Shapley Values .,3,0.4939256,127.857868762256,24
2389,Attention flow is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph .,2,0.39951897,53.27472200400709,21
2389,"Perhaps surprisingly , we prove that attention flows are indeed Shapley Values , at least at the layerwise level .",3,0.95340985,177.7863455316567,20
2389,"Given the many desirable theoretical qualities of Shapley Values — which has driven their adoption among the ML community — we argue that NLP practitioners should , when possible , adopt attention flow explanations alongside more traditional ones .",3,0.7346086,115.84980184185947,39
2390,Video paragraph captioning aims to generate a set of coherent sentences to describe a video that contains several events .,0,0.918013,32.551885486066624,20
2390,Most previous methods simplify this task by using ground-truth event segments .,0,0.776188,73.18358404860531,14
2390,"In this work , we propose a novel framework by taking this task as a text summarization task .",1,0.79583704,19.483580149973257,19
2390,We first generate lots of sentence-level captions focusing on different video clips and then summarize these captions to obtain the final paragraph caption .,2,0.8472544,39.510364287734554,26
2390,Our method does not depend on ground-truth event segments .,3,0.5748079,46.92746023455065,12
2390,Experiments on two popular datasets ActivityNet Captions and YouCookII demonstrate the advantages of our new framework .,3,0.7676725,64.02680077498941,17
2390,"On the ActivityNet dataset , our method even outperforms some previous methods using ground-truth event segment labels .",3,0.9098456,75.14154545706722,20
2391,"Deep learning algorithms have shown promising results in visual question answering ( VQA ) tasks , but a more careful look reveals that they often do not understand the rich signal they are being fed with .",0,0.94180954,29.12658733159903,37
2391,"To understand and better measure the generalization capabilities of VQA systems , we look at their robustness to counterfactually augmented data .",1,0.5875775,35.88854306548593,22
2391,Our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes .,3,0.5466977,64.90378366515621,23
2391,"Using these augmentations , we propose a new robustness measure , Robustness to Augmented Data ( RAD ) , which measures the consistency of model predictions between original and augmented examples .",2,0.5880514,57.09485308592782,32
2391,"Through extensive experimentation , we show that RAD , unlike classical accuracy measures , can quantify when state-of-the-art systems are not robust to counterfactuals .",3,0.83831114,45.7585674000509,31
2391,We find substantial failure cases which reveal that current VQA systems are still brittle .,3,0.96824217,78.81836398179699,15
2391,"Finally , we connect between robustness and generalization , demonstrating the predictive power of RAD for performance on unseen augmentations .",3,0.6845592,94.44057226773606,21
2392,"Existing approaches for the Table-to-Text task suffer from issues such as missing information , hallucination and repetition .",0,0.9118864,35.443039392448895,21
2392,"Many approaches to this problem use Reinforcement Learning ( RL ) , which maximizes a single manually defined reward , such as BLEU .",0,0.8293003,53.27214362464804,24
2392,"In this work , we instead pose the Table-to-Text task as Inverse Reinforcement Learning ( IRL ) problem .",2,0.5578024,48.26586015184326,22
2392,We explore using multiple interpretable unsupervised reward components that are combined linearly to form a composite reward function .,2,0.68992025,49.513413575881636,19
2392,The composite reward function and the description generator are learned jointly .,2,0.655671,108.34880114825027,12
2392,We find that IRL outperforms strong RL baselines marginally .,3,0.9828273,78.52227476278962,10
2392,We further study the generalization of learned IRL rewards in scenarios involving domain adaptation .,3,0.5124463,90.97943287667039,15
2392,Our experiments reveal significant challenges in using IRL for this task .,3,0.979795,77.31013428925812,12
2393,"Most fact checking models for automatic fake news detection are based on reasoning : given a claim with associated evidence , the models aim to estimate the claim veracity based on the supporting or refuting content within the evidence .",0,0.9013351,55.95306669632273,40
2393,"When these models perform well , it is generally assumed to be due to the models having learned to reason over the evidence with regards to the claim .",0,0.81639224,29.264235482787733,29
2393,"In this paper , we investigate this assumption of reasoning , by exploring the relationship and importance of both claim and evidence .",1,0.92386276,68.86211753890476,23
2393,"Surprisingly , we find on political fact checking datasets that most often the highest effectiveness is obtained by utilizing only the evidence , as the impact of including the claim is either negligible or harmful to the effectiveness .",3,0.9685025,85.86648571255684,39
2393,This highlights an important problem in what constitutes evidence in existing approaches for automatic fake news detection .,3,0.48542562,58.80712940010543,18
2394,"Despite end-to-end neural systems making significant progress in the last decade for task-oriented as well as chit-chat based dialogue systems , most dialogue systems rely on hybrid approaches which use a combination of rule-based , retrieval and generative approaches for generating a set of ranked responses .",0,0.8900482,28.96481202012178,50
2394,Such dialogue systems need to rely on a fallback mechanism to respond to out-of-domain or novel user queries which are not answerable within the scope of the dialogue system .,0,0.8071784,19.29863044518888,33
2394,"While , dialogue systems today rely on static and unnatural responses like “ I do n’t know the answer to that question ” or “ I ’m not sure about that ” , we design a neural approach which generates responses which are contextually aware with the user query as well as say no to the user .",0,0.51774514,31.62974534485093,58
2394,Such customized responses provide paraphrasing ability and contextualization as well as improve the interaction with the user and reduce dialogue monotonicity .,3,0.6637906,53.33873881556969,22
2394,"Our simple approach makes use of rules over dependency parses and a text-to-text transformer fine-tuned on synthetic data of question-response pairs generating highly relevant , grammatical as well as diverse questions .",2,0.63744557,49.74390821251944,38
2394,We perform automatic and manual evaluations to demonstrate the efficacy of the system .,2,0.644613,15.731421112354257,14
2395,Spoken Language Understanding ( SLU ) systems parse speech into semantic structures like dialog acts and slots .,0,0.8890875,130.27107374822126,18
2395,This involves the use of an Automatic Speech Recognizer ( ASR ) to transcribe speech into multiple text alternatives ( hypotheses ) .,0,0.6516876,61.67590931199701,23
2395,"Transcription errors , ordinary in ASRs , impact downstream SLU performance negatively .",0,0.5718475,568.3262928600313,13
2395,"Common approaches to mitigate such errors involve using richer information from the ASR , either in form of N-best hypotheses or word-lattices .",0,0.8221061,80.42087553594047,23
2395,"We hypothesize that transformer models will learn better with a simpler utterance representation using the concatenation of the N-best ASR alternatives , where each alternative is separated by a special delimiter [ SEP ] .",3,0.41901106,56.840947749671656,35
2395,"In our work , we test our hypothesis by using the concatenated N-best ASR alternatives as the input to the transformer encoder models , namely BERT and XLM-RoBERTa , and achieve equivalent performance to the prior state-of-the-art model on DSTC2 dataset .",2,0.7228774,30.0605804264672,50
2395,We also show that our approach significantly outperforms the prior state-of-the-art when subjected to the low data regime .,3,0.956393,10.208519708369417,25
2395,"Additionally , this methodology is accessible to users of third-party ASR APIs which do not provide word-lattice information .",3,0.73384225,47.71104151581375,21
2396,"We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models , such as greedy search , quantization , average attention networks ( AANs ) and shallow decoder models and show their effect on gendered noun translation .",1,0.41145265,73.38143585873553,43
2396,"We construct a new gender bias test set , SimpleGEN , based on gendered noun phrases in which there is a single , unambiguous , correct answer .",2,0.81642216,105.60489086521731,28
2396,"While we find minimal overall BLEU degradation as we apply speed optimizations , we observe that gendered noun translation performance degrades at a much faster rate .",3,0.96258515,84.5097774461383,27
2397,State-of-the-art machine translation ( MT ) systems are typically trained to generate “ standard ” target language ;,0,0.934583,38.99197389286416,22
2397,"however , many languages have multiple varieties ( regional varieties , dialects , sociolects , non-native varieties ) that are different from the standard language .",0,0.92562896,60.69137418819436,26
2397,"Such varieties are often low-resource , and hence do not benefit from contemporary NLP solutions , MT included .",0,0.7041041,113.54185500321594,19
2397,"We propose a general framework to rapidly adapt MT systems to generate language varieties that are close to , but different from , the standard target language , using no parallel ( source–variety ) data .",3,0.3438373,95.03982522908798,36
2397,This also includes adaptation of MT systems to low-resource typologically-related target languages .,0,0.52638227,53.013443393318795,15
2397,"We experiment with adapting an English –Russian MT system to generate Ukrainian and Belarusian , an English –Norwegian Bokmål system to generate Nynorsk , and an English –Arabic system to generate four Arabic dialects , obtaining significant improvements over competitive baselines .",2,0.66829556,22.18605502911956,45
2398,Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs .,0,0.93643475,99.57291198783572,18
2398,"Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves , suggesting this assumption may not have merit .",0,0.5420694,71.80488314595048,26
2398,We build on the recent work exploring the interpretability of attention ;,2,0.3876934,80.39399831733125,12
2398,we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool .,2,0.667441,30.866664141153525,23
2398,"On three text classification tasks , we verify that only a weak relationship between inputs and co-indexed intermediate representations exists — under sparse attention and otherwise .",3,0.816278,91.69907105899213,27
2398,"Further , we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues .",3,0.97659415,102.74422830617469,24
2398,"Rather , we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior .",3,0.9035782,37.61290580176245,28
2399,Mechanisms for encoding positional information are central for transformer-based language models .,0,0.8430545,54.99297700700642,14
2399,"In this paper , we analyze the position embeddings of existing language models , finding strong evidence of translation invariance , both for the embeddings themselves and for their effect on self-attention .",1,0.7639973,25.096364805981867,33
2399,The degree of translation invariance increases during training and correlates positively with model performance .,3,0.8645672,40.504667229924486,15
2399,"Our findings lead us to propose translation-invariant self-attention ( TISA ) , which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings .",3,0.97893435,47.34422097985434,32
2399,Our proposal has several theoretical advantages over existing position-representation approaches .,3,0.80117404,49.201780574647174,13
2399,"Proof-of-concept experiments show that it improves on regular ALBERT on GLUE tasks , while only adding orders of magnitude less positional parameters .",3,0.88217115,50.052995855435455,23
2400,Determining the relative importance of the elements in a sentence is a key factor for effortless natural language understanding .,0,0.9116334,17.114654149284803,20
2400,"For human language processing , we can approximate patterns of relative importance by measuring reading fixations using eye-tracking technology .",0,0.5632889,82.69689976890501,20
2400,"In neural language models , gradient-based saliency methods indicate the relative importance of a token for the target objective .",0,0.76251346,40.672118266361,22
2400,"In this work , we compare patterns of relative importance in English language processing by humans and models and analyze the underlying linguistic patterns .",1,0.9046234,50.697742643203064,25
2400,We find that human processing patterns in English correlate strongly with saliency-based importance in language models and not with attention-based importance .,3,0.97982043,53.92734934663532,26
2400,Our results indicate that saliency could be a cognitively more plausible metric for interpreting neural language models .,3,0.99076337,37.19391356214939,18
2400,The code is available on github : https://github.com/beinborn/relative_importance .,3,0.60248613,24.235442868100158,9
2401,Pretrained language models ( PLM ) achieve surprising performance on the Choice of Plausible Alternatives ( COPA ) task .,0,0.71086764,22.620728632803175,20
2401,"However , whether PLMs have truly acquired the ability of causal reasoning remains a question .",0,0.9026981,53.34505951236551,16
2401,"In this paper , we investigate the problem of semantic similarity bias and reveal the vulnerability of current COPA models by certain attacks .",1,0.93561345,63.811071157245344,24
2401,"Previous solutions that tackle the superficial cues of unbalanced token distribution still encounter the same problem of semantic bias , even more seriously due to the utilization of more training data .",0,0.77382404,92.51038068250266,32
2401,"We mitigate this problem by simply adding a regularization loss and experimental results show that this solution not only improves the model ’s generalization ability , but also assists the models to perform more robustly on a challenging dataset , BCOPA-CE , which has unbiased token distribution and is more difficult for models to distinguish cause and effect .",3,0.69817096,42.81050367356127,61
2402,"A current open question in natural language processing is to what extent language models , which are trained with access only to the form of language , are able to capture the meaning of language .",0,0.9442032,18.39789422337856,36
2402,"This question is challenging to answer in general , as there is no clear line between meaning and form , but rather meaning constrains form in consistent ways .",0,0.8206,39.7723800499819,29
2402,"Focusing on several formal languages ( propositional logic and a set of programming languages ) , we generate training corpora using a variety of motivated constraints , and measure a distributional language model ’s ability to differentiate logical symbols ( AND , OR , and NOT ) .",2,0.84803647,69.30637093933213,48
2402,"Our findings are largely negative : none of our simulated training corpora result in models which definitively differentiate meaningfully different symbols ( e.g. , AND vs .",3,0.98591566,149.66578852646893,27
2402,"OR ) , suggesting a limitation to the types of semantic signals that current models are able to exploit .",3,0.6173454,52.782772259558776,20
2403,The predominant challenge in weakly supervised semantic parsing is that of spurious programs that evaluate to correct answers for the wrong reasons .,0,0.9109002,49.62760416504918,23
2403,Prior work uses elaborate search strategies to mitigate the prevalence of spurious programs ;,0,0.84439546,372.57710574522224,14
2403,"however , they typically consider only one input at a time .",0,0.9076892,46.37671381456155,12
2403,In this work we explore the use of consistency between the output programs for related inputs to reduce the impact of spurious programs .,1,0.7262498,47.796340664020384,24
2403,We bias the program search ( and thus the model ’s training signal ) towards programs that map the same phrase in related inputs to the same sub-parts in their respective programs .,2,0.7270589,95.83594332927723,33
2403,"Additionally , we study the importance of designing logical formalisms that facilitate this kind of consistency-based training .",1,0.46407837,42.27360085353311,20
2403,We find that a more consistent formalism leads to improved model performance even without consistency-based training .,3,0.98474044,36.7830438466454,19
2403,"When combined together , these two insights lead to a 10 % absolute improvement over the best prior result on the Natural Language Visual Reasoning dataset .",3,0.94206303,38.19355624552027,27
2404,"In this paper , we present an improved model for voicing silent speech , where audio is synthesized from facial electromyography ( EMG ) signals .",1,0.8494904,48.34933845781039,26
2404,"To give our model greater flexibility to learn its own input features , we directly use EMG signals as input in the place of hand-designed features used by prior work .",2,0.6393589,61.32347834953996,33
2404,Our model uses convolutional layers to extract features from the signals and Transformer layers to propagate information across longer distances .,2,0.81095743,29.090076169494044,21
2404,"To provide better signal for learning , we also introduce an auxiliary task of predicting phoneme labels in addition to predicting speech audio features .",2,0.6482902,60.60484773840782,25
2404,"On an open vocabulary intelligibility evaluation , our model improves the state of the art for this task by an absolute 25.8 % .",3,0.92136085,41.56398824349661,24
2405,"Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora , relatively little research has been dedicated to analyzing these massive sources of textual data .",0,0.95187426,20.84803745848843,37
2405,"In this exploratory analysis , we delve deeper into the Common Crawl , a colossal web corpus that is extensively used for training language models .",2,0.5221255,54.169789589633524,26
2405,"We find that it contains a significant amount of undesirable content , including hate speech and sexually explicit content , even after filtering procedures .",3,0.9792042,41.255286122968535,25
2405,We discuss the potential impacts of this content on language models and conclude with future research directions and a more mindful approach to corpus collection and analysis .,3,0.7468672,33.371121710069445,28
2406,Most current quality estimation ( QE ) models for machine translation are trained and evaluated in a static setting where training and test data are assumed to be from a fixed distribution .,0,0.9327424,31.504795141831014,33
2406,"However , in real-life settings , the test data that a deployed QE model would be exposed to may differ from its training data .",0,0.74174994,39.58367275105564,26
2406,"In particular , training samples are often labelled by one or a small set of annotators , whose perceptions of translation quality and needs may differ substantially from those of end-users , who will employ predictions in practice .",0,0.77658254,64.68170639306156,39
2406,"To address this challenge , we propose an online Bayesian meta-learning framework for the continuous training of QE models that is able to adapt them to the needs of different users , while being robust to distributional shifts in training and test data .",1,0.5308027,27.45841326182177,44
2406,Experiments on data with varying number of users and language characteristics validate the effectiveness of the proposed approach .,3,0.8018586,20.756571631891653,19
2407,Sequential sentence classification aims to classify each sentence in the document based on the context in which sentences appear .,0,0.72301847,19.37511010016643,20
2407,Most existing work addresses this problem using a hierarchical sequence labeling network .,0,0.87515295,34.38483202527139,13
2407,"However , they ignore considering the latent segment structure of the document , in which contiguous sentences often have coherent semantics .",0,0.81977946,123.0380677162193,22
2407,"In this paper , we proposed a span-based dynamic local attention model that could explicitly capture the structural information by the proposed supervised dynamic local attention .",1,0.79835474,46.72757044078658,27
2407,We further introduce an auxiliary task called span-based classification to explore the span-level representations .,2,0.61472845,33.73001294691385,15
2407,Extensive experiments show that our model achieves better or competitive performance against state-of-the-art baselines on two benchmark datasets .,3,0.8962726,6.400720123850576,25
2408,Ordered word sequences contain the rich structures that define language .,0,0.91648114,139.76745838434942,11
2408,"However , it ’s often not clear if or how modern pretrained language models utilize these structures .",0,0.947218,41.73960998575341,18
2408,"We show that the token representations and self-attention activations within BERT are surprisingly resilient to shuffling the order of input tokens , and that for several GLUE language understanding tasks , shuffling only minimally degrades performance , e.g. , by 4 % for QNLI .",3,0.9197703,43.31382881769164,45
2408,"While bleak from the perspective of language understanding , our results have positive implications for cases where copyright or ethics necessitates the consideration of bag-of-words data ( vs .",3,0.9832255,79.65164258594744,29
2408,full documents ) .,4,0.61188114,1455.9007083048593,4
2408,"We simulate such a scenario for three sensitive classification tasks , demonstrating minimal performance degradation vs .",2,0.61932206,153.06509448528922,17
2408,releasing full language sequences .,0,0.38750857,903.7272629487009,5
2409,"Recent works made significant advances on summarization tasks , facilitated by summarization datasets .",0,0.9313877,117.0916099285552,14
2409,Several existing datasets have the form of coherent-paragraph summaries .,0,0.8722413,67.5491081977448,12
2409,"However , these datasets were curated from academic documents that were written for experts , thus making the essential step of assessing the summarization output through human-evaluation very demanding .",0,0.6305609,96.63757823859996,30
2409,"To overcome these limitations , we present a dataset based on article summaries appearing on the WikiHow website , composed of how-to articles and coherent-paragraph summaries written in plain language .",2,0.703466,38.781293479047235,34
2409,"We compare our dataset attributes to existing ones , including readability and world-knowledge , showing our dataset makes human evaluation significantly easier and thus , more effective .",3,0.85417134,117.24156332992423,29
2409,A human evaluation conducted on PubMed and the proposed dataset reinforces our findings .,3,0.96002126,67.28665755006146,14
2410,"Despite the success of various text generation metrics such as BERTScore , it is still difficult to evaluate the image captions without enough reference captions due to the diversity of the descriptions .",0,0.87447083,25.131020861206675,33
2410,"In this paper , we introduce a new metric UMIC , an Unreferenced Metric for Image Captioning which does not require reference captions to evaluate image captions .",1,0.8602035,30.67539053734144,28
2410,"Based on Vision-and-Language BERT , we train UMIC to discriminate negative captions via contrastive learning .",2,0.8241463,83.72694140382801,19
2410,"Also , we observe critical problems of the previous benchmark dataset ( i.e. , human annotations ) on image captioning metric , and introduce a new collection of human annotations on the generated captions .",3,0.5465113,50.91088938639915,35
2410,"We validate UMIC on four datasets , including our new dataset , and show that UMIC has a higher correlation than all previous metrics that require multiple references .",3,0.8360891,56.220079907680294,29
2411,Good quality monolingual word embeddings ( MWEs ) can be built for languages which have large amounts of unlabeled text .,0,0.6834184,14.360301700247192,21
2411,MWEs can be aligned to bilingual spaces using only a few thousand word translation pairs .,0,0.4502325,78.11439259353999,16
2411,"For low resource languages training MWEs monolingually results in MWEs of poor quality , and thus poor bilingual word embeddings ( BWEs ) as well .",0,0.5915493,47.50096183272945,26
2411,This paper proposes a new approach for building BWEs in which the vector space of the high resource source language is used as a starting point for training an embedding space for the low resource target language .,1,0.8146365,15.215677011623235,38
2411,By using the source vectors as anchors the vector spaces are automatically aligned during training .,2,0.53569347,52.30330811832324,16
2411,"We experiment on English-German , English-Hiligaynon and English-Macedonian .",2,0.8525399,37.58625452728428,11
2411,"We show that our approach results not only in improved BWEs and bilingual lexicon induction performance , but also in improved target language MWE quality as measured using monolingual word similarity .",3,0.9688204,32.22172747205662,32
2412,"Although multilingual neural machine translation ( MNMT ) enables multiple language translations , the training process is based on independent multilingual objectives .",0,0.93267834,59.6889322685116,23
2412,"Most multilingual models can not explicitly exploit different language pairs to assist each other , ignoring the relationships among them .",0,0.85280824,50.98401322391917,21
2412,"In this work , we propose a novel agreement-based method to encourage multilingual agreement among different translation directions , which minimizes the differences among them .",1,0.89623535,29.02098253365153,28
2412,We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages .,2,0.8415154,64.75197322625726,27
2412,"To examine the effectiveness of our method , we conduct experiments on the multilingual translation task of 10 language pairs .",2,0.7937857,23.949128395967755,21
2412,Experimental results show that our method achieves significant improvements over the previous multilingual baselines .,3,0.96842366,7.2844997431548695,15
2413,Weighted finite-state machines are a fundamental building block of NLP systems .,0,0.9115597,20.320176783795116,13
2413,They have withstood the test of time — from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields .,0,0.92266935,53.03241851255454,30
2413,This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines .,1,0.7680251,45.159597974323184,21
2413,"We provide a general algorithm for evaluating derivatives of all orders , which has not been previously described in the literature .",3,0.45431006,60.344569095953496,22
2413,"In the case of second-order derivatives , our scheme runs in the optimal O( Aˆ2 Nˆ4 ) time where A is the alphabet size and N is the number of states .",3,0.6915662,51.18704921855973,32
2413,Our algorithm is significantly faster than prior algorithms .,3,0.8841346,51.459651995265034,9
2413,"Additionally , our approach leads to a significantly faster algorithm for computing second-order expectations , such as covariance matrices and gradients of first-order expectations .",3,0.83111423,40.15171282368517,25
2414,The growth of online consumer health questions has led to the necessity for reliable and accurate question answering systems .,0,0.9700304,31.48410822660731,20
2414,A recent study showed that manual summarization of consumer health questions brings significant improvement in retrieving relevant answers .,0,0.84473693,56.764620320221994,19
2414,"However , the automatic summarization of long questions is a challenging task due to the lack of training data and the complexity of the related subtasks , such as the question focus and type recognition .",0,0.92906356,22.347735086062546,36
2414,"In this paper , we introduce a reinforcement learning-based framework for abstractive question summarization .",1,0.8801206,16.083871679858643,17
2414,We propose two novel rewards obtained from the downstream tasks of ( i ) question-type identification and ( ii ) question-focus recognition to regularize the question generation model .,2,0.5728702,55.67274552538991,33
2414,These rewards ensure the generation of semantically valid questions and encourage the inclusion of key medical entities / foci in the question summary .,3,0.7195912,105.92210769631399,24
2414,We evaluated our proposed method on two benchmark datasets and achieved higher performance over state-of-the-art models .,3,0.6274777,9.136790016510178,23
2414,The manual evaluation of the summaries reveals that the generated questions are more diverse and have fewer factual inconsistencies than the baseline summaries .,3,0.9573617,19.093035865154995,24
2414,The source code is available here : https://github.com/shwetanlp/CHQ-Summ .,3,0.44937572,20.23071329889334,9
2415,Relation linking is a crucial component of Knowledge Base Question Answering systems .,0,0.902944,21.24970639593901,13
2415,"Existing systems use a wide variety of heuristics , or ensembles of multiple systems , heavily relying on the surface question text .",0,0.86522144,73.0609902159527,23
2415,"However , the explicit semantic parse of the question is a rich source of relation information that is not taken advantage of .",0,0.79675055,43.956520690392296,23
2415,We propose a simple transformer-based neural model for relation linking that leverages the AMR semantic parse of a sentence .,2,0.3730215,36.1947182063962,22
2415,Our system significantly outperforms the state-of-the-art on 4 popular benchmark datasets .,3,0.9020414,5.667254878584495,16
2415,"These are based on either DBpedia or Wikidata , demonstrating that our approach is effective across KGs .",3,0.8880902,31.087534446866314,18
2416,"Early fusion models with cross-attention have shown better-than-human performance on some question answer benchmarks , while it is a poor fit for retrieval since it prevents pre-computation of the answer representations .",0,0.80837226,38.18452410867967,35
2416,We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model .,1,0.47166044,53.09143518450559,25
2416,We first train an accurate classification model with cross-attention between questions and answers .,2,0.7554205,37.16919857379837,14
2416,The cross-attention model is then used to annotate additional passages in order to generate weighted training examples for a neural retrieval model .,2,0.71164507,35.34336511306797,23
2416,The resulting retrieval model with additional data significantly outperforms retrieval models directly trained with gold annotations on Precision at N ( P@N ) and Mean Reciprocal Rank ( MRR ) .,3,0.8393089,86.00068315357379,31
2417,"Generating descriptive sentences that convey non-trivial , detailed , and salient information about images is an important goal of image captioning .",0,0.92131305,32.96237672288396,22
2417,"In this paper we propose a novel approach to encourage captioning models to produce more detailed captions using natural language inference , based on the motivation that , among different captions of an image , descriptive captions are more likely to entail less descriptive captions .",1,0.859278,27.298066149184365,46
2417,"Specifically , we construct directed inference graphs for reference captions based on natural language inference .",2,0.87428415,48.34215743304094,16
2417,A PageRank algorithm is then employed to estimate the descriptiveness score of each node .,2,0.70927346,44.297727182088636,15
2417,"Built on that , we use reference sampling and weighted designated rewards to guide captioning to generate descriptive captions .",2,0.8022919,128.110523028104,20
2417,The results on MSCOCO show that the proposed method outperforms the baselines significantly on a wide range of conventional and descriptiveness-related evaluation metrics .,3,0.9820716,22.533742169077534,26
2418,We present an instance-based nearest neighbor approach to entity linking .,2,0.40706605,61.91871918668024,13
2418,"In contrast to most prior entity retrieval systems which represent each entity with a single vector , we build a contextualized mention-encoder that learns to place similar mentions of the same entity closer in vector space than mentions of different entities .",2,0.6447869,30.741693897872047,44
2418,This approach allows all mentions of an entity to serve as “ class prototypes ” as inference involves retrieving from the full set of labeled entity mentions in the training set and applying the nearest mention neighbor ’s entity label .,2,0.6234705,61.616149043068845,41
2418,"Our model is trained on a large multilingual corpus of mention pairs derived from Wikipedia hyperlinks , and performs nearest neighbor inference on an index of 700 million mentions .",2,0.78075397,41.21150953851171,30
2418,"It is simpler to train , gives more interpretable predictions , and outperforms all other systems on two multilingual entity linking benchmarks .",3,0.80654925,60.8349119340251,23
2419,"BERT has been shown to be extremely effective on a wide variety of natural language processing tasks , including sentiment analysis and emotion detection .",0,0.75932413,10.911538574356861,25
2419,"However , the proposed pretraining objectives of BERT do not induce any sentiment or emotion-specific biases into the model .",3,0.7946969,35.09443343309491,21
2419,"In this paper , we present Emotion Masked Language Modelling , a variation of Masked Language Modelling aimed at improving the BERT language representation model for emotion detection and sentiment analysis tasks .",1,0.8905796,18.813762892054275,33
2419,"Using the same pre-training corpora as the original model , Wikipedia and BookCorpus , our BERT variation manages to improve the downstream performance on 4 tasks from emotion detection and sentiment analysis by an average of 1.2 % F-1 .",3,0.8068375,55.95061214404143,42
2419,"Moreover , our approach shows an increased performance in our task-specific robustness tests .",3,0.9658919,35.033706947318386,15
2420,"Prior work has revealed that positive words occur more frequently than negative words in human expressions , which is typically attributed to positivity bias , a tendency for people to report positive views of reality .",0,0.890449,38.81553795099123,36
2420,"Consistent with prior work , we show that English negative reviews tend to contain more positive words than negative words , using a variety of datasets .",3,0.87124825,31.996178206323126,27
2420,"We reconcile this observation with prior findings on the pragmatics of negation , and show that negations are commonly associated with positive words in negative reviews .",3,0.84179133,25.488080627629852,27
2420,"Furthermore , in negative reviews , the majority of sentences with positive words express negative opinions based on sentiment classifiers , indicating some form of negation .",3,0.92194307,69.93423188423921,27
2421,Large pre-trained language generation models such as GPT-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks .,0,0.861227,8.876243675753171,33
2421,"However , the performance of pre-trained models on task-oriented dialog tasks is still under-explored .",0,0.8963594,10.057068288801926,17
2421,"We propose a Pre-trainedRole Alternating Language model ( PRAL ) , explicitly designed for task-oriented conversational systems .",2,0.4817244,92.5962190744552,20
2421,"We design several techniques : start position randomization , knowledge distillation , and history discount to improve pre-training performance .",2,0.81118035,123.32699985174384,20
2421,"In addition , we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing 13 dialog datasets .",2,0.78346616,62.25280853954668,20
2421,We effectively adapt PRALon three downstream tasks .,3,0.59966356,1083.5717462327407,8
2421,The results show that PRAL outperforms or is on par with state-of-the-art models .,3,0.98483443,13.380943115231213,20
2422,Natural reading orders of words are crucial for information extraction from form-like documents .,0,0.91830057,62.232121914130985,14
2422,"Despite recent advances in Graph Convolutional Networks ( GCNs ) on modeling spatial layout patterns of documents , they have limited ability to capture reading orders of given word-level node representations in a graph .",0,0.94557756,50.64019151357338,37
2422,"We propose Reading Order Equivariant Positional Encoding ( ROPE ) , a new positional encoding technique designed to apprehend the sequential presentation of words in documents .",1,0.5821344,94.496474478966,27
2422,ROPE generates unique reading order codes for neighboring words relative to the target word given a word-level graph connectivity .,0,0.36571142,119.66196744328597,20
2422,We study two fundamental document entity extraction tasks including word labeling and word grouping on the public FUNSD dataset and a large-scale payment dataset .,2,0.70091593,104.16409409553431,25
2422,We show that ROPE consistently improves existing GCNs with a margin up to 8.4 % F1-score .,3,0.9742235,56.29500403883841,19
2423,"Event extraction has long been a challenging task , addressed mostly with supervised methods that require expensive annotation and are not extensible to new event ontologies .",0,0.947939,54.97449311415904,27
2423,"In this work , we explore the possibility of zero-shot event extraction by formulating it as a set of Textual Entailment ( TE ) and / or Question Answering ( QA ) queries ( e.g .",1,0.60950536,20.146239868471508,36
2423,"“ There is an attack ” ) , exploiting pretrained TE / QA models for direct transfer .",2,0.54734397,233.48288751195184,18
2423,"On ACE-2005 and ERE , our system achieves acceptable results , yet there is still a large gap from supervised approaches , showing that current QA and TE technologies fail in transferring to a different domain .",3,0.97006935,84.68400126777456,39
2423,"To investigate the reasons behind the gap , we analyze the remaining key challenges , their respective impact , and possible improvement directions .",1,0.6706232,93.45667849171285,24
2424,"Pre-trained language models have achieved human-level performance on many Machine Reading Comprehension ( MRC ) tasks , but it remains unclear whether these models truly understand language or answer questions by exploiting statistical biases in datasets .",0,0.9557293,17.614420424353426,37
2424,"Here , we demonstrate a simple yet effective method to attack MRC models and reveal the statistical biases in these models .",1,0.6895989,28.82070342938973,22
2424,"We apply the method to the RACE dataset , for which the answer to each MRC question is selected from 4 options .",2,0.82085407,47.582754178432076,23
2424,"It is found that several pre-trained language models , including BERT , ALBERT , and RoBERTa , show consistent preference to some options , even when these options are irrelevant to the question .",3,0.96374786,28.591824028673948,34
2424,"When interfered by these irrelevant options , the performance of MRC models can be reduced from human-level performance to the chance-level performance .",0,0.6304244,64.94864358937879,25
2424,"Human readers , however , are not clearly affected by these irrelevant options .",0,0.71908796,190.12869178438828,14
2424,"Finally , we propose an augmented training method that can greatly reduce models ’ statistical biases .",3,0.52648693,50.21832141749407,17
2425,Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S .,0,0.89157015,23.76332135515431,21
2425,federal minimum wage .,3,0.36345342,139.716016895969,4
2425,"Meanwhile , research on collecting high quality annotations suggests using a qualification that requires workers to have previously completed a certain number of tasks .",0,0.83869976,72.21297174043652,25
2425,If most requesters who pay fairly require workers to have completed a large number of tasks already then workers need to complete a substantial amount of poorly paid work before they can earn a fair wage .,0,0.5408083,35.47073332064268,37
2425,"Through analysis of worker discussions and guidance for researchers , we estimate that workers spend approximately 2.25 months of full time effort on poorly paid tasks in order to get the qualifications needed for better paid tasks .",3,0.88220334,69.09482466752176,38
2425,We discuss alternatives to this qualification and conduct a study of the correlation between qualifications and work quality on two NLP tasks .,1,0.63981485,42.71774267195662,23
2425,We find that it is possible to reduce the burden on workers while still collecting high quality data .,3,0.98573756,14.955028643936588,19
2426,"Human activities can be seen as sequences of events , which are crucial to understanding societies .",0,0.9435943,48.592032057138844,17
2426,"Disproportional event distribution for different demographic groups can manifest and amplify social stereotypes , and potentially jeopardize the ability of members in some groups to pursue certain goals .",0,0.8477892,66.05803263287619,29
2426,"In this paper , we present the first event-centric study of gender biases in a Wikipedia corpus .",1,0.8762278,29.11706128327408,18
2426,"To facilitate the study , we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities .",2,0.89959466,72.98421236627205,26
2426,"Then we detect events with a state-of-the-art event detection model , calibrate the results using strategically generated templates , and extract events that have asymmetric associations with genders .",2,0.81004155,54.344087246538066,35
2426,"Our study discovers that the Wikipedia pages tend to intermingle personal life events with professional events for females but not for males , which calls for the awareness of the Wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry .",3,0.97882915,74.79800418046892,48
2426,Our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level .,3,0.97362804,42.676656979265005,20
2427,"Neural machine translation has achieved great success in bilingual settings , as well as in multilingual settings .",0,0.9322126,17.074700191722858,18
2427,"With the increase of the number of languages , multilingual systems tend to underperform their bilingual counterparts .",0,0.6888256,19.524985367254168,18
2427,Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics .,3,0.5013613,53.936607384280606,19
2427,Previous work increases the modeling capacity by deepening or widening the Transformer .,0,0.7967016,143.6710678785719,13
2427,"However , modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity .",0,0.77773005,72.31351958667103,29
2427,"In this paper , we propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality .",1,0.9008447,29.54316271520717,19
2427,"Unlike previous work which feeds the same input to several transformations and merges their outputs into one , we present a Multi-Input-Multi-Output ( MIMO ) architecture that allows each transformation of the block to have its own input .",2,0.55965656,29.415077902247297,39
2427,We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions .,3,0.46991545,30.682390434837618,26
2427,Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast .,3,0.8951512,14.923446248121051,31
2428,"kNN-MT , recently proposed by Khandelwal et al .",0,0.4670959,64.40379609220983,11
2428,"( 2020a ) , successfully combines pre-trained neural machine translation ( NMT ) model with token-level k-nearest-neighbor ( kNN ) retrieval to improve the translation accuracy .",0,0.64541084,21.88574380074209,30
2428,"However , the traditional kNN algorithm used in kNN-MT simply retrieves a same number of nearest neighbors for each target token , which may cause prediction errors when the retrieved neighbors include noises .",0,0.63149333,77.22970129474514,36
2428,"In this paper , we propose Adaptive kNN-MT to dynamically determine the number of k for each target token .",1,0.78180385,41.885953935211234,22
2428,"We achieve this by introducing a light-weight Meta-k Network , which can be efficiently trained with only a few training samples .",2,0.60566187,27.77762028941667,23
2428,"On four benchmark machine translation datasets , we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla kNN-MT model .",3,0.8772873,30.8591071821405,34
2428,"Even more noteworthy is that the Meta-k Network learned on one domain could be directly applied to other domains and obtain consistent improvements , illustrating the generality of our method .",3,0.9725485,42.86350985450044,31
2428,Our implementation is open-sourced at https://github.com/zhengxxn/adaptive-knn-mt .,3,0.75808144,14.288837876659162,7
2429,Orthogonality constraints encourage matrices to be orthogonal for numerical stability .,0,0.35719994,54.77596772396498,11
2429,"These plug-and-play constraints , which can be conveniently incorporated into model training , have been studied for popular architectures in natural language processing , such as convolutional neural networks and recurrent neural networks .",0,0.7450754,20.89789203780311,35
2429,"However , a dedicated study on such constraints for transformers has been absent .",0,0.9217591,145.02979460616808,14
2429,"To fill this gap , this paper studies orthogonality constraints for transformers , showing the effectiveness with empirical evidence from ten machine translation tasks and two dialogue generation tasks .",1,0.74130136,53.635522931132776,30
2429,"For example , on the large-scale WMT ’16 En→De benchmark , simply plugging-and-playing orthogonality constraints on the original transformer model ( Vaswani et al. , 2017 ) increases the BLEU from 28.4 to 29.6 , coming close to the 29.7 BLEU achieved by the very competitive dynamic convolution ( Wu et al. , 2019 ) .",3,0.66755164,41.36533055516344,61
2430,Imagine you are in a supermarket .,0,0.6191018,48.2951210264296,7
2430,You have two bananas in your basket and want to buy four apples .,0,0.776784,42.751671201367955,14
2430,"This seemingly straightforward question can be challenging for data-driven language models , even if trained at scale .",0,0.7432651,34.90918973117223,18
2430,"However , we would expect such generic language models to possess some mathematical abilities in addition to typical linguistic competence .",3,0.598939,48.404620773557056,21
2430,"Towards this goal , we investigate if a commonly used language model , BERT , possesses such mathematical abilities and , if so , to what degree .",1,0.9174752,62.29803438045622,28
2430,"For that , we fine-tune BERT on a popular dataset for word math problems , AQuA-RAT , and conduct several tests to understand learned representations better .",2,0.8107514,71.59704420390761,30
2430,"Since we teach models trained on natural language to do formal mathematics , we hypothesize that such models would benefit from training on semi-formal steps that explain how math results are derived .",0,0.32029954,43.751184064018425,33
2430,"To better accommodate such training , we also propose new pretext tasks for learning mathematical rules .",2,0.40821338,304.0691417488611,17
2430,We call them ( Neighbor ) Reasoning Order Prediction ( ROP or NROP ) .,0,0.41238832,201.65573209352914,15
2430,"With this new model , we achieve significantly better outcomes than data-driven baselines and even on-par with more tailored models .",3,0.9545414,39.31568184421076,23
2431,Datasets with induced emotion labels are scarce but of utmost importance for many NLP tasks .,0,0.9117989,45.723070126223895,16
2431,"We present a new , automated method for collecting texts along with their induced reaction labels .",1,0.4264005,181.75750984660309,17
2431,"The method exploits the online use of reaction GIFs , which capture complex affective states .",2,0.705444,105.34868697604463,16
2431,We show how to augment the data with induced emotion and induced sentiment labels .,3,0.58547205,62.41343017814264,15
2431,"We use our method to create and publish ReactionGIF , a first-of-its-kind affective dataset of 30 K tweets .",2,0.8225727,45.10398896764426,24
2431,"We provide baselines for three new tasks , including induced sentiment prediction and multilabel classification of induced emotions .",3,0.42363328,62.91659373856046,19
2431,Our method and dataset open new research opportunities in emotion detection and affective computing .,3,0.9722662,39.081759584939405,15
2432,"This work explores a framework for fact verification that leverages pretrained sequence-to-sequence transformer models for sentence selection and label prediction , two key sub-tasks in fact verification .",1,0.67694455,23.60773947503332,30
2432,"Most notably , improving on previous pointwise aggregation approaches for label prediction , we take advantage of T5 using a listwise approach coupled with data augmentation .",2,0.5728126,100.17683072365857,27
2432,"With this enhancement , we observe that our label prediction stage is more robust to noise and capable of verifying complex claims by jointly reasoning over multiple pieces of evidence .",3,0.92617697,58.0671840063507,31
2432,Experimental results on the FEVER task show that our system attains a FEVER score of 75.87 % on the blind test set .,3,0.96526194,19.314976054435377,23
2432,"This puts our approach atop the competitive FEVER leaderboard at the time of our work , scoring higher than the second place submission by almost two points in label accuracy and over one point in FEVER score .",3,0.92849165,31.721026311501838,38
2433,Sentence embedding methods using natural language inference ( NLI ) datasets have been successfully applied to various tasks .,0,0.91662806,24.575436244937887,19
2433,"However , these methods are only available for limited languages due to relying heavily on the large NLI datasets .",0,0.90294296,35.74458715723127,20
2433,"In this paper , we propose DefSent , a sentence embedding method that uses definition sentences from a word dictionary , which performs comparably on unsupervised semantics textual similarity ( STS ) tasks and slightly better on SentEval tasks than conventional methods .",1,0.82429266,60.16958638342041,43
2433,"Since dictionaries are available for many languages , DefSent is more broadly applicable than methods using NLI datasets without constructing additional datasets .",3,0.62733674,100.81488550260879,23
2433,We demonstrate that DefSent performs comparably on unsupervised semantics textual similarity ( STS ) tasks and slightly better on SentEval tasks to the methods using large NLI datasets .,3,0.96110576,73.70114319547798,29
2433,Our code is publicly available at https://github.com/hpprc/defsent .,3,0.57753724,18.19727431706143,8
2434,"Modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words , including phrases , sentences , or paragraphs .",0,0.89316183,39.60707518664719,30
2434,"These kinds of representations are ideal for training a classifier for an end task such as sentiment analysis , question answering and text classification .",0,0.74225795,26.75232357452059,25
2434,Different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols .,0,0.8696672,34.198686039555156,19
2434,"While averaging is the most commonly used efficient sentence encoder , Discrete Cosine Transform ( DCT ) was recently proposed as an alternative that captures the underlying syntactic characteristics of a given text without compromising practical efficiency compared to averaging .",0,0.9290165,46.712265597173904,41
2434,"However , as with most other sentence encoders , the DCT sentence encoder was only evaluated in English .",3,0.48895597,50.25451680342325,19
2434,"To this end , we utilize DCT encoder to generate universal sentence representation for different languages such as German , French , Spanish and Russian .",2,0.8343067,37.980370623778725,26
2434,The experimental results clearly show the superior effectiveness of DCT encoding in which consistent performance improvements are achieved over strong baselines on multiple standardized datasets .,3,0.9847413,45.27210111248274,26
2435,High-quality alignment between movie scripts and plot summaries is an asset for learning to summarize stories and to generate dialogues .,0,0.8479357,41.44880009355743,21
2435,The alignment task is challenging as scripts and summaries substantially differ in details and abstraction levels as well as in linguistic register .,0,0.79765975,103.07418097494643,23
2435,This paper addresses the alignment problem by devising a fully unsupervised approach based on a global optimization model .,1,0.69789845,26.1127576082458,19
2435,Experimental results on ten movies show the viability of our method with 76 % F1-score and its superiority over a previous baseline .,3,0.95508707,36.198558534627004,25
2435,We publish alignments for 914 movies to foster research in this new topic .,2,0.43885082,154.34359175320833,14
2436,Most studies on word-level Quality Estimation ( QE ) of machine translation focus on language-specific models .,0,0.91124195,41.07805875553571,18
2436,The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models .,0,0.7238239,31.67825682116181,28
2436,"To overcome these problems , we explore different approaches to multilingual , word-level QE .",1,0.36107022,84.13146847729973,16
2436,We show that multilingual QE models perform on par with the current language-specific models .,3,0.96402353,34.82816794756609,16
2436,"In the cases of zero-shot and few-shot QE , we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs .",3,0.91210216,23.547372073638634,36
2436,"Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages , making them more useful in real-world scenarios .",3,0.98828566,26.713017804018435,33
2437,"A sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for Chinese Spelling Correction ( CSC ) , which takes a sentence with some spelling errors as input and outputs the corrected one .",0,0.9410741,22.647413744102273,39
2437,"However , CSC models may fail to correct spelling errors covered by the confusion sets , and also will encounter unseen ones .",0,0.53520364,190.9410170488021,23
2437,"We propose a method , which continually identifies the weak spots of a model to generate more valuable training instances , and apply a task-specific pre-training strategy to enhance the model .",2,0.37687504,42.6372677496101,33
2437,The generated adversarial examples are gradually added to the training set .,2,0.60904735,32.343305217846314,12
2437,"Experimental results show that such an adversarial training method combined with the pre-training strategy can improve both the generalization and robustness of multiple CSC models across three different datasets , achieving state-of-the-art performance for CSC task .",3,0.9625637,15.592624881838704,43
2438,Adaptive Computation ( AC ) has been shown to be effective in improving the efficiency of Open-Domain Question Answering ( ODQA ) systems .,0,0.9522318,11.766401127048683,24
2438,"However , the current AC approaches require tuning of all model parameters , and training state-of-the-art ODQA models requires significant computational resources that may not be available for most researchers .",0,0.7995443,46.40382261357971,36
2438,"We propose Adaptive Passage Encoder , an AC method that can be applied to an existing ODQA model and can be trained efficiently on a single GPU .",2,0.36879832,60.42210868740421,28
2438,"It keeps the parameters of the base ODQA model fixed , but it overrides the default layer-by-layer computation of the encoder with an AC policy that is trained to optimise the computational efficiency of the model .",3,0.47267917,47.27045667158398,41
2438,"Our experimental results show that our method improves upon a state-of-the-art model on two datasets , and is also more accurate than previous AC methods due to the stronger base ODQA model .",3,0.96784645,23.85491783455715,39
2438,All source code and datasets are available at https://github.com/uclnlp/APE .,3,0.51730216,11.04436736217578,10
2439,"In this paper , we empirically investigate adversarial attack on NMT from two aspects : languages ( the source vs .",1,0.7845955,82.56884191878515,21
2439,the target language ) and positions ( front vs .,2,0.37265047,345.94757648135686,10
2439,rear ) .,3,0.43697095,256.1313852604639,3
2439,"For autoregressive NMT models that generate target words from left to right , we observe that adversarial attack on the source language is more effective than on the target language , and that attacking front positions of target sentences or positions of source sentences aligned to the front positions of corresponding target sentences is more effective than attacking other positions .",3,0.95076275,19.216413247793735,61
2439,We further exploit the attention distribution of the victim model to attack source sentences at positions that have a strong association with front target words .,2,0.6176892,105.4173292031988,26
2439,Experiment results demonstrate that our attention-based adversarial attack is more effective than adversarial attacks by sampling positions randomly or according to gradients .,3,0.9817447,30.87755765856612,25
2440,SOTA coreference resolution produces increasingly impressive scores on the OntoNotes benchmark .,3,0.7030843,98.03006521343073,12
2440,However lack of comparable data following the same scheme for more genres makes it difficult to evaluate generalizability to open domain data .,0,0.65280056,44.09412910874474,23
2440,This paper provides a dataset and comprehensive evaluation showing that the latest neural LM based end-to-end systems degrade very substantially out of domain .,1,0.461181,65.03172705835162,26
2440,"We make an OntoNotes-like coreference dataset called OntoGUM publicly available , converted from GUM , an English corpus covering 12 genres , using deterministic rules , which we evaluate .",2,0.85697806,116.0867536208419,32
2440,"Thanks to the rich syntactic and discourse annotations in GUM , we are able to create the largest human-annotated coreference corpus following the OntoNotes guidelines , and the first to be evaluated for consistency with the OntoNotes scheme .",3,0.7120994,30.295395031806333,39
2440,"Out-of-domain evaluation across 12 genres shows nearly 15-20 % degradation for both deterministic and deep learning systems , indicating a lack of generalizability or covert overfitting in existing coreference resolution models .",3,0.9271976,59.18216850599864,35
2441,Visual Question Answering ( VQA ) methods aim at leveraging visual input to answer questions that may require complex reasoning over entities .,0,0.9426618,28.08357963647992,23
2441,Current models are trained on labelled data that may be insufficient to learn complex knowledge representations .,0,0.8618184,33.729490229949796,17
2441,"In this paper , we propose a new method to enhance the reasoning capabilities of a multi-modal pretrained model ( Vision + Language BERT ) by integrating facts extracted from an external knowledge base .",1,0.87868667,28.203183513917015,35
2441,"Evaluation on the KVQA dataset benchmark demonstrates that our method outperforms competitive baselines by 19 % , achieving new state-of-the-art results .",3,0.919794,12.734481980723237,28
2441,We also perform an extensive analysis highlighting the limitations of our best performing model through an ablation study .,3,0.57087,27.350091473878667,19
2442,"Neural models for automated fact verification have achieved promising results thanks to the availability of large , human-annotated datasets .",0,0.9303557,22.11661141178346,20
2442,"However , for each new domain that requires fact verification , creating a dataset by manually writing claims and linking them to their supporting evidence is expensive .",0,0.80371445,78.56459600599804,28
2442,"We develop QACG , a framework for training a robust fact verification model by using automatically generated claims that can be supported , refuted , or unverifiable from evidence from Wikipedia .",2,0.5403911,83.88435028116491,32
2442,QACG generates question-answer pairs from the evidence and then converts them into different types of claims .,0,0.42515564,32.16150881306238,19
2442,Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data .,3,0.9597081,21.117014866171388,19
2442,"In a zero-shot scenario , QACG improves a RoBERTa model ’s F1 from 50 % to 77 % , equivalent in performance to 2K + manually-curated examples .",3,0.88497835,68.91063341776133,31
2442,Our QACG code is publicly available .,3,0.39892793,69.46114135018456,7
2443,Scarcity of parallel data causes formality style transfer models to have scarce success in preserving content .,0,0.7078013,211.652373844443,17
2443,"We show that fine-tuning pre-trained language ( GPT-2 ) and sequence-to-sequence ( BART ) models boosts content preservation , and that this is possible even with limited amounts of parallel data .",3,0.92280954,27.42251557610473,35
2443,Augmenting these models with rewards that target style and content – the two core aspects of the task – we achieve a new state-of-the-art .,3,0.482927,17.248524153197415,30
2444,"Existing works for aspect-based sentiment analysis ( ABSA ) have adopted a unified approach , which allows the interactive relations among subtasks .",0,0.89526975,67.2080957414545,23
2444,"However , we observe that these methods tend to predict polarities based on the literal meaning of aspect and opinion terms and mainly consider relations implicitly among subtasks at the word level .",3,0.94534534,66.37807853552275,33
2444,"In addition , identifying multiple aspect–opinion pairs with their polarities is much more challenging .",0,0.6921191,96.02756414779522,15
2444,"Therefore , a comprehensive understanding of contextual information w.r.t .",0,0.84865,30.239209036236186,10
2444,the aspect and opinion are further required in ABSA .,3,0.7893314,353.24714861702853,10
2444,"In this paper , we propose Deep Contextualized Relation-Aware Network ( DCRAN ) , which allows interactive relations among subtasks with deep contextual information based on two modules ( i.e. , Aspect and Opinion Propagation and Explicit Self-Supervised Strategies ) .",1,0.7515662,50.32047088400027,43
2444,"Especially , we design novel self-supervised strategies for ABSA , which have strengths in dealing with multiple aspects .",3,0.42935959,71.98108866780927,19
2444,Experimental results show that DCRAN significantly outperforms previous state-of-the-art methods by large margins on three widely used benchmarks .,3,0.95916057,9.747881879789539,25
2445,Aspect-based sentiment analysis ( ABSA ) has received increasing attention recently .,0,0.9592922,24.806206962152398,13
2445,"Most existing work tackles ABSA in a discriminative manner , designing various task-specific classification networks for the prediction .",0,0.8571947,97.56968913928462,20
2445,"Despite their effectiveness , these methods ignore the rich label semantics in ABSA problems and require extensive task-specific designs .",0,0.72271293,132.4387559555234,20
2445,"In this paper , we propose to tackle various ABSA tasks in a unified generative framework .",1,0.873564,39.43979618566308,17
2445,"Two types of paradigms , namely annotation-style and extraction-style modeling , are designed to enable the training process by formulating each ABSA task as a text generation problem .",2,0.65483814,56.11469002892798,31
2445,We conduct experiments on four ABSA tasks across multiple benchmark datasets where our proposed generative approach achieves new state-of-the-art results in almost all cases .,3,0.47142553,16.490586255296616,31
2445,This also validates the strong generality of the proposed framework which can be easily adapted to arbitrary ABSA task without additional task-specific model design .,3,0.9800163,41.92464908492292,26
2446,"Recently , token-level adaptive training has achieved promising improvement in machine translation , where the cross-entropy loss function is adjusted by assigning different training weights to different tokens , in order to alleviate the token imbalance problem .",0,0.91614217,33.088295238372574,40
2446,"However , previous approaches only use static word frequency information in the target language without considering the source language , which is insufficient for bilingual tasks like machine translation .",0,0.87058413,30.809874093663574,30
2446,"In this paper , we propose a novel bilingual mutual information ( BMI ) based adaptive objective , which measures the learning difficulty for each target token from the perspective of bilingualism , and assigns an adaptive weight accordingly to improve token-level adaptive training .",1,0.8543359,57.46428781654192,47
2446,"This method assigns larger training weights to tokens with higher BMI , so that easy tokens are updated with coarse granularity while difficult tokens are updated with fine granularity .",2,0.59465635,47.98211753414747,30
2446,Experimental results on WMT14 English-to-German and WMT19 Chinese-to-English demonstrate the superiority of our approach compared with the Transformer baseline and previous token-level adaptive training approaches .,3,0.9556535,8.726906227343916,32
2446,Further analyses confirm that our method can improve the lexical diversity .,3,0.9859049,41.19567373145099,12
2447,This ability to learn consecutive tasks without forgetting how to perform previously trained problems is essential for developing an online dialogue system .,0,0.87034774,69.89196036401336,23
2447,"This paper proposes an effective continual learning method for the task-oriented dialogue system with iterative network pruning , expanding , and masking ( TPEM ) , which preserves performance on previously encountered tasks while accelerating learning progress on subsequent tasks .",1,0.8652156,82.54006608123834,43
2447,"Specifically , TPEM ( i ) leverages network pruning to keep the knowledge for old tasks , ( ii ) adopts network expanding to create free weights for new tasks , and ( iii ) introduces task-specific network masking to alleviate the negative impact of fixed weights of old tasks on new tasks .",2,0.60100603,48.43486648874332,55
2447,We conduct extensive experiments on seven different tasks from three benchmark datasets and show empirically that TPEM leads to significantly improved results over the strong competitors .,3,0.73739415,30.842756049896153,27
2448,"We present TIMERS-a TIME , Rhetorical and Syntactic-aware model for document-level temporal relation classification in the English language .",1,0.37863183,84.0548397892367,24
2448,"Our proposed method leverages rhetorical discourse features and temporal arguments from semantic role labels , in addition to traditional local syntactic features , trained through a Gated Relational-GCN .",2,0.6696829,143.62284654939356,31
2448,"Extensive experiments show that the proposed model outperforms previous methods by 5-18 % on the TDDiscourse , TimeBank-Dense , and MATRES datasets due to our discourse-level modeling .",3,0.8771095,56.74613624132684,33
2449,Arabic diacritization is a fundamental task for Arabic language processing .,0,0.9244807,20.835411091929508,11
2449,Previous studies have demonstrated that automatically generated knowledge can be helpful to this task .,0,0.9174193,28.70818472968952,15
2449,"However , these studies regard the auto-generated knowledge instances as gold references , which limits their effectiveness since such knowledge is not always accurate and inferior instances can lead to incorrect predictions .",0,0.80247843,72.94472325487547,33
2449,"In this paper , we propose to use regularized decoding and adversarial training to appropriately learn from such noisy knowledge for diacritization .",1,0.83480346,58.13106901432041,23
2449,"Experimental results on two benchmark datasets show that , even with quite flawed auto-generated knowledge , our model can still learn adequate diacritics and outperform all previous studies , on both datasets .",3,0.9473809,41.632381239654784,33
2450,Subword segmentation algorithms have been a de facto choice when building neural machine translation systems .,0,0.9125969,30.076782213018312,16
2450,"However , most of them need to learn a segmentation model based on some heuristics , which may produce sub-optimal segmentation .",0,0.81270796,29.040757254816448,22
2450,This can be problematic in some scenarios when the target language has rich morphological changes or there is not enough data for learning compact composition rules .,0,0.6596024,45.289341949439546,27
2450,"Translating at fully character level has the potential to alleviate the issue , but empirical performances of character-based models has not been fully explored .",0,0.89270633,41.08055624341668,27
2450,"In this paper , we present an in-depth comparison between character-based and subword-based NMT systems under three settings : translating to typologically diverse languages , training with low resource , and adapting to unseen domains .",1,0.89975464,34.73181269314701,41
2450,Experiment results show strong competitiveness of character-based models .,3,0.979591,53.78334753539016,11
2450,"Further analyses show that compared to subword-based models , character-based models are better at handling morphological phenomena , generating rare and unknown words , and more suitable for transferring to unseen domains .",3,0.9810098,47.74059205440875,37
2451,Chinese word segmentation ( CWS ) is undoubtedly an important basic task in natural language processing .,0,0.9645199,37.556756674580086,17
2451,"Previous works only focus on the textual modality , but there are often audio and video utterances ( such as news broadcast and face-to-face dialogues ) , where textual , acoustic and visual modalities normally exist .",0,0.8994979,56.862987472633556,41
2451,"To this end , we attempt to combine the multi-modality ( mainly the converted text and actual voice information ) to perform CWS .",2,0.528778,123.55002059034521,24
2451,"In this paper , we annotate a new dataset for CWS containing text and audio .",1,0.7667657,41.30496827689724,16
2451,"Moreover , we propose a time-dependent multi-modal interactive model based on Transformer framework to integrate multi-modal information for word sequence labeling .",2,0.4598043,18.41935622688486,23
2451,The experimental results on three different training sets show the effectiveness of our approach with fusing text and audio .,3,0.9715124,19.696413503143894,20
2452,Discrimination between antonyms and synonyms is an important and challenging NLP task .,0,0.95700824,23.98358967641936,13
2452,Antonyms and synonyms often share the same or similar contexts and thus are hard to make a distinction .,0,0.86142945,33.08396453528526,19
2452,This paper proposes two underlying hypotheses and employs the mixture-of-experts framework as a solution .,1,0.56882864,40.95741050798022,17
2452,"It works on the basis of a divide-and-conquer strategy , where a number of localized experts focus on their own domains ( or subspaces ) to learn their specialties , and a gating mechanism determines the space partitioning and the expert mixture .",0,0.38848883,44.7783886126871,46
2452,Experimental results have shown that our method achieves the state-of-the-art performance on the task .,3,0.9644171,5.041515122530586,21
2453,"Injecting external domain-specific knowledge ( e.g. , UMLS ) into pretrained language models ( LMs ) advances their capability to handle specialised in-domain tasks such as biomedical entity linking ( BEL ) .",0,0.7928822,46.56124653242046,33
2453,"However , such abundant expert knowledge is available only for a handful of languages ( e.g. , English ) .",0,0.9224876,36.40544013922193,20
2453,"In this work , by proposing a novel cross-lingual biomedical entity linking task ( XL-BEL ) and establishing a new XL-BEL benchmark spanning 10 typologically diverse languages , we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual LMs beyond the standard monolingual English BEL task .",1,0.8312066,34.01045821055649,60
2453,The scores indicate large gaps to English performance .,3,0.95327246,255.1931725365909,9
2453,We then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones .,1,0.5542414,15.130844810256589,20
2453,"To this end , we propose and evaluate a series of cross-lingual transfer methods for the XL-BEL task , and demonstrate that general-domain bitext helps propagate the available English knowledge to languages with little to no in-domain data .",1,0.43268153,39.41185043642883,44
2453,"Remarkably , we show that our proposed domain-specific transfer methods yield consistent gains across all target languages , sometimes up to 20 Precision@1 points , without any in-domain knowledge in the target language , and without any in-domain parallel data .",3,0.9666881,38.852156148838176,43
2454,The representation degeneration problem in Contextual Word Representations ( CWRs ) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations .,0,0.9402561,92.29148971446986,32
2454,Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy .,0,0.8313019,76.98618747637619,26
2454,Our quantitative analysis over isotropy shows that a local assessment could be more accurate due to the clustered structure of CWRs .,3,0.98663914,93.83592470393954,22
2454,"Based on this observation , we propose a local cluster-based method to address the degeneration issue in contextual embedding spaces .",2,0.31647485,28.607227377358587,23
2454,"We show that in clusters including punctuations and stop words , local dominant directions encode structural information , removing which can improve CWRs performance on semantic tasks .",3,0.96420074,294.064259487408,28
2454,"Moreover , we find that tense information in verb representations dominates sense semantics .",3,0.98076916,179.71185300708774,14
2454,We show that removing dominant directions of verb representations can transform the space to better suit semantic applications .,3,0.9303964,151.24598437883301,19
2454,Our experiments demonstrate that the proposed cluster-based method can mitigate the degeneration problem on multiple tasks .,3,0.97292554,23.396790571188856,19
2455,"Humans often refer to personal narratives , life experiences , and events to make a conversation more engaging and rich .",0,0.9265252,67.00675620693568,21
2455,"While persona-grounded dialog models are able to generate responses that follow a given persona , they often miss out on stating detailed experiences or events related to a persona , often leaving conversations shallow and dull .",0,0.8601609,51.07728277201351,37
2455,"In this work , we equip dialog models with ‘ background stories ’ related to a persona by leveraging fictional narratives from existing story datasets ( e.g .",2,0.5705733,103.45771901123656,28
2455,ROCStories ) .,4,0.665626,420.2106888368971,3
2455,"Since current dialog datasets do not contain such narratives as responses , we perform an unsupervised adaptation of a retrieved story for generating a dialog response using a gradient-based rewriting technique .",2,0.8088283,42.947493766346696,34
2455,"Our proposed method encourages the generated response to be fluent ( i.e. , highly likely ) with the dialog history , minimally different from the retrieved story to preserve event ordering and consistent with the original persona .",3,0.6200889,110.03048937356412,38
2455,"We demonstrate that our method can generate responses that are more diverse , and are rated more engaging and human-like by human evaluators , compared to outputs from existing dialog models .",3,0.9465812,25.735200304280056,32
2456,The famous “ laurel / yanny ” phenomenon references an audio clip that elicits dramatically different responses from different listeners .,0,0.96377677,115.27456878216401,21
2456,"For the original clip , roughly half the population hears the word “ laurel , ” while the other half hears “ yanny .",3,0.74918216,78.28715261862723,24
2456,In this paper we apply ML techniques to study the prevalence of polyperceivability in spoken language .,1,0.8285184,47.744519090137,17
2456,"We devise a metric that correlates with polyperceivability of audio clips , use it to efficiently find new “ laurel / yanny ”-type examples , and validate these results with human experiments .",2,0.4487116,157.56976864800518,35
2456,"Our results suggest that polyperceivable examples are surprisingly prevalent in natural language , existing for > 2 % of English words .",3,0.9904479,162.85724998577157,22
2457,Event language models represent plausible sequences of events .,0,0.7215842,117.08580337495809,9
2457,"Most existing approaches train autoregressive models on text , which successfully capture event co-occurrence but unfortunately constrain the model to follow the discourse order in which events are presented .",0,0.8574923,32.9110748653862,30
2457,"Other domains may employ different discourse orders , and for many applications , we may care about different notions of ordering ( e.g. , temporal ) or not care about ordering at all ( e.g. , when predicting related events in a schema ) .",0,0.5844692,57.77114719809521,45
2457,We propose a simple yet surprisingly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order .,3,0.50204486,48.450077292479946,27
2457,"Despite generating completely synthetic event orderings , we show that this technique improves the performance of the event language models on both applications and out-of-domain events data .",3,0.9484625,51.885096524773104,29
2458,Information Retrieval using dense low-dimensional representations recently became popular and showed out-performance to traditional sparse-representations like BM25 .,0,0.8901226,76.50518181876048,19
2458,"However , no previous work investigated how dense representations perform with large index sizes .",0,0.8953683,100.01655401450279,15
2458,We show theoretically and empirically that the performance for dense representations decreases quicker than sparse representations for increasing index sizes .,3,0.8752102,76.64729580759891,21
2458,"In extreme cases , this can even lead to a tipping point where at a certain index size sparse representations outperform dense representations .",3,0.5249371,67.32189586093949,24
2458,"The lower the dimension , the higher the chance for false positives , i.e .",3,0.6997739,28.76431705311956,15
2458,returning irrelevant documents .,3,0.42028198,407.3165412403984,4
2459,"Cross-lingual text classification aims at training a classifier on the source language and transferring the knowledge to target languages , which is very useful for low-resource languages .",0,0.9155855,11.673809984915993,28
2459,"Recent multilingual pretrained language models ( mPLM ) achieve impressive results in cross-lingual classification tasks , but rarely consider factors beyond semantic similarity , causing performance degradation between some language pairs .",0,0.9207209,37.80609637332614,32
2459,In this paper we propose a simple yet effective method to incorporate heterogeneous information within and across languages for cross-lingual text classification using graph convolutional networks ( GCN ) .,1,0.8861531,11.956788135477971,30
2459,"In particular , we construct a heterogeneous graph by treating documents and words as nodes , and linking nodes with different relations , which include part-of-speech roles , semantic similarity , and document translations .",2,0.82121974,78.09335039577485,36
2459,"Extensive experiments show that our graph-based method significantly outperforms state-of-the-art models on all tasks , and also achieves consistent performance gain over baselines in low-resource settings where external tools like translators are unavailable .",3,0.926921,11.626737355158614,41
2460,"Question answering ( QA ) in English has been widely explored , but multilingual datasets are relatively new , with several methods attempting to bridge the gap between high-and low-resourced languages using data augmentation through translation and cross-lingual transfer .",0,0.9615947,26.923089043464394,42
2460,In this project we take a step back and study which approaches allow us to take the most advantage of existing resources in order to produce QA systems in many languages .,1,0.7831197,25.29510370286383,32
2460,"Specifically , we perform extensive analysis to measure the efficacy of few-shot approaches augmented with automatic translations and permutations of context-question-answer pairs .",2,0.72339445,46.634004202472994,25
2460,"In addition , we make suggestions for future dataset development efforts that make better use of a fixed annotation budget , with a goal of increasing the language coverage of QA datasets and systems .",3,0.72737545,48.48429305970764,35
2461,"We propose an effective context-sensitive neural model for time to event ( TTE ) prediction task , which aims to predict the amount of time to / from the occurrence of given events in streaming content .",1,0.60244983,42.48232253010134,37
2461,"We investigate this problem in the context of a multi-task learning framework , which we enrich with time difference embeddings .",2,0.49697652,29.596326573691716,21
2461,"In addition , we develop a multi-genre dataset of English events about soccer competitions and academy awards ceremonies , and their relevant tweets obtained from Twitter .",2,0.8811803,122.0049250049246,27
2461,Our model is 1.4 and 3.3 hours more accurate than the current state-of-the-art model in estimating TTE on English and Dutch tweets respectively .,3,0.90436494,21.966897626068,30
2461,We examine different aspects of our model to illustrate its source of improvement .,2,0.35312688,74.70247909677047,14
2462,Compositional generalization is the ability to generalize systematically to a new data distribution by combining known components .,0,0.8802908,40.652942144826646,18
2462,"Although humans seem to have a great ability to generalize compositionally , state-of-the-art neural models struggle to do so .",0,0.9405877,15.961689280215266,26
2462,"In this work , we study compositional generalization in classification tasks and present two main contributions .",1,0.8175252,38.63669456274153,17
2462,"First , we study ways to convert a natural language sequence-to-sequence dataset to a classification dataset that also requires compositional generalization .",2,0.4824701,28.708266864582356,24
2462,"Second , we show that providing structural hints ( specifically , providing parse trees and entity links as attention masks for a Transformer model ) helps compositional generalization .",3,0.7944145,105.53260391828579,29
2463,Pre-trained text-to-text transformers such as BART have achieved impressive performance across a range of NLP tasks .,0,0.8439302,6.235672197270125,20
2463,"Recent study further shows that they can learn to generalize to novel tasks , by including task descriptions as part of the source sequence and training the model with ( source , target ) examples .",0,0.81514776,45.522981098115245,36
2463,"At test time , these fine-tuned models can make inferences on new tasks using the new task descriptions as part of the input .",3,0.6898698,32.399885786687065,24
2463,"However , this approach has potential limitations , as the model learns to solve individual ( source , target ) examples ( i.e. , at the instance level ) , instead of learning to solve tasks by taking all examples within a task as a whole ( i.e. , at the task level ) .",3,0.4485479,45.88643276053475,55
2463,"To this end , we introduce Hypter , a framework that improves text-to-text transformer ’s generalization ability to unseen tasks by training a hypernetwork to generate task-specific , light-weight adapters from task descriptions .",2,0.53683597,63.08828311194633,41
2463,Experiments on ZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves upon fine-tuning baselines .,3,0.87345386,70.61810445146932,17
2463,"Notably , when using BART-Large as the main network , Hypter brings 11.3 % comparative improvement on ZEST dataset .",3,0.9686508,653.423781423012,22
2464,Slot-filling is an essential component for building task-oriented dialog systems .,0,0.8192799,20.27325675173729,13
2464,"In this work , we focus on the zero-shot slot-filling problem , where the model needs to predict slots and their values , given utterances from new domains without training on the target domain .",1,0.47580746,34.87840821227829,35
2464,Prior methods directly encode slot descriptions to generalize to unseen slot types .,0,0.8136467,73.92658103658636,13
2464,"However , raw slot descriptions are often ambiguous and do not encode enough semantic information , limiting the models ’ zero-shot capability .",0,0.8664974,80.3434505534897,23
2464,"To address this problem , we introduce QA-driven slot filling ( QASF ) , which extracts slot-filler spans from utterances with a span-based QA model .",0,0.33851606,35.19669501527571,28
2464,"We use a linguistically motivated questioning strategy to turn descriptions into questions , allowing the model to generalize to unseen slot types .",2,0.8016501,65.98672608251225,23
2464,"Moreover , our QASF model can benefit from weak supervision signals from QA pairs synthetically generated from unlabeled conversations .",3,0.9092695,55.47608962135282,20
2464,Our full system substantially outperforms baselines by over 5 % on the SNIPS benchmark .,3,0.906737,37.64934990056508,15
2465,Language models like BERT and SpanBERT pretrained on open-domain data have obtained impressive gains on various NLP tasks .,0,0.7342789,17.59390890611676,19
2465,"In this paper , we probe the effectiveness of domain-adaptive pretraining objectives on downstream tasks .",1,0.9024057,24.429752028322024,16
2465,"In particular , three objectives , including a novel objective focusing on modeling predicate-argument relations , are evaluated on two challenging dialogue understanding tasks .",2,0.5372026,146.30311214324465,27
2465,"Experimental results demonstrate that domain-adaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks , achieving the new state-of-the-art performances .",3,0.97858626,16.27700988097952,34
2466,"One group introduces a language task , exemplified by a dataset , which they argue is challenging enough to serve as a benchmark .",0,0.68642294,55.32519532557585,24
2466,"They also provide a baseline model for it , which then soon is improved upon by other groups .",3,0.53371716,112.05585792970689,19
2466,"Often , research efforts then move on , and the pattern repeats itself .",0,0.8831391,141.99481171484453,14
2466,"What is typically left implicit is the argumentation for why this constitutes progress , and progress towards what .",0,0.9299674,143.91715253794854,19
2466,"In this paper , we try to step back for a moment from this pattern and work out possible argumentations and their parts .",1,0.89812195,52.22236517308418,24
2467,"In this work , we introduce : the largest publicly available multilingual dataset for factual verification of naturally existing real-world claims .",1,0.79941714,53.7697825659645,22
2467,The dataset contains short statements in 25 languages and is labeled for veracity by expert fact-checkers .,2,0.67234725,44.03257726502012,17
2467,"The dataset includes a multilingual evaluation benchmark that measures both out-of-domain generalization , and zero-shot capabilities of the multilingual models .",2,0.639261,28.937843819913937,24
2467,"Using state-of-the-art multilingual transformer-based models , we develop several automated fact-checking models that , along with textual claims , make use of additional metadata and evidence from news stories retrieved using a search engine .",2,0.7542516,41.104236052686915,44
2467,"Empirically , our best model attains an F-score of around 40 % , suggesting that our dataset is a challenging benchmark for the evaluation of multilingual fact-checking models .",3,0.96402043,30.364518283292124,31
2468,"Recently , mT5-a massively multilingual version of T5-leveraged a unified text-to-text format to attain state-of-the-art results on a wide variety of multilingual NLP tasks .",0,0.94617313,18.414815958243565,39
2468,"In this paper , we investigate the impact of incorporating parallel data into mT5 pre-training .",1,0.92665935,26.60088765714345,16
2468,We find that multi-tasking language modeling with objectives such as machine translation during pre-training is a straightforward way to improve performance on downstream multilingual and cross-lingual tasks .,3,0.97984856,15.823618987373031,28
2468,"However , the gains start to diminish as the model capacity increases , suggesting that parallel data might not be as essential for larger models .",3,0.951185,40.068830038175484,26
2468,"At the same time , even at larger model sizes , we find that pre-training with parallel data still provides benefits in the limited labelled data regime .",3,0.9683903,45.74475781342571,28
2469,Intelligent and adaptive online education systems aim to make high-quality education available for a diverse range of students .,0,0.92345864,26.552541072184052,19
2469,"However , existing systems usually depend on a pool of hand-made questions , limiting how fine-grained and open-ended they can be in adapting to individual students .",0,0.8740282,41.252797683265946,29
2469,We explore targeted question generation as a controllable sequence generation task .,2,0.4998281,33.32831249732968,12
2469,We first show how to fine-tune pre-trained language models for deep knowledge tracing ( LM-KT ) .,2,0.4318248,35.23119277316093,19
2469,"This model accurately predicts the probability of a student answering a question correctly , and generalizes to questions not seen in training .",3,0.55000657,22.03426882045496,23
2469,We then use LM-KT to specify the objective and data for training a model to generate questions conditioned on the student and target difficulty .,2,0.8210903,76.38645700349842,27
2469,"Our results show we succeed at generating novel , well-calibrated language translation questions for second language learners from a real online education platform .",3,0.98769367,81.5390104896571,26
2470,This paper presents a simple recipe to trainstate-of-the-art multilingual Grammatical Error Correction ( GEC ) models .,1,0.8436216,21.30116647069456,22
2470,We achieve this by first proposing a language-agnostic method to generate a large number of synthetic examples .,2,0.6627439,17.937979299623255,20
2470,The second ingredient is to use large-scale multilingual language models ( up to 11B parameters ) .,0,0.3788871,70.61598306315013,17
2470,"Once fine-tuned on language-specific supervised sets we surpass the previous state-of-the-art results on GEC benchmarks in four languages : English , Czech , German and Russian .",3,0.7661849,24.100989166589265,34
2470,"Having established a new set of baselines for GEC , we make our results easily reproducible and accessible by releasing a CLANG-8 dataset .",3,0.7252428,47.856261733147036,26
2470,"It is produced by using our best model , which we call gT5 , to clean the targets of a widely used yet noisy Lang-8 dataset .",2,0.60036045,124.4850180105415,29
2470,cLang-8 greatly simplifies typical GEC training pipelines composed of multiple fine-tuning stages – we demonstrate that performing a single fine-tuning stepon cLang-8 with the off-the-shelf language models yields further accuracy improvements over an already top-performing gT5 model for English .,3,0.8757578,36.1067010135153,46
2471,Pathology imaging is broadly used for identifying the causes and effects of diseases or injuries .,0,0.9602027,44.10325521516429,16
2471,"Given a pathology image , being able to answer questions about the clinical findings contained in the image is very important for medical decision making .",0,0.916407,44.117000507982496,26
2471,"In this paper , we aim to develop a pathological visual question answering framework to analyze pathology images and answer medical questions related to these images .",1,0.95246774,33.38136303344405,27
2471,"To build such a framework , we create PathVQA , a VQA dataset with 32,795 questions asked from 4,998 pathology images .",2,0.8638876,61.433787953269764,22
2471,We also propose a three-level optimization framework which performs self-supervised pretraining and VQA finetuning end-to-end to learn powerful visual and textual representations jointly and automatically identifies and excludes noisy self-supervised examples from pretraining .,2,0.594542,31.875798905908354,37
2471,We perform experiments on our created PathVQA dataset and the results demonstrate the effectiveness of our proposed methods .,3,0.75495934,26.84878953537568,19
2471,The datasets and code are available at https://github.com/UCSD-AI4H/PathVQA .,3,0.56705195,19.72227250119992,9
2472,Text-based games ( TBGs ) have emerged as useful benchmarks for evaluating progress at the intersection of grounded language understanding and reinforcement learning ( RL ) .,0,0.9591712,43.10256327300646,29
2472,Recent work has proposed the use of external knowledge to improve the efficiency of RL agents for TBGs .,0,0.8632862,43.033263269012686,19
2472,"In this paper , we posit that to act efficiently in TBGs , an agent must be able to track the state of the game while retrieving and using relevant commonsense knowledge .",1,0.7930858,37.96307002842658,33
2472,"Thus , we propose an agent for TBGs that induces a graph representation of the game state and jointly grounds it with a graph of commonsense knowledge from ConceptNet .",2,0.37723485,54.24747781667161,30
2472,This combination is achieved through bidirectional knowledge graph attention between the two symbolic representations .,2,0.48521867,55.221640983111946,15
2472,We show that agents that incorporate commonsense into the game state graph outperform baseline agents .,3,0.92872715,74.89854413016855,16
2473,"We introduce mTVR , a large-scale multilingual video moment retrieval dataset , containing 218 K English and Chinese queries from 21.8 K TV show video clips .",2,0.73077077,135.2985174653443,27
2473,The dataset is collected by extending the popular TVR dataset ( in English ) with paired Chinese queries and subtitles .,2,0.79875547,124.95597120001662,21
2473,"Compared to existing moment retrieval datasets , mTVR is multilingual , larger , and comes with diverse annotations .",3,0.50067914,265.79416329891467,19
2473,"We further propose mXML , a multilingual moment retrieval model that learns and operates on data from both languages , via encoder parameter sharing and language neighborhood constraints .",2,0.39066285,106.9404572584728,29
2473,"We demonstrate the effectiveness of mXML on the newly collected mTVR dataset , where mXML outperforms strong monolingual baselines while using fewer parameters .",3,0.8184519,25.561247194494893,24
2473,"In addition , we also provide detailed dataset analyses and model ablations .",3,0.5105989,66.20674871210245,13
2473,Data and code are publicly available at https://github.com/jayleicn/mTVRetrieval .,3,0.45996365,16.713706060427803,9
2474,"Named entity recognition ( NER ) is well studied for the general domain , and recent systems have achieved human-level performance for identifying common entity types .",0,0.9612818,40.8640561734117,27
2474,"However , the NER performance is still moderate for specialized domains that tend to feature complicated contexts and jargonistic entity types .",3,0.56499404,163.2790034047655,22
2474,"To address these challenges , we propose explicitly connecting entity mentions based on both global coreference relations and local dependency relations for building better entity mention representations .",1,0.33383954,57.421338937691296,28
2474,"In our experiments , we incorporate entity mention relations by Graph Neural Networks and show that our system noticeably improves the NER performance on two datasets from different domains .",3,0.5317029,45.78604625354562,30
2474,"We further show that the proposed lightweight system can effectively elevate the NER performance to a higher level even when only a tiny amount of labeled data is available , which is desirable for domain-specific NER .",3,0.9576234,20.896955358999037,37
2475,Accurate terminology translation is crucial for ensuring the practicality and reliability of neural machine translation ( NMT ) systems .,0,0.95128006,24.72119353211427,20
2475,"To address this , lexically constrained NMT explores various methods to ensure pre-specified words and phrases appear in the translation output .",0,0.7758415,92.58584362606399,22
2475,"However , in many cases , those methods are studied on general domain corpora , where the terms are mostly uni-and bi-grams ( > 98 % ) .",0,0.7632106,107.16046387823722,28
2475,"In this paper , we instead tackle a more challenging setup consisting of domain-specific corpora with much longer n-gram and highly specialized terms .",1,0.41265506,52.65161733041187,24
2475,"Inspired by the recent success of masked span prediction models , we propose a simple and effective training strategy that achieves consistent improvements on both terminology and sentence-level translation for three domain-specific corpora in two language pairs .",2,0.45845535,23.42771996676396,40
2476,"To help individuals express themselves better , quotation recommendation is receiving growing attention .",0,0.9516208,265.643004841041,14
2476,"Nevertheless , most prior efforts focus on modeling quotations and queries separately and ignore the relationship between the quotations and the queries .",0,0.8980905,32.29361361139524,23
2476,"In this work , we introduce a transformation matrix that directly maps the query representations to quotation representations .",1,0.52993447,56.13189778387331,19
2476,"To better learn the mapping relationship , we employ a mapping loss that minimizes the distance of two semantic spaces ( one for quotation and another for mapped-query ) .",2,0.8051038,97.17118236271206,32
2476,"Furthermore , we explore using the words in history queries to interpret the figurative language of quotations , where quotation-aware attention is applied on top of history queries to highlight the indicator words .",2,0.64377743,78.13510514089742,36
2476,Experiments on two datasets in English and Chinese show that our model outperforms previous state-of-the-art models .,3,0.9046229,4.465393949236557,22
2477,"Topic models extract groups of words from documents , whose interpretation as a topic hopefully allows for a better understanding of the data .",0,0.87649786,104.16583253591249,24
2477,"However , the resulting word groups are often not coherent , making them harder to interpret .",0,0.7659983,59.70598338211436,17
2477,"Recently , neural topic models have shown improvements in overall coherence .",0,0.90490717,64.29627766876095,12
2477,"Concurrently , contextual embeddings have advanced the state of the art of neural models in general .",0,0.8952426,33.12756562377701,17
2477,"In this paper , we combine contextualized representations with neural topic models .",1,0.6377456,40.88251310924544,13
2477,We find that our approach produces more meaningful and coherent topics than traditional bag-of-words topic models and recent neural models .,3,0.9759604,27.76680090399752,21
2477,Our results indicate that future improvements in language models will translate into better topic models .,3,0.9901703,45.36948942066332,16
2478,Neural semantic parsers have obtained acceptable results in the context of parsing DRSs ( Discourse Representation Structures ) .,0,0.8694178,50.3814422578383,19
2478,In particular models with character sequences as input showed remarkable performance for English .,3,0.93404555,202.60945328019432,14
2478,The results are promising .,3,0.92994523,83.47922013050588,5
2478,"Even with DRSs based on English , good results for Chinese are obtained .",3,0.83050543,126.55744741426075,14
2478,"Tokenisation offers a small advantage for English , but not for Chinese .",0,0.5846585,92.27072027030343,13
2478,"Overall , characters are preferred as input , both for English and Chinese .",3,0.906145,126.14384058152707,14
2479,Training datasets for semantic parsing are typically small due to the higher expertise required for annotation than most other NLP tasks .,0,0.84835947,43.82719882662245,22
2479,"As a result , models for this application usually need additional prior knowledge to be built into the architecture or algorithm .",0,0.82706517,50.918779764017856,22
2479,The increased dependency on human experts hinders automation and raises the development and maintenance costs in practice .,0,0.84903544,58.567611945783405,18
2479,This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance with minimal code-generation-specific inductive bias design .,1,0.9063537,38.870112137872326,23
2479,"By exploiting a relatively sizeable monolingual corpus of the target programming language , which is cheap to mine from the web , we achieved 81.03 % exact match accuracy on Django and 32.57 BLEU score on CoNaLa .",3,0.7940419,55.67909058020721,38
2479,Both are SOTA to the best of our knowledge .,3,0.5346076,25.52482330439554,10
2479,This positive evidence highlights a potentially easier path toward building accurate semantic parsers in practice .,3,0.9787644,110.12995856331598,16
2480,"The general format of natural language inference ( NLI ) makes it tempting to be used for zero-shot text classification by casting any target label into a sentence of hypothesis and verifying whether or not it could be entailed by the input , aiming at generic classification applicable on any specified label space .",0,0.91550523,64.36032523533517,54
2480,"In this opinion piece , we point out a few overlooked issues that are yet to be discussed in this line of work .",1,0.52907985,24.87369159080785,24
2480,We observe huge variance across different classification datasets amongst standard BERT-based NLI models and surprisingly find that pre-trained BERT without any fine-tuning can yield competitive performance against BERT fine-tuned for NLI .,3,0.9462538,21.118062107383363,34
2480,"With the concern that these models heavily rely on spurious lexical patterns for prediction , we also experiment with preliminary approaches for more robust NLI , but the results are in general negative .",3,0.56093574,53.30686695724557,34
2480,Our observations reveal implicit but challenging difficulties in entailment-based zero-shot text classification .,3,0.9830237,55.34545972751974,15
2481,"Commonsense reasoning aims to incorporate sets of commonsense facts , retrieved from Commonsense Knowledge Graphs ( CKG ) , to draw conclusion about ordinary situations .",0,0.9419823,68.5813197770445,26
2481,The dynamic nature of commonsense knowledge postulates models capable of performing multi-hop reasoning over new situations .,0,0.87185204,42.75502476169136,17
2481,"This feature also results in having large-scale sparse Knowledge Graphs , where such reasoning process is needed to predict relations between new events .",0,0.5666868,119.76431846612208,24
2481,"However , existing approaches in this area are limited by considering CKGs as a limited set of facts , thus rendering them unfit for reasoning over new unseen situations and events .",0,0.9161211,65.65194189806587,32
2481,"In this paper , we present a neural-symbolic reasoner , which is capable of reasoning over large-scale dynamic CKGs .",1,0.8461544,31.94232072167569,20
2481,The logic rules for reasoning over CKGs are learned during training by our model .,2,0.6208095,98.70865402437984,15
2481,"In addition to providing interpretable explanation , the learned logic rules help to generalise prediction to newly introduced events .",3,0.70360476,112.21899881487859,20
2481,Experimental results on the task of link prediction on CKGs prove the effectiveness of our model by outperforming the state-of-the-art models .,3,0.93313736,11.141378709580955,28
2482,"According to the self-determination theory , the levels of satisfaction of three basic needs ( competence , autonomy and relatedness ) have implications on people ’s everyday life and career .",0,0.9088153,51.69769067079202,31
2482,"We benchmark the novel task of automatically detecting those needs on short posts in English , by modelling it as a ternary classification task , and as three binary classification tasks .",2,0.69736516,98.66399664086995,32
2482,"A detailed manual analysis shows that the latter has advantages in the real-world scenario , and that our best models achieve similar performances as a trained human annotator .",3,0.9330562,35.2487526690252,29
2483,Recent studies on semantic frame induction show that relatively high performance has been achieved by using clustering-based methods with contextualized word embeddings .,0,0.86818695,22.123003224652322,25
2483,"However , there are two potential drawbacks to these methods : one is that they focus too much on the superficial information of the frame-evoking verb and the other is that they tend to divide the instances of the same verb into too many different frame clusters .",0,0.7624077,26.98274871446236,49
2483,"To overcome these drawbacks , we propose a semantic frame induction method using masked word embeddings and two-step clustering .",2,0.58006805,32.56636292156955,21
2483,"Through experiments on the English FrameNet data , we demonstrate that using the masked word embeddings is effective for avoiding too much reliance on the surface information of frame-evoking verbs and that two-step clustering can improve the number of resulting frame clusters for the instances of the same verb .",3,0.8414134,40.8792674369113,52
2484,Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP .,0,0.86387515,18.60869687725516,14
2484,"Adapter tuning consists in freezing pre-trained parameters of a model and injecting lightweight modules between layers , resulting in the addition of only a small number of task-specific trainable parameters .",0,0.4005097,45.743874403543,32
2484,"While adapter tuning was investigated for multilingual neural machine translation , this paper proposes a comprehensive analysis of adapters for multilingual speech translation ( ST ) .",1,0.6426179,62.40411566689668,27
2484,"Starting from different pre-trained models ( a multilingual ST trained on parallel data or a multilingual BART ( mBART ) trained on non parallel multilingual data ) , we show that adapters can be used to : ( a ) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters , and ( b ) transfer from an automatic speech recognition ( ASR ) task and an mBART pre-trained model to a multilingual ST task .",3,0.38976264,41.14938008739264,82
2484,"Experiments show that adapter tuning offer competitive results to full fine-tuning , while being much more parameter-efficient .",3,0.9597853,53.07212261352686,20
2485,The importance of parameter selection in supervised learning is well known .,0,0.8856982,25.105640851290364,12
2485,"However , due to the many parameter combinations , an incomplete or an insufficient procedure is often applied .",0,0.8277399,98.45256059548836,19
2485,This situation may cause misleading or confusing conclusions .,0,0.6693667,175.28610174073532,9
2485,"In this opinion paper , through an intriguing example we point out that the seriousness goes beyond what is generally recognized .",1,0.448514,124.88592047004931,22
2485,"In the topic of multilabel classification for medical code prediction , one influential paper conducted a proper parameter selection on a set , but when moving to a subset of frequently occurring labels , the authors used the same parameters without a separate tuning .",0,0.6593746,125.24319607957582,45
2485,"The set of frequent labels became a popular benchmark in subsequent studies , which kept pushing the state of the art .",0,0.7309041,62.97422197659228,22
2485,"However , we discovered that most of the results in these studies cannot surpass the approach in the original paper if a parameter tuning had been conducted at the time .",3,0.9777859,55.60416423626222,31
2485,Thus it is unclear how much progress the subsequent developments have actually brought .,0,0.90197283,54.358522860521624,14
2485,"The lesson clearly indicates that without enough attention on parameter selection , the research progress in our field can be uncertain or even illusive .",3,0.95033664,73.80612106772773,25
2486,Few-shot text classification aims to classify inputs whose label has only a few examples .,0,0.80076504,37.23007595731769,17
2486,Previous studies overlooked the semantic relevance between label representations .,0,0.9347788,232.08230142374146,10
2486,"Therefore , they are easily confused by labels that are relevant .",0,0.7363122,134.8735140336833,12
2486,"To address this problem , we propose a method that generates distinct label representations that embed information specific to each label .",1,0.37336668,28.0643563200767,22
2486,Our method is applicable to conventional few-shot classification models .,3,0.84690356,38.962422509763925,10
2486,Experimental results show that our method significantly improved the performance of few-shot text classification across models and datasets .,3,0.9772495,10.817343465273849,19
2487,"In real scenarios , a multilingual model trained to solve NLP tasks on a set of languages can be required to support new languages over time .",0,0.6560016,39.889632657400256,27
2487,"Unfortunately , the straightforward retraining on a dataset containing annotated examples for all the languages is both expensive and time-consuming , especially when the number of target languages grows .",0,0.69734716,38.308154493297614,32
2487,"Moreover , the original annotated material may no longer be available due to storage or business constraints .",0,0.8554969,63.861022208605306,18
2487,Re-training only with the new language data will inevitably result in Catastrophic Forgetting of previously acquired knowledge .,0,0.58223075,128.0856016064519,18
2487,"We propose a Continual Learning strategy that updates a model to support new languages over time , while maintaining consistent results on previously learned languages .",3,0.39277783,66.92769144342287,26
2487,"We define a Teacher-Student framework where the existing model “ teaches ” to a student model its knowledge about the languages it supports , while the student is also trained on a new language .",2,0.73982245,59.03473229549237,37
2487,"We report an experimental evaluation in several tasks including Sentence Classification , Relational Learning and Sequence Labeling .",3,0.5982644,89.22964448819823,18
2488,Transformer is important for text modeling .,0,0.70174956,62.385133842838826,7
2488,"However , it has difficulty in handling long documents due to the quadratic complexity with input text length .",0,0.89823955,49.784236255567,19
2488,"In order to handle this problem , we propose a hierarchical interactive Transformer ( Hi-Transformer ) for efficient and effective long document modeling .",1,0.36340353,56.55154003245077,24
2488,"Hi-Transformer models documents in a hierarchical way , i.e. , first learns sentence representations and then learns document representations .",2,0.55293053,59.63445260605581,21
2488,It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence .,3,0.7261199,111.94877716990402,19
2488,"More specifically , we first use a sentence Transformer to learn the representations of each sentence .",2,0.83731693,39.00294520108643,17
2488,Then we use a document Transformer to model the global document context from these sentence representations .,2,0.83771026,50.57396307433259,17
2488,"Next , we use another sentence Transformer to enhance sentence modeling using the global document context .",2,0.77449626,95.04032373411734,17
2488,"Finally , we use hierarchical pooling method to obtain document embedding .",2,0.7809021,92.16052686269191,12
2488,Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling .,3,0.74374723,28.2766194002097,18
2489,Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks .,0,0.8903372,16.785483584570073,22
2489,Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust .,0,0.7676887,31.801309410287757,22
2489,"In particular , the performance considerably varies as the random seed changes or the number of pretraining and / or fine-tuning iterations varies , and the fine-tuned model is vulnerable to adversarial attack .",3,0.8871344,39.974584551848814,34
2489,We propose a simple yet effective adapter-based approach to mitigate these issues .,1,0.45999527,18.16307518974363,14
2489,"Specifically , we insert small bottleneck layers ( i.e. , adapter ) within each layer of a pretrained model , then fix the pretrained layers and train the adapter layers on the downstream task data , with ( 1 ) task-specific unsupervised pretraining and then ( 2 ) task-specific supervised training ( e.g. , classification , sequence labeling ) .",2,0.8290815,53.35183887871162,64
2489,Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks .,3,0.9664649,38.294466009235535,23
2490,Natural Language Inference ( NLI ) datasets contain examples with highly ambiguous labels .,0,0.84846836,60.12005725943909,14
2490,"While many research works do not pay much attention to this fact , several recent efforts have been made to acknowledge and embrace the existence of ambiguity , such as UNLI and ChaosNLI .",0,0.8854627,45.2392677647554,34
2490,"In this paper , we explore the option of training directly on the estimated label distribution of the annotators in the NLI task , using a learning loss based on this ambiguity distribution instead of the gold-labels .",1,0.65112734,51.84480950494188,38
2490,"We prepare AmbiNLI , a trial dataset obtained from readily available sources , and show it is possible to reduce ChaosNLI divergence scores when finetuning on this data , a promising first step towards learning how to capture linguistic ambiguity .",3,0.6935908,118.4507227320737,41
2490,"Additionally , we show that training on the same amount of data but targeting the ambiguity distribution instead of gold-labels can result in models that achieve higher performance and learn better representations for downstream tasks .",3,0.9441456,36.78334201972959,38
2491,Detecting Out-of-Domain ( OOD ) or unknown intents from user queries is essential in a task-oriented dialog system .,0,0.9238219,27.328275162588298,21
2491,A key challenge of OOD detection is to learn discriminative semantic features .,0,0.8903453,34.0173513253507,13
2491,"Traditional cross-entropy loss only focuses on whether a sample is correctly classified , and does not explicitly distinguish the margins between categories .",0,0.70688635,60.24457421139252,23
2491,"In this paper , we propose a supervised contrastive learning objective to minimize intra-class variance by pulling together in-domain intents belonging to the same class and maximize inter-class variance by pushing apart samples from different classes .",1,0.8277884,26.328018057558033,37
2491,"Besides , we employ an adversarial augmentation mechanism to obtain pseudo diverse views of a sample in the latent space .",2,0.71638477,42.43629285433905,21
2491,Experiments on two public datasets prove the effectiveness of our method capturing discriminative representations for OOD detection .,3,0.7940961,23.757792355971993,18
2492,"Existing dialog state tracking ( DST ) models are trained with dialog data in a random order , neglecting rich structural information in a dataset .",0,0.855486,66.45709610831784,26
2492,"In this paper , we propose to use curriculum learning ( CL ) to better leverage both the curriculum structure and schema structure for task-oriented dialogs .",1,0.907943,42.74403748125216,29
2492,"Specifically , we propose a model-agnostic framework called Schema-aware Curriculum Learning for Dialog State Tracking ( SaCLog ) , which consists of a preview module that pre-trains a DST model with schema information , a curriculum module that optimizes the model with CL , and a review module that augments mispredicted data to reinforce the CL training .",2,0.7141364,32.27290117789757,59
2492,We show that our proposed approach improves DST performance over both a transformer-based and RNN-based DST model ( TripPy and TRADE ) and achieves new state-of-the-art results on WOZ2.0 and MultiWOZ2.1 .,3,0.91891843,14.190897887265368,41
2493,"Under the pandemic of COVID-19 , people experiencing COVID19-related symptoms have a pressing need to consult doctors .",0,0.9447273,22.436207276842403,21
2493,"Because of the shortage of medical professionals , many people cannot receive online consultations timely .",0,0.92242485,65.82776826990633,16
2493,"To address this problem , we aim to develop a medical dialog system that can provide COVID19-related consultations .",1,0.9112766,52.63506236332179,21
2493,We collected two dialog datasets – CovidDialog – ( in English and Chinese respectively ) containing conversations between doctors and patients about COVID-19 .,2,0.9427415,63.50046506733373,24
2493,"While the largest of their kind , these two datasets are still relatively small compared with general-domain dialog datasets .",0,0.5296442,33.65974407857858,22
2493,Training complex dialog generation models on small datasets bears high risk of overfitting .,0,0.7981088,58.2150629323413,14
2493,"To alleviate overfitting , we develop a multi-task learning approach , which regularizes the data-deficient dialog generation task with a masked token prediction task .",2,0.79821247,40.20740834022821,25
2493,Experiments on the CovidDialog datasets demonstrate the effectiveness of our approach .,3,0.83056056,17.946842909665058,12
2493,We perform both human evaluation and automatic evaluation of dialogs generated by our method .,2,0.7773233,25.791631217720642,15
2493,"Results show that the generated responses are promising in being doctor-like , relevant to conversation history , clinically informative and correct .",3,0.9881742,169.97900908581676,22
2493,The code and the data are available at https://github.com/UCSD-AI4H/COVID-Dialogue .,3,0.5808831,14.50776881246462,10
2494,"In multi-modal dialogue systems , it is important to allow the use of images as part of a multi-turn conversation .",0,0.848356,17.760598751573387,21
2494,"Training such dialogue systems generally requires a large-scale dataset consisting of multi-turn dialogues that involve images , but such datasets rarely exist .",0,0.82333875,38.49123986083577,23
2494,"In response , this paper proposes a 45 k multi-modal dialogue dataset created with minimal human intervention .",0,0.41886723,64.6401438720437,18
2494,"Our method to create such a dataset consists of ( 1 ) preparing and pre-processing text dialogue datasets , ( 2 ) creating image-mixed dialogues by using a text-to-image replacement technique , and ( 3 ) employing a contextual-similarity-based filtering step to ensure the contextual coherence of the dataset .",2,0.8544538,37.10068012256496,55
2494,"To evaluate the validity of our dataset , we devise a simple retrieval model for dialogue sentence prediction tasks .",2,0.6036946,63.11768089155442,20
2494,Automatic metrics and human evaluation results on such tasks show that our dataset can be effectively used as training data for multi-modal dialogue systems which require an understanding of images and text in a context-aware manner .,3,0.9678553,18.04396533433738,39
2494,Our dataset and generation code is available at https://github.com/shh1574/multi-modal-dialogue-dataset .,3,0.5863458,14.895435317389541,10
2495,Reducing and counter-acting hate speech on Social Media is a significant concern .,0,0.7832296,37.43198128413905,13
2495,"Most of the proposed automatic methods are conducted exclusively on English and very few consistently labeled , non-English resources have been proposed .",0,0.81010234,60.759015465721504,23
2495,Learning to detect hate speech on English and transferring to unseen languages seems an immediate solution .,3,0.57902646,90.89570006738856,17
2495,"This work is the first to shed light on the limits of this zero-shot , cross-lingual transfer learning framework for hate speech detection .",3,0.9385585,17.426880462576847,24
2495,"We use benchmark data sets in English , Italian , and Spanish to detect hate speech towards immigrants and women .",2,0.8587408,48.03463218303522,21
2495,"Investigating post-hoc explanations of the model , we discover that non-hateful , language-specific taboo interjections are misinterpreted as signals of hate speech .",3,0.9347197,60.524447293460405,24
2495,"Our findings demonstrate that zero-shot , cross-lingual models cannot be used as they are , but need to be carefully designed .",3,0.99122155,40.74551303711955,22
2496,Neural machine translation models are often biased toward the limited translation references seen during training .,0,0.9242924,60.62658341499456,16
2496,"To amend this form of overfitting , in this paper we propose fine-tuning the models with a novel training objective based on the recently-proposed BERTScore evaluation metric .",1,0.43884403,24.773345458659474,30
2496,BERTScore is a scoring function based on contextual embeddings that overcomes the typical limitations of n-gram-based metrics ( e.g .,0,0.4503013,25.634250979397773,21
2496,"synonyms , paraphrases ) , allowing translations that are different from the references , yet close in the contextual embedding space , to be treated as substantially correct .",3,0.57313895,137.32382510027946,29
2496,"To be able to use BERTScore as a training objective , we propose three approaches for generating soft predictions , allowing the network to remain completely differentiable end-to-end .",2,0.6095956,42.51969314195981,30
2496,"Experiments carried out over four , diverse language pairs show improvements of up to 0.58 pp ( 3.28 % ) in BLEU score and up to 0.76 pp ( 0.98 % ) in BERTScore ( F_BERT ) when fine-tuning a strong baseline .",3,0.9052048,25.77308573981142,43
2497,"Implicit discourse relation classification is a challenging task , in particular when the text domain is different from the standard Penn Discourse Treebank ( PDTB ; Prasad et al. , 2008 ) training corpus domain ( Wall Street Journal in 1990s ) .",0,0.8357986,89.6523358351453,43
2497,"We here tackle the task of implicit discourse relation classification on the biomedical domain , for which the Biomedical Discourse Relation Bank ( BioDRB ; Prasad et al. , 2011 ) is available .",2,0.38346297,48.7416071660884,34
2497,We show that entity information can be used to improve discourse relational argument representation .,3,0.89381313,79.34606651063314,15
2497,"In a first step , we show that explicitly marked instances that are content-wise similar to the target relations can be used to achieve good performance in the cross-domain setting using a simple unsupervised voting pipeline .",3,0.4498208,36.15845799167743,38
2497,"As a further step , we show that with the linked entity information from the first step , a transformer which is augmented with entity-related information ( KBERT ; Liu et al. , 2020 ) sets the new state of the art performance on the dataset , outperforming the large pre-trained BioBERT ( Lee et al. , 2020 ) model by 2 % points .",3,0.81763744,35.02220718563032,67
2498,"In this work , we propose Masked Noun-Phrase Prediction ( MNPP ) , a pre-training strategy to tackle pronoun resolution in a fully unsupervised setting .",1,0.6659307,35.89427637534177,27
2498,We evaluate our pre-trained model on various pronoun resolution datasets without any finetuning .,2,0.52387303,32.50342326811513,14
2498,Our method outperforms all previous unsupervised methods on all datasets by large margins .,3,0.8912433,12.69998362626339,14
2498,"Secondly , we proceed to a few-shot setting where we finetune our pre-trained model on WinoGrande-S and XS separately .",2,0.8121917,44.17750121518864,21
2498,"Our method outperforms RoBERTa-large baseline with large margins , meanwhile , achieving a higher AUC score after further finetuning on the remaining three official splits of WinoGrande .",3,0.87892145,117.44758683917586,30
2499,"Recently , question answering ( QA ) based on machine reading comprehension has become popular .",0,0.9649508,26.532379378083867,16
2499,This work focuses on generative QA which aims to generate an abstractive answer to a given question instead of extracting an answer span from a provided passage .,1,0.6532335,21.554056416080567,28
2499,"Generative QA often suffers from two critical problems : ( 1 ) summarizing content irrelevant to a given question , ( 2 ) drifting away from a correct answer during generation .",0,0.9288192,46.07691261618114,32
2499,"In this paper , we address these problems by a novel Rationale-Enriched Answer Generator ( REAG ) , which incorporates an extractive mechanism into a generative model .",1,0.7589406,37.151452570955726,30
2499,"Specifically , we add an extraction task on the encoder to obtain the rationale for an answer , which is the most relevant piece of text in an input document to a given question .",2,0.7972819,33.9240157181477,35
2499,"Based on the extracted rationale and original input , the decoder is expected to generate an answer with high confidence .",3,0.6087434,45.2087755714303,21
2499,We jointly train REAG on the MS MARCO QA + NLG task and the experimental results show that REAG improves the quality and semantic accuracy of answers over baseline models .,3,0.75616646,62.152911542290624,31
2500,"In news articles the lead bias is a common phenomenon that usually dominates the learning signals for neural extractive summarizers , severely limiting their performance on data with different or even no bias .",0,0.86991686,84.66926367566194,34
2500,"In this paper , we introduce a novel technique to demote lead bias and make the summarizer focus more on the content semantics .",1,0.8775664,55.57831896630327,24
2500,"Experiments on two news corpora with different degrees of lead bias show that our method can effectively demote the model ’s learned lead bias and improve its generality on out-of-distribution data , with little to no performance loss on in-distribution data .",3,0.9101504,21.388371272878903,46
2501,Machine reading comprehension ( MRC ) is a crucial task in natural language processing and has achieved remarkable advancements .,0,0.97416997,21.208435712922334,20
2501,"However , most of the neural MRC models are still far from robust and fail to generalize well in real-world applications .",0,0.85936254,15.242705589018247,22
2501,"In order to comprehensively verify the robustness and generalization of MRC models , we introduce a real-world Chinese dataset – DuReader_robust .",2,0.679911,50.036530245792754,22
2501,"It is designed to evaluate the MRC models from three aspects : over-sensitivity , over-stability and generalization .",2,0.38615853,40.80669218999217,18
2501,"Comparing to previous work , the instances in DuReader_robust are natural texts , rather than the altered unnatural texts .",3,0.9011773,288.3130198439767,20
2501,It presents the challenges when applying MRC models to real-world applications .,3,0.586331,30.44715318725103,12
2501,The experimental results show that MRC models do not perform well on the challenge test set .,3,0.9808996,18.72588154264796,17
2501,"Moreover , we analyze the behavior of existing models on the challenge test set , which may provide suggestions for future model development .",3,0.7192967,41.59651444992129,24
2501,The dataset and codes are publicly available at https://github.com/baidu/DuReader .,3,0.5835862,11.061506865794067,10
2502,"With the recent advancements in deep learning , neural solvers have gained promising results in solving math word problems .",0,0.9352692,37.349850287285236,20
2502,"However , these SOTA solvers only generate binary expression trees that contain basic arithmetic operators and do not explicitly use the math formulas .",0,0.83322984,63.496226095443234,24
2502,"As a result , the expression trees they produce are lengthy and uninterpretable because they need to use multiple operators and constants to represent one single formula .",0,0.81427056,66.83134983494028,28
2502,"In this paper , we propose sequence-to-general tree ( S2G ) that learns to generate interpretable and executable operation trees where the nodes can be formulas with an arbitrary number of arguments .",1,0.7630814,51.20272148081722,36
2502,"With nodes now allowed to be formulas , S2G can learn to incorporate mathematical domain knowledge into problem-solving , making the results more interpretable .",3,0.725719,93.35655315398637,26
2502,Experiments show that S2G can achieve a better performance against strong baselines on problems that require domain knowledge .,3,0.9411013,22.62399175533642,19
2503,Understanding the multi-scale visual information in a video is essential for Video Question Answering ( VideoQA ) .,0,0.9535512,22.56701150447986,18
2503,"Therefore , we propose a novel Multi-Scale Progressive Attention Network ( MSPAN ) to achieve relational reasoning between cross-scale video information .",1,0.77523965,56.05176477590197,22
2503,We construct clips of different lengths to represent different scales of the video .,2,0.8162944,56.49709518880292,14
2503,"Then , the clip-level features are aggregated into node features by using max-pool , and a graph is generated for each scale of clips .",2,0.8628885,70.89973588636019,25
2503,"For cross-scale feature interaction , we design a message passing strategy between adjacent scale graphs , i.e. , top-down scale interaction and bottom-up scale interaction .",2,0.905263,80.10940698244569,28
2503,"Under the question ’s guidance of progressive attention , we realize the fusion of all-scale video features .",3,0.4871715,431.06952919697073,18
2503,"Experimental evaluations on three benchmarks : TGIF-QA , MSVD-QA and MSRVTT-QA show our method has achieved state-of-the-art performance .",3,0.83041316,22.85931672701684,30
2504,Most state-of-the-art open-domain question answering systems use a neural retrieval model to encode passages into continuous vectors and extract them from a knowledge source .,0,0.90055186,19.14014931176391,29
2504,"However , such retrieval models often require large memory to run because of the massive size of their passage index .",0,0.88289493,67.49611146213644,21
2504,"In this paper , we introduce Binary Passage Retriever ( BPR ) , a memory-efficient neural retrieval model that integrates a learning-to-hash technique into the state-of-the-art Dense Passage Retriever ( DPR ) to represent the passage index using compact binary codes rather than continuous vectors .",1,0.7498908,33.383687065586344,57
2504,BPR is trained with a multi-task objective over two tasks : efficient candidate generation based on binary codes and accurate reranking based on continuous vectors .,2,0.6787937,65.70433653664396,26
2504,"Compared with DPR , BPR substantially reduces the memory cost from 65 GB to 2GB without a loss of accuracy on two standard open-domain question answering benchmarks : Natural Questions and TriviaQA .",3,0.87960535,43.30678649797481,33
2504,Our code and trained models are available at https://github.com/studio-ousia/bpr .,3,0.5923473,16.80757698653253,10
2505,"Few-shot relation extraction ( FSRE ) is of great importance in long-tail distribution problem , especially in special domain with low-resource data .",0,0.9093917,47.796534388518644,25
2505,"Most existing FSRE algorithms fail to accurately classify the relations merely based on the information of the sentences together with the recognized entity pairs , due to limited samples and lack of knowledge .",0,0.797088,81.82840740635629,34
2505,"To address this problem , in this paper , we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction scheme ( ConceptFERE ) , which introduces the inherent concepts of entities to provide clues for relation prediction and boost the relations classification performance .",1,0.7386676,68.75282758618786,47
2505,"Firstly , a concept-sentence attention module is developed to select the most appropriate concept from multiple concepts of each entity by calculating the semantic similarity between sentences and concepts .",2,0.8086877,32.95198111318398,30
2505,"Secondly , a self-attention based fusion module is presented to bridge the gap of concept embedding and sentence embedding from different semantic spaces .",2,0.5413985,33.05239672684992,24
2505,Extensive experiments on the FSRE benchmark dataset FewRel have demonstrated the effectiveness and the superiority of the proposed ConceptFERE scheme as compared to the state-of-the-art baselines .,3,0.8415387,21.73631274927391,33
2505,Code is available at https://github.com/LittleGuoKe/ConceptFERE .,3,0.5241225,24.67132179384254,6
2506,Generalization is an important ability that helps to ensure that a machine learning model can perform well on unseen data .,0,0.92079145,16.840970377288162,21
2506,"In this paper , we study the effect of data bias on model generalization , using Chinese Named Entity Recognition ( NER ) as a case study .",1,0.9089902,23.327277733989146,28
2506,"Specifically , we analyzed five benchmarking datasets for Chinese NER , and observed the following two types of data bias that can compromise model generalization ability .",2,0.6003595,58.28152838471993,27
2506,"Firstly , the test sets of all the five datasets contain a significant proportion of entities that have been seen in the training sets .",3,0.85962266,27.854802310417234,25
2506,Such test data would therefore not be able to reflect the true generalization ability of a model .,0,0.5353083,24.57445777123884,18
2506,"Secondly , all datasets are dominated by a few fat-head entities , i.e. , entities appearing with particularly high frequency .",3,0.8996754,82.37469150707423,21
2506,"As a result , a model might be able to produce high prediction accuracy simply by keyword memorization without leveraging context knowledge .",3,0.7852085,64.44068951314043,23
2506,"To address these data biases , we first refine each test set by excluding seen entities from it , so as to better evaluate a model ’s generalization ability .",2,0.8136594,62.04122194010541,30
2506,"Then , we propose a simple yet effective entity resampling method to make entities within the same category distributed equally , encouraging a model to leverage both name and context knowledge in the training process .",2,0.5331757,46.01045381639055,36
2506,"Experimental results demonstrate that the proposed entity resampling method significantly improves a model ’s ability in detecting unseen entities , especially for company , organization and position categories .",3,0.97958857,55.988002374484424,29
2507,Document-level Relation Extraction ( RE ) is a more challenging task than sentence RE as it often requires reasoning over multiple sentences .,0,0.9309797,24.075489876721686,23
2507,"Yet , human annotators usually use a small number of sentences to identify the relationship between a given entity pair .",0,0.9252527,23.449530724430215,21
2507,"In this paper , we present an embarrassingly simple but effective method to heuristically select evidence sentences for document-level RE , which can be easily combined with BiLSTM to achieve good performance on benchmark datasets , even better than fancy graph neural network based methods .",1,0.7561376,31.693630264251414,48
2507,We have released our code at https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need .,3,0.47806406,12.32959203805056,8
2508,Learning prerequisite chains is an important task for one to pick up knowledge efficiently in both known and unknown domains .,0,0.9060247,72.35587560383571,21
2508,"For example , one may be an expert in the natural language processing ( NLP ) domain , but want to determine the best order in which to learn new concepts in an unfamiliar Computer Vision domain ( CV ) .",0,0.884392,42.816332164198556,41
2508,"Both domains share some common concepts , such as machine learning basics and deep learning models .",0,0.7962481,58.405523687715,17
2508,"In this paper , we solve the task of unsupervised cross-domain concept prerequisite chain learning , using an optimized variational graph autoencoder .",1,0.69849634,48.030498063048036,23
2508,"Our model learns to transfer concept prerequisite relations from an information-rich domain ( source domain ) to an information-poor domain ( target domain ) , substantially surpassing other baseline models .",3,0.77786565,49.872706840946314,35
2508,"In addition , we expand an existing dataset by introducing two new domains —-CV and Bioinformatics ( BIO ) .",2,0.7177374,65.17160628368373,22
2508,The annotated data and resources as well as the code will be made publicly available .,3,0.55565816,15.369062316567057,16
2509,We present a text representation approach that can combine different views ( representations ) of the same input through effective data fusion and attention strategies for ranking purposes .,1,0.37240887,117.32517130240896,29
2509,"We apply our model to the problem of differential diagnosis , which aims to find the most probable diseases that match with clinical descriptions of patients , using data from the Undiagnosed Diseases Network .",2,0.6862556,29.323755093661024,35
2509,Our model outperforms several ranking approaches ( including a commercially-supported system ) by effectively prioritizing and combining representations obtained from traditional and recent text representation techniques .,3,0.83792627,94.12833694154295,29
2509,We elaborate on several aspects of our model and shed light on its improved performance .,3,0.5075932,21.05725224791211,16
2510,Crowdworker-constructed natural language inference ( NLI ) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance ( CITATION ) .,0,0.93961275,46.684230865901334,38
2510,"We investigate whether MedNLI , a physician-annotated dataset with premises extracted from clinical notes , contains such artifacts ( CITATION ) .",1,0.8188301,130.22027106025402,24
2510,"We find that entailed hypotheses contain generic versions of specific concepts in the premise , as well as modifiers related to responsiveness , duration , and probability .",3,0.97311497,163.71855454873233,28
2510,"Neutral hypotheses feature conditions and behaviors that co-occur with , or cause , the condition ( s ) in the premise .",0,0.755349,129.27782218803196,22
2510,Contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health .,0,0.7129126,152.03834040968894,17
2510,Adversarial filtering demonstrates that performance degrades when evaluated on the difficult subset .,3,0.8634632,92.08066844706516,13
2510,We provide partition information and recommendations for alternative dataset construction strategies for knowledge-intensive domains .,3,0.71706116,122.9229541120083,16
2511,"With the explosion of chatbot applications , Conversational Question Answering ( CQA ) has generated a lot of interest in recent years .",0,0.9710575,11.130479884384881,23
2511,"Among proposals , reading comprehension models which take advantage of the conversation history ( previous QA ) seem to answer better than those which only consider the current question .",0,0.5958495,75.62778815786108,30
2511,"Nevertheless , we note that the CQA evaluation protocol has a major limitation .",3,0.7879121,38.317133653833416,14
2511,"In particular , models are allowed , at each turn of the conversation , to access the ground truth answers of the previous turns .",2,0.49751073,72.88307949255613,25
2511,"Not only does this severely prevent their applications in fully autonomous chatbots , it also leads to unsuspected biases in their behavior .",0,0.81850994,55.86405138450912,23
2511,"In this paper , we highlight this effect and propose new tools for evaluation and training in order to guard against the noted issues .",1,0.8011846,65.17903392912315,25
2511,The new results that we bring come to reinforce methods of the current state of the art .,3,0.89099383,57.21386830416923,18
2512,"Existing models on Machine Reading Comprehension ( MRC ) require complex model architecture for effectively modeling long texts with paragraph representation and classification , thereby making inference computationally inefficient for production use .",0,0.9351986,61.52513563226613,33
2512,"In this work , we propose VAULT : a light-weight and parallel-efficient paragraph representation for MRC based on contextualized representation from long document input , trained using a new Gaussian distribution-based objective that pays close attention to the partially correct instances that are close to the ground-truth .",1,0.49094456,59.74084087187586,54
2512,We validate our VAULT architecture showing experimental results on two benchmark MRC datasets that require long context modeling ;,3,0.7062531,354.01523287634893,19
2512,one Wikipedia-based ( Natural Questions ( NQ ) ) and the other on TechNotes ( TechQA ) .,2,0.67696536,164.34130613595954,19
2512,"VAULT can achieve comparable performance on NQ with a state-of-the-art ( SOTA ) complex document modeling approach while being 16 times faster , demonstrating the efficiency of our proposed model .",3,0.9325395,36.04055364078446,34
2512,We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM .,3,0.9543453,38.62844175585412,33
2513,Leveraging additional unlabeled data to boost model performance is common practice in machine learning and natural language processing .,0,0.90792245,15.442209772123535,19
2513,"For generation tasks , if there is overlap between the additional data and the target text evaluation data , then training on the additional data is training on answers of the test set .",2,0.44233274,53.45816999937878,34
2513,This leads to overly-inflated scores with the additional data compared to real-world testing scenarios and problems when comparing models .,3,0.607624,55.10561378494763,22
2513,"We study the AMR dataset and Gigaword , which is popularly used for improving AMR-to-text generators , and find significant overlap between Gigaword and a subset of the AMR dataset .",2,0.5053717,27.36383418768107,35
2513,"We propose methods for excluding parts of Gigaword to remove this overlap , and show that our approach leads to a more realistic evaluation of the task of AMR-to-text generation .",3,0.64742917,40.9627913806084,35
2513,"Going forward , we give simple best-practice recommendations for leveraging additional data in AMR-to-text generation .",3,0.8765678,41.14926235824028,20
2514,Social media has become a valuable resource for the study of suicidal ideation and the assessment of suicide risk .,0,0.95458376,12.210840274548696,20
2514,"Among social media platforms , Reddit has emerged as the most promising one due to its anonymity and its focus on topic-based communities ( subreddits ) that can be indicative of someone ’s state of mind or interest regarding mental health disorders such as r/SuicideWatch , r/ Anxiety , r/depression .",0,0.87599933,42.97048755309381,51
2514,A challenge for previous work on suicide risk assessment has been the small amount of labeled data .,0,0.91233945,30.237291346305227,18
2514,"We propose an empirical investigation into several classes of weakly-supervised approaches , and show that using pseudo-labeling based on related issues around mental health ( e.g. , anxiety , depression ) helps improve model performance for suicide risk assessment .",3,0.7052012,57.93851943860876,42
2515,The Shuffle Test is the most common task to evaluate whether NLP models can measure coherence in text .,0,0.8384149,30.450790247133234,19
2515,Most recent work uses direct supervision on the task ;,0,0.73308927,179.459321092881,10
2515,"we show that by simply finetuning a RoBERTa model , we can achieve a near perfect accuracy of 97.8 % , a state-of-the-art .",3,0.886384,16.410731586215228,29
2515,"We argue that this outstanding performance is unlikely to lead to a good model of text coherence , and suggest that the Shuffle Test should be approached in a Zero-Shot setting : models should be evaluated without being trained on the task itself .",3,0.9251075,62.59613225985484,46
2515,"We evaluate common models in this setting , such as Generative and Bi-directional Transformers , and find that larger architectures achieve high-performance out-of-the-box .",3,0.7925775,32.19243298062431,29
2515,"Finally , we suggest the k-Block Shuffle Test , a modification of the original by increasing the size of blocks shuffled .",3,0.8827047,85.21959858162377,22
2515,"Even though human reader performance remains high ( around 95 % accuracy ) , model performance drops from 94 % to 78 % as block size increases , creating a conceptually simple challenge to benchmark NLP models .",3,0.7279384,65.40241329550243,38
2516,"In this paper , we present a conceptually simple while empirically powerful framework for abstractive summarization , SimCLS , which can bridge the gap between the learning objective and evaluation metrics resulting from the currently dominated sequence-to-sequence learning framework by formulating text generation as a reference-free evaluation problem ( i.e. , quality estimation ) assisted by contrastive learning .",1,0.73690826,44.02432646511038,62
2516,"Experimental results show that , with minor modification over existing top-scoring systems , SimCLS can improve the performance of existing top-performing models by a large margin .",3,0.96472466,23.535686353477146,28
2516,"Particularly , 2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on the CNN / DailyMail dataset , driving the state-of-the-art performance to a new level .",3,0.95929027,32.84450884797958,37
2516,We have open-sourced our codes and results : https://github.com/yixinL7/SimCLS .,3,0.81456035,31.38037595061925,10
2516,"Results of our proposed models have been deployed into ExplainaBoard platform , which allows researchers to understand our systems in a more fine-grained way .",3,0.89514244,65.0818267600022,27
2517,"In this work , we introduce a corpus for satire detection in Romanian news .",1,0.8185623,71.06741685264655,15
2517,"We gathered 55,608 public news articles from multiple real and satirical news sources , composing one of the largest corpora for satire detection regardless of language and the only one for the Romanian language .",2,0.8650315,56.56853108609607,35
2517,"We provide an official split of the text samples , such that training news articles belong to different sources than test news articles , thus ensuring that models do not achieve high performance simply due to overfitting .",2,0.63012594,74.72050545580807,38
2517,"We conduct experiments with two state-of-the-art deep neural models , resulting in a set of strong baselines for our novel corpus .",2,0.6943548,16.990522154469833,28
2517,"Our results show that the machine-level accuracy for satire detection in Romanian is quite low ( under 73 % on the test set ) compared to the human-level accuracy ( 87 % ) , leaving enough room for improvement in future research .",3,0.98731893,33.556779242328936,45
2518,Faceted summarization provides briefings of a document from different perspectives .,0,0.7026264,84.68315328197154,11
2518,Readers can quickly comprehend the main points of a long document with the help of a structured outline .,0,0.4992501,27.164406705607192,19
2518,"However , little research has been conducted on this subject , partially due to the lack of large-scale faceted summarization datasets .",0,0.957829,24.608046958597456,22
2518,"In this study , we present FacetSum , a faceted summarization benchmark built on Emerald journal articles , covering a diverse range of domains .",1,0.88645315,89.3127794561841,25
2518,"Different from traditional document-summary pairs , FacetSum provides multiple summaries , each targeted at specific sections of a long document , including the purpose , method , findings , and value .",0,0.5725277,123.6357101441045,32
2518,Analyses and empirical results on our dataset reveal the importance of bringing structure into summaries .,3,0.9696341,45.31715487406213,16
2518,We believe FacetSum will spur further advances in summarization research and foster the development of NLP systems that can leverage the structured information in both long texts and summaries .,3,0.95254636,28.40594985561901,30
2519,Søgaard ( 2020 ) obtained results suggesting the fraction of trees occurring in the test data isomorphic to trees in the training set accounts for a non-trivial variation in parser performance .,0,0.5079809,43.36948480531415,32
2519,"Similar to other statistical analyses in NLP , the results were based on evaluating linear regressions .",3,0.66008013,50.95159256639431,17
2519,"However , the study had methodological issues and was undertaken using a small sample size leading to unreliable results .",3,0.8578459,30.8805098748411,20
2519,We present a replication study in which we also bin sentences by length and find that only a small subset of sentences vary in performance with respect to graph isomorphism .,3,0.80801576,30.997329993952167,31
2519,"Further , the correlation observed between parser performance and graph isomorphism in the wild disappears when controlling for covariants .",3,0.9682768,114.13434427087192,20
2519,"However , in a controlled experiment , where covariants are kept fixed , we do observe a correlation .",3,0.9289212,103.93437885829317,19
2519,We suggest that conclusions drawn from statistical analyses like this need to be tempered and that controlled experiments can complement them by more readily teasing factors apart .,3,0.964915,63.866351409915055,28
2520,High-performing machine translation ( MT ) systems can help overcome language barriers while making it possible for everyone to communicate and use language technologies in the language of their choice .,0,0.94459677,31.28670344579825,33
2520,"However , such systems require large amounts of parallel sentences for training , and translators can be difficult to find and expensive .",0,0.86783826,43.47783932882537,23
2520,"Here , we present a data collection strategy for MT which , in contrast , is cheap and simple , as it does not require bilingual speakers .",1,0.79682624,94.23589526041835,28
2520,"Based on the insight that humans pay specific attention to movements , we use graphics interchange formats ( GIFs ) as a pivot to collect parallel sentences from monolingual annotators .",2,0.6705041,71.68172763029206,31
2520,"We use our strategy to collect data in Hindi , Tamil and English .",2,0.85317796,48.089176307555924,14
2520,"As a baseline , we also collect data using images as a pivot .",2,0.80084485,65.69619117160595,14
2520,"We perform an intrinsic evaluation by manually evaluating a subset of the sentence pairs and an extrinsic evaluation by finetuning mBART ( Liu et al. , 2020 ) on the collected data .",2,0.84927416,29.722387192853272,33
2520,We find that sentences collected via GIFs are indeed of higher quality .,3,0.98063517,70.56751155887483,13
2521,Pre-trained language models have shown stellar performance in various downstream tasks .,0,0.7711256,6.571074178794407,12
2521,"But , this usually comes at the cost of high latency and computation , hindering their usage in resource-limited settings .",0,0.90056837,32.3832046552929,21
2521,"In this work , we propose a novel approach for reducing the computational cost of BERT with minimal loss in downstream performance .",1,0.8954162,16.89283008369394,23
2521,"Our method dynamically eliminates less contributing tokens through layers , resulting in shorter lengths and consequently lower computational cost .",3,0.73084325,269.16703791833584,20
2521,"To determine the importance of each token representation , we train a Contribution Predictor for each layer using a gradient-based saliency method .",2,0.8798601,41.29129184759402,25
2521,Our experiments on several diverse classification tasks show speedups up to 22x during inference time without much sacrifice in performance .,3,0.90774953,52.88733875082687,21
2521,We also validate the quality of the selected tokens in our method using human annotations in the ERASER benchmark .,3,0.5840349,39.72659658821089,20
2521,"In comparison to other widely used strategies for selecting important tokens , such as saliency and attention , our proposed method has a significantly lower false positive rate in generating rationales .",3,0.93803364,41.630912227462076,32
2521,Our code is freely available at https://github.com/amodaresi/AdapLeR .,3,0.52279276,18.573950386888743,8
2522,This paper describes and tests a method for carrying out quantified reproducibility assessment ( QRA ) that is based on concepts and definitions from metrology .,1,0.84770405,46.50652825111049,26
2522,"QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure , on the basis of the scores from , and differences between , different reproductions .",0,0.5095154,65.79127283684184,33
2522,"We test QRA on 18 different system and evaluation measure combinations ( involving diverse NLP tasks and types of evaluation ) , for each of which we have the original results and one to seven reproduction results .",2,0.89322937,146.66472009948808,38
2522,"The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same , but also of different , original studies .",3,0.9330273,55.113287029182125,29
2522,"We find that the proposed method facilitates insights into causes of variation between reproductions , and as a result , allows conclusions to be drawn about what aspects of system and / or evaluation design need to be changed in order to improve reproducibility .",3,0.9762364,36.74778001244235,45
2523,Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape .,0,0.9175049,26.834173059848677,26
2523,"This phenomenon , called the representation degeneration problem , facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models .",0,0.7379334,45.148789313291005,28
2523,"Although the existing methods that address the degeneration problem based on observations of the phenomenon triggered by the problem improves the performance of the text generation , the training dynamics of token embeddings behind the degeneration problem are still not explored .",0,0.7804481,39.265106395086995,42
2523,"In this study , we analyze the training dynamics of the token embeddings focusing on rare token embedding .",1,0.8387535,48.810754066592814,19
2523,We demonstrate that the specific part of the gradient for rare token embeddings is the key cause of the degeneration problem for all tokens during training stage .,3,0.9669952,47.01949646310622,28
2523,"Based on the analysis , we propose a novel method called , adaptive gradient gating ( AGG ) .",2,0.4657696,60.50560444774668,19
2523,AGG addresses the degeneration problem by gating the specific part of the gradient for rare token embeddings .,3,0.4017367,49.896993316568675,18
2523,"Experimental results from language modeling , word similarity , and machine translation tasks quantitatively and qualitatively verify the effectiveness of AGG .",3,0.88980407,46.90997605705345,22
2524,Large Pre-trained Language Models ( PLMs ) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances .,0,0.95728105,17.78703294490424,28
2524,"While advances reported for English using PLMs are unprecedented , reported advances using PLMs for Hebrew are few and far between .",0,0.7796738,128.26792087292014,22
2524,The problem is twofold .,0,0.4442156,25.740397809212215,5
2524,"First , so far , Hebrew resources for training large language models are not of the same magnitude as their English counterparts .",0,0.7549157,49.65027972960463,23
2524,"Second , most benchmarks available to evaluate progress in Hebrew NLP require morphological boundaries which are not available in the output of standard PLMs .",0,0.7067643,71.8328620517099,25
2524,In this work we remedy both aspects .,1,0.36457834,163.4975397977006,8
2524,"We present AlephBERT , a large PLM for Modern Hebrew , trained on larger vocabulary and a larger dataset than any Hebrew PLM before .",3,0.33773845,115.36574043132389,25
2524,"Moreover , we introduce a novel neural architecture that recovers the morphological segments encoded in contextualized embedding vectors .",2,0.5077522,35.12579920846658,19
2524,"Based on this new morphological component we offer an evaluation suite consisting of multiple tasks and benchmarks that cover sentence-level , word-level and sub-word level analyses .",2,0.4876681,38.80451759033919,30
2524,"On all tasks , AlephBERT obtains state-of-the-art results beyond contemporary Hebrew baselines .",3,0.95794034,47.71319147681235,18
2524,"We make our AlephBERT model , the morphological extraction model , and the Hebrew evaluation suite publicly available , for evaluating future Hebrew PLMs .",3,0.43142557,223.24362933122293,25
2525,Neural discrete reasoning ( NDR ) has shown remarkable progress in combining deep models with discrete reasoning .,0,0.9505077,66.70994880358074,18
2525,"However , we find that existing NDR solution suffers from large performance drop on hypothetical questions , e.g .",3,0.9577823,74.22215020329068,19
2525,“ what the annualized rate of return would be if the revenue in 2020 was doubled ” .,0,0.54488665,94.49561835366754,18
2525,"The key to hypothetical question answering ( HQA ) is counterfactual thinking , which is a natural ability of human reasoning but difficult for deep models .",0,0.96150315,89.33241452176166,27
2525,"In this work , we devise a Learning to Imagine ( L2I ) module , which can be seamlessly incorporated into NDR models to perform the imagination of unseen counterfactual .",1,0.6753384,76.38918884492847,31
2525,"In particular , we formulate counterfactual thinking into two steps : 1 ) identifying the fact to intervene , and 2 ) deriving the counterfactual from the fact and assumption , which are designed as neural networks .",2,0.73358756,51.423716562638326,38
2525,"Based on TAT-QA , we construct a very challenging HQA dataset with 8,283 hypothetical questions .",2,0.6643874,87.78153152674362,18
2525,"We apply the proposed L2I to TAGOP , the state-of-the-art solution on TAT-QA , validating the rationality and effectiveness of our approach .",3,0.6680719,42.9031183204019,30
2526,Complex word identification ( CWI ) is a cornerstone process towards proper text simplification .,0,0.9650145,102.09673768980892,15
2526,"CWI is highly dependent on context , whereas its difficulty is augmented by the scarcity of available datasets which vary greatly in terms of domains and languages .",0,0.87273043,42.29305743651087,28
2526,"As such , it becomes increasingly more difficult to develop a robust model that generalizes across a wide array of input examples .",0,0.93357736,25.61082339206467,23
2526,"In this paper , we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations .",1,0.8941546,27.636024818779745,27
2526,"This technique addresses the problem of working with multiple domains , inasmuch as it creates a way of smoothing the differences between the explored datasets .",2,0.43237188,45.840244709682025,26
2526,"Moreover , we also propose a similar auxiliary task , namely text simplification , that can be used to complement lexical complexity prediction .",3,0.46992987,60.32831367470767,24
2526,"Our model obtains a boost of up to 2.42 % in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques , when considering the CompLex from the Lexical Complexity Prediction 2021 dataset .",3,0.9150324,63.08653833149092,35
2526,"At the same time , we obtain an increase of 3 % in Pearson scores , while considering a cross-lingual setup relying on the Complex Word Identification 2018 dataset .",3,0.93118864,79.6944965232352,30
2526,"In addition , our model yields state-of-the-art results in terms of Mean Absolute Error .",3,0.93874526,14.180261165291986,21
2527,Zero-shot stance detection ( ZSSD ) aims to detect the stance for an unseen target during the inference stage .,0,0.8590128,29.800119838583548,20
2527,"In this paper , we propose a joint contrastive learning ( JointCL ) framework , which consists of stance contrastive learning and target-aware prototypical graph contrastive learning .",1,0.80957055,43.777165202160994,30
2527,"Specifically , a stance contrastive learning strategy is employed to better generalize stance features for unseen targets .",2,0.72346395,34.56983996442328,18
2527,"Further , we build a prototypical graph for each instance to learn the target-based representation , in which the prototypes are deployed as a bridge to share the graph structures between the known targets and the unseen ones .",2,0.78640515,42.82956404731993,41
2527,Then a novel target-aware prototypical graph contrastive learning strategy is devised to generalize the reasoning ability of target-based stance representations to the unseen targets .,2,0.678622,35.597856086031584,29
2527,Extensive experiments on three benchmark datasets show that the proposed approach achieves state-of-the-art performance in the ZSSD task .,3,0.8947793,8.227231209660554,25
2528,The recent success of reinforcement learning ( RL ) in solving complex tasks is often attributed to its capacity to explore and exploit an environment .,0,0.9688561,33.95596263171651,26
2528,Sample efficiency is usually not an issue for tasks with cheap simulators to sample data online .,0,0.7424636,119.21830023520454,17
2528,"On the other hand , Task-oriented Dialogues ( ToD ) are usually learnt from offline data collected using human demonstrations .",0,0.86650354,51.54647881784077,21
2528,Collecting diverse demonstrations and annotating them is expensive .,0,0.8818445,42.16305006761385,9
2528,"Unfortunately , RL policy trained on off-policy data are prone to issues of bias and generalization , which are further exacerbated by stochasticity in human response and non-markovian nature of annotated belief state of a dialogue management system .",0,0.7611724,78.83610540188361,39
2528,"To this end , we propose a batch-RL framework for ToD policy learning : Causal-aware Safe Policy Improvement ( CASPI ) .",2,0.43920812,145.86983063292328,26
2528,CASPI includes a mechanism to learn fine-grained reward that captures intention behind human response and also offers guarantee on dialogue policy ’s performance against a baseline .,3,0.3537302,166.83500803449195,28
2528,We demonstrate the effectiveness of this framework on end-to-end dialogue task of the Multiwoz 2.0 dataset .,3,0.73514605,21.1560445540742,19
2528,The proposed method outperforms the current state of the art .,3,0.9054143,10.947676073279768,11
2528,"Further more we demonstrate sample efficiency , where our method trained only on 20 % of the data , are comparable to current state of the art method trained on 100 % data on two out of there evaluation metrics .",3,0.896807,72.30565816973608,41
2529,"As a more natural and intelligent interaction manner , multimodal task-oriented dialog system recently has received great attention and many remarkable progresses have been achieved .",0,0.94190055,64.24050275933698,28
2529,"Nevertheless , almost all existing studies follow the pipeline to first learn intra-modal features separately and then conduct simple feature concatenation or attention-based feature fusion to generate responses , which hampers them from learning inter-modal interactions and conducting cross-modal feature alignment for generating more intention-aware responses .",0,0.84059435,46.65002858815523,51
2529,"To address these issues , we propose UniTranSeR , a Unified Transformer Semantic Representation framework with feature alignment and intention reasoning for multimodal dialog systems .",1,0.5265941,43.04216980936424,26
2529,"Specifically , we first embed the multimodal features into a unified Transformer semantic space to prompt inter-modal interactions , and then devise a feature alignment and intention reasoning ( FAIR ) layer to perform cross-modal entity alignment and fine-grained key-value reasoning , so as to effectively identify user ’s intention for generating more accurate responses .",2,0.83462924,47.91200833734362,59
2529,"Experimental results verify the effectiveness of UniTranSeR , showing that it significantly outperforms state-of-the-art approaches on the representative MMD dataset .",3,0.96447533,15.283167246217987,27
2530,Dialogue State Tracking ( DST ) aims to keep track of users ’ intentions during the course of a conversation .,0,0.9324511,27.040522612386262,21
2530,"In DST , modelling the relations among domains and slots is still an under-studied problem .",0,0.85518664,35.82725394295373,16
2530,"Existing approaches that have considered such relations generally fall short in : ( 1 ) fusing prior slot-domain membership relations and dialogue-aware dynamic slot relations explicitly , and ( 2 ) generalizing to unseen domains .",0,0.7784528,141.54573642456006,38
2530,"To address these issues , we propose a novel Dynamic Schema Graph Fusion Network ( DSGFNet ) , which generates a dynamic schema graph to explicitly fuse the prior slot-domain membership relations and dialogue-aware dynamic slot relations .",2,0.49125567,75.69384676876533,40
2530,It also uses the schemata to facilitate knowledge transfer to new domains .,2,0.36575717,35.38735359267196,13
2530,"DSGFNet consists of a dialogue utterance encoder , a schema graph encoder , a dialogue-aware schema graph evolving network , and a schema graph enhanced dialogue state decoder .",2,0.5705829,58.899738945190784,31
2530,"Empirical results on benchmark datasets ( i.e. , SGD , MultiWOZ2.1 , and MultiWOZ2.2 ) , show that DSGFNet outperforms existing methods .",3,0.86673814,24.089855751392196,23
2531,"Recent progress of abstractive text summarization largely relies on large pre-trained sequence-to-sequence Transformer models , which are computationally expensive .",0,0.9421636,14.167203623897919,20
2531,This paper aims to distill these large models into smaller ones for faster inference and with minimal performance loss .,1,0.91776955,43.050000468007546,20
2531,Pseudo-labeling based methods are popular in sequence-to-sequence model distillation .,0,0.80032825,18.196766711651797,11
2531,"In this paper , we find simply manipulating attention temperatures in Transformers can make pseudo labels easier to learn for student models .",3,0.551309,210.50115065136524,23
2531,Our experiments on three summarization datasets show our proposed method consistently improves vanilla pseudo-labeling based methods .,3,0.94016933,51.251917635249754,17
2531,Further empirical analysis shows that both pseudo labels and summaries produced by our students are shorter and more abstractive .,3,0.98136693,68.34727852364875,20
2532,"This paper demonstrates that multilingual pretraining and multilingual fine-tuning are both critical for facilitating cross-lingual transfer in zero-shot translation , where the neural machine translation ( NMT ) model is tested on source languages unseen during supervised training .",1,0.57592314,21.030841227755353,39
2532,"Following this idea , we present SixT + , a strong many-to-English NMT model that supports 100 source languages but is trained with a parallel dataset in only six source languages .",2,0.5870494,92.07254592074472,35
2532,SixT + initializes the decoder embedding and the full encoder with XLM-R large and then trains the encoder and decoder layers with a simple two-stage training strategy .,2,0.66997534,32.62989900198933,30
2532,SixT + achieves impressive performance on many-to-English translation .,3,0.77055335,166.50407744483442,11
2532,"It significantly outperforms CRISS and m2m-100 , two strong multilingual NMT systems , with an average gain of 7.2 and 5.0 BLEU respectively .",3,0.9495111,41.298301805687174,26
2532,"Additionally , SixT + offers a set of model parameters that can be further fine-tuned to other unsupervised tasks .",3,0.7389676,45.07936987868124,20
2532,We demonstrate that adding SixT + initialization outperforms state-of-the-art explicitly designed unsupervised NMT models on Si <-> En and Ne <-> En by over 1.2 average BLEU .,3,0.9229931,43.25655286103564,33
2532,"When applied to zero-shot cross-lingual abstractive summarization , it produces an average performance gain of 12.3 ROUGE-L over mBART-ft .",3,0.8885428,27.645593618414818,24
2532,"We conduct detailed analyses to understand the key ingredients of SixT + , including multilinguality of the auxiliary parallel data , positional disentangled encoder , and the cross-lingual transferability of its encoder .",2,0.5149262,78.65827064123191,33
2533,"Processing open-domain Chinese texts has been a critical bottleneck in computational linguistics for decades , partially because text segmentation and word discovery often entangle with each other in this challenging scenario .",0,0.94714856,41.17304066981352,32
2533,No existing methods yet can achieve effective text segmentation and word discovery simultaneously in open domain .,0,0.8296586,106.32050135078416,17
2533,"This study fills in this gap by proposing a novel method called TopWORDS-Seg based on Bayesian inference , which enjoys robust performance and transparent interpretation when no training corpus and domain vocabulary are available .",1,0.8476886,90.4569665092702,37
2533,Advantages of TopWORDS-Seg are demonstrated by a series of experimental studies .,3,0.6364912,101.23876213087024,14
2534,Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages .,0,0.9184248,22.591742579709805,23
2534,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer .,0,0.81072515,26.175850917485253,19
2534,"However , existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains .",0,0.90885645,81.01271465748567,20
2534,Other possible auxiliary tasks to improve the learning performance have not been fully investigated .,0,0.7005664,26.356311533900257,15
2534,"In this study , based on the knowledge distillation framework and multi-task learning , we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on the target domain .",1,0.4866971,24.74508162714658,35
2534,"Specifically , an entity recognizer and a similarity evaluator are first trained in parallel as two teachers from the source domain .",2,0.716979,50.350833298343474,22
2534,"Then , two tasks in the student model are supervised by these teachers simultaneously .",2,0.75194937,136.5663411819833,15
2534,Empirical studies on the three datasets across 7 different languages confirm the effectiveness of the proposed model .,3,0.90535474,15.006185409381207,18
2535,"Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks , they still struggle to address multi-input tasks such as multi-document summarization .",0,0.8524471,13.175802656000114,34
2535,"Many solutions truncate the inputs , thus ignoring potential summary-relevant contents , which is unacceptable in the medical domain where each information can be vital .",0,0.71738994,167.00286997902285,28
2535,"Others leverage linear model approximations to apply multi-input concatenation , worsening the results because all information is considered , even if it is conflicting or noisy with respect to a shared background .",3,0.46870047,93.13059737319422,33
2535,"Despite the importance and social impact of medicine , there are no ad-hoc solutions for multi-document summarization .",0,0.9305391,31.21980519280006,18
2535,"For this reason , we propose a novel discriminative marginalized probabilistic method ( DAMEN ) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization .",2,0.43317258,72.92809899284474,37
2535,Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews .,3,0.9814767,18.13696857869478,25
2535,"Moreover , we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method .",3,0.57993656,25.70268911131869,23
2536,Conventional wisdom in pruning Transformer-based language models is that pruning reduces the model expressiveness and thus is more likely to underfit rather than overfit .,0,0.8588809,20.581887467846602,27
2536,"However , under the trending pretrain-and-finetune paradigm , we postulate a counter-traditional hypothesis , that is : pruning increases the risk of overfitting when performed at the fine-tuning phase .",3,0.71374464,54.196956822019324,33
2536,"In this paper , we aim to address the overfitting problem and improve pruning performance via progressive knowledge distillation with error-bound properties .",1,0.9250132,50.855496969181615,23
2536,We show for the first time that reducing the risk of overfitting can help the effectiveness of pruning under the pretrain-and-finetune paradigm .,3,0.9291316,20.0336125321055,27
2536,Ablation studies and experiments on the GLUE benchmark show that our method outperforms the leading competitors across different tasks .,3,0.92574596,16.516827366464,20
2537,We propose a novel data-augmentation technique for neural machine translation based on ROT-k ciphertexts .,1,0.5433057,24.236095811219023,16
2537,ROT-k is a simple letter substitution cipher that replaces a letter in the plaintext with the kth letter after it in the alphabet .,0,0.827239,42.937040529160896,25
2537,We first generate multiple ROT-k ciphertexts using different values of k for the plaintext which is the source side of the parallel data .,2,0.8885202,73.89806852533002,25
2537,We then leverage this enciphered training data along with the original parallel data via multi-source training to improve neural machine translation .,2,0.73151934,48.101663698095294,22
2537,"Our method , CipherDAug , uses a co-regularization-inspired training procedure , requires no external data sources other than the original training data , and uses a standard Transformer to outperform strong data augmentation techniques on several datasets by a significant margin .",2,0.5161192,51.75048388712015,44
2537,"This technique combines easily with existing approaches to data augmentation , and yields particularly strong results in low-resource settings .",3,0.6779676,37.1387618031713,20
2538,Pre-trained multilingual language models such as mBERT and XLM-R have demonstrated great potential for zero-shot cross-lingual transfer to low web-resource languages ( LRL ) .,0,0.8723583,12.970071118992646,26
2538,"However , due to limited model capacity , the large difference in the sizes of available monolingual corpora between high web-resource languages ( HRL ) and LRLs does not provide enough scope of co-embedding the LRL with the HRL , thereby affecting the downstream task performance of LRLs .",0,0.52013236,34.28323724507886,49
2538,"In this paper , we argue that relatedness among languages in a language family along the dimension of lexical overlap may be leveraged to overcome some of the corpora limitations of LRLs .",1,0.80665463,27.17078680649681,33
2538,"We propose Overlap BPE ( OBPE ) , a simple yet effective modification to the BPE vocabulary generation algorithm which enhances overlap across related languages .",1,0.4820798,104.50428715899787,26
2538,"Through extensive experiments on multiple NLP tasks and datasets , we observe that OBPE generates a vocabulary that increases the representation of LRLs via tokens shared with HRLs .",3,0.8499706,65.15103705713858,29
2538,This results in improved zero-shot transfer from related HRLs to LRLs without reducing HRL representation and accuracy .,3,0.9305651,54.87015614882423,18
2538,"Unlike previous studies that dismissed the importance of token-overlap , we show that in the low-resource related language setting , token overlap matters .",3,0.94687754,60.369118764820044,25
2538,Synthetically reducing the overlap to zero can cause as much as a four-fold drop in zero-shot transfer accuracy .,3,0.80168235,40.329827877307316,19
2539,"Self-attention mechanism has been shown to be an effective approach for capturing global context dependencies in sequence modeling , but it suffers from quadratic complexity in time and memory usage .",0,0.81446284,21.968474114907192,31
2539,"Due to the sparsity of the attention matrix , much computation is redundant .",0,0.49384296,51.42620547786361,14
2539,"Therefore , in this paper , we design an efficient Transformer architecture , named Fourier Sparse Attention for Transformer ( FSAT ) , for fast long-range sequence modeling .",1,0.8308915,52.117709300237905,30
2539,"We provide a brand-new perspective for constructing sparse attention matrix , i.e .",3,0.5525404,82.7725273032015,14
2539,making the sparse attention matrix predictable .,3,0.5547985,887.9616088304532,7
2539,"Two core sub-modules are : ( 1 ) A fast Fourier transform based hidden state cross module , which captures and pools L2 semantic combinations in 𝒪( Llog L ) time complexity .",2,0.5277155,144.53031595082106,34
2539,"( 2 ) A sparse attention matrix estimation module , which predicts dominant elements of an attention matrix based on the output of the previous hidden state cross module .",2,0.6680339,116.03760925001195,30
2539,"By reparameterization and gradient truncation , FSAT successfully learned the index of dominant elements .",3,0.71556914,164.08110862621433,15
2539,The overall complexity about the sequence length is reduced from 𝒪 ( L2 ) to 𝒪( Llog L ) .,3,0.78767204,44.46864381718894,20
2539,"Extensive experiments ( natural language , vision , and math ) show that FSAT remarkably outperforms the standard multi-head attention and its variants in various long-sequence tasks with low computational costs , and achieves new state-of-the-art results on the Long Range Arena benchmark .",3,0.869008,34.79455426540881,50
2540,"In modern recommender systems , there are usually comments or reviews from users that justify their ratings for different items .",0,0.89895505,81.45444968111504,21
2540,"Trained on such textual corpus , explainable recommendation models learn to discover user interests and generate personalized explanations .",0,0.5218674,195.7393531153273,19
2540,"Though able to provide plausible explanations , existing models tend to generate repeated sentences for different items or empty sentences with insufficient details .",0,0.8636196,111.8949283562465,24
2540,"To this end , we propose a visually-enhanced approach named METER with the help of visualization generation and text–image matching discrimination : the explainable recommendation model is encouraged to visualize what it refers to while incurring a penalty if the visualization is incongruent with the textual explanation .",2,0.6804723,57.92191784694253,50
2540,Experimental results and a manual assessment demonstrate that our approach can improve not only the text quality but also the diversity and explainability of the generated explanations .,3,0.96899545,13.209801364249802,28
2541,New intent discovery aims to uncover novel intent categories from user utterances to expand the set of supported intent classes .,0,0.80445576,44.52145265606228,21
2541,It is a critical task for the development and service expansion of a practical dialogue system .,0,0.77500814,35.440073468591244,17
2541,"Despite its importance , this problem remains under-explored in the literature .",0,0.9264681,18.27508756930761,12
2541,"Existing approaches typically rely on a large amount of labeled utterances and employ pseudo-labeling methods for representation learning and clustering , which are label-intensive , inefficient , and inaccurate .",0,0.78785604,47.08328157667802,32
2541,"In this paper , we provide new solutions to two important research questions for new intent discovery : ( 1 ) how to learn semantic utterance representations and ( 2 ) how to better cluster utterances .",1,0.88802594,57.287658278264225,37
2541,"Particularly , we first propose a multi-task pre-training strategy to leverage rich unlabeled data along with external labeled data for representation learning .",2,0.59844244,27.864506651062264,23
2541,"Then , we design a new contrastive loss to exploit self-supervisory signals in unlabeled data for clustering .",2,0.8263033,29.06907577662927,18
2541,"Extensive experiments on three intent recognition benchmarks demonstrate the high effectiveness of our proposed method , which outperforms state-of-the-art methods by a large margin in both unsupervised and semi-supervised scenarios .",3,0.82618576,8.089891121075897,37
2541,The source code will be available at https://github.com/zhang-yu-wei/MTP-CLNN .,3,0.60630244,10.566382083766612,9
2542,"Decisions on state-level policies have a deep effect on many aspects of our everyday life , such as health-care and education access .",0,0.83018446,29.839528346142163,26
2542,"However , there is little understanding of how these policies and decisions are being formed in the legislative process .",0,0.9324036,38.57499781116713,20
2542,"We take a data-driven approach by decoding the impact of legislation on relevant stakeholders ( e.g. , teachers in education bills ) to understand legislators ’ decision-making process and votes .",2,0.86892366,55.41121177592752,32
2542,"We build a new dataset for multiple US states that interconnects multiple sources of data including bills , stakeholders , legislators , and money donors .",2,0.79831827,109.67900092108584,26
2542,"Next , we develop a textual graph-based model to embed and analyze state bills .",2,0.77997667,74.94420105499333,17
2542,"Our model predicts winners / losers of bills and then utilizes them to better determine the legislative body ’s vote breakdown according to demographic / ideological criteria , e.g. , gender .",2,0.7156523,116.48432447064104,32
2543,"Tangled multi-party dialogue contexts lead to challenges for dialogue reading comprehension , where multiple dialogue threads flow simultaneously within a common dialogue record , increasing difficulties in understanding the dialogue history for both human and machine .",0,0.9078288,75.3511577798204,37
2543,Previous studies mainly focus on utterance encoding methods with carefully designed features but pay inadequate attention to characteristic features of the structure of dialogues .,0,0.85799557,61.94585874196637,25
2543,We specially take structure factors into account and design a novel model for dialogue disentangling .,2,0.55824846,63.99945141733213,16
2543,"Based on the fact that dialogues are constructed on successive participation and interactions between speakers , we model structural information of dialogues in two aspects : 1 ) speaker property that indicates whom a message is from , and 2 ) reference dependency that shows whom a message may refer to .",2,0.76431096,54.80410534491506,52
2543,The proposed method achieves new state-of-the-art on the Ubuntu IRC benchmark dataset and contributes to dialogue-related comprehension .,3,0.9091773,23.761559413493398,24
2544,"Empathetic dialogue assembles emotion understanding , feeling projection , and appropriate response generation .",0,0.5643354,520.7544301433624,14
2544,Existing work for empathetic dialogue generation concentrates on the two-party conversation scenario .,0,0.82229936,38.97605239228027,14
2544,"Multi-party dialogues , however , are pervasive in reality .",0,0.9145868,156.90824401527777,10
2544,"Furthermore , emotion and sensibility are typically confused ;",0,0.83872384,537.7596374323845,9
2544,a refined empathy analysis is needed for comprehending fragile and nuanced human feelings .,3,0.5199083,127.79539246403337,14
2544,We address these issues by proposing a novel task called Multi-Party Empathetic Dialogue Generation in this study .,1,0.64659995,36.7589261441384,18
2544,"Additionally , a Static-Dynamic model for Multi-Party Empathetic Dialogue Generation , SDMPED , is introduced as a baseline by exploring the static sensibility and dynamic emotion for the multi-party empathetic dialogue learning , the aspects that help SDMPED achieve the state-of-the-art performance .",2,0.6283854,42.16418601027278,49
2545,"Applying existing methods to emotional support conversation — which provides valuable assistance to people who are in need — has two major limitations : ( a ) they generally employ a conversation-level emotion label , which is too coarse-grained to capture user ’s instant mental state ;",0,0.80068237,72.52291965772712,49
2545,( b ) most of them focus on expressing empathy in the response ( s ) rather than gradually reducing user ’s distress .,3,0.7059102,188.2406962553359,24
2545,"To address the problems , we propose a novel model \textbf { MISC } , which firstly infers the user ’s fine-grained emotional status , and then responds skillfully using a mixture of strategy .",2,0.67595106,61.7477982555475,36
2545,Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling .,3,0.95861816,16.88527606208832,30
2546,"There have been various types of pretraining architectures including autoencoding models ( e.g. , BERT ) , autoregressive models ( e.g. , GPT ) , and encoder-decoder models ( e.g. , T5 ) .",0,0.90347916,12.638231611300714,34
2546,"However , none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding ( NLU ) , unconditional generation , and conditional generation .",3,0.92332655,67.17999611995714,32
2546,We propose a General Language Model ( GLM ) based on autoregressive blank infilling to address this challenge .,1,0.5517204,49.45216036268149,19
2546,"GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans , which results in performance gains over BERT and T5 on NLU tasks .",3,0.7054645,78.70772065002838,32
2546,"Meanwhile , GLM can be pretrained for different types of tasks by varying the number and lengths of blanks .",3,0.54407996,36.07568927490581,20
2546,"On a wide range of tasks across NLU , conditional and unconditional generation , GLM outperforms BERT , T5 , and GPT given the same model sizes and data , and achieves the best performance from a single pretrained model with 1.25 × parameters of BERT Large , demonstrating its generalizability to different downstream tasks .",3,0.8926677,47.042415944521515,56
2547,It is very common to use quotations ( quotes ) to make our writings more elegant or convincing .,0,0.91825575,98.49237871698394,19
2547,"To help people find appropriate quotes efficiently , the task of quote recommendation is presented , aiming to recommend quotes that fit the current context of writing .",1,0.44172966,66.90379241742328,28
2547,"There have been various quote recommendation approaches , but they are evaluated on different unpublished datasets .",0,0.863188,90.82313045275397,17
2547,"To facilitate the research on this task , we build a large and fully open quote recommendation dataset called QuoteR , which comprises three parts including English , standard Chinese and classical Chinese .",2,0.67892504,101.43997644202888,34
2547,Any part of it is larger than previous unpublished counterparts .,3,0.75462604,118.65191179079278,11
2547,We conduct an extensive evaluation of existing quote recommendation methods on QuoteR .,2,0.47638428,114.64783872767062,13
2547,"Furthermore , we propose a new quote recommendation model that significantly outperforms previous methods on all three parts of QuoteR .",3,0.57765996,40.874530972174554,21
2547,All the code and data of this paper can be obtained at https://github.com/thunlp/QuoteR .,3,0.49795407,5.172726063855665,14
2548,Predicting the approval chance of a patent application is a challenging problem involving multiple facets .,0,0.93422824,41.85229336188639,16
2548,The most crucial facet is arguably the novelty — 35 U.S .,0,0.7046937,184.12270271173034,12
2548,Code § 102 rejects more recent applications that have very similar prior arts .,0,0.65364593,467.49178787502905,14
2548,Successful patent applications may share similar writing patterns ;,0,0.7125698,950.8103173325973,9
2548,"however , too-similar newer applications would receive the opposite label , thus confusing standard document classifiers ( e.g. , BERT ) .",0,0.59464586,146.90332303305482,22
2548,"To address this issue , we propose a novel framework that unifies the document classifier with handcrafted features , particularly time-dependent novelty scores .",1,0.41557446,51.287733012983644,25
2548,"Specifically , we formulate the novelty scores by comparing each application with millions of prior arts using a hybrid of efficient filters and a neural bi-encoder .",2,0.8486408,107.04933209708278,27
2548,"Moreover , we impose a new regularization term into the classification objective to enforce the monotonic change of approval prediction w.r.t .",2,0.6368102,49.98242282239821,22
2548,novelty scores .,3,0.5824704,674.1152103177826,3
2548,"From extensive experiments on a large-scale USPTO dataset , we find that standard BERT fine-tuning can partially learn the correct relationship between novelty and approvals from inconsistent data .",3,0.9062088,43.94627239517476,31
2548,"However , our time-dependent novelty features offer a boost on top of it .",3,0.93341553,125.0735850587433,16
2548,"Also , our monotonic regularization , while shrinking the search space , can drive the optimizer to better local optima , yielding a further small performance gain .",3,0.9187841,118.03118708447492,28
2549,Knowledge-based visual question answering ( QA ) aims to answer a question which requires visually-grounded external knowledge beyond image content itself .,0,0.9363127,31.074277482200053,26
2549,Answering complex questions that require multi-hop reasoning under weak supervision is considered as a challenging problem since i ) no supervision is given to the reasoning process and ii ) high-order semantics of multi-hop knowledge facts need to be captured .,0,0.8665663,24.479736590713305,42
2549,"In this paper , we introduce a concept of hypergraph to encode high-level semantics of a question and a knowledge base , and to learn high-order associations between them .",1,0.7856382,34.49141488711306,32
2549,"The proposed model , Hypergraph Transformer , constructs a question hypergraph and a query-aware knowledge hypergraph , and infers an answer by encoding inter-associations between two hypergraphs and intra-associations in both hypergraph itself .",2,0.58176446,42.13043242348152,36
2549,"Extensive experiments on two knowledge-based visual QA and two knowledge-based textual QA demonstrate the effectiveness of our method , especially for multi-hop reasoning problem .",3,0.79785883,22.529874327857858,29
2549,Our source code is available at https://github.com/yujungheo/kbvqa-public .,3,0.5323868,17.9326512636305,8
2550,Modelling prosody variation is critical for synthesizing natural and expressive speech in end-to-end text-to-speech ( TTS ) systems .,0,0.94645107,27.038936706001255,23
2550,"In this paper , a cross-utterance conditional VAE ( CUC-VAE ) is proposed to estimate a posterior probability distribution of the latent prosody features for each phoneme by conditioning on acoustic features , speaker information , and text features obtained from both past and future sentences .",1,0.5389167,55.08388748271311,49
2550,"At inference time , instead of the standard Gaussian distribution used by VAE , CUC-VAE allows sampling from an utterance-specific prior distribution conditioned on cross-utterance information , which allows the prosody features generated by the TTS system to be related to the context and is more similar to how humans naturally produce prosody .",3,0.6272371,71.84176827401443,57
2550,"The performance of CUC-VAE is evaluated via a qualitative listening test for naturalness , intelligibility and quantitative measurements , including word error rates and the standard deviation of prosody attributes .",2,0.70847386,105.68191377771687,33
2550,Experimental results on LJ-Speech and LibriTTS data show that the proposed CUC-VAE TTS system improves naturalness and prosody diversity with clear margins .,3,0.9759045,65.89638925163496,27
2551,"Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model ( LM ) , or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM .",0,0.92286557,35.74677742296921,41
2551,"In this work , we propose Mix and Match LM , a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models .",1,0.5321337,33.01259364856115,51
2551,"We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency , the control attribute , and faithfulness to any conditioning context .",2,0.6137804,79.33312794338026,48
2551,We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features .,2,0.87105083,76.27246248927553,22
2551,"We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training , fine-tuning , or restrictive assumptions over the form of models .",3,0.71376824,59.38459918304138,40
2552,Automatic transfer of text between domains has become popular in recent times .,0,0.96244246,22.835648365642466,13
2552,One of its aims is to preserve the semantic content while adapting to the target domain .,0,0.8733362,22.457325186516343,17
2552,"However , it does not explicitly maintain other attributes between the source and translated text : e.g. , text length and descriptiveness .",0,0.52266437,78.9119649087982,23
2552,"Maintaining constraints in transfer has several downstream applications , including data augmentation and debiasing .",0,0.8727809,51.21427124863464,15
2552,We introduce a method for such constrained unsupervised text style transfer by introducing two complementary losses to the generative adversarial network ( GAN ) family of models .,2,0.6492988,50.07499424691942,28
2552,"Unlike the competing losses used in GANs , we introduce cooperative losses where the discriminator and the generator cooperate and reduce the same loss .",2,0.6100369,68.66093020985201,25
2552,The first is a contrastive loss and the second is a classification loss — aiming to regularize the latent space further and bring similar sentences closer together .,2,0.41000888,36.84110158650996,28
2552,"We demonstrate that such training retains lexical , syntactic and domain-specific constraints between domains for multiple benchmark datasets , including ones where more than one attribute change .",3,0.90544945,115.19379531382465,28
2552,"We show that the complementary cooperative losses improve text quality , according to both automated and human evaluation measures .",3,0.96435124,101.47979305240109,20
2553,Understanding causality has vital importance for various Natural Language Processing ( NLP ) applications .,0,0.9626692,35.91451286431213,15
2553,"Beyond the labeled instances , conceptual explanations of the causality can provide deep understanding of the causal fact to facilitate the causal reasoning process .",0,0.61026174,65.93174831268617,25
2553,"However , such explanation information still remains absent in existing causal reasoning resources .",0,0.9134122,158.7908380863077,14
2553,"In this paper , we fill this gap by presenting a human-annotated explainable CAusal REasoning dataset ( e-CARE ) , which contains over 20 K causal reasoning questions , together with natural language formed explanations of the causal questions .",1,0.7288366,59.826618732571696,40
2553,"Experimental results show that generating valid explanations for causal facts still remains especially challenging for the state-of-the-art models , and the explanation information can be helpful for promoting the accuracy and stability of causal reasoning models .",3,0.9496223,24.142062941273057,43
2554,"Question answering ( QA ) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children , yet there is scarcity of high-quality QA datasets carefully designed to serve this purpose .",0,0.960185,39.778895043395224,40
2554,"In particular , existing datasets rarely distinguish fine-grained reading skills , such as the understanding of varying narrative elements .",0,0.8979612,125.28506722457175,21
2554,"Drawing on the reading education research , we introduce FairytaleQA , a dataset focusing on narrative comprehension of kindergarten to eighth-grade students .",2,0.5041391,61.16714920094342,25
2554,"Generated by educational experts based on an evidence-based theoretical framework , FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories , covering seven types of narrative elements or relations .",2,0.5643648,79.34814747132853,38
2554,"First , we ran existing QA models on our dataset and confirmed that this annotation helps assess models ’ fine-grained learning skills .",2,0.5006672,46.71914881474732,24
2554,"Second , the dataset supports question generation ( QG ) task in the education domain .",2,0.44104898,90.70160340642549,16
2554,"Through benchmarking with QG models , we show that the QG model trained on FairytaleQA is capable of asking high-quality and more diverse questions .",3,0.9183958,25.763943925181724,25
2555,The actions defined in grammar are not sufficient to handle uncertain reasoning common in real-world scenarios .,3,0.616212,72.05526509770795,17
2555,( 2 ) Knowledge base information is not well exploited and incorporated into semantic parsing .,0,0.6962925,102.52253286717878,16
2555,"To mitigate the two issues , we propose a knowledge-aware fuzzy semantic parsing framework ( KaFSP ) .",2,0.45616892,77.58574299865529,20
2555,It defines fuzzy comparison operations in the grammar system for uncertain reasoning based on the fuzzy set theory .,2,0.6096546,215.46190904003026,19
2555,"In order to enhance the interaction between semantic parsing and knowledge base , we incorporate entity triples from the knowledge base into a knowledge-aware entity disambiguation module .",2,0.7230415,22.06623252661912,30
2555,"Additionally , we propose a multi-label classification framework to not only capture correlations between entity types and relations but also detect knowledge base information relevant to the current utterance .",2,0.5974837,26.625132037997776,30
2555,Both enhancements are based on pre-trained language models .,3,0.37767994,12.8444676933244,9
2555,"Experiments on a large-scale conversational question answering benchmark demonstrate that the proposed KaFSP achieves significant improvements over previous state-of-the-art models , setting new SOTA results on 8 out of 10 question types , gaining improvements of over 10 % F1 or accuracy on 3 question types , and improving overall F1 from 83.01 % to 85.33 % .",3,0.91009283,20.714786891964305,64
2555,The source code of KaFSP is available at https://github.com/tjunlp-lab/KaFSP .,3,0.5467713,9.481110336987507,10
2556,Predicting missing facts in a knowledge graph ( KG ) is crucial as modern KGs are far from complete .,0,0.9413872,35.206329828586355,20
2556,"Due to labor-intensive human labeling , this phenomenon deteriorates when handling knowledge represented in various languages .",0,0.8642816,109.27015887566628,17
2556,"In this paper , we explore multilingual KG completion , which leverages limited seed alignment as a bridge , to embrace the collective knowledge from multiple languages .",1,0.89760846,81.71104498646548,28
2556,"However , language alignment used in prior works is still not fully exploited : ( 1 ) alignment pairs are treated equally to maximally push parallel entities to be close , which ignores KG capacity inconsistency ;",0,0.67913693,214.60491494586643,37
2556,( 2 ) seed alignment is scarce and new alignment identification is usually in a noisily unsupervised manner .,0,0.7760237,107.89644171190724,19
2556,"To tackle these issues , we propose a novel self-supervised adaptive graph alignment ( SS-AGA ) method .",1,0.34311196,31.29357424432895,20
2556,"Specifically , SS-AGA fuses all KGs as a whole graph by regarding alignment as a new edge type .",3,0.6312464,260.10470733829806,21
2556,"As such , information propagation and noise influence across KGs can be adaptively controlled via relation-aware attention weights .",3,0.6014482,179.09455286140596,21
2556,"Meanwhile , SS-AGA features a new pair generator that dynamically captures potential alignment pairs in a self-supervised paradigm .",3,0.46887565,118.73873364105822,21
2556,Extensive experiments on both the public multilingual DBPedia KG and newly-created industrial multilingual E-commerce KG empirically demonstrate the effectiveness of SS-AGA .,3,0.82925993,57.669146575967794,25
2557,"Automatic code summarization , which aims to describe the source code in natural language , has become an essential task in software maintenance .",0,0.9629729,31.843721174946364,24
2557,Our fellow researchers have attempted to achieve such a purpose through various machine learning-based approaches .,0,0.88350624,29.84160579345663,18
2557,"One key challenge keeping these approaches from being practical lies in the lacking of retaining the semantic structure of source code , which has unfortunately been overlooked by the state-of-the-art .",0,0.90591365,27.91908254380312,36
2557,Existing approaches resort to representing the syntax structure of code by modeling the Abstract Syntax Trees ( ASTs ) .,0,0.87792027,51.48973194808494,20
2557,"However , the hierarchical structures of ASTs have not been well explored .",0,0.9496426,29.467681060993225,13
2557,"In this paper , we propose CODESCRIBE to model the hierarchical syntax structure of code by introducing a novel triplet position for code summarization .",1,0.8563421,47.42659147799134,25
2557,"Specifically , CODESCRIBE leverages the graph neural network and Transformer to preserve the structural and sequential information of code , respectively .",2,0.48243412,53.02997829332152,22
2557,"In addition , we propose a pointer-generator network that pays attention to both the structure and sequential tokens of code for a better summary generation .",2,0.47689295,55.41802910345333,28
2557,Experiments on two real-world datasets in Java and Python demonstrate the effectiveness of our proposed approach when compared with several state-of-the-art baselines .,3,0.7806808,5.688336576567454,29
2558,The few-shot natural language understanding ( NLU ) task has attracted much recent attention .,0,0.9616398,21.565987074705717,15
2558,"However , prior methods have been evaluated under a disparate set of protocols , which hinders fair comparison and measuring the progress of the field .",0,0.91687536,72.61832388947286,26
2558,"To address this issue , we introduce an evaluation framework that improves previous evaluation procedures in three key aspects , i.e. , test performance , dev-test correlation , and stability .",1,0.4898853,52.29444264259557,31
2558,"Under this new evaluation framework , we re-evaluate several state-of-the-art few-shot methods for NLU tasks .",3,0.4371736,15.421922163144727,23
2558,Our framework reveals new insights : ( 1 ) both the absolute performance and relative gap of the methods were not accurately estimated in prior literature ;,3,0.93948025,112.42134251242848,27
2558,( 2 ) no single method dominates most tasks with consistent performance ;,3,0.8283605,536.6522591917011,13
2558,( 3 ) improvements of some methods diminish with a larger pretrained model ;,3,0.9032545,296.0617271570631,14
2558,and ( 4 ) gains from different methods are often complementary and the best combined model performs close to a strong fully-supervised baseline .,3,0.9160564,74.73796595715923,26
2558,"We open-source our toolkit , FewNLU , that implements our evaluation framework along with a number of state-of-the-art methods .",2,0.61559105,25.954853464335113,26
2559,Generalized zero-shot text classification aims to classify textual instances from both previously seen classes and incrementally emerging unseen classes .,0,0.85551405,72.42857324894189,20
2559,"Most existing methods generalize poorly since the learned parameters are only optimal for seen classes rather than for both classes , and the parameters keep stationary in predicting procedures .",0,0.7008248,162.05162279424772,30
2559,"To address these challenges , we propose a novel Learn to Adapt ( LTA ) network using a variant meta-learning framework .",1,0.4428965,44.642497582889575,22
2559,"Specifically , LTA trains an adaptive classifier by using both seen and virtual unseen classes to simulate a generalized zero-shot learning ( GZSL ) scenario in accordance with the test time , and simultaneously learns to calibrate the class prototypes and sample representations to make the learned parameters adaptive to incoming unseen classes .",2,0.7532257,105.23998611965455,54
2559,We claim that the proposed model is capable of representing all prototypes and samples from both classes to a more consistent distribution in a global space .,3,0.9558456,45.97048645753934,27
2559,Extensive experiments on five text classification datasets show that our model outperforms several competitive previous approaches by large margins .,3,0.8773395,13.231515836215154,20
2559,The code and the whole datasets are available at https://github.com/Quareia/LTA .,3,0.6341132,19.364557677323187,11
2560,Understanding tables is an important aspect of natural language understanding .,0,0.9414989,14.612998904220039,11
2560,"Existing models for table understanding require linearization of the table structure , where row or column order is encoded as an unwanted bias .",0,0.87300247,77.2839650346043,24
2560,Such spurious biases make the model vulnerable to row and column order perturbations .,3,0.5197982,48.261729144107996,14
2560,"Additionally , prior work has not thoroughly modeled the table structures or table-text alignments , hindering the table-text understanding ability .",0,0.8381851,65.31399757716363,25
2560,"In this work , we propose a robust and structurally aware table-text encoding architecture TableFormer , where tabular structural biases are incorporated completely through learnable attention biases .",1,0.64178556,192.28100367119424,30
2560,"TableFormer is ( 1 ) strictly invariant to row and column orders , and , ( 2 ) could understand tables better due to its tabular inductive biases .",3,0.7022151,103.95172622667916,29
2560,"Our evaluations showed that TableFormer outperforms strong baselines in all settings on SQA , WTQ and TabFact table reasoning datasets , and achieves state-of-the-art performance on SQA , especially when facing answer-invariant row and column order perturbations ( 6 % improvement over the best baseline ) , because previous SOTA models ’ performance drops by 4 %-6 % when facing such perturbations while TableFormer is not affected .",3,0.911263,51.19820483840008,76
2561,Text-based games provide an interactive way to study natural language processing .,0,0.92879325,14.798978285384209,14
2561,"While deep reinforcement learning has shown effectiveness in developing the game playing agent , the low sample efficiency and the large action space remain to be the two major challenges that hinder the DRL from being applied in the real world .",0,0.7689521,52.13575472902164,42
2561,"In this paper , we address the challenges by introducing world-perceiving modules , which automatically decompose tasks and prune actions by answering questions about the environment .",1,0.8408046,67.00713962376716,29
2561,"We then propose a two-phase training framework to decouple language learning from reinforcement learning , which further improves the sample efficiency .",2,0.5736231,36.8123377373757,24
2561,The experimental results show that the proposed method significantly improves the performance and sample efficiency .,3,0.97531223,13.714380030134489,16
2561,"Besides , it shows robustness against compound error and limited pre-training data .",3,0.66280365,61.85143843364504,13
2562,"In zero-shot multilingual extractive text summarization , a model is typically trained on English summarization dataset and then applied on summarization datasets of other languages .",0,0.8309652,24.913830901365994,26
2562,"Given English gold summaries and documents , sentence-level labels for extractive summarization are usually generated using heuristics .",0,0.70168334,56.62134379806778,20
2562,"However , these monolingual labels created on English datasets may not be optimal on datasets of other languages , for that there is the syntactic or semantic discrepancy between different languages .",0,0.594922,47.317160711081314,32
2562,"In this way , it is possible to translate the English dataset to other languages and obtain different sets of labels again using heuristics .",3,0.58605975,35.85257248410749,25
2562,"To fully leverage the information of these different sets of labels , we propose NLSSum ( Neural Label Search for Summarization ) , which jointly learns hierarchical weights for these different sets of labels together with our summarization model .",2,0.6877011,55.95915018131165,40
2562,"We conduct multilingual zero-shot summarization experiments on MLSUM and WikiLingua datasets , and we achieve state-of-the-art results using both human and automatic evaluations across these two datasets .",2,0.5188831,22.344293388663218,34
2563,Previous work of class-incremental learning for Named Entity Recognition ( NER ) relies on the assumption that there exists abundance of labeled data for the training of new classes .,0,0.9298856,29.619967606706243,30
2563,"In this work , we study a more challenging but practical problem , i.e. , few-shot class-incremental learning for NER , where an NER model is trained with only few labeled samples of the new classes , without forgetting knowledge of the old ones .",1,0.47774273,37.57013667487064,46
2563,"To alleviate the problem of catastrophic forgetting in few-shot class-incremental learning , we reconstruct synthetic training data of the old classes using the trained NER model , augmenting the training of new classes .",2,0.77266,55.24118258387137,34
2563,"We further develop a framework that distills from the existing model with both synthetic data , and real data from the current training set .",2,0.5656474,36.12774633883435,25
2563,Experimental results show that our approach achieves significant improvements over existing baselines .,3,0.9612681,5.517277599276139,13
2564,Building models of natural language processing ( NLP ) is challenging in low-resource scenarios where limited data are available .,0,0.9578785,17.739667472343644,20
2564,Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks .,3,0.50975686,26.18262307133498,24
2564,"Nonetheless , these approaches suffer from the memorization overfitting issue , where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks .",0,0.74190265,49.4807011549358,30
2564,"To address this issue , we propose a memory imitation meta-learning ( MemIML ) method that enhances the model ’s reliance on support sets for task adaptation .",1,0.48527598,65.81659468947416,28
2564,"Specifically , we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of support sets stored in the memory .",2,0.84222406,58.616952176727814,35
2564,"A theoretical analysis is provided to prove the effectiveness of our method , and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks .",3,0.8149394,13.75446694156288,32
2565,Paraphrase generation has been widely used in various downstream tasks .,0,0.9454258,15.566007509158945,11
2565,"Most tasks benefit mainly from high quality paraphrases , namely those that are semantically similar to , yet linguistically diverse from , the original sentence .",0,0.6340445,53.5135520302552,26
2565,Generating high-quality paraphrases is challenging as it becomes increasingly hard to preserve meaning as linguistic diversity increases .,0,0.9116848,28.26293711853763,18
2565,"Recent works achieve nice results by controlling specific aspects of the paraphrase , such as its syntactic tree .",0,0.7507092,67.71577436197767,19
2565,"However , they do not allow to directly control the quality of the generated paraphrase , and suffer from low flexibility and scalability .",0,0.8445434,37.79421821501817,24
2565,"Here we propose QCPG , a quality-guided controlled paraphrase generation model , that allows directly controlling the quality dimensions .",1,0.78569376,133.83208823355525,22
2565,"Furthermore , we suggest a method that given a sentence , identifies points in the quality control space that are expected to yield optimal generated paraphrases .",3,0.6434706,74.73461607534124,27
2565,We show that our method is able to generate paraphrases which maintain the original meaning while achieving higher diversity than the uncontrolled baseline .,3,0.96550536,28.01001733549638,24
2565,"The models , the code , and the data can be found in https://github.com/IBM/quality-controlled-paraphrase-generation .",3,0.6613374,18.169857885100743,15
2566,Example sentences for targeted words in a dictionary play an important role to help readers understand the usage of words .,0,0.67130715,42.21062499789607,21
2566,"Traditionally , example sentences in a dictionary are usually created by linguistics experts , which are labor-intensive and knowledge-intensive .",0,0.86773324,40.025711214009235,21
2566,"In this paper , we introduce the problem of dictionary example sentence generation , aiming to automatically generate dictionary example sentences for targeted words according to the corresponding definitions .",1,0.88708156,47.93474572559771,30
2566,"This task is challenging especially for polysemous words , because the generated sentences need to reflect different usages and meanings of these targeted words .",0,0.7772275,43.46918464323547,25
2566,Targeted readers may also have different backgrounds and educational levels .,0,0.64173156,80.8924719274671,11
2566,It is essential to generate example sentences that can be understandable for different backgrounds and levels of audiences .,0,0.8196835,42.3913136208071,19
2566,"To solve these problems , we propose a controllable target-word-aware model for this task .",1,0.43470317,24.664575945859802,19
2566,"Our proposed model can generate reasonable examples for targeted words , even for polysemous words .",3,0.9296665,95.90616138206279,16
2566,"In addition , our model allows users to provide explicit control over attributes related to readability , such as length and lexical complexity , thus generating suitable examples for targeted audiences .",3,0.72598463,71.39464088088555,32
2566,Automatic and human evaluations on the Oxford dictionary dataset show that our model can generate suitable examples for targeted words with specific definitions while meeting the desired readability .,3,0.9323054,57.257890611094965,29
2567,Transfer learning with a unified Transformer framework ( T5 ) that converts all language problems into a text-to-text format was recently proposed as a simple and effective transfer learning approach .,0,0.90598357,29.233175806927306,35
2567,"Although a multilingual version of the T5 model ( mT5 ) was also introduced , it is not clear how well it can fare on non-English tasks involving diverse data .",0,0.90849906,28.532034931540824,31
2567,"To investigate this question , we apply mT5 on a language with a wide variety of dialects –Arabic .",2,0.5066271,63.635563779131004,20
2567,"For evaluation , we introduce a novel benchmark for ARabic language GENeration ( ARGEN ) , covering seven important tasks .",2,0.63707864,155.15513502617694,21
2567,"For model comparison , we pre-train three powerful Arabic T5-style models and evaluate them on ARGEN .",2,0.8185214,87.92354318674205,19
2567,"Although pre-trained with ~49 less data , our new models perform significantly better than mT5 on all ARGEN tasks ( in 52 out of 59 test sets ) and set several new SOTAs .",3,0.9378446,109.639574545668,34
2567,"Our models also establish new SOTA on the recently-proposed , large Arabic language understanding evaluation benchmark ARLUE ( Abdul-Mageed et al. , 2021 ) .",3,0.9067551,134.09821039757486,29
2567,Our new models are publicly available .,3,0.6202613,27.316894834576203,7
2567,We also link to ARGEN datasets through our repository : https://github.com/UBC-NLP/araT5 .,3,0.48304707,62.8843509605953,12
2568,"While significant progress has been made on the task of Legal Judgment Prediction ( LJP ) in recent years , the incorrect predictions made by SOTA LJP models can be attributed in part to their failure to ( 1 ) locate the key event information that determines the judgment , and ( 2 ) exploit the cross-task consistency constraints that exist among the subtasks of LJP .",0,0.8616445,27.614862655429924,68
2568,"To address these weaknesses , we propose EPM , an Event-based Prediction Model with constraints , which surpasses existing SOTA models in performance on a standard LJP dataset .",2,0.38121673,66.44597412537547,29
2569,"Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks , including commonsense reasoning tasks .",0,0.9182275,14.082699412642711,33
2569,"This is achieved using text interactions with the model , usually by posing the task as a natural language text completion problem .",2,0.40683654,50.01045885072763,23
2569,"While using language model probabilities to obtain task specific scores has been generally useful , it often requires task-specific heuristics such as length normalization , or probability calibration .",0,0.7998839,89.65229308559043,30
2569,"In this work , we consider the question answering format , where we need to choose from a set of ( free-form ) textual choices of unspecified lengths given a context .",2,0.3954273,60.97117146717313,32
2569,"We present ALC ( Answer-Level Calibration ) , where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context .",2,0.47735256,61.00935668890175,48
2569,We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks .,3,0.93987375,27.36259464692809,29
2569,"Further , we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context .",3,0.9644011,104.17651216334795,22
2569,We analyze such biases using an associated F1-score .,2,0.7978471,104.97765926284829,11
2569,Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability .,3,0.98506296,41.417998956165135,23
2570,"Existing approaches waiting-and-translating for a fixed duration often break the acoustic units in speech , since the boundaries between acoustic units in speech are not even .",0,0.83540606,71.56325345053212,31
2570,"In this paper , we propose MoSST , a simple yet effective method for translating streaming speech content .",1,0.90668404,41.306908353211824,19
2570,"Given a usually long speech sequence , we develop an efficient monotonic segmentation module inside an encoder-decoder model to accumulate acoustic information incrementally and detect proper speech unit boundaries for the input in speech translation task .",2,0.6000858,61.78010638302196,37
2570,Experiments on multiple translation directions of the MuST-C dataset show that outperforms existing methods and achieves the best trade-off between translation quality ( BLEU ) and latency .,3,0.90323657,24.069457846164056,32
2570,Our code is available at https://github.com/dqqcasia/mosst .,3,0.58487594,22.408980290884863,7
2571,Transformer based re-ranking models can achieve high search relevance through context-aware soft matching of query tokens with document tokens .,3,0.50011873,74.39573769282613,22
2571,"To alleviate runtime complexity of such inference , previous work has adopted a late interaction architecture with pre-computed contextual token representations at the cost of a large online storage .",0,0.8841569,88.32079889072551,30
2571,This paper proposes contextual quantization of token embeddings by decoupling document-specific and document-independent ranking contributions during codebook-based compression .,1,0.8015582,75.8431688691477,22
2571,This allows effective online decompression and embedding composition for better search relevance .,3,0.63078475,194.9959487396465,13
2571,This paper presents an evaluation of the above compact token representation model in terms of relevance and space efficiency .,1,0.8609853,49.195833510452445,20
2572,"Early stopping , which is widely used to prevent overfitting , is generally based on a separate validation set .",0,0.5359986,58.99049714897002,20
2572,"However , in low resource settings , validation-based stopping can be risky because a small validation set may not be sufficiently representative , and the reduction in the number of samples by validation split may result in insufficient samples for training .",3,0.5061315,47.77586727260252,44
2572,"In this study , we propose an early stopping method that uses unlabeled samples .",1,0.854162,47.859353894781734,15
2572,The proposed method is based on confidence and class distribution similarities .,2,0.50735795,84.08646923031957,12
2572,"To further improve the performance , we present a calibration method to better estimate the class distribution of the unlabeled samples .",2,0.5666917,20.80959584244819,22
2572,The proposed method is advantageous because it does not require a separate validation set and provides a better stopping point by using a large unlabeled set .,3,0.7761943,23.88898155399132,27
2572,Extensive experiments are conducted on five text classification datasets and several stop-methods are compared .,2,0.7382456,52.00907181739446,15
2572,"Our results show that the proposed model even performs better than using an additional validation set as well as the existing stop-methods , in both balanced and imbalanced data settings .",3,0.98535407,55.550048729865004,31
2572,Our code is available at https://github.com/DMCB-GIST/BUS-stop .,3,0.583359,17.302546402433578,7
2573,The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples .,0,0.86119515,10.029945140023425,20
2573,"Inspired by the recent progress in large language models , we propose \textit{in-context tuning } ( ICT ) , which recasts task adaptation and prediction as a simple sequence prediction problem : to form the input sequence , we concatenate the task instruction , labeled in-context examples , and the target input to predict ;",2,0.46982202,87.01161060163217,58
2573,"to meta-train the model to learn from in-context examples , we fine-tune a pre-trained language model ( LM ) to predict the target label given the input sequence on a collection of tasks .",2,0.76718134,30.333484086738906,37
2573,We benchmark our method on two collections of text classification tasks : LAMA and BinaryClfs .,2,0.71680355,82.8883313933631,16
2573,"Compared to MAML which adapts the model through gradient descent , our method leverages the inductive bias of pre-trained LMs to perform pattern matching , and outperforms MAML by an absolute 6 % average AUC-ROC score on BinaryClfs , gaining more advantage with increasing model size .",3,0.85776323,45.67934441373765,49
2573,Compared to non-fine-tuned in-context learning ( i.e .,3,0.58103806,19.92312054408945,11
2573,"prompting a raw LM ) , in-context tuning meta-trains the model to learn from in-context examples .",2,0.5317813,118.4079737965351,19
2573,"On BinaryClfs , ICT improves the average AUC-ROC score by an absolute 10 % , and reduces the variance due to example ordering by 6x and example choices by 2x .",3,0.9278122,106.87877330285063,33
2574,Existing question answering ( QA ) techniques are created mainly to answer questions asked by humans .,0,0.9353999,43.949049048962245,17
2574,"But in educational applications , teachers often need to decide what questions they should ask , in order to help students to improve their narrative understanding capabilities .",0,0.84313947,37.133750454223545,28
2574,"We design an automated question-answer generation ( QAG ) system for this education scenario : given a story book at the kindergarten to eighth-grade level as input , our system can automatically generate QA pairs that are capable of testing a variety of dimensions of a student ’s comprehension skills .",2,0.72583777,41.79006469887775,55
2574,"Our proposed QAG model architecture is demonstrated using a new expert-annotated FairytaleQA dataset , which has 278 child-friendly storybooks with 10,580 QA pairs .",3,0.47860798,57.3145990588211,25
2574,Automatic and human evaluations show that our model outperforms state-of-the-art QAG baseline systems .,3,0.93185705,10.058852404230418,19
2574,"On top of our QAG system , we also start to build an interactive story-telling application for the future real-world deployment in this educational scenario .",3,0.8763686,49.155583638695,28
2575,"Weakly-supervised learning ( WSL ) has shown promising results in addressing label scarcity on many NLP tasks , but manually designing a comprehensive , high-quality labeling rule set is tedious and difficult .",0,0.9452475,56.818374704332214,35
2575,We study interactive weakly-supervised learning — the problem of iteratively and automatically discovering novel labeling rules from data to improve the WSL model .,1,0.38433993,64.6275077500563,26
2575,"Our proposed model , named PRBoost , achieves this goal via iterative prompt-based rule discovery and model boosting .",2,0.49316758,145.9001602693958,20
2575,It uses boosting to identify large-error instances and discovers candidate rules from them by prompting pre-trained LMs with rule templates .,2,0.5614112,190.37281558080986,21
2575,"The candidate rules are judged by human experts , and the accepted rules are used to generate complementary weak labels and strengthen the current model .",2,0.64724374,101.7156744493737,26
2575,"Experiments on four tasks show PRBoost outperforms state-of-the-art WSL baselines up to 7.1 % , and bridges the gaps with fully supervised models .",3,0.91489655,35.2896364019711,29
2576,"We examine the extent to which supervised bridging resolvers can be improved without employing additional labeled bridging data by proposing a novel constrained multi-task learning framework for bridging resolution , within which we ( 1 ) design cross-task consistency constraints to guide the learning process ;",1,0.5368696,47.80512742955915,48
2576,( 2 ) pre-train the entity coreference model in the multi-task framework on the large amount of publicly available coreference data ;,2,0.67415464,62.478938826684285,22
2576,and ( 3 ) integrating prior knowledge encoded in rule-based resolvers .,2,0.42810097,98.03067289222449,13
2576,Our approach achieves state-of-the-art results on three standard evaluation corpora .,3,0.8660453,7.600153465102907,16
2577,Automatic evaluation metrics are essential for the rapid development of open-domain dialogue systems as they facilitate hyper-parameter tuning and comparison between models .,0,0.7381394,20.93178517642687,23
2577,"Although recently proposed trainable conversation-level metrics have shown encouraging results , the quality of the metrics is strongly dependent on the quality of training data .",0,0.8467575,29.531782387254708,28
2577,Prior works mainly resort to heuristic text-level manipulations ( e.g .,0,0.8318978,58.699464882830874,11
2577,utterances shuffling ) to bootstrap incoherent conversations ( negative examples ) from coherent dialogues ( positive examples ) .,2,0.61338955,110.7621448736165,19
2577,Such approaches are insufficient to appropriately reflect the incoherence that occurs in interactions between advanced dialogue models and humans .,0,0.80733114,53.66248618873924,20
2577,"To tackle this problem , we propose DEAM , a Dialogue coherence Evaluation metric that relies on Abstract Meaning Representation ( AMR ) to apply semantic-level Manipulations for incoherent ( negative ) data generation .",1,0.37347725,80.38920660442507,35
2577,"AMRs naturally facilitate the injection of various types of incoherence sources , such as coreference inconsistency , irrelevancy , contradictions , and decrease engagement , at the semantic level , thus resulting in more natural incoherent samples .",0,0.57910895,112.9104385132944,38
2577,Our experiments show that DEAM achieves higher correlations with human judgments compared to baseline methods on several dialog datasets by significant margins .,3,0.9777899,36.39883546135642,23
2577,"We also show that DEAM can distinguish between coherent and incoherent dialogues generated by baseline manipulations , whereas those baseline models cannot detect incoherent examples generated by DEAM .",3,0.96635133,53.85766936218146,29
2577,Our results demonstrate the potential of AMR-based semantic manipulations for natural negative example generation .,3,0.99006104,41.09832706928023,17
2578,Document structure is critical for efficient information consumption .,0,0.87978697,96.57704757367641,9
2578,"However , it is challenging to encode it efficiently into the modern Transformer architecture .",0,0.8932323,47.55399293659868,15
2578,"In this work , we present HIBRIDS , which injects Hierarchical Biases foR Incorporating Document Structure into attention score calculation .",1,0.7072245,120.78010020685298,21
2578,"We further present a new task , hierarchical question-summary generation , for summarizing salient content in the source document into a hierarchy of questions and summaries , where each follow-up question inquires about the content of its parent question-summary pair .",2,0.4953883,34.99286108129325,47
2578,"We also annotate a new dataset with 6,153 question-summary hierarchies labeled on government reports .",2,0.7128531,88.96831510823384,17
2578,"Experiment results show that our model produces better question-summary hierarchies than comparisons on both hierarchy quality and content coverage , a finding also echoed by human judges .",3,0.9833845,79.14635947270327,30
2578,"Additionally , our model improves the generation of long-form summaries from long government reports and Wikipedia articles , as measured by ROUGE scores .",3,0.8766119,36.28332193318459,24
2579,Named entity recognition ( NER ) is a fundamental task to recognize specific types of entities from a given sentence .,0,0.94890565,21.77383934156687,21
2579,"Depending on how the entities appear in the sentence , it can be divided into three subtasks , namely , Flat NER , Nested NER , and Discontinuous NER .",0,0.46519396,26.20965422549318,30
2579,"Among the existing approaches , only the generative model can be uniformly adapted to these three subtasks .",0,0.6439717,36.494986058372476,18
2579,"However , when the generative model is applied to NER , its optimization objective is not consistent with the task , which makes the model vulnerable to the incorrect biases .",0,0.59042585,36.77393319339354,31
2579,"In this paper , we analyze the incorrect biases in the generation process from a causality perspective and attribute them to two confounders : pre-context confounder and entity-order confounder .",1,0.8879931,31.536591607313447,31
2579,"Furthermore , we design Intra-and Inter-entity Deconfounding Data Augmentation methods to eliminate the above confounders according to the theory of backdoor adjustment .",2,0.89415,84.31001790241412,23
2579,Experiments show that our method can improve the performance of the generative NER model in various datasets .,3,0.9502978,11.417037466189011,18
2580,"Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained , and prompt engineering seeks to align these models to specific tasks .",0,0.9159745,37.672417090419515,31
2580,"Unfortunately , existing prompt engineering methods require significant amounts of labeled data , access to model parameters , or both .",0,0.85689276,116.5357696218804,21
2580,We introduce a new method for selecting prompt templates without labeled examples and without direct access to the model .,2,0.47698894,59.9841552482151,20
2580,"Specifically , over a set of candidate templates , we choose the template that maximizes the mutual information between the input and the corresponding model output .",2,0.81740874,23.654118980896484,27
2580,"Across 8 datasets representing 7 distinct NLP tasks , we show that when a template has high mutual information , it also has high accuracy on the task .",3,0.90387917,48.956321707142564,29
2580,"On the largest model , selecting prompts with our method gets 90 % of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels .",3,0.8994203,82.51492004143854,33
2581,The performance of multilingual pretrained models is highly dependent on the availability of monolingual or parallel text present in a target language .,0,0.7631668,16.138143428037516,23
2581,"Thus , the majority of the world ’s languages cannot benefit from recent progress in NLP as they have no or limited textual data .",0,0.7650376,37.52419531443409,25
2581,"To expand possibilities of using NLP technology in these under-represented languages , we systematically study strategies that relax the reliance on conventional language resources through the use of bilingual lexicons , an alternative resource with much better language coverage .",1,0.47931057,68.51947526443297,40
2581,"We analyze different strategies to synthesize textual or labeled data using lexicons , and how this data can be combined with monolingual or parallel text when available .",2,0.4173563,52.22764457153669,28
2581,"For 19 under-represented languages across 3 tasks , our methods lead to consistent improvements of up to 5 and 15 points with and without extra monolingual text respectively .",3,0.90920246,37.336646657575116,29
2581,"Overall , our study highlights how NLP methods can be adapted to thousands more languages that are under-served by current technology .",3,0.9886903,25.103186854233538,22
2582,While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored .,0,0.64985096,11.97144125778718,30
2582,"We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including : masked language modeling ( MLM ) , translation language modeling ( TLM ) , dual encoder translation ranking , and additive margin softmax .",2,0.67520934,37.31728147279498,46
2582,We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80 % .,3,0.94956917,16.629471927101008,26
2582,"Composing the best of these methods produces a model that achieves 83.7 % bi-text retrieval accuracy over 112 languages on Tatoeba , well above the 65.5 % achieved by LASER , while still performing competitively on monolingual transfer learning benchmarks .",3,0.87123954,41.985246273122684,41
2582,Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de .,3,0.7896869,82.92928861793199,21
2582,We publicly release our best multilingual sentence embedding model for 109 + languages at https://tfhub.dev/google/LaBSE .,2,0.4306897,97.55582571800433,16
2583,Span-based methods with the neural networks backbone have great potential for the nested named entity recognition ( NER ) problem .,0,0.77548856,44.18798253174508,22
2583,"However , they face problems such as degenerating when positive instances and negative instances largely overlap .",0,0.8287488,126.06627085041352,17
2583,"Besides , the generalization ability matters a lot in nested NER , as a large proportion of entities in the test set hardly appear in the training set .",3,0.7679977,38.686156208898815,29
2583,"In this work , we try to improve the span representation by utilizing retrieval-based span-level graphs , connecting spans and entities in the training data based on n-gram features .",1,0.51757616,46.24343579581601,32
2583,"Specifically , we build the entity-entity graph and span-entity graph globally based on n-gram similarity to integrate the information of similar neighbor entities into the span representation .",2,0.8650632,36.699711647387865,29
2583,"To evaluate our method , we conduct experiments on three common nested NER datasets , ACE2004 , ACE2005 , and GENIA datasets .",2,0.8587285,47.115150408762574,23
2583,"Experimental results show that our method achieves general improvements on all three benchmarks ( + 0.30 ∼ 0.85 micro-F1 ) , and obtains special superiority on low frequency entities ( + 0.56 ∼ 2.08 recall ) .",3,0.950254,42.40040067683138,38
2584,"Taxonomy ( Zamir et al. , 2018 ) finds that a structure exists among visual tasks , as a principle underlying transfer learning for them .",0,0.5977139,178.94627632142783,26
2584,"In this paper , we propose a cognitively inspired framework , CogTaskonomy , to learn taxonomy for NLP tasks .",1,0.86383593,49.86189947495826,20
2584,The framework consists of Cognitive Representation Analytics ( CRA ) and Cognitive-Neural Mapping ( CNM ) .,2,0.64834976,45.88157557854884,18
2584,"The former employs Representational Similarity Analysis , which is commonly used in computational neuroscience to find a correlation between brain-activity measurement and computational modeling , to estimate task similarity with task-specific sentence representations .",2,0.44304913,65.74257058505006,35
2584,"The latter learns to detect task relations by projecting neural representations from NLP models to cognitive signals ( i.e. , fMRI voxels ) .",2,0.5401715,99.4196220140561,24
2584,"Experiments on 12 NLP tasks , where BERT / TinyBERT are used as the underlying models for transfer learning , demonstrate that the proposed CogTaxonomy is able to guide transfer learning , achieving performance competitive to the Analytic Hierarchy Process ( Saaty , 1987 ) used in visual Taskonomy ( Zamir et al. , 2018 ) but without requiring exhaustive pairwise O( m2 ) task transferring .",3,0.8651452,120.20010160877307,68
2584,Analyses further discover that CNM is capable of learning model-agnostic task taxonomy .,3,0.9531785,71.91425800528108,14
2585,Large-scale pretrained language models have achieved SOTA results on NLP tasks .,0,0.7746738,8.878207783610765,12
2585,"However , they have been shown vulnerable to adversarial attacks especially for logographic languages like Chinese .",0,0.90662575,42.8606485020173,17
2585,"In this work , we propose RoCBert : a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation , synonyms , typos , etc .",1,0.5947942,59.0466972382767,31
2585,It is pretrained with the contrastive learning objective which maximizes the label consistency under different synthesized adversarial examples .,2,0.48354393,32.54878124437689,19
2585,"The model takes as input multimodal information including the semantic , phonetic and visual features .",2,0.55164117,34.83744439587881,16
2585,We show all these features areimportant to the model robustness since the attack can be performed in all the three forms .,3,0.8883435,79.5013044634223,22
2585,"Across 5 Chinese NLU tasks , RoCBert outperforms strong baselines under three blackbox adversarial algorithms without sacrificing the performance on clean testset .",3,0.8227174,139.90181525095258,23
2585,It also performs the best in the toxic content detection task under human-made attacks .,3,0.91507864,68.04686942090888,15
2586,It is a common practice for recent works in vision language cross-modal reasoning to adopt a binary or multi-choice classification formulation taking as input a set of source image ( s ) and textual query .,0,0.8568601,66.82105736986341,36
2586,"In this work , we take a sober look at such an “ unconditional ” formulation in the sense that no prior knowledge is specified with respect to the source image ( s ) .",1,0.4900163,62.13975418808403,35
2586,"Inspired by the designs of both visual commonsense reasoning and natural language inference tasks , we propose a new task termed “ Premise-based Multi-modal Reasoning ” ( PMR ) where a textual premise is the background presumption on each source image .",2,0.550517,36.35576522791406,44
2586,"The PMR dataset contains 15,360 manually annotated samples which are created by a multi-phase crowd-sourcing process .",2,0.723112,21.034446705797738,19
2586,"With selected high-quality movie screenshots and human-curated premise templates from 6 pre-defined categories , we ask crowd-source workers to write one true hypothesis and three distractors ( 4 choices ) given the premise and image through a cross-check procedure .",2,0.9022047,121.99468637377687,40
2587,Named entity recognition ( NER ) is a fundamental task in natural language processing .,0,0.96024036,11.593808003540836,15
2587,"Recent works treat named entity recognition as a reading comprehension task , constructing type-specific queries manually to extract entities .",0,0.9159453,53.918439986245886,20
2587,This paradigm suffers from three issues .,0,0.9003955,127.77851386583198,7
2587,"First , type-specific queries can only extract one type of entities per inference , which is inefficient .",0,0.46504107,81.10957898098577,18
2587,"Second , the extraction for different types of entities is isolated , ignoring the dependencies between them .",2,0.47200847,73.35166456253359,18
2587,"Third , query construction relies on external knowledge and is difficult to apply to realistic scenarios with hundreds of entity types .",0,0.76270163,52.6823186656295,22
2587,"To deal with them , we propose Parallel Instance Query Network ( PIQN ) , which sets up global and learnable instance queries to extract entities from a sentence in a parallel manner .",2,0.54367656,46.39574693694768,34
2587,"Each instance query predicts one entity , and by feeding all instance queries simultaneously , we can query all entities in parallel .",2,0.47782332,158.10413204993665,23
2587,"Instead of being constructed from external knowledge , instance queries can learn their different query semantics during training .",0,0.43117934,125.9726494486826,19
2587,"For training the model , we treat label assignment as a one-to-many Linear Assignment Problem ( LAP ) and dynamically assign gold entities to instance queries with minimal assignment cost .",2,0.82931423,83.48443487860213,31
2587,Experiments on both nested and flat NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models .,3,0.93412274,7.639574322019598,24
2588,Typical generative dialogue models utilize the dialogue history to generate the response .,0,0.85690683,34.98685467412532,13
2588,"However , since one dialogue utterance can often be appropriately answered by multiple distinct responses , generating a desired response solely based on the historical information is not easy .",0,0.8802195,67.22790390204048,30
2588,"Intuitively , if the chatbot can foresee in advance what the user would talk about ( i.e. , the dialogue future ) after receiving its response , it could possibly provide a more informative response .",3,0.8354775,48.27180988454571,36
2588,"Accordingly , we propose a novel dialogue generation framework named Prophet Chat that utilizes the simulated dialogue futures in the inference phase to enhance response generation .",1,0.5028324,76.26846194432602,27
2588,"To enable the chatbot to foresee the dialogue future , we design a beam-search-like roll-out strategy for dialogue future simulation using a typical dialogue generation model and a dialogue selector .",2,0.68393904,63.52596542520906,35
2588,"With the simulated futures , we then utilize the ensemble of a history-to-response generator and a future-to-response generator to jointly generate a more informative response .",2,0.75084907,29.972962773153274,32
2588,"Experiments on two popular open-domain dialogue datasets demonstrate that Prophet Chat can generate better responses over strong baselines , which validates the advantages of incorporating the simulated dialogue futures .",3,0.90619224,45.03613089757864,30
2589,"Fusion-in-decoder ( Fid ) ( Izacard and Grave , 2020 ) is a generative question answering ( QA ) model that leverages passage retrieval with a pre-trained transformer and pushed the state of the art on single-hop QA .",0,0.86485773,43.0922469500777,41
2589,"However , the complexity of multi-hop QA hinders the effectiveness of the generative QA approach .",0,0.70257866,14.56189095306705,16
2589,"In this work , we propose a simple generative approach ( PathFid ) that extends the task beyond just answer generation by explicitly modeling the reasoning process to resolve the answer for multi-hop questions .",1,0.6797147,47.77529774327766,35
2589,"By linearizing the hierarchical reasoning path of supporting passages , their key sentences , and finally the factoid answer , we cast the problem as a single sequence prediction task .",2,0.60700905,194.98534915794014,31
2589,"To facilitate complex reasoning with multiple clues , we further extend the unified flat representation of multiple input documents by encoding cross-passage interactions .",2,0.5454489,79.35325551533197,26
2589,Our extensive experiments demonstrate that PathFid leads to strong performance gains on two multi-hop QA datasets : HotpotQA and IIRC .,3,0.96198344,43.61442753553014,21
2589,"Besides the performance gains , PathFid is more interpretable , which in turn yields answers that are more faithfully grounded to the supporting passages and facts compared to the baseline Fid model .",3,0.9087197,67.86977590399651,33
2590,Multilingual pre-trained models are able to zero-shot transfer knowledge from rich-resource to low-resource languages in machine reading comprehension ( MRC ) .,0,0.53541076,22.728687680712284,22
2590,"However , inherent linguistic discrepancies in different languages could make answer spans predicted by zero-shot transfer violate syntactic constraints of the target language .",0,0.62489706,109.83511991294836,24
2590,"In this paper , we propose a novel multilingual MRC framework equipped with a Siamese Semantic Disentanglement Model ( S2 DM ) to disassociate semantics from syntax in representations learned by multilingual pre-trained models .",1,0.8039008,32.233283660604805,35
2590,"To explicitly transfer only semantic knowledge to the target language , we propose two groups of losses tailored for semantic and syntactic encoding and disentanglement .",2,0.59002453,50.108613274604004,26
2590,"Experimental results on three multilingual MRC datasets ( i.e. , XQuAD , MLQA , and TyDi QA ) demonstrate the effectiveness of our proposed approach over models based on mBERT and XLM-100 .",3,0.8834569,26.03966997114952,35
2591,Transferring the knowledge to a small model through distillation has raised great interest in recent years .,0,0.9500078,33.97899470226448,17
2591,"Prevailing methods transfer the knowledge derived from mono-granularity language units ( e.g. , token-level or sample-level ) , which is not enough to represent the rich semantics of a text and may lose some vital knowledge .",0,0.78955567,55.3933797116307,41
2591,"Besides , these methods form the knowledge as individual representations or their simple dependencies , neglecting abundant structural relations among intermediate representations .",0,0.69764763,211.6243189107964,23
2591,"To overcome the problems , we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities ( e.g. , tokens , spans and samples ) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations .",2,0.60280883,57.29282140202562,55
2591,"Moreover , we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers .",3,0.6133446,73.22257399179135,19
2591,Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods .,3,0.9476584,12.666928611566401,14
2592,Human-like biases and undesired social stereotypes exist in large pretrained language models .,0,0.5803898,34.56734270071297,13
2592,"Given the wide adoption of these models in real-world applications , mitigating such biases has become an emerging and important task .",0,0.9376475,28.345974991317604,22
2592,"In this paper , we propose an automatic method to mitigate the biases in pretrained language models .",1,0.87477094,13.42847295785954,18
2592,"Different from previous debiasing work that uses external corpora to fine-tune the pretrained models , we instead directly probe the biases encoded in pretrained models through prompts .",2,0.77437717,29.707134154846006,29
2592,"Specifically , we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups .",2,0.608316,46.064280917107894,33
2592,"Given the identified biased prompts , we then propose a distribution alignment loss to mitigate the biases .",2,0.5457993,114.66385767311094,18
2592,"Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases , including gender and racial bias , in pretrained language models such as BERT , RoBERTa and ALBERT .",3,0.9626947,24.14300693000848,36
2592,"Moreover , the improvement in fairness does not decrease the language models ’ understanding abilities , as shown using the GLUE benchmark .",3,0.95476604,71.73017778131519,23
2593,Most dialog systems posit that users have figured out clear and specific goals before starting an interaction .,0,0.90630406,90.09958218289292,18
2593,"For example , users have determined the departure , the destination , and the travel time for booking a flight .",0,0.66517204,67.64538779294229,21
2593,"However , in many scenarios , limited by experience and knowledge , users may know what they need , but still struggle to figure out clear and specific goals by determining all the necessary slots .",0,0.7597249,86.60587094708167,36
2593,"In this paper , we identify this challenge , and make a step forward by collecting a new human-to-human mixed-type dialog corpus .",1,0.89335185,48.737900238449505,26
2593,It contains 5 k dialog sessions and 168 k utterances for 4 dialog types and 5 domains .,2,0.6094711,109.21978585244034,18
2593,"Within each session , an agent first provides user-goal-related knowledge to help figure out clear and specific goals , and then help achieve them .",2,0.50539887,73.91516061398478,27
2593,"Furthermore , we propose a mixed-type dialog model with a novel Prompt-based continual learning mechanism .",3,0.42801756,58.375175158167195,19
2593,"Specifically , the mechanism enables the model to continually strengthen its ability on any specific type by utilizing existing dialog corpora effectively .",3,0.71869457,123.08683141777841,23
2594,Supervised parsing models have achieved impressive results on in-domain texts .,0,0.9016643,30.8384766179147,12
2594,"However , their performances drop drastically on out-of-domain texts due to the data distribution shift .",3,0.5283399,39.9364704682141,19
2594,"The shared-private model has shown its promising advantages for alleviating this problem via feature separation , whereas prior works pay more attention to enhance shared features but neglect the in-depth relevance of specific ones .",0,0.8481577,81.89941343916804,38
2594,"To address this issue , we for the first time apply a dynamic matching network on the shared-private model for semi-supervised cross-domain dependency parsing .",2,0.36722314,30.47083453556976,27
2594,"Meanwhile , considering the scarcity of target-domain labeled data , we leverage unlabeled data from two aspects , i.e. , designing a new training strategy to improve the capability of the dynamic matching network and fine-tuning BERT to obtain domain-related contextualized representations .",2,0.70450765,38.581123496940414,44
2594,"Experiments on benchmark datasets show that our proposed model consistently outperforms various baselines , leading to new state-of-the-art results on all domains .",3,0.9356788,8.16526968482281,29
2594,Detailed analysis on different matching strategies demonstrates that it is essential to learn suitable matching weights to emphasize useful features and ignore useless or even harmful ones .,3,0.7755634,68.72955495485932,28
2594,"Besides , our proposed model can be directly extended to multi-source domain adaptation and achieves best performances among various baselines , further verifying the effectiveness and robustness .",3,0.927488,36.179015935641424,28
2595,"Given the prevalence of pre-trained contextualized representations in today ’s NLP , there have been many efforts to understand what information they contain , and why they seem to be universally successful .",0,0.93904066,35.982316205003436,33
2595,The most common approach to use these representations involves fine-tuning them for an end task .,0,0.8015183,28.274711571561735,16
2595,"Yet , how fine-tuning changes the underlying embedding space is less studied .",0,0.9223396,37.25356128575109,13
2595,"In this work , we study the English BERT family and use two probing techniques to analyze how fine-tuning changes the space .",1,0.6250494,41.54085570403288,23
2595,We hypothesize that fine-tuning affects classification performance by increasing the distances between examples associated with different labels .,3,0.40078214,29.497400454668277,18
2595,We confirm this hypothesis with carefully designed experiments on five different NLP tasks .,3,0.5799091,17.94564486863439,14
2595,"Via these experiments , we also discover an exception to the prevailing wisdom that “ fine-tuning always improves performance ” .",3,0.88268125,38.24747478666626,23
2595,"Finally , by comparing the representations before and after fine-tuning , we discover that fine-tuning does not introduce arbitrary changes to representations ;",3,0.940728,27.901821107882274,23
2595,"instead , it adjusts the representations to downstream tasks while largely preserving the original spatial structure of the data points .",3,0.40979394,63.12421224712299,21
2596,Training dense passage representations via contrastive learning has been shown effective for Open-Domain Passage Retrieval ( ODPR ) .,0,0.93036324,52.85424963038632,19
2596,Existing studies focus on further optimizing by improving negative sampling strategy or extra pretraining .,0,0.7478029,204.34240425175517,15
2596,"However , these studies keep unknown in capturing passage with internal representation conflicts from improper modeling granularity .",0,0.79965454,913.3395136486592,18
2596,"Specifically , under our observation that a passage can be organized by multiple semantically different sentences , modeling such a passage as a unified dense vector is not optimal .",3,0.6841871,58.36899601474407,30
2596,"This work thus presents a refined model on the basis of a smaller granularity , contextual sentences , to alleviate the concerned conflicts .",3,0.44438836,180.08552091416328,24
2596,"In detail , we introduce an in-passage negative sampling strategy to encourage a diverse generation of sentence representations within the same passage .",2,0.60374254,52.51629311466207,24
2596,"Experiments on three benchmark datasets verify the efficacy of our method , especially on datasets where conflicts are severe .",3,0.8315133,23.543560387233562,20
2596,Extensive experiments further present good transferability of our method across datasets .,3,0.96197027,83.94801311470198,12
2597,Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language .,0,0.7772653,26.824674059878667,24
2597,"Recent works show that such models can also produce the reasoning steps ( i.e. , the proof graph ) that emulate the model ’s logical reasoning process .",0,0.86498696,75.21593008352582,28
2597,"Currently , these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful .",0,0.79807854,48.844840242574,25
2597,"In this work , we frame the deductive logical reasoning task by defining three modular components : rule selection , fact selection , and knowledge composition .",2,0.40661266,89.38542092777804,27
2597,The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences .,2,0.61574066,69.7224314628883,27
2597,This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning .,3,0.59465146,185.0828823846253,17
2597,"To test our framework , we propose FaiRR ( Faithful and Robust Reasoner ) where the above three components are independently modeled by transformers .",2,0.682905,85.78025932997164,25
2597,"We observe that FaiRR is robust to novel language perturbations , and is faster at inference than previous works on existing reasoning datasets .",3,0.95277053,78.25606271749929,24
2597,"Additionally , in contrast to black-box generative models , the errors made by FaiRR are more interpretable due to the modular approach .",3,0.9362835,53.720359652255816,25
2598,"Tables are often created with hierarchies , but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables .",0,0.8704833,76.59172602963393,23
2598,"Hierarchical tables challenge numerical reasoning by complex hierarchical indexing , as well as implicit relationships of calculation and semantics .",0,0.8691539,161.88387438188315,20
2598,"We present a new dataset , HiTab , to study question answering ( QA ) and natural language generation ( NLG ) over hierarchical tables .",1,0.4428505,43.71008400387195,26
2598,"HiTab is a cross-domain dataset constructed from a wealth of statistical reports and Wikipedia pages , and has unique characteristics : ( 1 ) nearly all tables are hierarchical , and ( 2 ) QA pairs are not proposed by annotators from scratch , but are revised from real and meaningful sentences authored by analysts .",0,0.49730515,92.98407710947926,56
2598,"( 3 ) to reveal complex numerical reasoning in statistical reports , we provide fine-grained annotations of quantity and entity alignment .",3,0.4040753,142.83565697025696,23
2598,Experiments suggest that this HiTab presents a strong challenge for existing baselines and a valuable benchmark for future research .,3,0.97692454,41.65748156223381,20
2598,"Targeting hierarchical structure , we devise a hierarchy-aware logical form for symbolic reasoning over tables , which shows high effectiveness .",2,0.5392244,133.7142080027825,23
2598,"Targeting table reasoning , we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG , and largely reduce spurious predictions in QA and produce better descriptions in NLG .",2,0.556592,120.12429661983766,37
2599,"Huge volumes of patient queries are daily generated on online health forums , rendering manual doctor allocation a labor-intensive task .",0,0.94172126,109.09174403621864,21
2599,"To better help patients , this paper studies a novel task of doctor recommendation to enable automatic pairing of a patient to a doctor with relevant expertise .",1,0.7080295,60.694152482413934,28
2599,"While most prior work in recommendation focuses on modeling target users from their past behavior , we can only rely on the limited words in a query to infer a patient ’s needs for privacy reasons .",0,0.7224847,71.0978883223815,37
2599,"For doctor modeling , we study the joint effects of their profiles and previous dialogues with other patients and explore their interactions via self-learning .",2,0.8066392,93.51329142547685,26
2599,The learned doctor embeddings are further employed to estimate their capabilities of handling a patient query with a multi-head attention mechanism .,2,0.6254008,62.75551507401948,22
2599,"For experiments , a large-scale dataset is collected from Chunyu Yisheng , a Chinese online health forum , where our model exhibits the state-of-the-art results , outperforming baselines only consider profiles and past dialogues to characterize a doctor .",2,0.7368018,54.35908014689492,45
2600,"A desirable dialog system should be able to continually learn new skills without forgetting old ones , and thereby adapt to new domains or tasks in its life cycle .",0,0.6565857,37.92675628726933,30
2600,"However , continually training a model often leads to a well-known catastrophic forgetting issue .",0,0.92796236,63.39312464829766,16
2600,"In this paper , we present Continual Prompt Tuning , a parameter-efficient framework that not only avoids forgetting but also enables knowledge transfer between tasks .",1,0.8456474,37.73958894540163,28
2600,"To avoid forgetting , we only learn and store a few prompt tokens ’ embeddings for each task while freezing the backbone pre-trained model .",2,0.7640101,110.45018130542412,25
2600,"To achieve bi-directional knowledge transfer among tasks , we propose several techniques ( continual prompt initialization , query fusion , and memory replay ) to transfer knowledge from preceding tasks and a memory-guided technique to transfer knowledge from subsequent tasks .",2,0.6125943,50.958189252215156,43
2600,"Extensive experiments demonstrate the effectiveness and efficiency of our proposed method on continual learning for dialog state tracking , compared with state-of-the-art baselines .",3,0.867319,14.488321964757143,30
2601,"Images are often more significant than only the pixels to human eyes , as we can infer , associate , and reason with contextual information from other sources to establish a more complete picture .",0,0.81755227,86.55579229478167,35
2601,"For example , in Figure 1 , we can find a way to identify the news articles related to the picture through segment-wise understandings of the signs , the buildings , the crowds , and more .",3,0.6464542,73.44322096157771,39
2601,"This reasoning could provide the time and place the image was taken , which will help us in subsequent tasks , such as automatic storyline construction , correction of image source in intended effect photographs , and upper-stream processing such as image clustering for certain location or time .",3,0.7400484,158.75964559314193,49
2601,"In this work , we formulate this problem and introduce TARA : a dataset with 16 k images with their associated news , time , and location , automatically extracted from New York Times , and an additional 61 k examples as distant supervision from WIT .",2,0.5318325,114.663748321187,47
2601,"On top of the extractions , we present a crowdsourced subset in which we believe it is possible to find the images ’ spatio-temporal information for evaluation purpose .",3,0.556991,50.805906092086076,29
2601,"We show that there exists a 70 % gap between a state-of-the-art joint model and human performance , which is slightly filled by our proposed model that uses segment-wise reasoning , motivating higher-level vision-language joint models that can conduct open-ended reasoning with world knowledge .",3,0.9360616,45.25978716434177,57
2601,The data and code are publicly available at https://github.com/zeyofu/TARA .,3,0.58667094,11.2358207990789,10
2602,"Tables store rich numerical data , but numerical reasoning over tables is still a challenge .",0,0.9676086,122.78107238716515,16
2602,"In this paper , we find that the spreadsheet formula , a commonly used language to perform computations on numerical values in spreadsheets , is a valuable supervision for numerical reasoning in tables .",3,0.89719754,59.99697061500356,34
2602,"Considering large amounts of spreadsheets available on the web , we propose FORTAP , the first exploration to leverage spreadsheet formulas for table pretraining .",3,0.49167755,83.76915183178033,25
2602,"Two novel self-supervised pretraining objectives are derived from formulas , numerical reference prediction ( NRP ) and numerical calculation prediction ( NCP ) .",2,0.5538537,86.74060352559411,24
2602,"While our proposed objectives are generic for encoders , to better capture spreadsheet table layouts and structures , FORTAP is built upon TUTA , the first transformer-based method for spreadsheet table pretraining with tree attention .",3,0.4176149,179.33673705240207,38
2602,"FORTAP outperforms state-of-the-art methods by large margins on three representative datasets of formula prediction , question answering , and cell type classification , showing the great potential of leveraging formulas for table pretraining .",3,0.94319314,56.86637687181369,39
2603,Information integration from different modalities is an active area of research .,0,0.9311148,24.06819538596701,12
2603,"Human beings and , in general , biological neural systems are quite adept at using a multitude of signals from different sensory perceptive fields to interact with the environment and each other .",0,0.952012,64.12429730243899,33
2603,"Recent work in deep fusion models via neural networks has led to substantial improvements over unimodal approaches in areas like speech recognition , emotion recognition and analysis , captioning and image description .",0,0.92131823,56.80073982546675,33
2603,"However , such research has mostly focused on architectural changes allowing for fusion of different modalities while keeping the model complexity manageable .",0,0.9233704,52.54233043502414,23
2603,"Inspired by neuroscientific ideas about multisensory integration and processing , we investigate the effect of introducing neural dependencies in the loss functions .",1,0.41996452,42.099375687407914,23
2603,Experiments on multimodal sentiment analysis tasks with different models show that our approach provides a consistent performance boost .,3,0.9463226,16.09837719433174,19
2604,Procedural Multimodal Documents ( PMDs ) organize textual instructions and corresponding images step by step .,0,0.87823534,78.51482409096221,16
2604,Comprehending PMDs and inducing their representations for the downstream reasoning tasks is designated as Procedural MultiModal Machine Comprehension ( M3C ) .,0,0.8252872,90.77537446122291,22
2604,"In this study , we approach Procedural M3 C at a fine-grained level ( compared with existing explorations at a document or sentence level ) , that is , entity .",2,0.5433992,155.01766020590185,32
2604,"With delicate consideration , we model entity both in its temporal and cross-modal relation and propose a novel Temporal-Modal Entity Graph ( TMEG ) .",2,0.5723802,53.13983605036618,27
2604,"Specifically , graph structure is formulated to capture textual and visual entities and trace their temporal-modal evolution .",2,0.51019305,75.38508355377867,18
2604,"In addition , a graph aggregation module is introduced to conduct graph encoding and reasoning .",2,0.6883672,73.92802633928788,16
2604,"Comprehensive experiments across three Procedural M3C tasks are conducted on a traditional dataset RecipeQA and our new dataset CraftQA , which can better evaluate the generalization of TMEG .",2,0.7061379,82.2678033438669,29
2605,Pre-trained sequence-to-sequence language models have led to widespread success in many natural language generation tasks .,0,0.94338524,5.98373655410062,18
2605,"However , there has been relatively less work on analyzing their ability to generate structured outputs such as graphs .",0,0.9378824,37.3353737289898,20
2605,"Unlike natural language , graphs have distinct structural and semantic properties in the context of a downstream NLP task , e.g. , generating a graph that is connected and acyclic can be attributed to its structural constraints , while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node concepts .",0,0.7893469,36.24366323798481,59
2605,"In this work , we study pre-trained language models that generate explanation graphs in an end-to-end manner and analyze their ability to learn the structural constraints and semantics of such graphs .",1,0.79081124,17.49420725289155,34
2605,"We first show that with limited supervision , pre-trained language models often generate graphs that either violate these constraints or are semantically incoherent .",3,0.68472177,27.759566054217693,24
2605,"Since curating large amount of human-annotated graphs is expensive and tedious , we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs .",2,0.34787878,46.40258351437688,38
2605,"Next , we leverage these graphs in different contrastive learning models with Max-Margin and InfoNCE losses .",2,0.74251586,140.5664233972236,18
2605,Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks .,3,0.91707903,31.577927561828986,25
2605,"Lastly , we show that human errors are the best negatives for contrastive learning and also that automatically generating more such human-like negative graphs can lead to further improvements .",3,0.9759646,45.850726978190174,31
2606,Opinion summarization is the task of automatically generating summaries that encapsulate information expressed in multiple user reviews .,0,0.92688453,22.44124679729206,18
2606,We present Semantic Autoencoder ( SemAE ) to perform extractive opinion summarization in an unsupervised manner .,2,0.62389266,22.339126510307615,17
2606,SemAE uses dictionary learning to implicitly capture semantic information from the review text and learns a latent representation of each sentence over semantic units .,2,0.4847946,69.00474050654432,25
2606,Our extractive summarization algorithm leverages the representations to identify representative opinions among hundreds of reviews .,2,0.65516734,52.11377046091336,16
2606,SemAE is also able to perform controllable summarization to generate aspect-specific summaries using only a few samples .,3,0.77339834,27.677394773104794,18
2606,We report strong performance on SPACE and AMAZON datasets and perform experiments to investigate the functioning of our model .,3,0.7584879,61.44029154614796,20
2607,Lexical substitution is the task of generating meaningful substitutes for a word in a given textual context .,0,0.9193356,24.279692068261948,18
2607,Contextual word embedding models have achieved state-of-the-art results in the lexical substitution task by relying on contextual information extracted from the replaced word within the sentence .,0,0.86158687,15.850004202614148,33
2607,"However , such models do not take into account structured knowledge that exists in external lexical databases .",0,0.8831444,33.76804825641179,18
2607,"We introduce LexSubCon , an end-to-end lexical substitution framework based on contextual embedding models that can identify highly-accurate substitute candidates .",2,0.41678464,42.28789502486803,25
2607,This is achieved by combining contextual information with knowledge from structured lexical resources .,0,0.4944181,25.024564717329092,14
2607,Our approach involves : ( i ) introducing a novel mix-up embedding strategy to the target word ’s embedding through linearly interpolating the pair of the target input embedding and the average embedding of its probable synonyms ;,2,0.80807495,53.754055076034604,39
2607,( ii ) considering the similarity of the sentence-definition embeddings of the target word and its proposed candidates ;,2,0.5556403,77.86831202804069,20
2607,"and , ( iii ) calculating the effect of each substitution on the semantics of the sentence through a fine-tuned sentence similarity model .",2,0.7604088,42.20738457957272,24
2607,Our experiments show that LexSubCon outperforms previous state-of-the-art methods by at least 2 % over all the official lexical substitution metrics on LS07 and CoInCo benchmark datasets that are widely used for lexical substitution tasks .,3,0.953683,33.43937686916444,42
2608,"Implicit knowledge , such as common sense , is key to fluid human conversations .",0,0.94431174,162.42752388973759,15
2608,"Current neural response generation ( RG ) models are trained to generate responses directly , omitting unstated implicit knowledge .",0,0.8891469,208.90873618803352,20
2608,"In this paper , we present Think-Before-Speaking ( TBS ) , a generative approach to first externalize implicit commonsense knowledge ( think ) and use this knowledge to generate responses ( speak ) .",1,0.8578517,68.68578443006919,38
2608,"We argue that externalizing implicit knowledge allows more efficient learning , produces more informative responses , and enables more explainable models .",3,0.87592715,82.1468336958994,22
2608,"We analyze different choices to collect knowledge-aligned dialogues , represent implicit knowledge , and transition between knowledge and dialogues .",2,0.69492817,143.84079312623047,21
2608,"Empirical results show TBS models outperform end-to-end and knowledge-augmented RG baselines on most automatic metrics and generate more informative , specific , and commonsense-following responses , as evaluated by human annotators .",3,0.96734434,42.07624612843909,36
2608,TBS also generates knowledge that makes sense and is relevant to the dialogue around 85 % of the time .,3,0.7658525,84.9391452975312,20
2609,"In this work , we propose a flow-adapter architecture for unsupervised NMT .",1,0.8463734,23.505494254666804,13
2609,"It leverages normalizing flows to explicitly model the distributions of sentence-level latent representations , which are subsequently used in conjunction with the attention mechanism for the translation task .",2,0.590344,43.956405409980206,31
2609,The primary novelties of our model are : ( a ) capturing language-specific sentence representations separately for each language using normalizing flows and ( b ) using a simple transformation of these latent representations for translating from one language to another .,2,0.4597847,38.88031683712755,43
2609,This architecture allows for unsupervised training of each language independently .,2,0.47583124,34.9994693201707,11
2609,"While there is prior work on latent variables for supervised MT , to the best of our knowledge , this is the first work that uses latent variables and normalizing flows for unsupervised MT .",3,0.52962863,22.887385537361446,35
2609,We obtain competitive results on several unsupervised MT benchmarks .,3,0.9236074,29.87959448976124,10
2610,Sentence compression reduces the length of text by removing non-essential content while preserving important facts and grammaticality .,0,0.6476761,31.740400766679365,18
2610,"Unsupervised objective driven methods for sentence compression can be used to create customized models without the need for ground-truth training data , while allowing flexibility in the objective function ( s ) that are used for learning and inference .",0,0.5139223,63.21499877964224,42
2610,Recent unsupervised sentence compression approaches use custom objectives to guide discrete search ;,0,0.80421007,434.1617613828395,13
2610,"however , guided search is expensive at inference time .",0,0.7999592,221.89229706273852,10
2610,"In this work , we explore the use of reinforcement learning to train effective sentence compression models that are also fast when generating predictions .",1,0.8193873,42.85456876218601,25
2610,"In particular , we cast the task as binary sequence labelling and fine-tune a pre-trained transformer using a simple policy gradient approach .",2,0.7821964,32.22671364356453,24
2610,Our approach outperforms other unsupervised models while also being more efficient at inference time .,3,0.91038036,19.645654311190967,15
2611,"Machine reading comprehension is a heavily-studied research and test field for evaluating new pre-trained language models ( PrLMs ) and fine-tuning strategies , and recent studies have enriched the pre-trained language models with syntactic , semantic and other linguistic information to improve the performance of the models .",0,0.9188488,20.832689052844255,50
2611,"In this paper , we imitate the human reading process in connecting the anaphoric expressions and explicitly leverage the coreference information of the entities to enhance the word embeddings from the pre-trained language model , in order to highlight the coreference mentions of the entities that must be identified for coreference-intensive question answering in QUOREF , a relatively new dataset that is specifically designed to evaluate the coreference-related performance of a model .",3,0.26305035,38.311853689074766,77
2611,"We use two strategies to fine-tune a pre-trained language model , namely , placing an additional encoder layer after a pre-trained language model to focus on the coreference mentions or constructing a relational graph convolutional network to model the coreference relations .",2,0.8185181,19.094164827882675,43
2611,We demonstrate that the explicit incorporation of coreference information in the fine-tuning stage performs better than the incorporation of the coreference information in pre-training a language model .,3,0.96502864,17.241801740835,28
2612,We contribute a new dataset for the task of automated fact checking and an evaluation of state of the art algorithms .,3,0.35432786,22.227097674305888,22
2612,"The dataset includes claims ( from speeches , interviews , social media and news articles ) , review articles published by professional fact checkers and premise articles used by those professional fact checkers to support their review and verify the veracity of the claims .",2,0.82871306,47.455954523869465,45
2612,An important challenge in the use of premise articles is the identification of relevant passages that will help to infer the veracity of a claim .,0,0.8621856,28.854778171415646,26
2612,We show that transferring a dense passage retrieval model trained with review articles improves the retrieval quality of passages in premise articles .,3,0.97018886,166.73996912293063,23
2612,We report results for the prediction of claim veracity by inference from premise articles .,3,0.7006716,89.43914114964073,15
2613,Fast and reliable evaluation metrics are key to R&D progress .,0,0.7847556,30.882505173354538,11
2613,"While traditional natural language generation metrics are fast , they are not very reliable .",0,0.8851608,75.13190772960355,15
2613,"Conversely , new metrics based on large pretrained language models are much more reliable , but require significant computational resources .",0,0.699402,42.04783575154397,21
2613,"In this paper , we propose FrugalScore , an approach to learn a fixed , low cost version of any expensive NLG metric , while retaining most of its original performance .",1,0.8399022,67.55616253960095,32
2613,"Experiments with BERTScore and MoverScore on summarization and translation show that FrugalScore is on par with the original metrics ( and sometimes better ) , while having several orders of magnitude less parameters and running several times faster .",3,0.9211993,51.15904882932237,39
2613,"On average over all learned metrics , tasks , and variants , FrugalScore retains 96.8 % of the performance , runs 24 times faster , and has 35 times less parameters than the original metrics .",3,0.9200978,110.49084752139898,36
2613,"We make our trained metrics publicly available , to benefit the entire NLP community and in particular researchers and practitioners with limited resources .",3,0.49002257,34.37761855716196,24
2614,"We propose Composition Sampling , a simple but effective method to generate diverse outputs for conditional generation of higher quality compared to previous stochastic decoding strategies .",2,0.33917233,40.6888684139435,27
2614,"It builds on recently proposed plan-based neural generation models ( FROST , Narayan et al , 2021 ) that are trained to first create a composition of the output and then generate by conditioning on it and the input .",2,0.41644484,52.475791237817724,40
2614,Our approach avoids text degeneration by first sampling a composition in the form of an entity chain and then using beam search to generate the best possible text grounded to this entity chain .,2,0.71336955,45.958946796524295,34
2614,"Experiments on summarization ( CNN / DailyMail and XSum ) and question generation ( SQuAD ) , using existing and newly proposed automaticmetrics together with human-based evaluation , demonstrate that Composition Sampling is currently the best available decoding strategy for generating diverse meaningful outputs .",3,0.9056082,71.398930505574,46
2615,Synthesizing QA pairs with a question generator ( QG ) on the target domain has become a popular approach for domain adaptation of question answering ( QA ) models .,0,0.9580188,26.391436417135083,30
2615,"Since synthetic questions are often noisy in practice , existing work adapts scores from a pretrained QA ( or QG ) model as criteria to select high-quality questions .",0,0.7865852,70.46226808383335,29
2615,"However , these scores do not directly serve the ultimate goal of improving QA performance on the target domain .",0,0.77464914,36.217636764043306,20
2615,"In this paper , we introduce a novel idea of training a question value estimator ( QVE ) that directly estimates the usefulness of synthetic questions for improving the target-domain QA performance .",1,0.8317029,35.43730211735152,34
2615,"By conducting comprehensive experiments , we show that the synthetic questions selected by QVE can help achieve better target-domain QA performance , in comparison with existing techniques .",3,0.9166526,84.03712608783532,29
2615,"We additionally show that by using such questions and only around 15 % of the human annotations on the target domain , we can achieve comparable performance to the fully-supervised baselines .",3,0.931384,35.860206588773025,34
2616,Class-based language models ( LMs ) have been long devised to address context sparsity in n-gram LMs .,0,0.9178762,38.063062283892485,20
2616,"In this study , we revisit this approach in the context of neural LMs .",1,0.9128619,40.710002639928504,15
2616,We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words .,3,0.43632895,55.71685728280961,24
2616,We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training .,2,0.84551287,89.69235835878185,31
2616,"Empirically , this curriculum learning strategy consistently improves perplexity over various large , highly-performant state-of-the-art Transformer-based models on two datasets , WikiText-103 and ARXIV .",3,0.83554745,45.55227332728687,36
2616,Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words .,3,0.98232883,25.82274033987714,16
2616,"Finally , we document other attempts that failed to yield empirical gains , and discuss future directions for the adoption of class-based LMs on a larger scale .",3,0.7854255,57.17604109885491,30
2617,"Easy access , variety of content , and fast widespread interactions are some of the reasons making social media increasingly popular .",0,0.8934906,105.15150169825138,22
2617,"However , this rise has also enabled the propagation of fake news , text published by news sources with an intent to spread misinformation and sway beliefs .",0,0.91033196,61.598288063693914,28
2617,Detecting it is an important and challenging problem to prevent large scale misinformation and maintain a healthy society .,0,0.8898546,53.06015386577708,19
2617,"We view fake news detection as reasoning over the relations between sources , articles they publish , and engaging users on social media in a graph framework .",2,0.6719791,113.06646654871174,28
2617,"After embedding this information , we formulate inference operators which augment the graph edges by revealing unobserved interactions between its elements , such as similarity between documents ’ contents and users ’ engagement patterns .",2,0.7479561,102.05463513000744,35
2617,"Our experiments over two challenging fake news detection tasks show that using inference operators leads to a better understanding of the social media framework enabling fake news spread , resulting in improved performance .",3,0.92829835,56.211126807722934,34
2618,Knowledge base ( KB ) embeddings have been shown to contain gender biases .,0,0.93168026,35.8204210882237,14
2618,"Specifically , first , we develop two novel bias measures respectively for a group of person entities and an individual person entity .",2,0.8047835,112.45040108432767,23
2618,Evidence of their validity is observed by comparison with real-world census data .,3,0.50400215,46.185137742344075,13
2618,"Second , we use the influence function to inspect the contribution of each triple in KB to the overall group bias .",2,0.8689879,63.63122476346725,22
2618,"To exemplify the potential applications of our study , we also present two strategies ( by adding and removing KB triples ) to mitigate gender biases in KB embeddings .",3,0.4390122,51.25178322192284,30
2619,"South Asia is home to a plethora of languages , many of which severely lack access to new language technologies .",0,0.9503734,34.9807992574665,21
2619,"This linguistic diversity also results in a research environment conducive to the study of comparative , contact , and historical linguistics –fields which necessitate the gathering of extensive data from many languages .",0,0.7419942,75.75812071885184,33
2619,"We claim that data scatteredness ( rather than scarcity ) is the primary obstacle in the development of South Asian language technology , and suggest that the study of language history is uniquely aligned with surmounting this obstacle .",3,0.94154125,65.28223827742148,39
2619,"We review recent developments in and at the intersection of South Asian NLP and historical-comparative linguistics , describing our and others ’ current efforts in this area .",1,0.7572735,49.82277952246874,30
2619,We also offer new strategies towards breaking the data barrier .,3,0.5481891,92.54770729027878,11
2620,"Despite recent progress in abstractive summarization , systems still suffer from faithfulness errors .",0,0.9312334,46.355102279348394,14
2620,"While prior work has proposed models that improve faithfulness , it is unclear whether the improvement comes from an increased level of extractiveness of the model outputs as one naive way to improve faithfulness is to make summarization models more extractive .",0,0.83878446,33.14974336761665,42
2620,"In this work , we present a framework for evaluating the effective faithfulness of summarization systems , by generating a faithfulness-abstractiveness trade-off curve that serves as a control at different operating points on the abstractiveness spectrum .",1,0.8202112,42.77370350167867,41
2620,"We then show that the Maximum Likelihood Estimation ( MLE ) baseline as well as recently proposed methods for improving faithfulness , fail to consistently improve over the control at the same level of abstractiveness .",3,0.84957373,57.151672580387626,36
2620,"Finally , we learn a selector to identify the most faithful and abstractive summary for a given document , and show that this system can attain higher faithfulness scores in human evaluations while being more abstractive than the baseline system on two datasets .",3,0.7690712,34.097267298166535,44
2620,"Moreover , we show that our system is able to achieve a better faithfulness-abstractiveness trade-off than the control at the same level of abstractiveness .",3,0.97077954,18.82901994717534,29
2621,"Languages are continuously undergoing changes , and the mechanisms that underlie these changes are still a matter of debate .",0,0.945491,23.44278359510994,20
2621,"In this work , we approach language evolution through the lens of causality in order to model not only how various distributional factors associate with language change , but how they causally affect it .",1,0.74895364,37.51721772029709,35
2621,"In particular , we study slang , which is an informal language that is typically restricted to a specific group or social setting .",2,0.60483795,47.551578056010975,24
2621,"We analyze the semantic change and frequency shift of slang words and compare them to those of standard , nonslang words .",2,0.6108706,70.84782638101446,22
2621,"With causal discovery and causal inference techniques , we measure the effect that word type ( slang / nonslang ) has on both semantic change and frequency shift , as well as its relationship to frequency , polysemy and part of speech .",2,0.7991689,77.84154553562679,43
2621,"Our analysis provides some new insights in the study of language change , e.g. , we show that slang words undergo less semantic change but tend to have larger frequency shifts over time .",3,0.97616416,46.32316219441583,34
2622,"Model-based , reference-free evaluation metricshave been proposed as a fast and cost-effectiveapproach to evaluate Natural Language Generation ( NLG ) systems .",0,0.9347602,36.66102252782213,24
2622,"Despite promising recentresults , we find evidence that reference-freeevaluation metrics of summarization and dialoggeneration may be relying on spuriouscorrelations with measures such as word overlap , perplexity , and length .",3,0.8978145,147.52702836274074,32
2622,"We further observethat for text summarization , these metrics havehigh error rates when ranking current state-ofthe-art abstractive summarization systems .",3,0.93928766,91.6236318520668,23
2622,Wedemonstrate that these errors can be mitigatedby explicitly designing evaluation metrics toavoid spurious features in reference-free evaluation .,3,0.800418,89.68363397866825,19
2623,"Semantic parsers map natural language utterances into meaning representations ( e.g. , programs ) .",0,0.73987675,65.4317973898345,15
2623,Such models are typically bottlenecked by the paucity of training data due to the required laborious annotation efforts .,0,0.92056113,27.63302042465307,19
2623,"Recent studies have performed zero-shot learning by synthesizing training examples of canonical utterances and programs from a grammar , and further paraphrasing these utterances to improve linguistic diversity .",0,0.90677303,53.99964282826314,29
2623,"However , such synthetic examples cannot fully capture patterns in real data .",0,0.8075943,74.06980316257818,13
2623,"In this paper we analyze zero-shot parsers through the lenses of the language and logical gaps ( Herzig and Berant , 2019 ) , which quantify the discrepancy of language and programmatic patterns between the canonical examples and real-world user-issued ones .",1,0.7724713,83.016607648517,42
2623,"We propose bridging these gaps using improved grammars , stronger paraphrasers , and efficient learning methods using canonical examples that most likely reflect real user intents .",3,0.71944624,146.62017814956067,27
2623,"Our model achieves strong performance on two semantic parsing benchmarks ( Scholar , Geo ) with zero labeled data .",3,0.7622917,149.8301637393506,20
2624,Machine Translation Quality Estimation ( QE ) aims to build predictive models to assess the quality of machine-generated translations in the absence of reference translations .,0,0.9271021,32.19033769897695,28
2624,"While state-of-the-art QE models have been shown to achieve good results , they over-rely on features that do not have a causal impact on the quality of a translation .",0,0.84937084,14.262167705080623,34
2624,"In particular , there appears to be a partial input bias , i.e. , a tendency to assign high-quality scores to translations that are fluent and grammatically correct , even though they do not preserve the meaning of the source .",3,0.9171261,27.734090307152766,41
2624,We analyse the partial input bias in further detail and evaluate four approaches to use auxiliary tasks for bias mitigation .,2,0.5331828,143.11033330284005,21
2624,"Two approaches use additional data to inform and support the main task , while the other two are adversarial , actively discouraging the model from learning the bias .",0,0.4728741,75.71063217164,29
2624,We compare the methods with respect to their ability to reduce the partial input bias while maintaining the overall performance .,2,0.63617474,31.32693508676252,21
2624,We find that training a multitask architecture with an auxiliary binary classification task that utilises additional augmented data best achieves the desired effects and generalises well to different languages and quality metrics .,3,0.9693234,61.56675023328394,33
2625,"In this work , we describe a method to jointly pre-train speech and text in an encoder-decoder modeling framework for speech translation and recognition .",1,0.8492539,17.97784004180842,25
2625,The proposed method utilizes multi-task learning to integrate four self-supervised and supervised subtasks for cross modality learning .,2,0.6476548,30.690965108040636,18
2625,"A self-supervised speech subtask , which leverages unlabelled speech data , and a ( self-) supervised text to text subtask , which makes use of abundant text training data , take up the majority of the pre-training time .",3,0.41427606,55.51715996711206,42
2625,Two auxiliary supervised speech tasks are included to unify speech and text modeling space .,2,0.7151867,110.87707818959755,15
2625,Detailed analysis reveals learning interference among subtasks .,3,0.880465,246.51149343995067,8
2625,"In order to alleviate the subtask interference , two pre-training configurations are proposed for speech translation and speech recognition respectively .",2,0.5570656,49.410640526845,21
2625,Our experiments show the proposed method can effectively fuse speech and text information into one model .,3,0.97068787,33.988709457887,17
2625,It achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the MuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the Librispeech speech recognition task .,3,0.8566201,20.469191425675206,35
2626,"Pretrained multilingual models enable zero-shot learning even for unseen languages , and that performance can be further improved via adaptation prior to finetuning .",3,0.57036734,32.658314592845834,24
2626,"However , it is unclear how the number of pretraining languages influences a model ’s zero-shot learning for languages unseen during pretraining .",0,0.87357986,33.633664559720344,23
2626,Our experiments on pretraining with related languages indicate that choosing a diverse set of languages is crucial .,3,0.96648675,39.8205802029343,18
2626,"Without model adaptation , surprisingly , increasing the number of pretraining languages yields better results up to adding related languages , after which performance plateaus .",3,0.94705373,182.56853992634532,26
2626,"In contrast , with model adaptation via continued pretraining , pretraining on a larger number of languages often gives further improvement , suggesting that model adaptation is crucial to exploit additional pretraining languages .",3,0.9427379,66.66403128791116,34
2627,The growing size of neural language models has led to increased attention in model compression .,0,0.9487182,24.96525827637635,16
2627,"The two predominant approaches are pruning , which gradually removes weights from a pre-trained model , and distillation , which trains a smaller compact model to match a larger one .",0,0.72964245,33.433876249210854,31
2627,Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation .,3,0.8131114,55.34398186403422,16
2627,"However , distillation methods require large amounts of unlabeled data and are expensive to train .",0,0.92080086,13.699954887946364,16
2627,"In this work , we propose a task-specific structured pruning method CoFi ( Coarse-and Fine-grained Pruning ) , which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency , without resorting to any unlabeled data .",1,0.61057246,42.82774646313092,43
2627,"Our key insight is to jointly prune coarse-grained ( e.g. , layers ) and fine-grained ( e.g. , heads and hidden units ) modules , which controls the pruning decision of each parameter with masks of different granularity .",3,0.5037672,38.06795399151739,39
2627,We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization .,2,0.6928547,29.63623574474414,18
2627,"Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop , showing its effectiveness and efficiency compared to previous pruning and distillation approaches .",3,0.95455444,38.956431329746046,35
2628,"More than 43 % of the languages spoken in the world are endangered , and language loss currently occurs at an accelerated rate because of globalization and neocolonialism .",0,0.9291733,22.61198794154017,29
2628,Saving and revitalizing endangered languages has become very important for maintaining the cultural diversity on our planet .,0,0.9499884,29.06777285157421,18
2628,"In this work , we focus on discussing how NLP can help revitalize endangered languages .",1,0.89304906,42.69193224913866,16
2628,"We first suggest three principles that may help NLP practitioners to foster mutual understanding and collaboration with language communities , and we discuss three ways in which NLP can potentially assist in language education .",1,0.6092695,36.294491600015085,35
2628,"We then take Cherokee , a severely-endangered Native American language , as a case study .",2,0.70203644,61.69702885143337,18
2628,"After reviewing the language ’s history , linguistic features , and existing resources , we ( in collaboration with Cherokee community members ) arrive at a few meaningful ways NLP practitioners can collaborate with community partners .",3,0.63892317,114.4929040716357,37
2628,"We suggest two approaches to enrich the Cherokee language ’s resources with machine-in-the-loop processing , and discuss several NLP tools that people from the Cherokee community have shown interest in .",1,0.48049167,36.107975095229804,36
2628,"We hope that our work serves not only to inform the NLP community about Cherokee , but also to provide inspiration for future work on endangered languages in general .",3,0.9261488,25.7790714529214,30
2629,"The IMPRESSIONS section of a radiology report about an imaging study is a summary of the radiologist ’s reasoning and conclusions , and it also aids the referring physician in confirming or excluding certain diagnoses .",0,0.71724606,43.5042286124966,36
2629,A cascade of tasks are required to automatically generate an abstractive summary of the typical information-rich radiology report .,0,0.69617647,41.80667720084618,21
2629,"These tasks include acquisition of salient content from the report and generation of a concise , easily consumable IMPRESSIONS section .",0,0.6150449,122.50352450317617,21
2629,Prior research on radiology report summarization has focused on single-step end-to-end models – which subsume the task of salient content acquisition .,0,0.9372601,47.29831342972091,24
2629,"To fully explore the cascade structure and explainability of radiology report summarization , we introduce two innovations .",2,0.4430355,88.77748749021009,18
2629,"First , we design a two-step approach : extractive summarization followed by abstractive summarization .",2,0.9039658,25.184739878947468,15
2629,"Second , we additionally break down the extractive part into two independent tasks : extraction of salient ( 1 ) sentences and ( 2 ) keywords .",2,0.881705,61.64671272505557,27
2629,Experiments on English radiology reports from two clinical sites show our novel approach leads to a more precise summary compared to single-step and to two-step-with-single-extractive-process baselines with an overall improvement in F1 score of 3-4 % .,3,0.9352179,36.397611861556356,47
2630,"Standard conversational semantic parsing maps a complete user utterance into an executable program , after which the program is executed to respond to the user .",0,0.88229746,42.768115337747616,26
2630,This could be slow when the program contains expensive function calls .,3,0.5094328,110.50401984194976,12
2630,We investigate the opportunity to reduce latency by predicting and executing function calls while the user is still speaking .,1,0.5927442,62.72395302318799,20
2630,"We introduce the task of online semantic parsing for this purpose , with a formal latency reduction metric inspired by simultaneous machine translation .",1,0.49498904,140.08852867846875,24
2630,"We propose a general framework with first a learned prefix-to-program prediction module , and then a simple yet effective thresholding heuristic for subprogram selection for early execution .",2,0.4287977,79.44680975362336,30
2630,"Experiments on the SMCalFlow and TreeDST datasets show our approach achieves large latency reduction with good parsing quality , with a 30 % –65 % latency reduction depending on function execution time and allowed cost .",3,0.9342588,121.14463343082508,37
2631,The enrichment of tabular datasets using external sources has gained significant attention in recent years .,0,0.95774734,27.499333760725417,16
2631,"Existing solutions , however , either ignore external unstructured data completely or devise dataset-specific solutions .",0,0.92846787,106.2374911602894,17
2631,"In this study we proposed Few-Shot Transformer based Enrichment ( FeSTE ) , a generic and robust framework for the enrichment of tabular datasets using unstructured data .",1,0.74854517,60.62455981822487,30
2631,"By training over multiple datasets , our approach is able to develop generic models that can be applied to additional datasets with minimal training ( i.e. , few-shot ) .",3,0.69427395,32.69542203833896,30
2631,"Our approach is based on an adaptation of BERT , for which we present a novel fine-tuning approach that reformulates the tuples of the datasets as sentences .",2,0.7300776,21.578402788284688,28
2631,"Our evaluation , conducted on 17 datasets , shows that FeSTE is able to generate high quality features and significantly outperform existing fine-tuning solutions .",3,0.95257753,54.146160140640085,25
2632,"Text summarization helps readers capture salient information from documents , news , interviews , and meetings .",0,0.8950833,98.30807353775484,17
2632,"However , most state-of-the-art pretrained language models ( LM ) are unable to efficiently process long text for many summarization tasks .",0,0.927005,19.412942807124058,28
2632,"In this paper , we propose SummN , a simple , flexible , and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs .",1,0.821125,60.62753741975614,33
2632,SummN first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it .,2,0.5700442,35.878139875130096,27
2632,Our framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM input size fixed .,3,0.7440307,73.33103115353374,23
2632,"Moreover , it can deal with both single-source documents and dialogues , and it can be used on top of different backbone abstractive summarization models .",3,0.7041568,35.11357435729097,27
2632,"To the best of our knowledge , SummN is the first multi-stage split-then-summarize framework for long input summarization .",3,0.85729843,25.217245236412026,21
2632,"Our experiments demonstrate that SummN outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI , ICSI , and QMSum , two long TV series datasets from SummScreen , and a long document summarization dataset GovReport .",3,0.9069731,42.21783129178375,47
2632,Our data and code are available at https://github.com/psunlpgroup/Summ-N .,3,0.6560308,19.70515937939683,9
2633,The retriever-reader framework is popular for open-domain question answering ( ODQA ) due to its ability to use explicit knowledge .,0,0.92882603,28.99056118633958,23
2633,"Although prior work has sought to increase the knowledge coverage by incorporating structured knowledge beyond text , accessing heterogeneous knowledge sources through a unified interface remains an open question .",0,0.9184135,47.74537283630016,30
2633,"While data-to-text generation has the potential to serve as a universal interface for data and text , its feasibility for downstream tasks remains largely unknown .",0,0.94808495,21.064030932981588,28
2633,"In this work , we bridge this gap and use the data-to-text method as a means for encoding structured knowledge for open-domain question answering .",1,0.79324967,22.538545662716654,28
2633,"Specifically , we propose a verbalizer-retriever-reader framework for ODQA over data and text where verbalized tables from Wikipedia and graphs from Wikidata are used as augmented knowledge sources .",2,0.6596617,56.08144022495029,33
2633,"We show that our Unified Data and Text QA , UDT-QA , can effectively benefit from the expanded knowledge index , leading to large gains over text-only baselines .",3,0.9535207,68.53829728508103,31
2633,"Notably , our approach sets the single-model state-of-the-art on Natural Questions .",3,0.7804384,19.062388206794207,17
2633,"Furthermore , our analyses indicate that verbalized knowledge is preferred for answer reasoning for both adapted and hot-swap settings .",3,0.98798007,90.97678659032903,21
2634,"Round-trip Machine Translation ( MT ) is a popular choice for paraphrase generation , which leverages readily available parallel corpora for supervision .",0,0.96131104,47.37145484318296,23
2634,"In this paper , we formalize the implicit similarity function induced by this approach , and show that it is susceptible to non-paraphrase pairs sharing a single ambiguous translation .",1,0.6854169,47.62566749825603,30
2634,"Based on these insights , we design an alternative similarity metric that mitigates this issue by requiring the entire translation distribution to match , and implement a relaxation of it through the Information Bottleneck method .",2,0.5436853,66.49373893664416,36
2634,"Our approach incorporates an adversarial term into MT training in order to learn representations that encode as much information about the reference translation as possible , while keeping as little information about the input as possible .",2,0.67815924,32.0389109667499,37
2634,"Paraphrases can be generated by decoding back to the source from this representation , without having to generate pivot translations .",0,0.42479366,60.134679450729934,21
2634,"In addition to being more principled and efficient than round-trip MT , our approach offers an adjustable parameter to control the fidelity-diversity trade-off , and obtains better results in our experiments .",3,0.8583392,38.77013494434624,36
2635,"Over the last few years , there has been a move towards data curation for multilingual task-oriented dialogue ( ToD ) systems that can serve people speaking different languages .",0,0.9638039,22.696197276670706,32
2635,"However , existing multilingual ToD datasets either have a limited coverage of languages due to the high cost of data curation , or ignore the fact that dialogue entities barely exist in countries speaking these languages .",0,0.89588517,50.684460458534126,37
2635,"To tackle these limitations , we introduce a novel data curation method that generates GlobalWoZ — a large-scale multilingual ToD dataset globalized from an English ToD dataset for three unexplored use cases of multilingual ToD systems .",2,0.52494633,37.72944976544288,37
2635,Our method is based on translating dialogue templates and filling them with local entities in the target-language countries .,2,0.7680016,68.45654401991914,21
2635,"Besides , we extend the coverage of target languages to 20 languages .",2,0.4918568,37.790406820283444,13
2635,We will release our dataset and a set of strong baselines to encourage research on multilingual ToD systems for real use cases .,3,0.7121808,28.432015638872755,23
2636,"Since the development and wide use of pretrained language models ( PLMs ) , several approaches have been applied to boost their performance on downstream tasks in specific domains , such as biomedical or scientific domains .",0,0.95082575,25.217702172294945,37
2636,Additional pre-training with in-domain texts is the most common approach for providing domain-specific knowledge to PLMs .,0,0.67983425,19.111294096694305,18
2636,"However , these pre-training methods require considerable in-domain data and training resources and a longer training time .",0,0.7636886,33.24240698852188,19
2636,"Moreover , the training must be re-performed whenever a new PLM emerges .",3,0.57786965,64.31909190300763,13
2636,"In this study , we propose a domain knowledge transferring ( DoKTra ) framework for PLMs without additional in-domain pretraining .",1,0.8976853,98.0371706163374,22
2636,"Specifically , we extract the domain knowledge from an existing in-domain pretrained language model and transfer it to other PLMs by applying knowledge distillation .",2,0.8502122,18.39142980603289,25
2636,"In particular , we employ activation boundary distillation , which focuses on the activation of hidden neurons .",2,0.7558442,123.65221839431427,18
2636,"We also apply an entropy regularization term in both teacher training and distillation to encourage the model to generate reliable output probabilities , and thus aid the distillation .",2,0.6792293,76.1208069829519,29
2636,"By applying the proposed DoKTra framework to downstream tasks in the biomedical , clinical , and financial domains , our student models can retain a high percentage of teacher performance and even outperform the teachers in certain tasks .",3,0.8708478,84.57843179367997,39
2636,Our code is available at https://github.com/DMCB-GIST/DoKTra .,3,0.5511274,17.92661103178939,7
2637,Deep NLP models have been shown to be brittle to input perturbations .,0,0.9159021,11.4347989283159,13
2637,Recent work has shown that data augmentation using counterfactuals — i.e .,0,0.9136708,16.01013516653044,12
2637,minimally perturbed inputs — can help ameliorate this weakness .,0,0.6296226,66.55416761433455,10
2637,"We focus on the task of creating counterfactuals for question answering , which presents unique challenges related to world knowledge , semantic diversity , and answerability .",1,0.67189276,43.41977712497212,27
2637,"To address these challenges , we develop a Retrieve-Generate-Filter ( RGF ) technique to create counterfactual evaluation and training data with minimal human supervision .",2,0.3845572,33.85797591246042,26
2637,"Using an open-domain QA framework and question generation model trained on original task data , we create counterfactuals that are fluent , semantically diverse , and automatically labeled .",2,0.82652426,52.573845458070885,29
2637,"Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods , in both the reading comprehension and open-domain QA settings .",3,0.9242574,38.051457203565676,29
2637,"Moreover , we find that RGF data leads to significant improvements in a model ’s robustness to local perturbations .",3,0.9801237,25.422426164200424,20
2638,Transformer-based models have achieved state-of-the-art performance on short-input summarization .,0,0.75358045,7.786374262017343,18
2638,"However , they still struggle with summarizing longer text .",0,0.9064835,57.60601643019064,10
2638,"In this paper , we present DYLE , a novel dynamic latent extraction approach for abstractive long-input summarization .",1,0.8765076,41.77555035991754,20
2638,"DYLE jointly trains an extractor and a generator and treats the extracted text snippets as the latent variable , allowing dynamic snippet-level attention weights during decoding .",2,0.58949554,116.27017705358723,29
2638,"To provide adequate supervision , we propose simple yet effective heuristics for oracle extraction as well as a consistency loss term , which encourages the extractor to approximate the averaged dynamic weights predicted by the generator .",2,0.636547,65.02047156805251,37
2638,"We evaluate our method on different long-document and long-dialogue summarization tasks : GovReport , QMSum , and arXiv .",2,0.59586525,73.82322709125015,19
2638,"Experiment results show that DYLE outperforms all existing methods on GovReport and QMSum , with gains up to 6.1 ROUGE , while yielding strong results on arXiv .",3,0.96678144,41.37496710874326,28
2638,Further analysis shows that the proposed dynamic weights provide interpretability of our generation process .,3,0.98016936,73.56052801499769,15
2639,"Natural language processing for sign language video — including tasks like recognition , translation , and search — is crucial for making artificial intelligence technologies accessible to deaf individuals , and is gaining research interest in recent years .",0,0.9467036,76.03091556720996,39
2639,"In this paper , we address the problem of searching for fingerspelled keywords or key phrases in raw sign language videos .",1,0.9029951,100.17262722474746,22
2639,"This is an important task since significant content in sign language is often conveyed via fingerspelling , and to our knowledge the task has not been studied before .",0,0.91708887,62.07702837633812,29
2639,"We propose an end-to-end model for this task , FSS-Net , that jointly detects fingerspelling and matches it to a text sequence .",2,0.3953995,74.12962276899582,26
2639,"Our experiments , done on a large public dataset of ASL fingerspelling in the wild , show the importance of fingerspelling detection as a component of a search and retrieval model .",3,0.8958753,87.85862501053009,32
2639,Our model significantly outperforms baseline methods adapted from prior work on related tasks .,3,0.9341976,32.418391809453254,14
2640,"We present a framework for learning hierarchical policies from demonstrations , using sparse natural language annotations to guide the discovery of reusable skills for autonomous decision-making .",2,0.35228363,91.38522285347214,28
2640,"We formulate a generative model of action sequences in which goals generate sequences of high-level subtask descriptions , and these descriptions generate sequences of low-level actions .",2,0.7179555,39.72649240120169,28
2640,"We describe how to train this model using primarily unannotated demonstrations by parsing demonstrations into sequences of named high-level sub-tasks , using only a small number of seed annotations to ground language in action .",3,0.3929607,72.32976232575109,37
2640,"In trained models , natural language commands index a combinatorial library of skills ;",3,0.36021385,825.8947800865155,14
2640,agents can use these skills to plan by generating high-level instruction sequences tailored to novel goals .,0,0.67384505,79.7649060105542,19
2640,"We evaluate this approach in the ALFRED household simulation environment , providing natural language annotations for only 10 % of demonstrations .",3,0.4906382,256.42296034340757,22
2640,"It achieves performance comparable state-of-the-art models on ALFRED success rate , outperforming several recent methods with access to ground-truth plans during training and evaluation .",3,0.87160754,51.3830527429954,31
2641,A language-independent representation of meaning is one of the most coveted dreams in Natural Language Understanding .,0,0.9540317,53.04046065149095,19
2641,"With this goal in mind , several formalisms have been proposed as frameworks for meaning representation in Semantic Parsing .",0,0.90412456,30.455647597343376,20
2641,"And yet , the dependencies these formalisms share with respect to language-specific repositories of knowledge make the objective of closing the gap between high-and low-resourced languages hard to accomplish .",0,0.7842641,62.695635488669154,33
2641,"In this paper , we present the BabelNet Meaning Representation ( BMR ) , an interlingual formalism that abstracts away from language-specific constraints by taking advantage of the multilingual semantic resources of BabelNet and VerbAtlas .",1,0.8001947,38.296328597414245,37
2641,"We describe the rationale behind the creation of BMR and put forward BMR 1.0 , a dataset labeled entirely according to the new formalism .",2,0.3047785,46.52341837246924,25
2641,"Moreover , we show how BMR is able to outperform previous formalisms thanks to its fully-semantic framing , which enables top-notch multilingual parsing and generation .",3,0.84549266,34.0454978319305,27
2641,We release the code at https://github.com/SapienzaNLP/bmr .,3,0.5539087,7.614051649564963,7
2642,Personalized language models are designed and trained to capture language patterns specific to individual users .,0,0.5834294,34.19785438225007,16
2642,This makes them more accurate at predicting what a user will write .,0,0.5281889,28.04837591978897,13
2642,"However , when a new user joins a platform and not enough text is available , it is harder to build effective personalized language models .",0,0.8703217,56.72906479125756,26
2642,"We propose a solution for this problem , using a model trained on users that are similar to a new user .",2,0.41514707,42.41725580124389,22
2642,"In this paper , we explore strategies for finding the similarity between new users and existing ones and methods for using the data from existing users who are a good match .",1,0.93564487,36.98160784876568,32
2642,We further explore the trade-off between available data for new users and how well their language can be modeled .,3,0.5380477,34.88518613409958,21
2643,It has been shown that machine translation models usually generate poor translations for named entities that are infrequent in the training corpus .,0,0.9246812,15.233717367348097,23
2643,"Earlier named entity translation methods mainly focus on phonetic transliteration , which ignores the sentence context for translation and is limited in domain and language coverage .",0,0.92346764,49.64782941739678,27
2643,"To address this limitation , we propose DEEP , a DEnoising Entity Pre-training method that leverages large amounts of monolingual data and a knowledge base to improve named entity translation accuracy within sentences .",1,0.39870682,40.81869963303672,34
2643,"Besides , we investigate a multi-task learning strategy that finetunes a pre-trained neural machine translation model on both entity-augmented monolingual data and parallel data to further improve entity translation .",2,0.5901023,15.257663792998889,32
2643,"Experimental results on three language pairs demonstrate that DEEP results in significant improvements over strong denoising auto-encoding baselines , with a gain of up to 1.3 BLEU and up to 9.2 entity accuracy points for English-Russian translation .",3,0.9194747,17.348335262717054,39
2644,"With the increasing popularity of posting multimodal messages online , many recent studies have been carried out utilizing both textual and visual information for multi-modal sarcasm detection .",0,0.9482922,22.503665722365305,28
2644,"In this paper , we investigate multi-modal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic relations between textual and visual modalities .",1,0.8340704,30.235618873927507,33
2644,"Specifically , we first detect the objects paired with descriptions of the image modality , enabling the learning of important visual information .",2,0.75834036,83.59122966401448,23
2644,"Then , the descriptions of the objects are served as a bridge to determine the importance of the association between the objects of image modality and the contextual words of text modality , so as to build a cross-modal graph for each multi-modal instance .",2,0.6709549,24.65646220016713,45
2644,"Furthermore , we devise a cross-modal graph convolutional network to make sense of the incongruity relations between modalities for multi-modal sarcasm detection .",2,0.61936474,12.691341878846167,23
2644,Extensive experimental results and in-depth analysis show that our model achieves state-of-the-art performance in multi-modal sarcasm detection .,3,0.9565776,6.347621388446206,25
2645,Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning .,0,0.90382415,26.738422600242632,20
2645,"To increase its efficiency and prevent catastrophic forgetting and interference , techniques like adapters and sparse fine-tuning have been developed .",0,0.7838316,77.22723399274143,21
2645,"Adapters are modular , as they can be combined to adapt a model towards different facets of knowledge ( e.g. , dedicated language and / or task adapters ) .",0,0.5028693,117.89635702407345,30
2645,"Sparse fine-tuning is expressive , as it controls the behavior of all model components .",0,0.42333025,83.03133472466618,16
2645,"In this work , we introduce a new fine-tuning method with both these desirable properties .",1,0.80754745,30.560599761621027,16
2645,"In particular , we learn sparse , real-valued masks based on a simple variant of the Lottery Ticket Hypothesis .",2,0.7896075,42.463619170523,20
2645,"Task-specific masks are obtained from annotated data in a source language , and language-specific masks from masked language modeling in a target language .",2,0.43180835,26.295397421876515,27
2645,Both these masks can then be composed with the pretrained model .,3,0.5443621,54.77306856762688,12
2645,"Unlike adapter-based fine-tuning , this method neither increases the number of parameters at inference time nor alters the original model architecture .",3,0.65420127,29.01120051665229,23
2645,"Most importantly , it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks , including Universal Dependencies , MasakhaNER , and AmericasNLI .",3,0.8731976,52.66598006266384,30
2645,"Based on an in-depth analysis , we additionally find that sparsity is crucial to prevent both 1 ) interference between the fine-tunings to be composed and 2 ) overfitting .",3,0.94646645,40.549471803510095,31
2645,We release the code and models at https://github.com/cambridgeltl/composable-sft .,3,0.53843373,16.35286052590754,9
2646,Crowdsourcing has emerged as a popular approach for collecting annotated data to train supervised machine learning models .,0,0.96305823,15.94045695464572,18
2646,"However , annotator bias can lead to defective annotations .",0,0.7177095,111.67537207910144,10
2646,"Though there are a few works investigating individual annotator bias , the group effects in annotators are largely overlooked .",0,0.88595545,62.32349764002331,20
2646,"In this work , we reveal that annotators within the same demographic group tend to show consistent group bias in annotation tasks and thus we conduct an initial study on annotator group bias .",1,0.60085034,44.30036761107181,34
2646,We first empirically verify the existence of annotator group bias in various real-world crowdsourcing datasets .,2,0.68239313,33.974255816922685,16
2646,"Then , we develop a novel probabilistic graphical framework GroupAnno to capture annotator group bias with an extended Expectation Maximization ( EM ) algorithm .",2,0.8552946,105.53350971577815,25
2646,We conduct experiments on both synthetic and real-world datasets .,2,0.79636884,12.485176908025684,10
2646,Experimental results demonstrate the effectiveness of our model in modeling annotator group bias in label aggregation and model learning over competitive baselines .,3,0.96657234,40.103733347876954,23
2647,"Gender bias is largely recognized as a problematic phenomenon affecting language technologies , with recent studies underscoring that it might surface differently across languages .",0,0.9164562,67.75546966163645,25
2647,"However , most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions .",0,0.8652717,70.78184538426069,22
2647,"Such protocols overlook key features of grammatical gender languages , which are characterized by morphosyntactic chains of gender agreement , marked on a variety of lexical items and parts-of-speech ( POS ) .",0,0.8747465,93.71729271649701,34
2647,"To overcome this limitation , we enrich the natural , gender-sensitive MuST-SHE corpus ( Bentivogli et al. , 2020 ) with two new linguistic annotation layers ( POS and agreement chains ) , and explore to what extent different lexical categories and agreement phenomena are impacted by gender skews .",2,0.6245858,135.04739895141816,52
2647,"Focusing on speech translation , we conduct a multifaceted evaluation on three language directions ( English-French / Italian / Spanish ) , with models trained on varying amounts of data and different word segmentation techniques .",2,0.77277994,36.513349961627156,38
2647,"By shedding light on model behaviours , gender bias , and its detection at several levels of granularity , our findings emphasize the value of dedicated analyses beyond aggregated overall results .",3,0.9814184,132.02242091917196,32
2648,"Open-domain questions are likely to be open-ended and ambiguous , leading to multiple valid answers .",0,0.8300492,21.206226144038588,16
2648,"Existing approaches typically adopt the rerank-then-read framework , where a reader reads top-ranking evidence to predict answers .",0,0.870598,90.65278741638424,21
2648,"According to our empirical analysis , this framework faces three problems : first , to leverage a large reader under a memory constraint , the reranker should select only a few relevant passages to cover diverse answers , while balancing relevance and diversity is non-trivial ;",3,0.9320523,96.17562408296833,46
2648,"second , the small reading budget prevents the reader from accessing valuable retrieved evidence filtered out by the reranker ;",3,0.64144164,265.3434762706145,20
2648,"third , when using a generative reader to predict answers all at once based on all selected evidence , whether a valid answer will be predicted also pathologically depends on evidence of some other valid answer ( s ) .",0,0.5524954,111.44800920756228,40
2648,"To address these issues , we propose to answer open-domain multi-answer questions with a recall-then-verify framework , which separates the reasoning process of each answer so that we can make better use of retrieved evidence while also leveraging large models under the same memory constraint .",2,0.39487192,36.35780223910236,50
2648,"Our framework achieves state-of-the-art results on two multi-answer datasets , and predicts significantly more gold answers than a rerank-then-read system that uses an oracle reranker .",3,0.8455737,26.24426454442306,35
2649,Pre-trained contextual representations have led to dramatic performance improvements on a range of downstream tasks .,0,0.8551741,11.78996139063738,16
2649,Such performance improvements have motivated researchers to quantify and understand the linguistic information encoded in these representations .,0,0.9367058,32.04985905331509,18
2649,"In general , researchers quantify the amount of linguistic information through probing , an endeavor which consists of training a supervised model to predict a linguistic property directly from the contextual representations .",0,0.93352187,74.2594626195154,33
2649,"Unfortunately , this definition of probing has been subject to extensive criticism in the literature , and has been observed to lead to paradoxical and counter-intuitive results .",0,0.8294118,25.790180046387434,28
2649,"In the theoretical portion of this paper , we take the position that the goal of probing ought to be measuring the amount of inductive bias that the representations encode on a specific task .",1,0.47549573,32.013865917860414,35
2649,We further describe a Bayesian framework that operationalizes this goal and allows us to quantify the representations ’ inductive bias .,2,0.4281196,57.04761013338225,21
2649,"In the empirical portion of the paper , we apply our framework to a variety of NLP tasks .",2,0.4619716,17.65453867312135,19
2649,Our results suggest that our proposed framework alleviates many previous problems found in probing .,3,0.99023247,46.79251022368183,15
2649,"Moreover , we are able to offer concrete evidence that — for some tasks — fastText can offer a better inductive bias than BERT .",3,0.9499268,46.44508580143917,25
2650,Structured pruning has been extensively studied on monolingual pre-trained language models and is yet to be fully evaluated on their multilingual counterparts .,0,0.84292346,13.760223353206243,23
2650,"This work investigates three aspects of structured pruning on multilingual pre-trained language models : settings , algorithms , and efficiency .",1,0.85396945,60.829313580756526,21
2650,"Experiments on nine downstream tasks show several counter-intuitive phenomena : for settings , individually pruning for each language does not induce a better result ;",3,0.9308113,167.5874426079694,25
2650,"for algorithms , the simplest method performs the best ;",3,0.7707902,701.0411087466146,10
2650,"for efficiency , a fast model does not imply that it is also small .",3,0.5950255,154.2539769260318,15
2650,"To facilitate the comparison on all sparsity levels , we present Dynamic Sparsification , a simple approach that allows training the model once and adapting to different model sizes at inference .",2,0.6538828,62.69958183320713,32
2650,We hope this work fills the gap in the study of structured pruning on multilingual pre-trained models and sheds light on future research .,3,0.91352594,21.15709877510276,24
2651,"Deep learning ( DL ) techniques involving fine-tuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals , and those with Alzheimer ’s disease ( AD ) .",0,0.9504824,63.438120167069705,40
2651,"However , questions remain about their ability to generalize beyond the small reference sets that are publicly available for research .",0,0.8840775,37.054146662310494,21
2651,"As an alternative to fitting model parameters directly , we propose a novel method by which a Transformer DL model ( GPT-2 ) pre-trained on general English text is paired with an artificially degraded version of itself ( GPT-D ) , to compute the ratio between these two models ’ perplexities on language from cognitively healthy and impaired individuals .",2,0.71755224,49.22833412729464,63
2651,"This technique approaches state-of-the-art performance on text data from a widely used “ Cookie Theft ” picture description task , and unlike established alternatives also generalizes well to spontaneous conversations .",3,0.5419237,89.65118160431919,36
2651,"Furthermore , GPT-D generates text with characteristics known to be associated with AD , demonstrating the induction of dementia-related linguistic anomalies .",3,0.9496985,69.7887560037062,23
2651,"Our study is a step toward better understanding of the relationships between the inner workings of generative neural language models , the language that they produce , and the deleterious effects of dementia on human speech and language characteristics .",3,0.95929307,28.658365354138944,40
2652,Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on .,0,0.9263504,18.72956966379947,21
2652,This has attracted attention to developing techniques that mitigate such biases .,0,0.9445331,52.59434349383069,12
2652,"In this work , we perform an empirical survey of five recently proposed bias mitigation techniques : Counterfactual Data Augmentation ( CDA ) , Dropout , Iterative Nullspace Projection , Self-Debias , and SentenceDebias .",1,0.79886025,67.18050866460734,36
2652,"We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model ’s language modeling ability , as well as its performance on downstream NLU tasks .",2,0.6574877,37.39825303068099,38
2652,"We experimentally find that : ( 1 ) Self-Debias is the strongest debiasing technique , obtaining improved scores on all bias benchmarks ;",3,0.96441996,116.81171599800277,25
2652,( 2 ) Current debiasing techniques perform less consistently when mitigating non-gender biases ;,3,0.8910439,255.27081975161587,14
2652,"And ( 3 ) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability , making it difficult to determine whether the bias mitigation was effective .",3,0.6986145,70.05889480566252,42
2653,"While GPT has become the de-facto method for text generation tasks , its application to pinyin input method remains unexplored .",0,0.8738356,32.76881811557509,21
2653,"In this work , we make the first exploration to leverage Chinese GPT for pinyin input method .",1,0.5817318,60.74186636981214,18
2653,We find that a frozen GPT achieves state-of-the-art performance on perfect pinyin .,3,0.9794024,27.785078451534456,18
2653,"However , the performance drops dramatically when the input includes abbreviated pinyin .A reason is that an abbreviated pinyin can be mapped to many perfect pinyin , which links to even larger number of Chinese characters .",3,0.8557697,35.39396038234212,37
2653,"We mitigate this issue with two strategies , including enriching the context with pinyin and optimizing the training process to help distinguish homophones .",2,0.51397234,50.608616921664876,24
2653,"To further facilitate the evaluation of pinyin input method , we create a dataset consisting of 270 K instances from fifteen domains .",2,0.82826275,55.31947092110872,23
2653,Results show that our approach improves the performance on abbreviated pinyin across all domains .,3,0.9890798,32.730737745394144,15
2653,Model analysis demonstrates that both strategiescontribute to the performance boost .,3,0.94546616,78.37642271075991,11
2654,Cross-lingual natural language inference ( XNLI ) is a fundamental task in cross-lingual natural language understanding .,0,0.9465649,8.750728911204627,17
2654,Recently this task is commonly addressed by pre-trained cross-lingual language models .,0,0.93778217,11.963195420424965,12
2654,"Existing methods usually enhance pre-trained language models with additional data , such as annotated parallel corpora .",0,0.8471029,27.68528806937409,17
2654,"These additional data , however , are rare in practice , especially for low-resource languages .",0,0.73770845,64.34550396457635,16
2654,"Inspired by recent promising results achieved by prompt-learning , this paper proposes a novel prompt-learning based framework for enhancing XNLI .",1,0.69372773,42.59016644928472,22
2654,It reformulates the XNLI problem to a masked language modeling problem by constructing cloze-style questions through cross-lingual templates .,2,0.5405568,40.79695454937308,19
2654,"To enforce correspondence between different languages , the framework augments a new question for every question using a sampled template in another language and then introduces a consistency loss to make the answer probability distribution obtained from the new question as similar as possible with the corresponding distribution obtained from the original question .",2,0.6754031,43.429405628250564,54
2654,Experimental results on two benchmark datasets demonstrate that XNLI models enhanced by our proposed framework significantly outperform original ones under both the full-shot and few-shot cross-lingual transfer settings .,3,0.9380694,20.910785600790216,31
2655,Sense embedding learning methods learn different embeddings for the different senses of an ambiguous word .,0,0.60738695,32.09498199201626,16
2655,One sense of an ambiguous word might be socially biased while its other senses remain unbiased .,0,0.7942667,110.25254202909652,17
2655,"In comparison to the numerous prior work evaluating the social biases in pretrained word embeddings , the biases in sense embeddings have been relatively understudied .",0,0.9134458,25.74604445841746,26
2655,We create a benchmark dataset for evaluating the social biases in sense embeddings and propose novel sense-specific bias evaluation measures .,2,0.64357835,44.24180820290875,21
2655,We conduct an extensive evaluation of multiple static and contextualised sense embeddings for various types of social biases using the proposed measures .,2,0.5704355,61.8128440588627,23
2655,"Our experimental results show that even in cases where no biases are found at word-level , there still exist worrying levels of social biases at sense-level , which are often ignored by the word-level bias evaluation measures .",3,0.9847189,34.30433203496091,38
2656,We consider the problem of generating natural language given a communicative goal and a world description .,0,0.42445758,37.550381804939136,17
2656,"In particular , we consider using two meaning representations , one based on logical semantics and the other based on distributional semantics .",2,0.7903992,33.03267820648883,23
2656,"We build upon an existing goal-directed generation system , S-STRUCT , which models sentence generation as planning in a Markov decision process .",2,0.7601996,132.7798347828503,25
2656,"We develop a hybrid approach , which uses distributional semantics to quickly and imprecisely add the main elements of the sentence and then uses first-order logic based semantics to more slowly add the precise details .",2,0.78062314,60.40998026054811,36
2656,We find that our hybrid method allows S-STRUCT ’s generation to scale significantly better in early phases of generation and that the hybrid can often generate sentences with the same quality as S-STRUCT in substantially less time .,3,0.98219323,46.07034370043446,38
2656,"However , we also observe and give insight into cases where the imprecision in distributional semantics leads to generation that is not as good as using pure logical semantics .",3,0.82289004,45.070439375402856,30
2657,Clinical trials offer a fundamental opportunity to discover new treatments and advance the medical knowledge .,0,0.9461792,47.30929830509185,16
2657,"However , the uncertainty of the outcome of a trial can lead to unforeseen costs and setbacks .",0,0.8530052,32.8659407451086,18
2657,"In this study , we propose a new method to predict the effectiveness of an intervention in a clinical trial .",1,0.9366307,16.79659673368378,21
2657,Our method relies on generating an informative summary from multiple documents available in the literature about the intervention under study .,2,0.7970855,55.59814585410255,21
2657,"Specifically , our method first gathers all the abstracts of PubMed articles related to the intervention .",2,0.894163,69.28076358515986,17
2657,"Then , an evidence sentence , which conveys information about the effectiveness of the intervention , is extracted automatically from each abstract .",2,0.66091543,89.78106160234393,23
2657,"Based on the set of evidence sentences extracted from the abstracts , a short summary about the intervention is constructed .",2,0.6771048,60.03697903008021,21
2657,"Finally , the produced summaries are used to train a BERT-based classifier , in order to infer the effectiveness of an intervention .",2,0.66801065,31.807337689865637,25
2657,"To evaluate our proposed method , we introduce a new dataset which is a collection of clinical trials together with their associated PubMed articles .",2,0.7118933,37.51782597174607,25
2657,"Our experiments , demonstrate the effectiveness of producing short informative summaries and using them to predict the effectiveness of an intervention .",3,0.80416155,67.96132732007564,22
2658,Interactive neural machine translation ( INMT ) is able to guarantee high-quality translations by taking human interactions into account .,0,0.93692183,35.78871662425466,20
2658,Existing IMT systems relying on lexical constrained decoding ( LCD ) enable humans to translate in a flexible translation order beyond the left-to-right .,0,0.9270432,96.89625047544975,25
2658,"However , they typically suffer from two significant limitations in translation efficiency and quality due to the reliance on LCD .",0,0.9255532,81.62264713906943,21
2658,"In this work , we propose a novel BiTIIMT system , Bilingual Text-Infilling for Interactive Neural Machine Translation .",1,0.86109257,78.94256253100232,19
2658,The key idea to BiTIIMT is Bilingual Text-infilling ( BiTI ) which aims to fill missing segments in a manually revised translation for a given source sentence .,0,0.88156253,89.22432614543563,28
2658,We propose a simple yet effective solution by casting this task as a sequence-to-sequence task .,1,0.35586235,12.257214731518577,19
2658,"In this way , our system performs decoding without explicit constraints and makes full use of revised words for better translation prediction .",3,0.6699299,116.80430811073563,23
2658,Experiment results show that BiTiIMT performs significantly better and faster than state-of-the-art LCD-based IMT on three translation tasks .,3,0.9847572,23.10839762409821,25
2659,"In this study , we investigate robustness against covariate drift in spoken language understanding ( SLU ) .",1,0.9302365,71.58414039730879,18
2659,Covariate drift can occur in SLUwhen there is a drift between training and testing regarding what users request or how they request it .,0,0.68794125,94.54848737084743,24
2659,To study this we propose a method that exploits natural variations in data to create a covariate drift in SLU datasets .,1,0.56257653,79.01554789310691,22
2659,Experiments show that a state-of-the-art BERT-based model suffers performance loss under this drift .,3,0.8549775,15.700089310570059,19
2659,"To mitigate the performance loss , we investigate distributionally robust optimization ( DRO ) for finetuning BERT-based models .",2,0.4877465,66.62427649948916,21
2659,"We discuss some recent DRO methods , propose two new variants and empirically show that DRO improves robustness under drift .",1,0.42022538,100.26007721069085,21
2660,"Chinese pre-trained language models usually exploit contextual character information to learn representations , while ignoring the linguistics knowledge , e.g. , word and sentence information .",0,0.65223706,47.871462029427434,26
2660,"Hence , we propose a task-free enhancement module termed as Heterogeneous Linguistics Graph ( HLG ) to enhance Chinese pre-trained language models by integrating linguistics knowledge .",1,0.47545475,38.18956800237073,29
2660,"Specifically , we construct a hierarchical heterogeneous graph to model the characteristics linguistics structure of Chinese language , and conduct a graph-based method to summarize and concretize information on different granularities of Chinese linguistics hierarchies .",2,0.90493256,42.11869183128598,37
2660,"Experimental results demonstrate our model has the ability to improve the performance of vanilla BERT , BERTwwm and ERNIE 1.0 on 6 natural language processing tasks with 10 benchmark datasets .",3,0.9565195,36.18890239521746,31
2660,"Further , the detailed experimental analyses have proven that this kind of modelization achieves more improvements compared with previous strong baseline MWA .",3,0.9571517,117.89225322793787,23
2660,"Meanwhile , our model introduces far fewer parameters ( about half of MWA ) and the training / inference speed is about 7x faster than MWA .",3,0.88858336,61.89982597620021,27
2661,Fine-grained Entity Typing ( FET ) has made great progress based on distant supervision but still suffers from label noise .,0,0.9629911,50.16136235545694,21
2661,"Existing FET noise learning methods rely on prediction distributions in an instance-independent manner , which causes the problem of confirmation bias .",0,0.7454174,82.07432165100785,24
2661,"In this work , we propose a clustering-based loss correction framework named Feature Cluster Loss Correction ( FCLC ) , to address these two problems .",1,0.8112499,40.07569934432078,28
2661,FCLC first train a coarse backbone model as a feature extractor and noise estimator .,2,0.6032074,159.69238007016864,15
2661,"Loss correction is then applied to each feature cluster , learning directly from the noisy labels .",2,0.6480069,162.2220976782262,17
2661,Experimental results on three public datasets show that FCLC achieves the best performance over existing competitive systems .,3,0.8962451,21.548861626917635,18
2661,Auxiliary experiments further demonstrate that FCLC is stable to hyperparameters and it does help mitigate confirmation bias .,3,0.9776717,110.77888865458581,18
2661,"We also find that in the extreme case of no clean data , the FCLC framework still achieves competitive performance .",3,0.97687346,62.170281144122114,21
2662,The robustness of Text-to-SQL parsers against adversarial perturbations plays a crucial role in delivering highly reliable applications .,0,0.92434585,15.672574777703522,20
2662,"Previous studies along this line primarily focused on perturbations in the natural language question side , neglecting the variability of tables .",0,0.8899173,81.00834959947925,22
2662,"Motivated by this , we propose the Adversarial Table Perturbation ( ATP ) as a new attacking paradigm to measure robustness of Text-to-SQL models .",1,0.41776547,26.90491025668359,27
2662,"Following this proposition , we curate ADVETA , the first robustness evaluation benchmark featuring natural and realistic ATPs .",2,0.42556682,357.4591778404055,19
2662,"All tested state-of-the-art models experience dramatic performance drops on ADVETA , revealing significant room of improvement .",3,0.9490348,88.97590922066239,22
2662,"To defense against ATP , we build a systematic adversarial training example generation framework tailored for better contextualization of tabular data .",2,0.67284805,101.92872321525768,22
2662,"Experiments show that our approach brings models best robustness improvement against ATP , while also substantially boost model robustness against NL-side perturbations .",3,0.96253854,114.98032078863145,25
2662,We will release ADVETA and code to facilitate future research .,3,0.76330614,117.30922806040374,11
2663,Neural networks tend to gradually forget the previously learned knowledge when learning multiple tasks sequentially from dynamic data distributions .,0,0.8361554,103.05968286280867,20
2663,"This problem is called catastrophic forgetting , which is a fundamental challenge in the continual learning of neural networks .",0,0.9365589,30.117454143301213,20
2663,"In this work , we observe that catastrophic forgetting not only occurs in continual learning but also affects the traditional static training .",1,0.40211812,68.93620257945678,23
2663,"Neural networks , especially neural machine translation models , suffer from catastrophic forgetting even if they learn from a static training set .",0,0.91853034,45.62944823741729,23
2663,"To be specific , the final model pays imbalanced attention to training samples , where recently exposed samples attract more attention than earlier samples .",3,0.5416042,93.53126316888638,25
2663,"The underlying cause is that training samples do not get balanced training in each model update , so we name this problem imbalanced training .",0,0.7261624,92.2915777305352,25
2663,"To alleviate this problem , we propose Complementary Online Knowledge Distillation ( COKD ) , which uses dynamically updated teacher models trained on specific data orders to iteratively provide complementary knowledge to the student model .",2,0.3916409,45.2284184686726,36
2663,Experimental results on multiple machine translation tasks show that our method successfully alleviates the problem of imbalanced training and achieves substantial improvements over strong baseline systems .,3,0.9415243,11.915981447448738,27
2664,Human languages are full of metaphorical expressions .,0,0.94046015,47.292269452894715,8
2664,Metaphors help people understand the world by connecting new concepts and domains to more familiar ones .,0,0.92034805,31.152470646881078,17
2664,Large pre-trained language models ( PLMs ) are therefore assumed to encode metaphorical knowledge useful for NLP systems .,0,0.86106807,28.96004054039251,19
2664,"In this paper , we investigate this hypothesis for PLMs , by probing metaphoricity information in their encodings , and by measuring the cross-lingual and cross-dataset generalization of this information .",1,0.91945964,29.222033329143564,31
2664,"We present studies in multiple metaphor detection datasets and in four languages ( i.e. , English , Spanish , Russian , and Farsi ) .",2,0.70318604,57.06717202674273,25
2664,"Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge , and mostly in their middle layers .",3,0.98464566,106.70039623315584,21
2664,"The knowledge is transferable between languages and datasets , especially when the annotation is consistent across training and testing sets .",3,0.50051457,37.76278345008494,21
2664,Our findings give helpful insights for both cognitive and NLP scientists .,3,0.98817635,79.10006612852179,12
2665,Dependency trees have been intensively used with graph neural networks for aspect-based sentiment classification .,0,0.873948,32.28011938776484,16
2665,"Though being effective , such methods rely on external dependency parsers , which can be unavailable for low-resource languages or perform worse in low-resource domains .",0,0.78456855,30.370288685366983,26
2665,"In addition , dependency trees are also not optimized for aspect-based sentiment classification .",0,0.6860535,74.00219770003932,15
2665,"In this paper , we propose an aspect-specific and language-agnostic discrete latent opinion tree model as an alternative structure to explicit dependency trees .",1,0.8603636,39.81442858807831,26
2665,"To ease the learning of complicated structured latent variables , we build a connection between aspect-to-context attention scores and syntactic distances , inducing trees from the attention scores .",2,0.8245303,109.2071311329045,30
2665,Results on six English benchmarks and one Chinese dataset show that our model can achieve competitive performance and interpretability .,3,0.9627713,15.673281017254142,20
2666,"Thanks to the strong representation power of neural encoders , neural chart-based parsers have achieved highly competitive performance by using local features .",0,0.8543851,33.30992237708351,25
2666,"Recently , it has been shown that non-local features in CRF structures lead to improvements .",0,0.88699436,33.655386726801474,16
2666,"In this paper , we investigate injecting non-local features into the training process of a local span-based parser , by predicting constituent n-gram non-local patterns and ensuring consistency between non-local patterns and local constituents .",1,0.9016686,45.65652299043415,35
2666,Results show that our simple method gives better results than the self-attentive parser on both PTB and CTB .,3,0.9882338,21.58349663463325,19
2666,"Besides , our method achieves state-of-the-art BERT-based performance on PTB ( 95.92 F1 ) and strong performance on CTB ( 92.31 F1 ) .",3,0.8956644,10.648874405334437,30
2666,Our parser also outperforms the self-attentive parser in multi-lingual and zero-shot cross-domain settings .,3,0.9457109,18.29388949498829,14
2667,"In this paper , we firstly empirically find that existing models struggle to handle hard mentions due to their insufficient contexts , which consequently limits their overall typing performance .",1,0.6461452,73.1396274729801,30
2667,"To this end , we propose to exploit sibling mentions for enhancing the mention representations .",2,0.41094622,64.96360378663803,16
2667,"Specifically , we present two different metrics for sibling selection and employ an attentive graph neural network to aggregate information from sibling mentions .",2,0.73224795,46.67506032703535,24
2667,The proposed graph model is scalable in that unseen test mentions are allowed to be added as new nodes for inference .,3,0.5680248,82.23850498804323,22
2667,"Exhaustive experiments demonstrate the effectiveness of our sibling learning strategy , where our model outperforms ten strong baselines .",3,0.8655252,42.155360641190136,19
2667,"Moreover , our experiments indeed prove the superiority of sibling mentions in helping clarify the types for hard mentions .",3,0.98200905,189.60051386569359,20
2668,"The goal of the cross-lingual summarization ( CLS ) is to convert a document in one language ( e.g. , English ) to a summary in another one ( e.g. , Chinese ) .",0,0.93589634,13.346137999882016,34
2668,"The CLS task is essentially the combination of machine translation ( MT ) and monolingual summarization ( MS ) , and thus there exists the hierarchical relationship between MT&MS and CLS .",0,0.90857506,47.90088351708181,32
2668,Existing studies on CLS mainly focus on utilizing pipeline methods or jointly training an end-to-end model through an auxiliary MT or MS objective .,0,0.7852035,64.48458363310391,26
2668,"However , it is very challenging for the model to directly conduct CLS as it requires both the abilities to translate and summarize .",0,0.6324723,76.42248873713923,24
2668,"To address this issue , we propose a hierarchical model for the CLS task , based on the conditional variational auto-encoder .",2,0.4346655,27.00932442905122,22
2668,"The hierarchical model contains two kinds of latent variables at the local and global levels , respectively .",2,0.604415,34.75725206381913,18
2668,"At the local level , there are two latent variables , one for translation and the other for summarization .",0,0.35139555,26.35870578352834,20
2668,"As for the global level , there is another latent variable for cross-lingual summarization conditioned on the two local-level variables .",3,0.5987956,41.97518736520819,23
2668,Experiments on two language directions ( English-Chinese ) verify the effectiveness and superiority of the proposed approach .,3,0.7329534,26.03265549062833,20
2668,"In addition , we show that our model is able to generate better cross-lingual summaries than comparison models in the few-shot setting .",3,0.9500067,14.37990926825537,23
2669,"In conversational question answering ( CQA ) , the task of question rewriting ( QR ) in context aims to rewrite a context-dependent question into an equivalent self-contained question that gives the same answer .",0,0.930694,25.371539015753516,35
2669,"In this paper , we are interested in the robustness of a QR system to questions varying in rewriting hardness or difficulty .",1,0.85136896,65.3699875833636,23
2669,"Since there is a lack of questions classified based on their rewriting hardness , we first propose a heuristic method to automatically classify questions into subsets of varying hardness , by measuring the discrepancy between a question and its rewrite .",2,0.51771337,39.13128718819918,41
2669,"To find out what makes questions hard or easy for rewriting , we then conduct a human evaluation to annotate the rewriting hardness of questions .",2,0.79429287,55.41105324340577,26
2669,"Finally , to enhance the robustness of QR systems to questions of varying hardness , we propose a novel learning framework for QR that first trains a QR model independently on each subset of questions of a certain level of hardness , then combines these QR models as one joint model for inference .",2,0.5774207,32.98042556606207,54
2669,Experimental results on two datasets show that our framework improves the overall performance compared to the baselines .,3,0.9423339,7.809148603039686,18
2670,AI technologies for Natural Languages have made tremendous progress recently .,0,0.95298004,98.99845238417265,11
2670,"However , commensurate progress has not been made on Sign Languages , in particular , in recognizing signs as individual words or as complete sentences .",0,0.91253525,81.33611101702711,26
2670,"We introduce OpenHands , a library where we take four key ideas from the NLP community for low-resource languages and apply them to sign languages for word-level recognition .",2,0.5685994,34.63049722114676,30
2670,"First , we propose using pose extracted through pretrained models as the standard modality of data in this work to reduce training time and enable efficient inference , and we release standardized pose datasets for different existing sign language datasets .",2,0.63883954,90.58982810771306,41
2670,"Second , we train and release checkpoints of 4 pose-based isolated sign language recognition models across 6 languages ( American , Argentinian , Chinese , Greek , Indian , and Turkish ) , providing baselines and ready checkpoints for deployment .",2,0.87288237,99.82930149720364,41
2670,"Third , to address the lack of labelled data , we propose self-supervised pretraining on unlabelled data .",2,0.56087023,21.072761075510943,18
2670,We curate and release the largest pose-based pretraining dataset on Indian Sign Language ( Indian-SL ) .,2,0.69771034,69.59886505302777,19
2670,"Fourth , we compare different pretraining strategies and for the first time establish that pretraining is effective for sign language recognition by demonstrating ( a ) improved fine-tuning performance especially in low-resource settings , and ( b ) high crosslingual transfer from Indian-SL to few other sign languages .",3,0.78219783,42.47130406968189,51
2670,We open-source all models and datasets in OpenHands with a hope that it makes research in sign languages reproducible and more accessible .,3,0.68131876,54.06117943745546,23
2671,"In recent years , researchers tend to pre-train ever-larger language models to explore the upper limit of deep models .",0,0.93926156,36.82885049762727,20
2671,"However , large language model pre-training costs intensive computational resources , and most of the models are trained from scratch without reusing the existing pre-trained models , which is wasteful .",0,0.7734501,32.483133872351154,31
2671,"In this paper , we propose bert2BERT , which can effectively transfer the knowledge of an existing smaller pre-trained model to a large model through parameter initialization and significantly improve the pre-training efficiency of the large model .",1,0.8130563,25.757924877540333,38
2671,"Specifically , we extend the previous function-preserving method proposed in computer vision on the Transformer-based language model , and further improve it by proposing a novel method , advanced knowledge for large model ’s initialization .",2,0.6771367,60.800923650010816,40
2671,"In addition , a two-stage learning method is proposed to further accelerate the pre-training .",2,0.6397243,17.07944755100005,16
2671,"We conduct extensive experiments on representative PLMs ( e.g. , BERT and GPT ) and demonstrate that ( 1 ) our method can save a significant amount of training cost compared with baselines including learning from scratch , StackBERT and MSLT ;",3,0.79800564,56.98119331301893,42
2671,( 2 ) our method is generic and applicable to different types of pre-trained models .,3,0.81757015,27.211097009529,16
2671,"In particular , bert2BERT saves about 45 % and 47 % computational cost of pre-training BERT\rm BASE and GPT\rm BASE by reusing the models of almost their half sizes .",3,0.93887913,126.02173492494504,30
2672,"As an important task in sentiment analysis , Multimodal Aspect-Based Sentiment Analysis ( MABSA ) has attracted increasing attention inrecent years .",0,0.9675075,23.360175861130788,24
2672,"However , previous approaches either ( i ) use separately pre-trained visual and textual models , which ignore the crossmodalalignment or ( ii ) use vision-language models pre-trained with general pre-training tasks , which are inadequate to identify fine-grainedaspects , opinions , and their alignments across modalities .",0,0.8444904,62.74555111934318,52
2672,"To tackle these limitations , we propose a task-specific Vision-LanguagePre-training framework for MABSA ( VLP-MABSA ) , which is a unified multimodal encoder-decoder architecture for all the pretrainingand downstream tasks .",2,0.5567273,32.86808000202198,35
2672,"We further design three types of task-specific pre-training tasks from the language , vision , and multimodalmodalities , respectively .",2,0.7806099,58.627993759194844,21
2672,Experimental results show that our approach generally outperforms the state-of-the-art approaches on three MABSA subtasks .,3,0.9695318,9.06437506051086,22
2672,Further analysis demonstrates the effectiveness of each pre-training task .,3,0.97075474,21.88464283719979,10
2672,The source code is publicly released at https://github.com/NUSTM/VLP-MABSA .,3,0.5163194,17.444830524702056,9
2673,Hedges have an important role in the management of rapport .,0,0.7441541,30.688645612769285,11
2673,"In peer-tutoring , they are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback .",3,0.7621258,117.62289610009609,25
2673,"Pursuing the objective of building a tutoring agent that manages rapport with teenagers in order to improve learning , we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges .",2,0.43632036,45.41183581471121,34
2673,We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature .,2,0.7731462,65.20757141753528,18
2673,Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret .,3,0.88275164,32.014155961790394,18
2673,"We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations , and we identify some novel features , and the benefits of a such a hybrid model approach .",2,0.49045616,79.6230481300328,35
2674,k-Nearest-Neighbor Machine Translation ( kNN-MT ) has been recently proposed as a non-parametric solution for domain adaptation in neural machine translation ( NMT ) .,0,0.9559835,14.446223695224079,29
2674,It aims to alleviate the performance degradation of advanced MT systems in translating out-of-domain sentences by coordinating with an additional token-level feature-based retrieval module constructed from in-domain data .,1,0.4646658,34.20547050940547,38
2674,"Previous studies ( Khandelwal et al. , 2021 ; Zheng et al. , 2021 ) have already demonstrated that non-parametric NMT is even superior to models fine-tuned on out-of-domain data .",0,0.7249212,21.397802114481234,32
2674,"In spite of this success , kNN retrieval is at the expense of high latency , in particular for large datastores .",0,0.77000225,74.20969330727335,22
2674,"To make it practical , in this paper , we explore a more efficient kNN-MT and propose to use clustering to improve the retrieval efficiency .",1,0.8240231,49.6965387328707,28
2674,"Concretely , we first propose a cluster-based Compact Network for feature reduction in a contrastive learning manner to compress context features into 90 + % lower dimensional vectors .",2,0.68529975,103.8519441031424,31
2674,We then suggest a cluster-based pruning solution to filter out 10 % 40 % redundant nodes in large datastores while retaining translation quality .,3,0.80014074,102.95422750551259,26
2674,Our proposed methods achieve better or comparable performance while reducing up to 57 % inference latency against the advanced non-parametric MT model on several machine translation benchmarks .,3,0.89990747,82.17778434867043,28
2674,Experimental results indicate that the proposed methods maintain the most useful information of the original datastore and the Compact Network shows good generalization on unseen domains .,3,0.9765302,46.15721028050639,27
2674,Codes are available at https://github.com/tjunlp-lab/PCKMT .,3,0.54579544,16.800281388843352,6
2675,We propose a new method for projective dependency parsing based on headed spans .,1,0.5170885,62.78062646826967,14
2675,"In a projective dependency tree , the largest subtree rooted at each word covers a contiguous sequence ( i.e. , a span ) in the surface order .",0,0.5699369,111.32112393612557,28
2675,We call such a span marked by a root word headed span .,2,0.38998762,222.53473003576147,13
2675,A projective dependency tree can be represented as a collection of headed spans .,0,0.6166448,74.5971866958864,14
2675,We decompose the score of a dependency tree into the scores of the headed spans and design a novel O ( n3 ) dynamic programming algorithm to enable global training and exact inference .,2,0.7555177,128.02179311237433,34
2675,"Our model achieves state-of-the-art or competitive results on PTB , CTB , and UD .",3,0.9088165,17.43919991509373,20
2676,This work explores techniques to predict Part-of-Speech ( PoS ) tags from neural signals measured at millisecond resolution with electroencephalography ( EEG ) during text reading .,1,0.77607954,47.23541948050686,28
2676,"We first show that information about word length , frequency and word class is encoded by the brain at different post-stimulus latencies .",3,0.5618547,45.11318425759469,23
2676,We then demonstrate that pre-training on averaged EEG data and data augmentation techniques boost PoS decoding accuracy for single EEG trials .,3,0.8837824,95.14878804811616,22
2676,"Finally , applying optimised temporally-resolved decoding techniques we show that Transformers substantially outperform linear-SVMs on PoS tagging of unigram and bigram data .",3,0.92627656,88.42956368894629,27
2677,Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models ( PLMs ) contain smaller matching subnetworks ( winning tickets ) which are capable of reaching accuracy comparable to the original models .,0,0.90457493,39.27843947826949,35
2677,"However , these tickets are proved to be notrobust to adversarial examples , and even worse than their PLM counterparts .",3,0.5677551,98.52671598885831,21
2677,"To address this problem , we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs .",1,0.39978078,69.07888016925837,26
2677,"Since the loss is not differentiable for the binary mask , we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization .",2,0.6803005,87.66766933227893,32
2677,"Furthermore , we design an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well bothin accuracy and robustness .",2,0.7301156,74.09643865326406,27
2677,Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation .,3,0.9645126,15.52593341353956,18
2678,Tuning pre-trained language models ( PLMs ) with task-specific prompts has been a promising approach for text classification .,0,0.91504866,17.546668642381142,20
2678,"Particularly , previous studies suggest that prompt-tuning has remarkable superiority in the low-data scenario over the generic fine-tuning methods with extra classifiers .",0,0.8157273,68.68460537055593,23
2678,"The core idea of prompt-tuning is to insert text pieces , i.e. , template , to the input and transform a classification problem into a masked language modeling problem , where a crucial step is to construct a projection , i.e. , verbalizer , between a label space and a label word space .",0,0.3827548,53.923890848681594,54
2678,"A verbalizer is usually handcrafted or searched by gradient descent , which may lack coverage and bring considerable bias and high variances to the results .",0,0.6803726,128.24859484914722,26
2678,"In this work , we focus on incorporating external knowledge into the verbalizer , forming a knowledgeable prompttuning ( KPT ) , to improve and stabilize prompttuning .",1,0.84113455,132.14567782466239,28
2678,"Specifically , we expand the label word space of the verbalizer using external knowledge bases ( KBs ) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space .",2,0.83726984,54.492370621720035,38
2678,Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning .,3,0.7744932,28.43326295085334,16
2679,"Fine-grained entity typing ( FGET ) aims to classify named entity mentions into fine-grained entity types , which is meaningful for entity-related NLP tasks .",0,0.89760405,28.441107356746265,29
2679,"For FGET , a key challenge is the low-resource problem — the complex entity type hierarchy makes it difficult to manually label data .",0,0.7580555,95.13381696840284,24
2679,"Especially for those languages other than English , human-labeled data is extremely scarce .",0,0.92062086,28.79529049273952,15
2679,"In this paper , we propose a cross-lingual contrastive learning framework to learn FGET models for low-resource languages .",1,0.89636016,17.90864771172543,19
2679,"Specifically , we use multi-lingual pre-trained language models ( PLMs ) as the backbone to transfer the typing knowledge from high-resource languages ( such as English ) to low-resource languages ( such as Chinese ) .",2,0.8575004,9.734383703518578,37
2679,"Furthermore , we introduce entity-pair-oriented heuristic rules as well as machine translation to obtain cross-lingual distantly-supervised data , and apply cross-lingual contrastive learning on the distantly-supervised data to enhance the backbone PLMs .",2,0.72018075,25.363984881756586,39
2679,"Experimental results show that by applying our framework , we can easily learn effective FGET models for low-resource languages , even without any language-specific human-labeled data .",3,0.9642468,37.32013756185034,29
2679,Our code is also available at https://github.com/thunlp/CrossET .,3,0.67443454,8.76912257978352,8
2680,Data augmentation is an effective solution to data scarcity in low-resource scenarios .,0,0.8122439,12.866989635001477,13
2680,"However , when applied to token-level tasks such as NER , data augmentation methods often suffer from token-label misalignment , which leads to unsatsifactory performance .",0,0.8666449,42.93719408425838,30
2680,"In this work , we propose Masked Entity Language Modeling ( MELM ) as a novel data augmentation framework for low-resource NER .",1,0.82849,17.93441712266594,23
2680,"To alleviate the token-label misalignment issue , we explicitly inject NER labels into sentence context , and thus the fine-tuned MELM is able to predict masked entity tokens by explicitly conditioning on their labels .",2,0.6257906,54.645631427211455,37
2680,"Thereby , MELM generates high-quality augmented data with novel entities , which provides rich entity regularity knowledge and boosts NER performance .",3,0.7824885,143.98146857309845,22
2680,"When training data from multiple languages are available , we also integrate MELM with code-mixing for further improvement .",2,0.51814526,59.417909059014036,19
2680,"We demonstrate the effectiveness of MELM on monolingual , cross-lingual and multilingual NER across various low-resource levels .",3,0.8808101,26.673439090291513,18
2680,Experimental results show that our MELM consistently outperforms the baseline methods .,3,0.9719711,21.236877269443458,12
2681,"Learning representations of words in a continuous space is perhaps the most fundamental task in NLP , however words interact in ways much richer than vector dot product similarity can provide .",0,0.8918248,70.83536159633772,32
2681,"Many relationships between words can be expressed set-theoretically , for example , adjective-noun compounds ( eg .",0,0.7840218,76.08012866187757,18
2681,“ red cars ” ⊆ “ cars ” ) and homographs ( eg .,2,0.36218548,150.24583232752093,14
2681,"“ tongue ” ∩“ body ” should be similar to “ mouth ” , while “ tongue ” ∩“ language ” should be similar to “ dialect ” ) have natural set-theoretic interpretations .",3,0.42664847,26.840379609953015,36
2681,Box embeddings are a novel region-based representation which provide the capability to perform these set-theoretic operations .,0,0.49695045,41.05222111937013,18
2681,"In this work , we provide a fuzzy-set interpretation of box embeddings , and learn box representations of words using a set-theoretic training objective .",1,0.543755,47.84162517032996,26
2681,"We demonstrate improved performance on various word similarity tasks , particularly on less common words , and perform a quantitative and qualitative analysis exploring the additional unique expressivity provided by Word2Box .",3,0.6251464,101.52112596065989,32
2682,"Traditionally , a debate usually requires a manual preparation process , including reading plenty of articles , selecting the claims , identifying the stances of the claims , seeking the evidence for the claims , etc .",0,0.9004625,81.87785919465647,37
2682,"As the AI debate attracts more attention these years , it is worth exploring the methods to automate the tedious process involved in the debating system .",0,0.74599224,58.07593426033869,27
2682,"In this work , we introduce a comprehensive and large dataset named IAM , which can be applied to a series of argument mining tasks , including claim extraction , stance classification , evidence extraction , etc .",1,0.6929421,55.43510252551956,38
2682,Our dataset is collected from over 1 k articles related to 123 topics .,2,0.86093193,102.42236439919427,14
2682,"Near 70 k sentences in the dataset are fully annotated based on their argument properties ( e.g. , claims , stances , evidence , etc. ) .",2,0.49809977,67.25114907273762,27
2682,We further propose two new integrated argument mining tasks associated with the debate preparation process : ( 1 ) claim extraction with stance classification ( CESC ) and ( 2 ) claim-evidence pair extraction ( CEPE ) .,2,0.43643177,75.03094719191444,38
2682,We adopt a pipeline approach and an end-to-end method for each integrated task separately .,2,0.82480824,31.776018261714544,16
2682,"Promising experimental results are reported to show the values and challenges of our proposed tasks , and motivate future research on argument mining .",3,0.90550977,85.135848595028,24
2683,"Despite recent progress of pre-trained language models on generating fluent text , existing methods still suffer from incoherence problems in long-form text generation tasks that require proper content control and planning to form a coherent high-level logical flow .",0,0.89792484,33.42844826367461,43
2683,"In this work , we propose PLANET , a novel generation framework leveraging autoregressive self-attention mechanism to conduct content planning and surface realization dynamically .",1,0.8044536,51.70122826483896,25
2683,"To guide the generation of output sentences , our framework enriches the Transformer decoder with latent representations to maintain sentence-level semantic plans grounded by bag-of-words .",2,0.57324547,53.54224110682323,28
2683,"Moreover , we introduce a new coherence-based contrastive learning objective to further improve the coherence of output .",2,0.49405417,15.942790633127768,20
2683,Extensive experiments are conducted on two challenging long-form text generation tasks including counterargument generation and opinion article generation .,2,0.6629238,29.202838375627447,21
2683,Both automatic and human evaluations show that our method significantly outperforms strong baselines and generates more coherent texts with richer contents .,3,0.96195245,11.8352171147168,22
2684,Existing reference-free metrics have obvious limitations for evaluating controlled text generation models .,0,0.89449173,74.12810283095591,14
2684,"Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments , whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets .",0,0.7488739,39.01795671838992,34
2684,"In this paper , we propose an unsupervised reference-free metric called CTRLEval , which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks .",1,0.8526163,66.81930494087241,32
2684,"On top of these tasks , the metric assembles the generation probabilities from a pre-trained language model without any model training .",2,0.5589828,40.54156435720086,22
2684,"Experimental results show that our metric has higher correlations with human judgments than other baselines , while obtaining better generalization of evaluating generated texts from different models and with different qualities .",3,0.9749566,33.744418996853845,32
2685,"In dialogue state tracking , dialogue history is a crucial material , and its utilization varies between different models .",0,0.8744186,136.2078066049156,20
2685,"However , no matter how the dialogue history is used , each existing model uses its own consistent dialogue history during the entire state tracking process , regardless of which slot is updated .",0,0.72388583,92.81123005799736,34
2685,"Apparently , it requires different dialogue history to update different slots in different turns .",3,0.55188006,209.88344902001208,15
2685,"Therefore , using consistent dialogue contents may lead to insufficient or redundant information for different slots , which affects the overall performance .",3,0.7502303,132.0799097001585,23
2685,"To address this problem , we devise DiCoS-DST to dynamically select the relevant dialogue contents corresponding to each slot for state updating .",2,0.5672288,85.32954884530353,25
2685,"Specifically , it first retrieves turn-level utterances of dialogue history and evaluates their relevance to the slot from a combination of three perspectives : ( 1 ) its explicit connection to the slot name ;",2,0.6885393,67.48362497150848,35
2685,( 2 ) its relevance to the current turn dialogue ;,2,0.357393,928.4726427541614,11
2685,( 3 ) Implicit Mention Oriented Reasoning .,3,0.35701528,236.11111956463083,8
2685,"Then these perspectives are combined to yield a decision , and only the selected dialogue contents are fed into State Generator , which explicitly minimizes the distracting information passed to the downstream state prediction .",2,0.6735572,136.97048606155946,35
2685,"Experimental results show that our approach achieves new state-of-the-art performance on MultiWOZ 2.1 and MultiWOZ 2.2 , and achieves superior performance on multiple mainstream benchmark datasets ( including Sim-M , Sim-R , and DSTC2 ) .",3,0.9204339,11.905369390931837,41
2686,Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks .,0,0.81350386,9.97971005298399,26
2686,"However , models with a task-specific head require a lot of training data , making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets .",0,0.78882724,35.79643101181604,34
2686,Prompting has reduced the data requirement by reusing the language model head and formatting the task input to match the pre-training objective .,3,0.6640557,76.69003265555557,23
2686,"Therefore , it is expected that few-shot prompt-based models do not exploit superficial cues .",0,0.75712115,51.33963020947692,16
2686,This paper presents an empirical examination of whether few-shot prompt-based models also exploit superficial cues .,1,0.86517555,57.336002233166305,17
2686,"Analyzing few-shot prompt-based models on MNLI , SNLI , HANS , and COPA has revealed that prompt-based models also exploit superficial cues .",0,0.56091434,78.43732663781066,25
2686,"While the models perform well on instances with superficial cues , they often underperform or only marginally outperform random accuracy on instances without superficial cues .",3,0.8450173,41.49482710363722,26
2687,"Confidence estimation aims to quantify the confidence of the model prediction , providing an expectation of success .",0,0.79802465,59.197804593586895,18
2687,A well-calibrated confidence estimate enables accurate failure prediction and proper risk measurement when given noisy samples and out-of-distribution data in real-world settings .,3,0.5363214,34.65039297260979,26
2687,"However , this task remains a severe challenge for neural machine translation ( NMT ) , where probabilities from softmax distribution fail to describe when the model is probably mistaken .",0,0.95074075,69.17611970706561,31
2687,"To address this problem , we propose an unsupervised confidence estimate learning jointly with the training of the NMT model .",2,0.5309445,31.00687978318998,21
2687,"We explain confidence as how many hints the NMT model needs to make a correct prediction , and more hints indicate low confidence .",2,0.43415794,99.99838516178811,24
2687,"Specifically , the NMT model is given the option to ask for hints to improve translation accuracy at the cost of some slight penalty .",3,0.5023586,48.113638125149805,25
2687,"Then , we approximate their level of confidence by counting the number of hints the model uses .",2,0.84398675,38.28184114563055,18
2687,We demonstrate that our learned confidence estimate achieves high accuracy on extensive sentence / word-level quality estimation tasks .,3,0.93356,89.36104430236466,19
2687,Analytical results verify that our confidence estimate can correctly assess underlying risk in two real-world scenarios : ( 1 ) discovering noisy samples and ( 2 ) detecting out-of-domain data .,3,0.98103243,47.385811991022585,33
2687,"We further propose a novel confidence-based instance-specific label smoothing approach based on our learned confidence estimate , which outperforms standard label smoothing .",3,0.45586342,33.388900808434805,26
2688,"Spatial commonsense , the knowledge about spatial position and relationship between objects ( like the relative size of a lion and a girl , and the position of a boy relative to a bicycle when cycling ) , is an important part of commonsense knowledge .",0,0.9594621,34.17563542450973,46
2688,"Although pretrained language models ( PLMs ) succeed in many NLP tasks , they are shown to be ineffective in spatial commonsense reasoning .",0,0.88163036,23.32405775718173,24
2688,"Starting from the observation that images are more likely to exhibit spatial commonsense than texts , we explore whether models with visual signals learn more spatial commonsense than text-based PLMs .",1,0.49600464,28.083459115089777,33
2688,"We propose a spatial commonsense benchmark that focuses on the relative scales of objects , and the positional relationship between people and objects under different actions .",1,0.5840169,66.84059209851941,27
2688,"We probe PLMs and models with visual signals , including vision-language pretrained models and image synthesis models , on this benchmark , and find that image synthesis models are more capable of learning accurate and consistent spatial knowledge than other models .",3,0.81655324,67.97526353782813,44
2688,The spatial knowledge from image synthesis models also helps in natural language understanding tasks that require spatial commonsense .,0,0.5074321,54.39547164539944,19
2689,"Token-level adaptive training approaches can alleviate the token imbalance problem and thus improve neural machine translation , through re-weighting the losses of different target tokens based on specific statistical metrics ( e.g. , token frequency or mutual information ) .",0,0.65367186,51.18603630126786,42
2689,"Given that standard translation models make predictions on the condition of previous target contexts , we argue that the above statistical metrics ignore target context information and may assign inappropriate weights to target tokens .",3,0.54482293,83.8698318371139,35
2689,"While one possible solution is to directly take target contexts into these statistical metrics , the target-context-aware statistical computing is extremely expensive , and the corresponding storage overhead is unrealistic .",0,0.65165895,105.74225149261737,34
2689,"To solve the above issues , we propose a target-context-aware metric , named conditional bilingual mutual information ( CBMI ) , which makes it feasible to supplement target context information for statistical metrics .",2,0.37659082,74.19518649028461,37
2689,"Particularly , our CBMI can be formalized as the log quotient of the translation model probability and language model probability by decomposing the conditional joint distribution .",3,0.5159593,97.13782701093622,27
2689,Thus CBMI can be efficiently calculated during model training without any pre-specific statistical calculations and large storage overhead .,3,0.8766036,243.7604009597106,19
2689,"Furthermore , we propose an effective adaptive training approach based on both the token-and sentence-level CBMI .",3,0.5493779,53.43249414420629,21
2689,Experimental results on WMT14 English-German and WMT19 Chinese-English tasks show our approach can significantly outperform the Transformer baseline and other related methods .,3,0.9569683,8.7488660111613,26
2690,"Recently , a lot of research has been carried out to improve the efficiency of Transformer .",0,0.94913286,15.911485490746378,17
2690,"Among them , the sparse pattern-based method is an important branch of efficient Transformers .",0,0.7245281,118.27231854586994,17
2690,"However , some existing sparse methods usually use fixed patterns to select words , without considering similarities between words .",0,0.8210223,129.68104976186333,20
2690,"Other sparse methods use clustering patterns to select words , but the clustering process is separate from the training process of the target task , which causes a decrease in effectiveness .",0,0.6669127,51.73217712307335,32
2690,"To address these limitations , we design a neural clustering method , which can be seamlessly integrated into the Self-Attention Mechanism in Transformer .",2,0.67365503,22.87560738542745,24
2690,"The clustering task and the target task are jointly trained and optimized to benefit each other , leading to significant effectiveness improvement .",3,0.5472706,39.2823166594436,23
2690,"In addition , our method groups the words with strong dependencies into the same cluster and performs the attention mechanism for each cluster independently , which improves the efficiency .",3,0.4975462,47.4268854707969,30
2690,"We verified our method on machine translation , text classification , natural language inference , and text matching tasks .",3,0.63252586,50.88198463612297,20
2690,"Experimental results show that our method outperforms two typical sparse attention methods , Reformer and Routing Transformer while having a comparable or even better time and memory efficiency .",3,0.964093,36.85828632087045,29
2691,Constituency parsing and nested named entity recognition ( NER ) are similar tasks since they both aim to predict a collection of nested and non-crossing spans .,0,0.9186888,40.950907529214476,27
2691,"In this work , we cast nested NER to constituency parsing and propose a novel pointing mechanism for bottom-up parsing to tackle both tasks .",1,0.73181087,66.7914634182132,27
2691,"The key idea is based on the observation that if we traverse a constituency tree in post-order , i.e. , visiting a parent after its children , then two consecutively visited spans would share a boundary .",0,0.50014573,72.31382992329556,37
2691,Our model tracks the shared boundaries and predicts the next boundary at each step by leveraging a pointer network .,2,0.64852273,51.24638254171159,20
2691,"As a result , it needs only linear steps to parse and thus is efficient .",3,0.7004872,111.88367087839113,16
2691,"It also maintains a parsing configuration for structural consistency , i.e. , always outputting valid trees .",2,0.4142914,210.30430744482484,17
2691,"Experimentally , our model achieves the state-of-the-art performance on PTB among all BERT-based models ( 96.01 F1 score ) and competitive performance on CTB7 in constituency parsing ;",3,0.8986306,34.81613798999746,36
2691,"and it also achieves strong performance on three benchmark datasets of nested NER : ACE2004 , ACE2005 , and GENIA .",3,0.7830618,51.644889745880064,21
2691,Our code will be available at https://github.com/xxxxx .,3,0.6219587,9.551235090763152,8
2692,"Knowledge distillation ( KD ) is the preliminary step for training non-autoregressive translation ( NAT ) models , which eases the training of NAT models at the cost of losing important information for translating low-frequency words .",0,0.8637018,28.80606421401497,37
2692,"In this work , we provide an appealing alternative for NAT – monolingual KD , which trains NAT student on external monolingual data with AT teacher trained on the original bilingual data .",1,0.72172403,107.8823970401581,33
2692,Monolingual KD is able to transfer both the knowledge of the original bilingual data ( implicitly encoded in the trained AT teacher model ) and that of the new monolingual data to the NAT student model .,3,0.76843596,51.56036797992544,37
2692,"Extensive experiments on eight WMT benchmarks over two advanced NAT models show that monolingual KD consistently outperforms the standard KD by improving low-frequency word translation , without introducing any computational cost .",3,0.88947976,52.28337226008554,32
2692,"Monolingual KD enjoys desirable expandability , which can be further enhanced ( when given more computational budget ) by combining with the standard KD , a reverse monolingual KD , or enlarging the scale of monolingual data .",3,0.55316025,104.7330645944811,38
2692,Extensive analyses demonstrate that these techniques can be used together profitably to further recall the useful information lost in the standard KD .,3,0.95441866,86.18098043056301,23
2692,"Encouragingly , combining with standard KD , our approach achieves 30.4 and 34.1 BLEU points on the WMT14 English-German and German-English datasets , respectively .",3,0.9171148,20.73105671360605,27
2692,Our code and trained models are freely available at https://github.com/alphadl/RLFW-NAT.mono .,3,0.63889635,28.1084984927357,11
2693,Higher-order methods for dependency parsing can partially but not fully address the issue that edges in dependency trees should be constructed at the text span / subtree level rather than word level .,0,0.66918516,43.50551478633863,34
2693,"In this paper , we propose a new method for dependency parsing to address this issue .",1,0.9064955,15.632528219678763,17
2693,"The proposed method constructs dependency trees by directly modeling span-span ( in other words , subtree-subtree ) relations .",2,0.64278597,78.62021005840484,19
2693,"It consists of two modules : the text span proposal module which proposes candidate text spans , each of which represents a subtree in the dependency tree denoted by ( root , start , end ) ;",2,0.6156583,70.57387154646487,37
2693,"and the span linking module , which constructs links between proposed spans .",3,0.37089533,152.90227388341225,13
2693,"We use the machine reading comprehension ( MRC ) framework as the backbone to formalize the span linking module , where one span is used as query to extract the text span / subtree it should be linked to .",2,0.81728345,48.53629268965356,40
2693,The proposed method has the following merits : ( 1 ) it addresses the fundamental problem that edges in a dependency tree should be constructed between subtrees ;,3,0.7045764,58.01935782757595,28
2693,"( 2 ) the MRC framework allows the method to retrieve missing spans in the span proposal stage , which leads to higher recall for eligible spans .",3,0.82845825,120.70335387815157,28
2693,"Extensive experiments on the PTB , CTB and Universal Dependencies ( UD ) benchmarks demonstrate the effectiveness of the proposed method .",3,0.73617697,21.43340028218084,22
2693,The code is available at https://github.com/ShannonAI/mrc-for-dependency-parsing .,3,0.53891337,11.16876701587282,7
2694,Cross-domain sentiment analysis has achieved promising results with the help of pre-trained language models .,0,0.78401595,6.065340774085298,15
2694,"As GPT-3 appears , prompt tuning has been widely explored to enable better semantic modeling in many natural language processing tasks .",0,0.7475261,40.33785751906475,24
2694,"However , directly using a fixed predefined template for cross-domain research cannot model different distributions of the \operatorname { [ MASK ] } token in different domains , thus making underuse of the prompt tuning technique .",3,0.55492425,134.8918443995879,37
2694,"In this paper , we propose a novel Adversarial Soft Prompt Tuning method ( AdSPT ) to better model cross-domain sentiment analysis .",1,0.88786286,22.154340312365523,23
2694,"On the one hand , AdSPT adopts separate soft prompts instead of hard templates to learn different vectors for different domains , thus alleviating the domain discrepancy of the \operatorname { [ MASK ] } token in the masked language modeling task .",3,0.6051949,107.21949842847619,43
2694,"On the other hand , AdSPT uses a novel domain adversarial training strategy to learn domain-invariant representations between each source domain and the target domain .",0,0.37386376,19.971196632173474,26
2694,Experiments on a publicly available sentiment analysis dataset show that our model achieves the new state-of-the-art results for both single-source domain adaptation and multi-source domain adaptation .,3,0.94892347,6.7022611778181345,34
2695,"Automated scientific fact checking is difficult due to the complexity of scientific language and a lack of significant amounts of training data , as annotation requires domain expertise .",0,0.9170288,40.41507164653851,29
2695,"To address this challenge , we propose scientific claim generation , the task of generating one or more atomic and verifiable claims from scientific sentences , and demonstrate its usefulness in zero-shot fact checking for biomedical claims .",1,0.7265073,54.0505469017663,38
2695,"We propose CLAIMGEN-BART , a new supervised method for generating claims supported by the literature , as well as KBIN , a novel method for generating claim negations .",3,0.315663,77.25835723102315,31
2695,"Additionally , we adapt an existing unsupervised entity-centric method of claim generation to biomedical claims , which we call CLAIMGEN-ENTITY .",2,0.80745536,43.0773933177953,23
2695,"Experiments on zero-shot fact checking demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART , coupled with KBIN , achieve up to 90 % performance of fully supervised models trained on manually annotated claims and evidence .",3,0.9146763,49.411889266583174,37
2695,A rigorous evaluation study demonstrates significant improvement in generated claim and negation quality over existing baselines .,3,0.70281047,49.40144087949524,17
2696,"Simultaneous machine translation ( SiMT ) outputs translation while reading source sentence and hence requires a policy to decide whether to wait for the next source word ( READ ) or generate a target word ( WRITE ) , the actions of which form a read / write path .",0,0.9191776,68.63840872730293,50
2696,"Although the read / write path is essential to SiMT performance , no direct supervision is given to the path in the existing methods .",0,0.6401799,89.630664425374,25
2696,"In this paper , we propose a method of dual-path SiMT which introduces duality constraints to direct the read / write path .",1,0.84805053,70.65818709269575,24
2696,"According to duality constraints , the read / write path in source-to-target and target-to-source SiMT models can be mapped to each other .",3,0.62337285,31.27193745522106,29
2696,"As a result , the two SiMT models can be optimized jointly by forcing their read / write paths to satisfy the mapping .",3,0.76002336,99.12929978906529,24
2696,Experiments on En-Vi and De-En tasks show that our method can outperform strong baselines under all latency .,3,0.9431192,20.634529390093828,19
2697,"Local models for Entity Disambiguation ( ED ) have today become extremely powerful , in most part thanks to the advent of large pre-trained language models .",0,0.9615581,31.874924942881098,27
2697,"However , despite their significant performance achievements , most of these approaches frame ED through classification formulations that have intrinsic limitations , both computationally and from a modeling perspective .",0,0.90396965,147.1418931632011,30
2697,"In contrast with this trend , here we propose ExtEnD , a novel local formulation for ED where we frame this task as a text extraction problem , and present two Transformer-based architectures that implement it .",1,0.43947065,71.73031459590219,39
2697,"Based on experiments in and out of domain , and training over two different data regimes , we find our approach surpasses all its competitors in terms of both data efficiency and raw performance .",3,0.86602885,38.15813192374574,35
2697,"ExtEnD outperforms its alternatives by as few as 6 F1 points on the more constrained of the two data regimes and , when moving to the other higher-resourced regime , sets a new state of the art on 4 out of 4 benchmarks under consideration , with average improvements of 0.7 F1 points overall and 1.1 F1 points out of domain .",3,0.92665946,33.93296235355043,64
2697,"In addition , to gain better insights from our results , we also perform a fine-grained evaluation of our performances on different classes of label frequency , along with an ablation study of our architectural choices and an error analysis .",2,0.45850793,41.26576282144504,42
2697,We release our code and models for research purposes at https://github.com/SapienzaNLP/extend .,3,0.55890083,9.869890828277585,12
2698,"We propose a generative model of paraphrase generation , that encourages syntactic diversity by conditioning on an explicit syntactic sketch .",1,0.4754837,42.37430715944199,21
2698,"We introduce Hierarchical Refinement Quantized Variational Autoencoders ( HRQ-VAE ) , a method for learning decompositions of dense encodings as a sequence of discrete latent variables that make iterative refinements of increasing granularity .",2,0.5851171,24.186239677301646,36
2698,"This hierarchy of codes is learned through end-to-end training , and represents fine-to-coarse grained information about the input .",0,0.42370018,37.21760678564156,25
2698,"We use HRQ-VAE to encode the syntactic form of an input sentence as a path through the hierarchy , allowing us to more easily predict syntactic sketches at test time .",2,0.6857456,48.687948144231285,33
2698,"Extensive experiments , including a human evaluation , confirm that HRQ-VAE learns a hierarchical representation of the input space , and generates paraphrases of higher quality than previous systems .",3,0.90610236,51.51964523474273,32
2699,Progress with supervised Open Information Extraction ( OpenIE ) has been primarily limited to English due to the scarcity of training data in other languages .,0,0.9554296,23.466572092795165,26
2699,"In this paper , we explore techniques to automatically convert English text for training OpenIE systems in other languages .",1,0.909797,54.83152533502196,20
2699,We introduce the Alignment-Augmented Constrained Translation ( AACTrans ) model to translate English sentences and their corresponding extractions consistently with each other — with no changes to vocabulary or semantic meaning which may result from independent translations .,2,0.667426,46.87808953276288,38
2699,"Using the data generated with AACTrans , we train a novel two-stage generative OpenIE model , which we call Gen2OIE , that outputs for each sentence : 1 ) relations in the first stage and 2 ) all extractions containing the relation in the second stage .",2,0.84907466,61.28585632481942,48
2699,"Gen2OIE increases relation coverage using a training data transformation technique that is generalizable to multiple languages , in contrast to existing models that use an English-specific training loss .",3,0.7247055,60.75968182896979,29
2699,"Evaluations on 5 languages — Spanish , Portuguese , Chinese , Hindi and Telugu — show that the Gen2OIE with AACTrans data outperforms prior systems by a margin of 6-25 % in F1 .",3,0.8809221,41.7111285672391,36
2700,"We study a new problem setting of information extraction ( IE ) , referred to as text-to-table .",1,0.57321095,67.76681083298277,21
2700,"In text-to-table , given a text , one creates a table or several tables expressing the main content of the text , while the model is learned from text-table pair data .",2,0.49278262,45.246645909593745,37
2700,The problem setting differs from those of the existing methods for IE .,2,0.49573925,93.00491842845304,13
2700,"First , the extraction can be carried out from long texts to large tables with complex structures .",0,0.5848538,48.841218620930384,18
2700,"Second , the extraction is entirely data-driven , and there is no need to explicitly define the schemas .",0,0.4380395,32.12249520981612,19
2700,"As far as we know , there has been no previous work that studies the problem .",0,0.89898205,21.371906599514485,17
2700,"In this work , we formalize text-to-table as a sequence-to-sequence ( seq2seq ) problem .",1,0.39320335,12.743411352954492,21
2700,We first employ a seq2seq model fine-tuned from a pre-trained language model to perform the task .,2,0.87469923,14.690710102834313,17
2700,"We also develop a new method within the seq2seq approach , exploiting two additional techniques in table generation : table constraint and table relation embeddings .",2,0.68202615,68.71549685916035,26
2700,"We consider text-to-table as an inverse problem of the well-studied table-to-text , and make use of four existing table-to-text datasets in our experiments on text-to-table .",2,0.8248468,15.756461693809387,43
2700,Experimental results show that the vanilla seq2seq model can outperform the baseline methods of using relation extraction and named entity extraction .,3,0.9775702,18.95014459011352,22
2700,The results also show that our method can further boost the performances of the vanilla seq2seq model .,3,0.98774,16.062970851680458,18
2700,We further discuss the main challenges of the proposed task .,3,0.550957,26.29725320350067,11
2700,The code and data are available at https://github.com/shirley-wu/text_to_table .,3,0.5658637,8.306453695526262,9
2701,Code search is to search reusable code snippets from source code corpus based on natural languages queries .,0,0.85852134,82.13343842151309,18
2701,Deep learning-based methods on code search have shown promising results .,0,0.85951734,30.14216540405187,13
2701,"However , previous methods focus on retrieval accuracy , but lacked attention to the efficiency of the retrieval process .",0,0.87704843,67.73653962773167,20
2701,"We propose a novel method CoSHC to accelerate code search with deep hashing and code classification , aiming to perform efficient code search without sacrificing too much accuracy .",1,0.47555345,74.76006468398552,29
2701,"To evaluate the effectiveness of CoSHC , we apply our methodon five code search models .",2,0.78522646,125.34810938740716,16
2701,"Extensive experimental results indicate that compared with previous code search baselines , CoSHC can save more than 90 % of retrieval time meanwhile preserving at least 99 % of retrieval accuracy .",3,0.9426359,69.05118370706508,32
2702,"Role-oriented dialogue summarization is to generate summaries for different roles in the dialogue , e.g. , merchants and consumers .",0,0.845773,42.867618273141645,20
2702,Existing methods handle this task by summarizing each role ’s content separately and thus are prone to ignore the information from other roles .,0,0.81446004,40.09547308418215,24
2702,"However , we believe that other roles ’ content could benefit the quality of summaries , such as the omitted information mentioned by other roles .",3,0.9706085,121.26449992156977,26
2702,"Therefore , we propose a novel role interaction enhanced method for role-oriented dialogue summarization .",1,0.72034425,52.59499555024222,15
2702,It adopts cross attention and decoder self-attention interactions to interactively acquire other roles ’ critical information .,2,0.4600431,121.37308306500985,17
2702,"The cross attention interaction aims to select other roles ’ critical dialogue utterances , while the decoder self-attention interaction aims to obtain key information from other roles ’ summaries .",2,0.51642257,64.5124786864191,30
2702,Experimental results have shown that our proposed method significantly outperforms strong baselines on two public role-oriented dialogue summarization datasets .,3,0.96241343,12.788615666837797,20
2702,Extensive analyses have demonstrated that other roles ’ content could help generate summaries with more complete semantics and correct topic structures .,3,0.6380752,148.55781043037112,22
2703,Generating new events given context with correlated ones plays a crucial role in many event-centric reasoning tasks .,0,0.9110616,58.129350458391514,18
2703,Existing works either limit their scope to specific scenarios or overlook event-level correlations .,0,0.8434764,52.260603066447985,14
2703,"In this paper , we propose to pre-train a general Correlation-aware context-to-Event Transformer ( ClarET ) for event-centric reasoning .",1,0.8875346,36.07357346276769,26
2703,"To achieve this , we propose three novel event-centric objectives , i.e. , whole event recovering , contrastive event-correlation encoding and prompt-based event locating , which highlight event-level correlations with effective training .",2,0.45810726,83.21588184437913,34
2703,"The proposed ClarET is applicable to a wide range of event-centric reasoning scenarios , considering its versatility of ( i ) event-correlation types ( e.g. , causal , temporal , contrast ) , ( ii ) application formulations ( i.e. , generation and classification ) , and ( iii ) reasoning types ( e.g. , abductive , counterfactual and ending reasoning ) .",3,0.81593555,45.1938712083508,63
2703,"Empirical fine-tuning results , as well as zero-and few-shot learning , on 9 benchmarks ( 5 generation and 4 classification tasks covering 4 reasoning types with diverse event correlations ) , verify its effectiveness and generalization ability .",3,0.73001325,95.3984712226245,39
2704,"Neural Machine Translation ( NMT ) systems exhibit problematic biases , such as stereotypical gender bias in the translation of occupation terms into languages with grammatical gender .",0,0.96166384,47.31932676838103,28
2704,"In this paper we describe a new source of bias prevalent in NMT systems , relating to translations of sentences containing person names .",1,0.90498614,67.64390404082891,24
2704,"To correctly translate such sentences , a NMT system needs to determine the gender of the name .",0,0.75398725,69.83995614314694,18
2704,"We show that leading systems are particularly poor at this task , especially for female given names .",3,0.96409005,211.70849486991622,18
2704,"This bias is deeper than given name gender : we show that the translation of terms with ambiguous sentiment can also be affected by person names , and the same holds true for proper nouns denoting race .",3,0.92505157,85.77571919325013,38
2704,"To mitigate these biases we propose a simple but effective data augmentation method based on randomly switching entities during translation , which effectively eliminates the problem without any effect on translation quality .",2,0.5225788,34.13436575928291,33
2705,"In this paper , we present a substantial step in better understanding the SOTA sequence-to-sequence ( Seq2Seq ) pretraining for neural machine translation ( NMT ) .",1,0.9239769,18.038493998935824,28
2705,"We focus on studying the impact of the jointly pretrained decoder , which is the main difference between Seq2Seq pretraining and previous encoder-based pretraining approaches for NMT .",1,0.4036332,29.998048969567634,29
2705,"On one hand , it helps NMT models to produce more diverse translations and reduce adequacy-related translation errors .",3,0.6529403,55.44424927683672,21
2705,"On the other hand , the discrepancies between Seq2Seq pretraining and NMT finetuning limit the translation quality ( i.e. , domain discrepancy ) and induce the over-estimation issue ( i.e. , objective discrepancy ) .",3,0.5280945,35.38009851428484,35
2705,"Based on these observations , we further propose simple and effective strategies , named in-domain pretraining and input adaptation to remedy the domain and objective discrepancies , respectively .",3,0.75355875,80.0162552767425,31
2705,Experimental results on several language pairs show that our approach can consistently improve both translation performance and model robustness upon Seq2Seq pretraining .,3,0.96180606,14.565109690846548,23
2706,Multimodal machine translation and textual chat translation have received considerable attention in recent years .,0,0.95446503,14.218434781537677,15
2706,"Although the conversation in its natural form is usually multimodal , there still lacks work on multimodal machine translation in conversations .",0,0.94298023,49.46826855425584,22
2706,"In this work , we introduce a new task named Multimodal Chat Translation ( MCT ) , aiming to generate more accurate translations with the help of the associated dialogue history and visual context .",1,0.80960757,27.420488859471018,35
2706,"To this end , we firstly construct a Multimodal Sentiment Chat Translation Dataset ( MSCTD ) containing 142,871 English-Chinese utterance pairs in 14,762 bilingual dialogues .",2,0.9349706,26.108001555764744,28
2706,"Each utterance pair , corresponding to the visual context that reflects the current conversational scene , is annotated with a sentiment label .",2,0.6895239,49.89265133035676,23
2706,"Then , we benchmark the task by establishing multiple baseline systems that incorporate multimodal and sentiment features for MCT .",2,0.8341739,78.61976019124141,20
2706,Preliminary experiments on two language directions ( English-Chinese ) verify the potential of contextual and multimodal information fusion and the positive impact of sentiment on the MCT task .,3,0.89133334,40.190166476634374,31
2706,"Additionally , we provide a new benchmark on multimodal dialogue sentiment analysis with the constructed MSCTD .",3,0.67631614,78.09498887665934,17
2706,Our work can facilitate researches on both multimodal chat translation and multimodal dialogue sentiment analysis .,3,0.963323,35.49827961849394,16
2707,"When working with textual data , a natural application of disentangled representations is the fair classification where the goal is to make predictions without being biased ( or influenced ) by sensible attributes that may be present in the data ( e.g. , age , gender or race ) .",0,0.84399974,47.637568861531754,50
2707,"Dominant approaches to disentangle a sensitive attribute from textual representations rely on learning simultaneously a penalization term that involves either an adversary loss ( e.g. , a discriminator ) or an information measure ( e.g. , mutual information ) .",0,0.8376704,55.28532146470587,40
2707,"However , these methods require the training of a deep neural network with several parameter updates for each update of the representation model .",0,0.793556,28.513973032345024,24
2707,"As a matter of fact , the resulting nested optimization loop is both times consuming , adding complexity to the optimization dynamic , and requires a fine hyperparameter selection ( e.g. , learning rates , architecture ) .",3,0.5467688,141.52238533462796,38
2707,"In this work , we introduce a family of regularizers for learning disentangled representations that do not require training .",1,0.596953,25.98944332703799,20
2707,These regularizers are based on statistical measures of similarity between the conditional probability distributions with respect to the sensible attributes .,0,0.557391,37.64745594795331,21
2707,"Our novel regularizers do not require additional training , are faster and do not involve additional tuning while achieving better results both when combined with pretrained and randomly initialized text encoders .",3,0.8809133,60.373408066809205,32
2708,Recent years have witnessed the emergence of a variety of post-hoc interpretations that aim to uncover how natural language processing ( NLP ) models make predictions .,0,0.9641749,15.064397162798015,27
2708,"Despite the surge of new interpretation methods , it remains an open problem how to define and quantitatively measure the faithfulness of interpretations , i.e. , to what extent interpretations reflect the reasoning process by a model .",0,0.9257478,32.66466887168911,38
2708,"We propose two new criteria , sensitivity and stability , that provide complementary notions of faithfulness to the existed removal-based criteria .",3,0.37236106,168.65654362916942,24
2708,Our results show that the conclusion for how faithful interpretations are could vary substantially based on different notions .,3,0.990171,145.5843042006692,19
2708,"Motivated by the desiderata of sensitivity and stability , we introduce a new class of interpretation methods that adopt techniques from adversarial robustness .",2,0.4207087,44.746990834231596,24
2708,Empirical results show that our proposed methods are effective under the new criteria and overcome limitations of gradient-based methods on removal-based criteria .,3,0.97070426,28.574276099004887,27
2708,"Besides text classification , we also apply interpretation methods and metrics to dependency parsing .",2,0.5399208,114.09315311886003,15
2708,Our results shed light on understanding the diverse set of interpretations .,3,0.989584,42.395033114622834,12
2709,"Solving crossword puzzles requires diverse reasoning capabilities , access to a vast amount of knowledge about language and the world , and the ability to satisfy the constraints imposed by the structure of the puzzle .",0,0.91489655,24.699666264386998,36
2709,"In this work , we introduce solving crossword puzzles as a new natural language understanding task .",1,0.74391437,35.7519510737253,17
2709,We release a corpus of crossword puzzles collected from the New York Times daily crossword spanning 25 years and comprised of a total of around nine thousand puzzles .,2,0.8089549,31.501234973297358,29
2709,"These puzzles include a diverse set of clues : historic , factual , word meaning , synonyms / antonyms , fill-in-the-blank , abbreviations , prefixes / suffixes , wordplay , and cross-lingual , as well as clues that depend on the answers to other clues .",0,0.7908314,38.9578802748641,51
2709,We separately release the clue-answer pairs from these puzzles as an open-domain question answering dataset containing over half a million unique clue-answer pairs .,2,0.8446286,28.746841455436797,24
2709,"For the question answering task , our baselines include several sequence-to-sequence and retrieval-based generative models .",2,0.63896936,21.959231532666628,20
2709,We also introduce a non-parametric constraint satisfaction baseline for solving the entire crossword puzzle .,2,0.5826907,71.04722272160213,15
2709,"Finally , we propose an evaluation framework which consists of several complementary performance metrics .",2,0.38228133,27.89956606506214,15
2710,"Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on , while not generalising to different task distributions .",0,0.8925742,48.44128747557752,37
2710,"We propose to tackle this problem by generating a debiased version of a dataset , which can then be used to train a debiased , off-the-shelf model , by simply replacing its training data .",2,0.49173728,18.688897665448387,37
2710,"Our approach consists of 1 ) a method for training data generators to generate high-quality , label-consistent data samples ;",2,0.8024158,88.89707233829391,22
2710,"and 2 ) a filtering mechanism for removing data points that contribute to spurious correlations , measured in terms of z-statistics .",2,0.7026362,80.7203517829652,22
2710,"We generate debiased versions of the SNLI and MNLI datasets , and we evaluate on a large suite of debiased , out-of-distribution , and adversarial test sets .",2,0.7661291,27.689724086421148,31
2710,Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings .,3,0.9871192,16.795559568938497,22
2710,"On the majority of the datasets , our method outperforms or performs comparably to previous state-of-the-art debiasing strategies , and when combined with an orthogonal technique , product-of-experts , it improves further and outperforms previous best results of SNLI-hard and MNLI-hard .",3,0.9572943,27.29962169301127,54
2711,"Due to high data demands of current methods , attention to zero-shot cross-lingual spoken language understanding ( SLU ) has grown , as such approaches greatly reduce human annotation effort .",0,0.9543514,57.75018740722883,31
2711,"However , existing models solely rely on shared parameters , which can only perform implicit alignment across languages .",0,0.89716524,106.5461926295816,19
2711,We present Global-Local Contrastive Learning Framework ( GL-CLeF ) to address this shortcoming .,2,0.43658552,67.08134009413322,16
2711,"Specifically , we employ contrastive learning , leveraging bilingual dictionaries to construct multilingual views of the same utterance , then encourage their representations to be more similar than negative example pairs , which achieves to explicitly align representations of similar sentences across languages .",2,0.8366134,75.2898498284201,44
2711,"In addition , a key step in GL-CLeF is a proposed Local and Global component , which achieves a fine-grained cross-lingual transfer ( i.e. , sentence-level Local intent transfer , token-level Local slot transfer , and semantic-level Global transfer across intent and slot ) .",3,0.4306986,56.088901659634004,52
2711,Experiments on MultiATIS ++ show that GL-CLeF achieves the best performance and successfully pulls representations of similar sentences across languages closer .,3,0.9409191,205.28181361358193,24
2712,Recent advances in prompt-based learning have shown strong results on few-shot text classification by using cloze-style templates .,0,0.82382166,23.303302638785514,19
2712,Similar attempts have been made on named entity recognition ( NER ) which manually design templates to predict entity types for every text span in a sentence .,0,0.90561444,46.51135179010309,28
2712,"However , such methods may suffer from error propagation induced by entity span detection , high cost due to enumeration of all possible text spans , and omission of inter-dependencies among token labels in a sentence .",0,0.7683837,70.62359341858787,37
2712,"Here we present a simple demonstration-based learning method for NER , which lets the input be prefaced by task demonstrations for in-context learning .",1,0.6076644,63.47948492270369,27
2712,"We perform a systematic study on demonstration strategy regarding what to include ( entity examples , with or without surrounding context ) , how to select the examples , and what templates to use .",2,0.76054823,115.15331998770856,35
2712,"Results on in-domain learning and domain adaptation show that the model ’s performance in low-resource settings can be largely improved with a suitable demonstration strategy ( e.g. , a 4-17 % improvement on 25 train instances ) .",3,0.9784718,43.51434269896322,41
2712,We also find that good demonstration can save many labeled examples and consistency in demonstration contributes to better performance .,3,0.98167473,109.98988769807269,20
2713,"Currently , masked language modeling ( e.g. , BERT ) is the prime choice to learn contextualized representations .",0,0.85433835,46.510253975189066,19
2713,"In this work , we analyze the learning dynamics of MLMs and find that it adopts sampled embeddings as anchors to estimate and inject contextual semantics to representations , which limits the efficiency and effectiveness of MLMs .",1,0.5835637,43.981921190541044,38
2713,"To address these problems , we propose TACO , a simple yet effective representation learning approach to directly model global semantics .",1,0.47305736,35.072275810923394,22
2713,"To be specific , TACO extracts and aligns contextual semantics hidden in contextualized representations to encourage models to attend global semantics when generating contextualized representations .",2,0.41908777,70.14232753005278,26
2713,Experiments on the GLUE benchmark show that TACO achieves up to 5x speedup and up to 1.2 points average improvement over MLM .,3,0.9051826,16.055014683913672,23
2714,"While hyper-parameters ( HPs ) are important for knowledge graph ( KG ) learning , existing methods fail to search them efficiently .",0,0.882716,52.895648948269674,23
2714,"To solve this problem , we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph .",2,0.5944023,45.680498853590585,26
2714,"Based on the analysis , we propose an efficient two-stage search algorithm KGTuner , which efficiently explores HP configurations on small subgraph at the first stage and transfers the top-performed configurations for fine-tuning on the large full graph at the second stage .",3,0.5692834,54.64529268638259,44
2714,"Experiments show that our method can consistently find better HPs than the baseline algorithms within the same time budget , which achieves 9.1 % average relative improvement for four embedding models on the large-scale KGs in open graph benchmark .",3,0.92507255,58.21017753483311,40
2714,Our code is released in https://github .,3,0.49984694,14.072556468832127,7
2714,com/ AutoML-Research / KGTuner .,4,0.7756873,938.6291249133776,7
2715,"News events are often associated with quantities ( e.g. , the number of COVID-19 patients or the number of arrests in a protest ) , and it is often important to extract their type , time , and location from unstructured text in order to analyze these quantity events .",0,0.9213013,28.186788047975124,51
2715,"This paper thus formulates the NLP problem of spatiotemporal quantity extraction , and proposes the first meta-framework for solving it .",1,0.8200344,38.90080847089489,21
2715,"This meta-framework contains a formalism that decomposes the problem into several information extraction tasks , a shareable crowdsourcing pipeline , and transformer-based baseline models .",2,0.60718805,56.77433837739498,27
2715,"We demonstrate the meta-framework in three domains — the COVID-19 pandemic , Black Lives Matter protests , and 2020 California wildfires — to show that the formalism is general and extensible , the crowdsourcing pipeline facilitates fast and high-quality data annotation , and the baseline system can handle spatiotemporal quantity extraction well enough to be practically useful .",3,0.6720897,46.82931785291998,58
2715,We release all resources for future research on this topic at https://github.com/steqe .,3,0.6667859,19.47090275050914,13
2716,Pre-trained language models are still far from human performance in tasks that need understanding of properties ( e.g .,0,0.8099548,18.522718450046526,19
2716,"appearance , measurable quantity ) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias .",0,0.655003,132.6602245499765,25
2716,"In this work , we study whether integrating visual knowledge into a language model can fill the gap .",1,0.9092922,41.09682791009787,19
2716,We investigate two types of knowledge transfer : ( 1 ) text knowledge transfer using image captions that may contain enriched visual knowledge and ( 2 ) cross-modal knowledge transfer using both images and captions with vision-language training objectives .,2,0.53558016,30.30009033589137,42
2716,"On 5 downstream tasks that may need visual knowledge to solve the problem , we perform extensive empirical comparisons over the presented objectives .",2,0.61459976,71.86547790818872,24
2716,Our experiments show that visual knowledge transfer can improve performance in both low-resource and fully supervised settings .,3,0.9776368,20.65216404575835,18
2717,Large pre-trained vision-language ( VL ) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning .,0,0.8482251,18.299905134572306,27
2717,"However , these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed .",0,0.859489,29.534113025588447,23
2717,"To solve this limitation , we study prompt-based low-resource learning of VL tasks with our proposed method , FewVLM , relatively smaller than recent few-shot learners .",2,0.5357944,116.00751306579832,28
2717,"For FewVLM , we pre-train a sequence-to-sequence transformer model with prefix language modeling ( PrefixLM ) and masked language modeling ( MaskedLM ) .",2,0.8200649,31.468843917898106,26
2717,"Furthermore , we analyze the effect of diverse prompts for few-shot tasks .",2,0.45126086,40.551946822388295,13
2717,"Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2 % point and achieves comparable results to a 246x larger model , PICa .",3,0.94654495,65.81653192190814,35
2717,"In our analysis , we observe that ( 1 ) prompts significantly affect zero-shot performance but marginally affect few-shot performance , ( 2 ) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data , and ( 3 ) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance .",3,0.95096064,59.27926931987218,53
2717,Our code is publicly available at https://github.com/woojeongjin/FewVLM .,3,0.59356415,16.462608717401707,8
2718,"Existing continual relation learning ( CRL ) methods rely on plenty of labeled training data for learning a new task , which can be hard to acquire in real scenario as getting large and representative labeled data is often expensive and time-consuming .",0,0.94437814,37.33193792864253,45
2718,It is therefore necessary for the model to learn novel relational patterns with very few labeled data while avoiding catastrophic forgetting of previous task knowledge .,0,0.6546773,43.99552377457054,26
2718,"In this paper , we formulate this challenging yet practical problem as continual few-shot relation learning ( CFRL ) .",1,0.78316444,65.67367136134995,20
2718,"Based on the finding that learning for new emerging few-shot tasks often results in feature distributions that are incompatible with previous tasks ’ learned distributions , we propose a novel method based on embedding space regularization and data augmentation .",2,0.45770282,33.77363606416387,40
2718,Our method generalizes to new few-shot tasks and avoids catastrophic forgetting of previous tasks by enforcing extra constraints on the relational embeddings and by adding extra relevant data in a self-supervised manner .,3,0.66472644,28.066912422906277,33
2718,With extensive experiments we demonstrate that our method can significantly outperform previous state-of-the-art methods in CFRL task settings .,3,0.9209425,11.316363125726586,25
2719,Coreference resolution over semantic graphs like AMRs aims to group the graph nodes that represent the same entity .,0,0.79181105,124.16998315155807,19
2719,This is a crucial step for making document-level formal semantic representations .,0,0.4886873,20.46040400823433,13
2719,"With annotated data on AMR coreference resolution , deep learning approaches have recently shown great potential for this task , yet they are usually data hunger and annotations are costly .",0,0.9229718,76.44257041150199,31
2719,"We propose a general pretraining method using variational graph autoencoder ( VGAE ) for AMR coreference resolution , which can leverage any general AMR corpus and even automatically parsed AMR data .",2,0.421164,46.76386986087646,32
2719,Experiments on benchmarks show that the pretraining approach achieves performance gains of up to 6 % absolute F1 points .,3,0.90306884,18.432887065879985,20
2719,"Moreover , our model significantly improves on the previous state-of-the-art model by up to 11 % F1 .",3,0.9594641,7.476296769257018,24
2720,"Recent works of opinion expression identification ( OEI ) rely heavily on the quality and scale of the manually-constructed training corpus , which could be extremely difficult to satisfy .",0,0.9425044,58.68989305391113,32
2720,"Crowdsourcing is one practical solution for this problem , aiming to create a large-scale but quality-unguaranteed corpus .",0,0.87398815,30.054904697660128,20
2720,"In this work , we investigate Chinese OEI with extremely-noisy crowdsourcing annotations , constructing a dataset at a very low cost .",1,0.75129306,85.93058750563365,24
2720,Following Zhang el al .,0,0.5548879,4107.4888322609995,5
2720,"( 2021 ) , we train the annotator-adapter model by regarding all annotations as gold-standard in terms of crowd annotators , and test the model by using a synthetic expert , which is a mixture of all annotators .",2,0.7591363,48.42284674971586,42
2720,"As this annotator-mixture for testing is never modeled explicitly in the training phase , we propose to generate synthetic training samples by a pertinent mixup strategy to make the training and testing highly consistent .",2,0.61318666,82.87651448909605,35
2720,"The simulation experiments on our constructed dataset show that crowdsourcing is highly promising for OEI , and our proposed annotator-mixup can further enhance the crowdsourcing modeling .",3,0.957807,76.24202725467613,27
2721,Knowledge graph embedding ( KGE ) models represent each entity and relation of a knowledge graph ( KG ) with low-dimensional embedding vectors .,0,0.59900844,31.013201114331423,24
2721,These methods have recently been applied to KG link prediction and question answering over incomplete KGs ( KGQA ) .,0,0.7538655,35.99450881996198,20
2721,"KGEs typically create an embedding for each entity in the graph , which results in large model sizes on real-world graphs with millions of entities .",0,0.6984159,36.57560348999864,26
2721,"For downstream tasks these atomic entity representations often need to be integrated into a multi stage pipeline , limiting their utility .",0,0.83723164,88.59065978652913,22
2721,We show that an off-the-shelf encoder-decoder Transformer model can serve as a scalable and versatile KGE model obtaining state-of-the-art results for KG link prediction and incomplete KG question answering .,3,0.9230185,14.959739486113637,36
2721,We achieve this by posing KG link prediction as a sequence-to-sequence task and exchange the triple scoring approach taken by prior KGE methods with autoregressive decoding .,2,0.58053225,47.70489930390308,29
2721,Such a simple but powerful method reduces the model size up to 98 % compared to conventional KGE models while keeping inference time tractable .,3,0.8501623,48.32971131353345,25
2721,"After finetuning this model on the task of KGQA over incomplete KGs , our approach outperforms baselines on multiple large-scale datasets without extensive hyperparameter tuning .",3,0.74293846,22.543613544772125,26
2722,Human communication is a collaborative process .,0,0.94500107,40.75916417809748,7
2722,"Speakers , on top of conveying their own intent , adjust the content and language expressions by taking the listeners into account , including their knowledge background , personalities , and physical capabilities .",0,0.4300598,124.8763927939193,34
2722,"Towards building AI agents with similar abilities in language communication , we propose a novel rational reasoning framework , Pragmatic Rational Speaker ( PRS ) , where the speaker attempts to learn the speaker-listener disparity and adjust the speech accordingly , by adding a light-weighted disparity adjustment layer into working memory on top of speaker ’s long-term memory system .",2,0.4299918,64.82599475104472,64
2722,"By fixing the long-term memory , the PRS only needs to update its working memory to learn and adapt to different types of listeners .",3,0.53251314,45.95147441169884,25
2722,"To validate our framework , we create a dataset that simulates different types of speaker-listener disparities in the context of referential games .",2,0.7461746,38.2273546493716,25
2722,"Our empirical results demonstrate that the PRS is able to shift its output towards the language that listeners are able to understand , significantly improve the collaborative task outcome , and learn the disparity more efficiently than joint training .",3,0.98673254,82.43076216225613,40
2723,Recent research demonstrates the effectiveness of using fine-tuned language models ( LM ) for dense retrieval .,0,0.9390389,35.74492804580398,18
2723,"However , dense retrievers are hard to train , typically requiring heavily engineered fine-tuning pipelines to realize their full potential .",0,0.89759415,61.501552826558815,21
2723,"In this paper , we identify and address two underlying problems of dense retrievers : i ) fragility to training data noise and ii ) requiring large batches to robustly learn the embedding space .",1,0.88105,76.27013487395949,35
2723,"We use the recently proposed Condenser pre-training architecture , which learns to condense information into the dense vector through LM pre-training .",2,0.6976683,52.02022054832559,22
2723,"On top of it , we propose coCondenser , which adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space .",2,0.5466058,51.31329579357474,24
2723,"Experiments on MS-MARCO , Natural Question , and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation , synthesis , or filtering , and the need for large batch training .",3,0.886039,85.118597070303,40
2723,"It shows comparable performance to RocketQA , a state-of-the-art , heavily engineered system , using simple small batch fine-tuning .",3,0.90132457,69.34008794822739,23
2724,Responsing with image has been recognized as an important capability for an intelligent conversational agent .,0,0.93415695,36.39164197876653,16
2724,"Yet existing works only focus on exploring the multimodal dialogue models which depend on retrieval-based methods , but neglecting generation methods .",0,0.90332323,54.933614665617796,24
2724,"To fill in the gaps , we first present a new task : multimodal dialogue response generation ( MDRG )-given the dialogue history , one model needs to generate a text sequence or an image as response .",2,0.47693184,51.02709848279474,40
2724,Learning such a MDRG model often requires multimodal dialogues containing both texts and images which are difficult to obtain .,0,0.5781087,43.761073859817515,20
2724,"Motivated by the challenge in practice , we consider MDRG under a natural assumption that only limited training examples are available .",2,0.6364931,74.97025725859949,22
2724,"In such a low-resource setting , we devise a novel conversational agent , Divter , in order to isolate parameters that depend on multimodal dialogues from the entire generation model .",2,0.57490987,57.38217062375694,31
2724,"By this means , the major part of the model can be learned from a large number of text-only dialogues and text-image pairs respectively , then the whole parameters can be well fitted using the limited training examples .",3,0.54108274,45.31410812111397,39
2724,"Extensive experiments demonstrate our method achieves state-of-the-art results in both automatic and human evaluation , and can generate informative text and high-resolution image responses .",3,0.9206486,14.774768919933416,31
2725,"Knowledge graphs store a large number of factual triples while they are still incomplete , inevitably .",0,0.9172506,104.67430098010989,17
2725,"The previous knowledge graph completion ( KGC ) models predict missing links between entities merely relying on fact-view data , ignoring the valuable commonsense knowledge .",0,0.9038062,106.85737066246848,27
2725,"The previous knowledge graph embedding ( KGE ) techniques suffer from invalid negative sampling and the uncertainty of fact-view link prediction , limiting KGC ’s performance .",0,0.890807,146.5770475879764,29
2725,"To address the above challenges , we propose a novel and scalable Commonsense-Aware Knowledge Embedding ( CAKE ) framework to automatically extract commonsense from factual triples with entity concepts .",1,0.4287651,34.939715185350344,32
2725,The generated commonsense augments effective self-supervision to facilitate both high-quality negative sampling ( NS ) and joint commonsense and fact-view link prediction .,3,0.81514835,125.47566501157553,24
2725,"Experimental results on the KGC task demonstrate that assembling our framework could enhance the performance of the original KGE models , and the proposed commonsense-aware NS module is superior to other NS techniques .",3,0.970933,52.39439398586918,36
2725,"Besides , our proposed framework could be easily adaptive to various KGE models and explain the predicted results .",3,0.9425822,73.00575772655861,19
2726,Most dominant neural machine translation ( NMT ) models are restricted to make predictions only according to the local context of preceding words in a left-to-right manner .,0,0.92993265,24.533163068261633,29
2726,"Although many previous studies try to incorporate global information into NMT models , there still exist limitations on how to effectively exploit bidirectional global context .",0,0.89699966,32.55045749812069,26
2726,"In this paper , we propose a Confidence Based Bidirectional Global Context Aware ( CBBGCA ) training framework for NMT , where the NMT model is jointly trained with an auxiliary conditional masked language model ( CMLM ) .",1,0.69456935,31.161429292082836,39
2726,The training consists of two stages : ( 1 ) multi-task joint training ;,2,0.6642378,68.56640923069851,14
2726,( 2 ) confidence based knowledge distillation .,2,0.5109722,198.14547137457828,8
2726,"At the first stage , by sharing encoder parameters , the NMT model is additionally supervised by the signal from the CMLM decoder that contains bidirectional global contexts .",2,0.6794836,79.25693915005273,29
2726,"Moreover , at the second stage , using the CMLM as teacher , we further pertinently incorporate bidirectional global context to the NMT model on its unconfidently-predicted target words via knowledge distillation .",2,0.68465257,73.78581721091393,35
2726,"Experimental results show that our proposed CBBGCA training framework significantly improves the NMT model by + 1.02 , + 1.30 and + 0.57 BLEU scores on three large-scale translation datasets , namely WMT’14 English-to-German , WMT’19 Chinese-to-English and WMT’14 English-to-French , respectively .",3,0.9015309,10.333091033101317,49
2727,"Abstractive summarization models are commonly trained using maximum likelihood estimation , which assumes a deterministic ( one-point ) target distribution in which an ideal model will assign all the probability mass to the reference summary .",0,0.9156634,69.14261430759761,36
2727,"This assumption may lead to performance degradation during inference , where the model needs to compare several system-generated ( candidate ) summaries that have deviated from the reference summary .",0,0.518917,70.84779259814631,31
2727,"To address this problem , we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality .",2,0.50630337,30.851132807872826,30
2727,Our method achieves a new state-of-the-art result on the CNN / DailyMail ( 47.78 ROUGE-1 ) and XSum ( 49.07 ROUGE-1 ) datasets .,3,0.8553035,12.179955663420488,32
2727,Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality .,3,0.9808122,30.259481984273958,23
2728,"In sequence modeling , certain tokens are usually less ambiguous than others , and representations of these tokens require fewer refinements for disambiguation .",0,0.75061077,57.172415146926866,24
2728,"However , given the nature of attention-based models like Transformer and UT ( universal transformer ) , all tokens are equally processed towards depth .",0,0.6438198,149.00149178893452,27
2728,"Inspired by the equilibrium phenomenon , we present a lazy transition , a mechanism to adjust the significance of iterative refinements for each token representation .",2,0.59521145,108.59604023556275,26
2728,"Our lazy transition is deployed on top of UT to build LT ( lazy transformer ) , where all tokens are processed unequally towards depth .",2,0.58221936,480.223927177504,26
2728,"Eventually , LT is encouraged to oscillate around a relaxed equilibrium .",3,0.6001526,158.7095384482878,12
2728,"Our experiments show that LT outperforms baseline models on several tasks of machine translation , pre-training , Learning to Execute , and LAMBADA .",3,0.96179813,80.49657107416185,24
2729,"We propose fill-in-the-blanks as a video understanding evaluation framework and introduce FIBER – a novel dataset consisting of 28,000 videos and descriptions in support of this evaluation framework .",2,0.3654546,30.486683213111277,34
2729,"The fill-in-the-blanks setting tests a model ’s understanding of a video by requiring it to predict a masked noun phrase in the caption of the video , given the video and the surrounding text .",2,0.43682316,33.48730240027204,40
2729,"The FIBER benchmark does not share the weaknesses of the current state-of-the-art language-informed video understanding tasks , namely : ( 1 ) video question answering using multiple-choice questions , where models perform relatively well because they exploit linguistic biases in the task formulation , thus making our framework challenging for the current state-of-the-art systems to solve ;",3,0.86477286,43.84630416101458,72
2729,"and ( 2 ) video captioning , which relies on an open-ended evaluation framework that is often inaccurate because system answers may be perceived as incorrect if they differ in form from the ground truth .",0,0.68608975,52.308969841239275,36
2729,The FIBER dataset and our code are available at https://lit.eecs.umich.edu/fiber/ .,3,0.5921683,12.628924285438108,11
2730,"Currently , Medical Subject Headings ( MeSH ) are manually assigned to every biomedical article published and subsequently recorded in the PubMed database to facilitate retrieving relevant information .",0,0.79919446,72.3942174478268,29
2730,"With the rapid growth of the PubMed database , large-scale biomedical document indexing becomes increasingly important .",0,0.9557674,32.469088237000236,17
2730,"MeSH indexing is a challenging task for machine learning , as it needs to assign multiple labels to each article from an extremely large hierachically organized collection .",0,0.88934153,69.41270126965794,28
2730,"To address this challenge , we propose KenMeSH , an end-to-end model that combines new text features and a dynamic knowledge-enhanced mask attention that integrates document features with MeSH label hierarchy and journal correlation features to index MeSH terms .",1,0.40358636,103.12919405248712,44
2730,Experimental results show the proposed method achieves state-of-the-art performance on a number of measures .,3,0.9642081,6.884535459123251,21
2731,Effective question-asking is a crucial component of a successful conversational chatbot .,0,0.8424292,14.718890499068014,14
2731,It could help the bots manifest empathy and render the interaction more engaging by demonstrating attention to the speaker ’s emotions .,3,0.89091426,78.52122638431442,22
2731,"However , current dialog generation approaches do not model this subtle emotion regulation technique due to the lack of a taxonomy of questions and their purpose in social chitchat .",0,0.89403164,57.10024388001009,30
2731,"To address this gap , we have developed an empathetic question taxonomy ( EQT ) , with special attention paid to questions ’ ability to capture communicative acts and their emotion-regulation intents .",1,0.5414089,79.8949390052978,35
2731,We further design a crowd-sourcing task to annotate a large subset of the EmpatheticDialogues dataset with the established labels .,2,0.84567696,40.21317005510783,20
2731,We use the crowd-annotated data to develop automatic labeling tools and produce labels for the whole dataset .,2,0.8004933,41.45682518753226,19
2731,"Finally , we employ information visualization techniques to summarize co-occurrences of question acts and intents and their role in regulating interlocutor ’s emotion .",2,0.63083124,61.10351127014323,24
2731,These results reveal important question-asking strategies in social dialogs .,3,0.99012333,68.26965941733607,12
2731,The EQT classification scheme can facilitate computational analysis of questions in datasets .,3,0.7637419,144.87773207083475,13
2731,"More importantly , it can inform future efforts in empathetic question generation using neural or hybrid methods .",3,0.9305462,70.66037713582904,18
2732,Aspect Sentiment Triplet Extraction ( ASTE ) is an emerging sentiment analysis task .,0,0.94783133,73.98261600755151,14
2732,Most of the existing studies focus on devising a new tagging scheme that enables the model to extract the sentiment triplets in an end-to-end fashion .,0,0.84504575,16.662729955008217,28
2732,"However , these methods ignore the relations between words for ASTE task .",0,0.76244545,135.09886085283796,13
2732,"In this paper , we propose an Enhanced Multi-Channel Graph Convolutional Network model ( EMC-GCN ) to fully utilize the relations between words .",1,0.8400009,19.153433317999827,27
2732,"Specifically , we first define ten types of relations for ASTE task , and then adopt a biaffine attention module to embed these relations as an adjacent tensor between words in a sentence .",2,0.88165677,49.53264752166856,34
2732,"After that , our EMC-GCN transforms the sentence into a multi-channel graph by treating words and the relation adjacent tensor as nodes and edges , respectively .",2,0.630268,87.18712568361737,29
2732,"Thus , relation-aware node representations can be learnt .",3,0.5547223,108.25821908949875,11
2732,"Furthermore , we consider diverse linguistic features to enhance our EMC-GCN model .",3,0.52068883,71.38677723829974,15
2732,"Finally , we design an effective refining strategy on EMC-GCN for word-pair representation refinement , which considers the implicit results of aspect and opinion extraction when determining whether word pairs match or not .",3,0.54618174,96.2348018253531,38
2732,"Extensive experimental results on the benchmark datasets demonstrate that the effectiveness and robustness of our proposed model , which outperforms state-of-the-art methods significantly .",3,0.8934539,8.789958338162423,30
2733,"We present ProtoTEx , a novel white-box NLP classification architecture based on prototype networks ( Li et al. , 2018 ) .",3,0.33171365,97.84000008806929,24
2733,ProtoTEx faithfully explains model decisions based on prototype tensors that encode latent clusters of training examples .,3,0.48035157,224.5742763546047,17
2733,"At inference time , classification decisions are based on the distances between the input text and the prototype tensors , explained via the training examples most similar to the most influential prototypes .",2,0.4797523,107.85066178769746,33
2733,We also describe a novel interleaved training algorithm that effectively handles classes characterized by ProtoTEx indicative features .,3,0.6606952,138.1491966290842,18
2733,"On a propaganda detection task , ProtoTEx accuracy matches BART-large and exceeds BERTlarge with the added benefit of providing faithful explanations .",3,0.8747619,410.1955459858149,24
2733,A user study also shows that prototype-based explanations help non-experts to better recognize propaganda in online news .,3,0.9669202,40.814554038674196,19
2734,Procedures are inherently hierarchical .,0,0.8805333,48.95537627644221,5
2734,"To “ make videos ” , one may need to “ purchase a camera ” , which in turn may require one to “ set a budget ” .",0,0.8092302,22.125055113914687,29
2734,"While such hierarchical knowledge is critical for reasoning about complex procedures , most existing work has treated procedures as shallow structures without modeling the parent-child relation .",0,0.87446773,58.672124937709896,28
2734,"In this work , we attempt to construct an open-domain hierarchical knowledge-base ( KB ) of procedures based on wikiHow , a website containing more than 110k instructional articles , each documenting the steps to carry out a complex procedure .",1,0.7712388,66.69404580823517,43
2734,"To this end , we develop a simple and efficient method that links steps ( e.g. , “ purchase a camera ” ) in an article to other articles with similar goals ( e.g. , “ how to choose a camera ” ) , recursively constructing the KB .",2,0.80696446,43.56762826209302,49
2734,"Our method significantly outperforms several strong baselines according to automatic evaluation , human judgment , and application to downstream tasks such as instructional video retrieval .",3,0.919722,45.4483917309515,26
2735,"In contrast to recent advances focusing on high-level representation learning across modalities , in this work we present a self-supervised learning framework that is able to learn a representation that captures finer levels of granularity across different modalities such as concepts or events represented by visual objects or spoken words .",2,0.33261698,23.725278890256146,52
2735,Our framework relies on a discretized embedding space created via vector quantization that is shared across different modalities .,2,0.6905285,32.502090395206,19
2735,"Beyond the shared embedding space , we propose a Cross-Modal Code Matching objective that forces the representations from different views ( modalities ) to have a similar distribution over the discrete embedding space such that cross-modal objects / actions localization can be performed without direct supervision .",2,0.5552532,49.126843854005116,47
2735,"We show that the proposed discretized multi-modal fine-grained representation ( e.g. , pixel / word / frame ) can complement high-level summary representations ( e.g. , video/ sentence / waveform ) for improved performance on cross-modal retrieval tasks .",3,0.9441614,32.46429676680108,42
2735,We also observe that the discretized representation uses individual clusters to represent the same semantic concept across modalities .,3,0.9588529,56.37806551171696,19
2736,Representations of events described in text are important for various tasks .,0,0.9224454,36.98577857336511,12
2736,"In this work , we present SWCC : a Simultaneous Weakly supervised Contrastive learning and Clustering framework for event representation learning .",1,0.79988086,59.86648500352776,22
2736,SWCC learns event representations by making better use of co-occurrence information of events .,2,0.36127478,41.08252495852632,14
2736,"Specifically , we introduce a weakly supervised contrastive learning method that allows us to consider multiple positives and multiple negatives , and a prototype-based clustering method that avoids semantically related events being pulled apart .",2,0.73242384,36.90678236397855,36
2736,"For model training , SWCC learns representations by simultaneously performing weakly supervised contrastive learning and prototype-based clustering .",2,0.70047903,84.16750126665426,19
2736,Experimental results show that SWCC outperforms other baselines on Hard Similarity and Transitive Sentence Similarity tasks .,3,0.9709019,24.039509492993478,17
2736,"In addition , a thorough analysis of the prototype-based clustering method demonstrates that the learned prototype vectors are able to implicitly capture various relations between events .",3,0.8994586,53.938343443274,28
2737,"We examine the effects of contrastive visual semantic pretraining by comparing the geometry and semantic properties of contextualized English language representations formed by GPT-2 and CLIP , a zero-shot multimodal image classifier which adapts the GPT-2 architecture to encode image captions .",2,0.68145525,24.95268448453591,46
2737,"We find that contrastive visual semantic pretraining significantly mitigates the anisotropy found in contextualized word embeddings from GPT-2 , such that the intra-layer self-similarity ( mean pairwise cosine similarity ) of CLIP word embeddings is under .25 in all layers , compared to greater than .95 in the top layer of GPT-2 .",3,0.9678505,24.776594205862402,57
2737,"CLIP word embeddings outperform GPT-2 on word-level semantic intrinsic evaluation tasks , and achieve a new corpus-based state of the art for the RG65 evaluation , at .88 .",3,0.88092035,61.0267268035508,32
2737,"CLIP also forms fine-grained semantic representations of sentences , and obtains Spearman ’s 𝜌 = .73 on the SemEval-2017 Semantic Textual Similarity Benchmark with no fine-tuning , compared to no greater than 𝜌 = .45 in any layer of GPT-2 .",3,0.87710404,36.444980421977384,46
2737,"Finally , intra-layer self-similarity of CLIP sentence embeddings decreases as the layer index increases , finishing at .25 in the top layer , while the self-similarity of GPT-2 sentence embeddings formed using the EOS token increases layer-over-layer and never falls below .97 .",3,0.9504253,38.36300310921637,48
2737,"Our results indicate that high anisotropy is not an inevitable consequence of contextualization , and that visual semantic pretraining is beneficial not only for ordering visual representations , but also for encoding useful semantic representations of language , both on the word level and the sentence level .",3,0.9895756,31.469789279486378,48
2738,The mainstream machine learning paradigms for NLP often work with two underlying presumptions .,0,0.9358112,65.05039748339388,14
2738,"First , the target task is predefined and static ;",2,0.5952251,104.67809440615808,10
2738,a system merely needs to learn to solve it exclusively .,0,0.79391336,161.42777159577548,11
2738,"Second , the supervision of a task mainly comes from a set of labeled examples .",0,0.5733935,45.866864955348824,16
2738,"This work defines a new learning paradigm ConTinTin ( Continual Learning from Task Instructions ) , in which a system should learn a sequence of new tasks one by one , each task is explained by a piece of textual instruction .",1,0.43747428,76.75387151829288,42
2738,"The system is required to ( i ) generate the expected outputs of a new task by learning from its instruction , ( ii ) transfer the knowledge acquired from upstream tasks to help solve downstream tasks ( i.e. , forward-transfer ) , and ( iii ) retain or even improve the performance on earlier tasks after learning new tasks ( i.e. , backward-transfer ) .",3,0.3703412,35.484766062659254,70
2738,"This new problem is studied on a stream of more than 60 tasks , each equipped with an instruction .",2,0.5913884,108.09655800217269,20
2738,"Technically , our method InstructionSpeak contains two strategies that make full use of task instructions to improve forward-transfer and backward-transfer : one is to learn from negative outputs , the other is to re-visit instructions of previous tasks .",2,0.63136524,62.6995220382548,43
2738,"To our knowledge , this is the first time to study ConTinTin in NLP .",3,0.93156534,36.14658903214832,15
2738,"In addition to the problem formulation and our promising approach , this work also contributes to providing rich analyses for the community to better understand this novel learning problem .",3,0.903164,59.39357628187606,30
2739,"We present the Berkeley Crossword Solver , a state-of-the-art approach for automatically solving crossword puzzles .",1,0.52038234,17.023057335347026,21
2739,Our system works by generating answer candidates for each crossword clue using neural question answering models and then combines loopy belief propagation with local search to find full puzzle solutions .,2,0.69729275,91.21030262905353,31
2739,"Compared to existing approaches , our system improves exact puzzle accuracy from 57 % to 82 % on crosswords from The New York Times and obtains 99.9 % letter accuracy on themeless puzzles .",3,0.89564264,54.280869728199036,34
2739,"Our system also won first place at the top human crossword tournament , which marks the first time that a computer program has surpassed human performance at this event .",3,0.8452987,37.43369482347837,30
2739,"To facilitate research on question answering and crossword solving , we analyze our system ’s remaining errors and release a dataset of over six million question-answer pairs .",2,0.47702616,43.833510609031116,30
2740,"We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence , where the label is predicted using strictly incremental processing of a prefix of the sentence , and the sequence of labels for a sentence fully determines a parse tree .",2,0.64757466,44.67276759779392,51
2740,"Our goal is to induce a syntactic representation that commits to syntactic choices only as they are incrementally revealed by the input , in contrast with standard representations that must make output choices such as attachments speculatively and later throw out conflicting analyses .",1,0.5840111,71.25139306360627,44
2740,"Our learned representations achieve 93.72 F1 on the Penn Treebank with as few as 5 bits per word , and at 8 bits per word they achieve 94.97 F1 , which is comparable with other state of the art parsing models when using the same pre-trained embeddings .",3,0.8442324,14.60721310858875,48
2740,"We also provide an analysis of the representations learned by our system , investigating properties such as the interpretable syntactic features captured by the system and mechanisms for deferred resolution of syntactic ambiguities .",3,0.6013696,36.88098314825309,34
2741,"In this paper , we study the effect of commonsense and domain knowledge while generating responses in counseling conversations using retrieval and generative methods for knowledge integration .",1,0.9445605,65.09064085186232,28
2741,"We propose a pipeline that collects domain knowledge through web mining , and show that retrieval from both domain-specific and commonsense knowledge bases improves the quality of generated responses .",3,0.5064087,43.70736413021048,30
2741,We also present a model that incorporates knowledge generated by COMET using soft positional encoding and masked self-attention .,3,0.52441406,53.112692194904554,19
2741,We show that both retrieved and COMET-generated knowledge improve the system ’s performance as measured by automatic metrics and also by human evaluation .,3,0.9613858,53.71691442907719,26
2741,"Lastly , we present a comparative study on the types of knowledge encoded by our system showing that causal and intentional relationships benefit the generation task more than other types of commonsense relations .",3,0.9032271,52.22582660521972,34
2742,"Even to a simple and short news headline , readers react in a multitude of ways : cognitively ( e.g .",0,0.7880548,67.49910470132491,21
2742,"inferring the writer ’s intent ) , emotionally ( e.g .",0,0.47125491,159.79711688528187,11
2742,"feeling distrust ) , and behaviorally ( e.g .",0,0.50160307,289.6648418588712,9
2742,sharing the news with their friends ) .,3,0.39186502,163.10321801417518,8
2742,"Such reactions are instantaneous and yet complex , as they rely on factors that go beyond interpreting factual content of news .",0,0.83045405,107.33096040351086,22
2742,"We propose Misinfo Reaction Frames ( MRF ) , a pragmatic formalism for modeling how readers might react to a news headline .",1,0.5339052,120.91027162436941,23
2742,"In contrast to categorical schema , our free-text dimensions provide a more nuanced way of understanding intent beyond being benign or malicious .",3,0.8391268,135.6976163165908,23
2742,"We also introduce a Misinfo Reaction Frames corpus , a crowdsourced dataset of reactions to over 25 k news headlines focusing on global crises : the Covid-19 pandemic , climate change , and cancer .",2,0.76400274,104.1490950437223,35
2742,Empirical results confirm that it is indeed possible for neural models to predict the prominent patterns of readers ’ reactions to previously unseen news headlines .,3,0.9808096,34.55664689304421,26
2742,"Additionally , our user study shows that displaying machine-generated MRF implications alongside news headlines to readers can increase their trust in real news while decreasing their trust in misinformation .",3,0.9872153,46.45164169993741,32
2742,Our work demonstrates the feasibility and importance of pragmatic inferences on news headlines to help enhance AI-guided misinformation detection and mitigation .,3,0.98111963,52.00135963341305,24
2743,Real-world natural language processing ( NLP ) models need to be continually updated to fix the prediction errors in out-of-distribution ( OOD ) data streams while overcoming catastrophic forgetting .,0,0.9524052,24.37256997815528,32
2743,"However , existing continual learning ( CL ) problem setups cannot cover such a realistic and complex scenario .",0,0.9027455,290.50640207302615,19
2743,"In response to this , we propose a new CL problem formulation dubbed continual model refinement ( CMR ) .",1,0.347052,160.30600842996094,20
2743,"Compared to prior CL settings , CMR is more practical and introduces unique challenges ( boundary-agnostic and non-stationary distribution shift , diverse mixtures of multiple OOD data clusters , error-centric streams , etc. ) .",0,0.584819,158.05671889777713,37
2743,We extend several existing CL approaches to the CMR setting and evaluate them extensively .,2,0.31953907,106.57637518297648,15
2743,"For benchmarking and analysis , we propose a general sampling algorithm to obtain dynamic OOD data streams with controllable non-stationarity , as well as a suite of metrics measuring various aspects of online performance .",2,0.55801946,47.95407527277787,35
2743,"Our experiments and detailed analysis reveal the promise and challenges of the CMR problem , supporting that studying CMR in dynamic OOD streams can benefit the longevity of deployed NLP models in production .",3,0.97924894,95.80140176259047,34
2744,"A limitation of current neural dialog models is that they tend to suffer from a lack of specificity and informativeness in generated responses , primarily due to dependence on training data that covers a limited variety of scenarios and conveys limited knowledge .",0,0.87492466,25.574225202096972,43
2744,One way to alleviate this issue is to extract relevant knowledge from external sources at decoding time and incorporate it into the dialog response .,0,0.7873776,20.87712556390779,25
2744,"In this paper , we propose a post-hoc knowledge-injection technique where we first retrieve a diverse set of relevant knowledge snippets conditioned on both the dialog history and an initial response from an existing dialog model .",1,0.76700443,26.56204503194608,39
2744,"We construct multiple candidate responses , individually injecting each retrieved snippet into the initial response using a gradient-based decoding method , and then select the final response with an unsupervised ranking step .",2,0.8930442,59.50299625726062,35
2744,Our experiments in goal-oriented and knowledge-grounded dialog settings demonstrate that human annotators judge the outputs from the proposed method to be more engaging and informative compared to responses from prior dialog systems .,3,0.95866627,28.316700049423073,35
2744,We further show that knowledge-augmentation promotes success in achieving conversational goals in both experimental settings .,3,0.9746396,50.522550767205,16
2745,It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models .,0,0.718671,30.27962413884298,21
2745,"To investigate this question , we develop generated knowledge prompting , which consists of generating knowledge from a language model , then providing the knowledge as additional input when answering a question .",1,0.41657734,71.7663401138987,33
2745,"Our method does not require task-specific supervision for knowledge integration , or access to a structured knowledge base , yet it improves performance of large-scale , state-of-the-art models on four commonsense reasoning tasks , achieving state-of-the-art results on numerical commonsense ( NumerSense ) , general commonsense ( CommonsenseQA 2.0 ) , and scientific commonsense ( QASC ) benchmarks .",3,0.7239511,26.39455124427471,72
2745,Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning .,3,0.8815886,61.316022268002015,18
2745,Our code is available at github.com/liujch1998/GKP .,3,0.59809506,37.189533166422066,7
2746,Retrieval-based methods have been shown to be effective in NLP tasks via introducing external knowledge .,0,0.8941646,13.94832301778077,18
2746,"However , the indexing and retrieving of large-scale corpora bring considerable computational cost .",0,0.91459364,41.84288482203151,14
2746,"Surprisingly , we found that REtrieving from the traINing datA ( REINA ) only can lead to significant gains on multiple NLG and NLU tasks .",3,0.9859519,213.53372946204308,26
2746,We retrieve the labeled training instances most similar to the input text and then concatenate them with the input to feed into the model to generate the output .,2,0.7724352,25.66030608757122,29
2746,"Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks , including summarization , machine translation , language modeling , and question answering tasks .",3,0.96349806,17.685309235280556,35
2746,"For instance , our proposed method achieved state-of-the-art results on XSum , BigPatent , and CommonsenseQA .",3,0.9321753,27.866991399456406,22
2746,"Our code is released , https://github.com/microsoft/REINA .",3,0.4480768,31.568247054655373,7
2747,"Existing pre-trained transformer analysis works usually focus only on one or two model families at a time , overlooking the variability of the architecture and pre-training objectives .",0,0.87615424,41.37779833457632,28
2747,"In our work , we utilize the oLMpics bench-mark and psycholinguistic probing datasets for a diverse set of 29 models including T5 , BART , and ALBERT .",2,0.80091864,80.84955208462785,29
2747,"Additionally , we adapt the oLMpics zero-shot setup for autoregres-sive models and evaluate GPT networks of different sizes .",2,0.76925766,216.37132746537412,21
2747,"Our findings show that none of these models can resolve compositional questions in a zero-shot fashion , suggesting that this skill is not learnable using existing pre-training objectives .",3,0.99006224,30.6346736130118,29
2747,"Furthermore , we find that global model decisions such as architecture , directionality , size of the dataset , and pre-training objective are not predictive of a model ’s linguistic capabilities .",3,0.9696661,46.47915998481112,32
2748,Controlled text perturbation is useful for evaluating and improving model generalizability .,3,0.63710135,29.511975867567912,12
2748,"However , current techniques rely on training a model for every target perturbation , which is expensive and hard to generalize .",0,0.9218909,30.838711897717083,22
2748,"We present Tailor , a semantically-controlled text generation system .",1,0.37359354,73.66892370228148,12
2748,Tailor builds on a pretrained seq2seq model and produces textual outputs conditioned on control codes derived from semantic representations .,2,0.4978043,48.08415474510328,20
2748,"We craft a set of operations to modify the control codes , which in turn steer generation towards targeted attributes .",2,0.5309672,136.0277573418485,21
2748,"These operations can be further composed into higher-level ones , allowing for flexible perturbation strategies .",3,0.5567949,58.99632011057531,18
2748,We demonstrate the effectiveness of these perturbations in multiple applications .,3,0.75230044,24.337694847435927,11
2748,"First , we use Tailor to automatically create high-quality contrast sets for four distinct natural language processing ( NLP ) tasks .",2,0.88160527,40.189237025390575,22
2748,These contrast sets contain fewer spurious artifacts and are complementary to manually annotated ones in their lexical diversity .,3,0.8769368,84.43767587146021,19
2748,"Second , we show that Tailor perturbations can improve model generalization through data augmentation .",3,0.54801327,25.837655982188146,15
2748,Perturbing just ∼ 2 % of training data leads to a 5.8-point gain on an NLI challenge set measuring reliance on syntactic heuristics .,3,0.8788917,56.69017946800619,26
2749,We propose a benchmark to measure whether a language model is truthful in generating answers to questions .,1,0.40532658,22.547870801219908,18
2749,"The benchmark comprises 817 questions that span 38 categories , including health , law , finance and politics .",2,0.58351886,95.13762756518807,19
2749,We crafted questions that some humans would answer falsely due to a false belief or misconception .,2,0.63180137,103.51782342989745,17
2749,"To perform well , models must avoid generating false answers learned from imitating human texts .",0,0.8398227,236.88145223514243,16
2749,"We tested GPT-3 , GPT-Neo / J , GPT-2 and a T5-based model .",2,0.71147275,43.5185861286076,22
2749,"The best model was truthful on 58 % of questions , while human performance was 94 % .",3,0.96056145,89.43760583820614,18
2749,Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans .,3,0.5255586,116.85132553058206,17
2749,The largest models were generally the least truthful .,3,0.9488875,373.1930236999168,9
2749,"This contrasts with other NLP tasks , where performance improves with model size .",3,0.5801224,47.70486518271236,14
2749,"However , this result is expected if false answers are learned from the training distribution .",3,0.5475996,82.51885475324683,16
2749,We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web .,3,0.98185986,65.01786726935102,27
2750,"Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor , or only work for a very restrictive class of bugs .",0,0.8937792,132.4418504259557,29
2750,"We present AdaTest , a process which uses large scale language models ( LMs ) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model .",2,0.390212,85.47093562762485,32
2750,"Such bugs are then addressed through an iterative text-fix-retest loop , inspired by traditional software development .",2,0.39307734,157.7453820789675,17
2750,"In experiments with expert and non-expert users and commercial / research models for 8 different tasks , AdaTest makes users 5-10x more effective at finding bugs than current approaches , and helps users effectively fix bugs without adding new bugs .",3,0.8621182,55.65310433389756,43
2751,"When pre-trained contextualized embedding-based models developed for unstructured data are adapted for structured tabular data , they perform admirably .",3,0.8060702,27.567472975502454,22
2751,"However , recent probing studies show that these models use spurious correlations , and often predict inference labels by focusing on false evidence or ignoring it altogether .",0,0.9034747,91.15008545635494,28
2751,"To study this issue , we introduce the task of Trustworthy Tabular Reasoning , where a model needs to extract evidence to be used for reasoning , in addition to predicting the label .",1,0.562703,47.37000920316772,34
2751,"As a case study , we propose a two-stage sequential prediction approach , which includes an evidence extraction and an inference stage .",2,0.53708094,30.4371516981628,24
2751,"First , we crowdsource evidence row labels and develop several unsupervised and supervised evidence extraction strategies for InfoTabS , a tabular NLI benchmark .",2,0.91531545,95.89358601068832,24
2751,Our evidence extraction strategy outperforms earlier baselines .,3,0.9229804,118.35982208242464,8
2751,"On the downstream tabular inference task , using only the automatically extracted evidence as the premise , our approach outperforms prior benchmarks .",3,0.8514745,81.75212216507512,23
2752,The composition of richly-inflected words in morphologically complex languages can be a challenge for language learners developing literacy .,0,0.8924555,39.85098198809284,21
2752,"Accordingly , Lane and Bird ( 2020 ) proposed a finite state approach which maps prefixes in a language to a set of possible completions up to the next morpheme boundary , for the incremental building of complex words .",0,0.8689715,76.12759486745603,40
2752,"In this work , we develop an approach to morph-based auto-completion based on a finite state morphological analyzer of Plains Cree ( nêhiyawêwin ) , showing the portability of the concept to a much larger , more complete morphological transducer .",1,0.71891123,60.01617018107869,42
2752,"Additionally , we propose and compare various novel ranking strategies on the morph auto-complete output .",2,0.4841458,124.03086111277538,16
2752,"The best weighting scheme ranks the target completion in the top 10 results in 64.9 % of queries , and in the top 50 in 73.9 % of queries .",3,0.94963616,37.13901858661276,30
2753,Semantic parsing is the task of producing structured meaning representations for natural language sentences .,0,0.9236661,18.23261151341524,15
2753,"Recent research has pointed out that the commonly-used sequence-to-sequence ( seq2seq ) semantic parsers struggle to generalize systematically , i.e .",0,0.94461054,18.060634739577253,25
2753,to handle examples that require recombining known knowledge in novel settings .,0,0.40063718,178.70598283896595,12
2753,"In this work , we show that better systematic generalization can be achieved by producing the meaning representation directly as a graph and not as a sequence .",1,0.595002,29.423073924074647,28
2753,"To this end we propose LAGr ( Label Aligned Graphs ) , a general framework to produce semantic parses by independently predicting node and edge labels for a complete multi-layer input-aligned graph .",2,0.44895753,81.15027640403513,33
2753,"The strongly-supervised LAGr algorithm requires aligned graphs as inputs , whereas weakly-supervised LAGr infers alignments for originally unaligned target graphs using approximate maximum-a-posteriori inference .",2,0.49736094,67.59225105860965,30
2753,Experiments demonstrate that LAGr achieves significant improvements in systematic generalization upon the baseline seq2seq parsers in both strongly-and weakly-supervised settings .,3,0.9619492,31.41763429673913,25
2754,"Toxic language detection systems often falsely flag text that contains minority group mentions as toxic , as those groups are often the targets of online hate .",0,0.7834643,59.54250491747009,27
2754,Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language .,0,0.84385735,60.01805899484254,16
2754,"To help mitigate these issues , we create ToxiGen , a new large-scale and machine-generated dataset of 274 k toxic and benign statements about 13 minority groups .",2,0.49112296,82.25893821545552,30
2754,We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model .,2,0.7130181,39.55335229907296,33
2754,"Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale , and about more demographic groups , than previous resources of human-written text .",3,0.6230917,150.67817266489408,31
2754,We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language .,3,0.7107537,30.59492981477473,26
2754,We also find that 94.5 % of toxic examples are labeled as hate speech by human annotators .,3,0.96858585,35.60083521446766,18
2754,"Using three publicly-available datasets , we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially .",3,0.79463077,34.027068934627145,25
2754,We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset .,3,0.9699497,66.23482024746134,25
2755,We present a direct speech-to-speech translation ( S2ST ) model that translates speech from one language to speech in another language without relying on intermediate text generation .,1,0.6032441,25.050615292805997,29
2755,We tackle the problem by first applying a self-supervised discrete speech encoder on the target speech and then training a sequence-to-sequence speech-to-unit translation ( S2UT ) model to predict the discrete representations of the target speech .,2,0.77188915,17.489056895298148,45
2755,"When target text transcripts are available , we design a joint speech and text training framework that enables the model to generate dual modality output ( speech and text ) simultaneously in the same inference pass .",2,0.7258508,66.02169287319545,37
2755,Experiments on the Fisher Spanish-English dataset show that the proposed framework yields improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features .,3,0.9100036,42.57908956289482,30
2755,"When trained without any text transcripts , our model performance is comparable to models that predict spectrograms and are trained with text supervision , showing the potential of our system for translation between unwritten languages .",3,0.94120497,63.65875066667206,36
2756,State-of-the-art abstractive summarization systems often generate hallucinations ;,0,0.8663234,42.73463224836465,12
2756,"i.e. , content that is not directly inferable from the source text .",0,0.57937163,32.10438007284832,13
2756,"Despite being assumed to be incorrect , we find that much hallucinated content is actually consistent with world knowledge , which we call factual hallucinations .",3,0.9587432,61.46358708751636,26
2756,Including these factual hallucinations in a summary can be beneficial because they provide useful background information .,3,0.51585424,61.253368547380006,17
2756,"In this work , we propose a novel detection approach that separates factual from non-factual hallucinations of entities .",1,0.8774039,28.521520100584244,19
2756,"Our method is based on an entity ’s prior and posterior probabilities according to pre-trained and finetuned masked language models , respectively .",2,0.7689104,33.709592801716155,23
2756,Empirical results suggest that our method vastly outperforms two baselines in both accuracy and F1 scores and has a strong correlation with human judgments on factuality classification tasks .,3,0.97361416,15.017788969839769,29
2756,"Furthermore , we use our method as a reward signal to train a summarization system using an off-line reinforcement learning ( RL ) algorithm that can significantly improve the factuality of generated summaries while maintaining the level of abstractiveness .",2,0.6045595,28.550836616919952,40
2757,"Controllable summarization aims to provide summaries that take into account user-specified aspects and preferences to better assist them with their information need , as opposed to the standard summarization setup which build a single generic summary of a document .",0,0.8631291,34.424295148895666,40
2757,We introduce a human-annotated data set EntSUM for controllable summarization with a focus on named entities as the aspects to control .,2,0.6972211,51.43450681964454,22
2757,We conduct an extensive quantitative analysis to motivate the task of entity-centric summarization and show that existing methods for controllable summarization fail to generate entity-centric summaries .,3,0.4171614,17.61375269980055,29
2757,We propose extensions to state-of-the-art summarization approaches that achieve substantially better results on our data set .,3,0.542327,16.11865924195434,22
2757,Our analysis and results show the challenging nature of this task and of the proposed data set .,3,0.976374,32.56254304803388,18
2758,User language data can contain highly sensitive personal content .,0,0.860569,130.3986024473517,10
2758,"As such , it is imperative to offer users a strong and interpretable privacy guarantee when learning from their data .",0,0.84074515,35.87062168365192,21
2758,"In this work we propose SentDP , pure local differential privacy at the sentence level for a single user document .",1,0.53287005,190.49331455699982,21
2758,"We propose a novel technique , DeepCandidate , that combines concepts from robust statistics and language modeling to produce high ( 768 ) dimensional , general 𝜖-SentDP document embeddings .",2,0.3719563,211.9082691106571,31
2758,This guarantees that any single sentence in a document can be substituted with any other sentence while keeping the embedding 𝜖-indistinguishable .,3,0.59248936,27.48476940459196,22
2758,Our experiments indicate that these private document embeddings are useful for downstream tasks like sentiment analysis and topic classification and even outperform baseline methods with weaker guarantees like word-level Metric DP .,3,0.97403145,53.57821350048922,33
2759,"As language technologies become more ubiquitous , there are increasing efforts towards expanding the language diversity and coverage of natural language processing ( NLP ) systems .",0,0.9575465,31.868344390893576,27
2759,"Arguably , the most important factor influencing the quality of modern NLP systems is data availability .",0,0.8862278,20.933856347378978,17
2759,"In this work , we study the geographical representativeness of NLP datasets , aiming to quantify if and by how much do NLP datasets match the expected needs of the language speakers .",1,0.905464,40.407768442881704,33
2759,"In doing so , we use entity recognition and linking systems , also making important observations about their cross-lingual consistency and giving suggestions for more robust evaluation .",2,0.42960238,80.34674534491286,28
2759,"Last , we explore some geographical and economic factors that may explain the observed dataset distributions .",2,0.45927534,51.11739992528088,17
2760,"Knowledge of difficulty level of questions helps a teacher in several ways , such as estimating students ’ potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions .",3,0.47301245,95.85536700639119,37
2760,"To this end , we conduct Instance-Level Difficulty Analysis of Evaluation data ( ILDAE ) in a large-scale setup of 23 datasets and demonstrate its five novel applications : 1 ) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time , 2 ) improving quality of existing evaluation datasets by repairing erroneous and trivial instances , 3 ) selecting the best model based on application requirements , 4 ) analyzing dataset characteristics for guiding future data creation , 5 ) estimating Out-of-Domain performance reliably .",2,0.3831391,77.93342901434984,93
2760,"Comprehensive experiments for these applications lead to several interesting results , such as evaluation using just 5 % instances ( selected via ILDAE ) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2 % higher correlation with Out-of-Domain performance .",3,0.9347576,135.48393251270215,56
2760,We release the difficulty scores and hope our work will encourage research in this important yet understudied field of leveraging instance difficulty in evaluations .,3,0.936857,81.99976215741543,25
2761,"The ability to integrate context , including perceptual and temporal cues , plays a pivotal role in grounding the meaning of a linguistic utterance .",0,0.9381952,44.88556492059122,25
2761,"In order to measure to what extent current vision-and-language models master this ability , we devise a new multimodal challenge , Image Retrieval from Contextual Descriptions ( ImageCoDe ) .",1,0.42631257,59.65965215650507,34
2761,"In particular , models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description .",2,0.59828484,73.52258514654785,25
2761,"As such , each description contains only the details that help distinguish between images .",0,0.8008611,151.29525015960695,15
2761,"Because of this , descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences .",0,0.8961778,58.189668891296826,21
2761,Images are sourced from both static pictures and video frames .,0,0.49682808,67.87925887593317,11
2761,"We benchmark several state-of-the-art models , including both cross-encoders such as ViLBERT and bi-encoders such as CLIP , on ImageCoDe .",2,0.64460504,25.006982182278204,26
2761,"Our results reveal that these models dramatically lag behind human performance : the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures , compared with 90.8 in humans .",3,0.98347336,49.89051021599362,35
2761,"Furthermore , we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations , which achieve modest gains .",2,0.5581179,56.36435676884098,27
2761,Our hope is that ImageCoDE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences .,3,0.8349112,44.77919999602199,23
2762,Molecular representation learning plays an essential role in cheminformatics .,0,0.91409653,48.83261397971874,10
2762,"Recently , language model-based approaches have gained popularity as an alternative to traditional expert-designed features to encode molecules .",0,0.942432,47.60779832694063,20
2762,"However , these approaches only utilize a single molecular language for representation learning .",0,0.8876072,84.14218040094423,14
2762,"Motivated by the fact that a given molecule can be described using different languages such as Simplified Molecular Line Entry System ( SMILES ) , The International Union of Pure and Applied Chemistry ( IUPAC ) , and The IUPAC International Chemical Identifier ( InChI ) , we propose a multilingual molecular embedding generation approach called MM-Deacon ( multilingual molecular domain embedding analysis via contrastive learning ) .",2,0.389994,36.23528227932773,70
2762,MM-Deacon is pre-trained using SMILES and IUPAC as two different languages on large-scale molecules .,2,0.4837178,108.84742470644503,17
2762,"We evaluated the robustness of our method on seven molecular property prediction tasks from MoleculeNet benchmark , zero-shot cross-lingual retrieval , and a drug-drug interaction prediction task .",2,0.7805272,47.723840330758215,28
2763,"Transformer-based models are the modern work horses for neural machine translation ( NMT ) , reaching state of the art across several benchmarks .",0,0.90009594,32.82111110073471,26
2763,"Despite their impressive accuracy , we observe a systemic and rudimentary class of errors made by current state-of-the-art NMT models with regards to translating from a language that does n’t mark gender on nouns into others that do .",3,0.86465985,39.56479285147435,45
2763,"We find that even when the surrounding context provides unambiguous evidence of the appropriate grammatical gender marking , no tested model was able to accurately gender occupation nouns systematically .",3,0.98032355,113.5461322208398,30
2763,We release an evaluation scheme and dataset for measuring the ability of NMT models to translate gender morphology correctly in unambiguous contexts across syntactically diverse sentences .,2,0.45773762,54.4731848254911,27
2763,Our dataset translates from an English source into 20 languages from several different language families .,2,0.8241958,59.202914030112616,16
2763,"With the availability of this dataset , our hope is that the NMT community can iterate on solutions for this class of especially egregious errors .",3,0.83982354,33.03110312320166,26
2764,"Humans ( e.g. , crowdworkers ) have a remarkable ability in solving different tasks , by simply reading textual instructions that define them and looking at a few examples .",0,0.9354754,66.78168659779634,30
2764,"Despite the success of the conventional supervised learning on individual datasets , such models often struggle with generalization across tasks ( e.g. , a question-answering system cannot solve classification tasks ) .",0,0.9116049,38.37562729060575,34
2764,A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it .,0,0.9527348,20.795029328059524,24
2764,"To study this , we introduce NATURAL INSTRUCTIONS , a dataset of 61 distinct tasks , their human-authored instructions , and 193 k task instances ( input-output pairs ) .",2,0.82887,128.88260838622892,30
2764,The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema .,2,0.6371615,62.939038466432194,20
2764,"Using this meta-dataset , we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones .",2,0.8249744,35.99043271494044,23
2764,We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output .,2,0.7467205,30.46665755437135,19
2764,Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks ( 19 % better for models utilizing instructions ) .,3,0.9896236,94.25665759883464,27
2764,"These models , however , are far behind an estimated performance upperbound indicating significant room for more progress in this direction .",3,0.9224739,204.33792215054987,22
2765,"State-of-the-art NLP systems represent inputs with word embeddings , but these are brittle when faced with Out-of-Vocabulary ( OOV ) words .",0,0.92623985,21.271923703955782,26
2765,"To address this issue , we follow the principle of mimick-like models to generate vectors for unseen words , by learning the behavior of pre-trained embeddings using only the surface form of words .",2,0.6584931,35.72250450708258,36
2765,"We present a simple contrastive learning framework , LOVE , which extends the word representation of an existing pre-trained language model ( such as BERT ) and makes it robust to OOV with few additional parameters .",2,0.37417212,33.36472546161419,37
2765,"Extensive evaluations demonstrate that our lightweight model achieves similar or even better performances than prior competitors , both on original datasets and on corrupted variants .",3,0.91753966,52.31826188199609,26
2765,"Moreover , it can be used in a plug-and-play fashion with FastText and BERT , where it significantly improves their robustness .",3,0.81690556,25.21902492810372,24
2766,"Given the ubiquitous nature of numbers in text , reasoning with numbers to perform simple calculations is an important skill of AI systems .",0,0.9257742,46.508856793683385,24
2766,"While many datasets and models have been developed to this end , state-of-the-art AI systems are brittle ;",0,0.9400709,31.91016083153055,24
2766,failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario .,3,0.51798445,70.9744562837198,16
2766,"Drawing inspiration from GLUE that was proposed in the context of natural language understanding , we propose NumGLUE , a multi-task benchmark that evaluates the performance of AI systems on eight different tasks , that at their core require simple arithmetic understanding .",2,0.45522818,40.65563672596786,43
2766,We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans ( lower by 46.4 % ) .,3,0.95473105,31.74743175396242,36
2766,"Further , NumGLUE promotes sharing knowledge across tasks , especially those with limited training data as evidenced by the superior performance ( average gain of 3.4 % on each task ) when a model is jointly trained on all the tasks as opposed to task-specific modeling .",3,0.9286748,47.81083798351697,48
2766,"Finally , we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language , a first step towards being able to perform more complex mathematical reasoning .",3,0.92248094,50.81025487102097,32
2767,"A few large , homogenous , pre-trained models undergird many machine learning systems — and often , these models contain harmful stereotypes learned from the internet .",0,0.8823798,141.5078772049376,27
2767,We investigate the bias transfer hypothesis : the theory that social biases ( such as stereotypes ) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning .,1,0.6344337,77.73349661382329,33
2767,"For two classification tasks , we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier ’s discriminatory behavior after fine-tuning .",3,0.96245205,36.9340965783163,28
2767,Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset .,3,0.9786773,39.233870036265856,16
2767,"Still , pre-training plays a role : simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained .",3,0.58317417,33.66565910309961,25
2767,Our results encourage practitioners to focus more on dataset quality and context-specific harms .,3,0.9899446,80.16206269773791,14
2768,"A dialogue response is malevolent if it is grounded in negative emotions , inappropriate behavior , or an unethical value basis in terms of content and dialogue acts .",0,0.77205306,130.26088678001275,29
2768,The detection of malevolent dialogue responses is attracting growing interest .,0,0.94312763,122.35028306885026,11
2768,Current research on detecting dialogue malevolence has limitations in terms of datasets and methods .,0,0.8658289,62.81703936475356,15
2768,"First , available dialogue datasets related to malevolence are labeled with a single category , but in practice assigning a single category to each utterance may not be appropriate as some malevolent utterances belong to multiple labels .",0,0.72006935,45.109032701612854,38
2768,"Second , current methods for detecting dialogue malevolence neglect label correlation .",0,0.34772477,273.9202040198494,12
2768,"Therefore , we propose the task of multi-label dialogue malevolence detection and crowdsource a multi-label dataset , multi-label dialogue malevolence detection ( MDMD ) for evaluation .",1,0.66381234,35.2700295044984,27
2768,"We also propose a multi-label malevolence detection model , multi-faceted label correlation enhanced CRF ( MCRF ) , with two label correlation mechanisms , label correlation in taxonomy ( LCT ) and label correlation in context ( LCC ) .",2,0.6794992,56.27431146594034,40
2768,"Experiments on MDMD show that our method outperforms the best performing baseline by a large margin , i.e. , 16.1 % , 11.9 % , 12.0 % , and 6.1 % on precision , recall , F1 , and Jaccard score , respectively .",3,0.915794,32.45336959699651,44
2769,"Long-form answers , consisting of multiple sentences , can provide nuanced and comprehensive answers to a broader set of questions .",0,0.8738633,39.68456511826361,22
2769,"To better understand this complex and understudied task , we study the functional structure of long-form answers collected from three datasets , ELI5 , WebGPT and Natural Questions .",1,0.4219476,52.04607396206216,29
2769,Our main goal is to understand how humans organize information to craft complex answers .,1,0.638289,37.58274187696506,15
2769,"We develop an ontology of six sentence-level functional roles for long-form answers , and annotate 3.9 k sentences in 640 answer paragraphs .",2,0.8422913,100.58694444765248,26
2769,Different answer collection methods manifest in different discourse structures .,0,0.6741401,154.11987270605368,10
2769,We further analyze model-generated answers – finding that annotators agree less with each other when annotating model-generated answers compared to annotating human-written answers .,3,0.63400733,24.775507304832278,27
2769,Our annotated data enables training a strong classifier that can be used for automatic analysis .,3,0.7809841,29.05392941546019,16
2769,We hope our work can inspire future research on discourse-level modeling and evaluation of long-form QA systems .,3,0.9333007,19.779297896259713,20
2770,"Writing is , by nature , a strategic , adaptive , and , more importantly , an iterative process .",0,0.8296127,134.73165055804145,20
2770,A crucial part of writing is editing and revising the text .,0,0.6355227,29.85211619356875,12
2770,"Previous works on text revision have focused on defining edit intention taxonomies within a single domain or developing computational models with a single level of edit granularity , such as sentence-level edits , which differ from human ’s revision cycles .",0,0.9106402,80.36655525282326,43
2770,"This work describes IteraTeR : the first large-scale , multi-domain , edit-intention annotated corpus of iteratively revised text .",1,0.65191835,124.07901247103689,19
2770,"In particular , IteraTeR is collected based on a new framework to comprehensively model the iterative text revisions that generalizes to a variety of domains , edit intentions , revision depths , and granularities .",2,0.6683366,118.63940879105789,35
2770,"When we incorporate our annotated edit intentions , both generative and action-based text revision models significantly improve automatic evaluations .",3,0.91181135,108.1910287018749,22
2770,"Through our work , we better understand the text revision process , making vital connections between edit intentions and writing quality , enabling the creation of diverse corpora to support computational modeling of iterative text revisions .",3,0.9290519,75.01098597827445,37
2771,"Several studies have reported the inability of Transformer models to generalize compositionally , a key type of generalization in many NLP tasks such as semantic parsing .",0,0.9191196,23.13021440381011,27
2771,In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization .,1,0.8256709,30.61511277858438,29
2771,We identified Transformer configurations that generalize compositionally significantly better than previously reported in the literature in many compositional tasks .,3,0.93679893,43.38085005416892,20
2771,"We achieve state-of-the-art results in a semantic parsing compositional generalization benchmark ( COGS ) , and a string edit operation composition benchmark ( PCFG ) .",3,0.63873976,40.54500555016991,30
2772,"Unlike literal expressions , idioms ’ meanings do not directly follow from their parts , posing a challenge for neural machine translation ( NMT ) .",0,0.9489585,80.34873760992589,26
2772,"NMT models are often unable to translate idioms accurately and over-generate compositional , literal translations .",0,0.8763751,73.88940064685133,16
2772,"In this work , we investigate whether the non-compositionality of idioms is reflected in the mechanics of the dominant NMT model , Transformer , by analysing the hidden states and attention patterns for models with English as source language and one of seven European languages as target language .",1,0.87692386,42.27284494940276,49
2772,When Transformer emits a non-literal translation-i.e .,3,0.42114687,49.01283557684693,9
2772,identifies the expression as idiomatic-the encoder processes idioms more strongly as single lexical units compared to literal expressions .,3,0.607908,122.05595644897724,21
2772,This manifests in idioms ’ parts being grouped through attention and in reduced interaction between idioms and their context .,3,0.6367012,278.81912082611024,20
2772,"In the decoder ’s cross-attention , figurative inputs result in reduced attention on source-side tokens .",3,0.6292218,72.93341973553662,16
2772,These results suggest that Transformer ’s tendency to process idioms as compositional expressions contributes to literal translations of idioms .,3,0.99027807,36.97316202924054,20
2773,"We describe a Question Answering ( QA ) dataset that contains complex questions with conditional answers , i.e .",1,0.42999023,27.88452380677188,19
2773,the answers are only applicable when certain conditions apply .,0,0.6827412,68.98766543084263,10
2773,We call this dataset ConditionalQA .,2,0.5616315,60.322934526367774,6
2773,"In addition to conditional answers , the dataset also features : ( 1 ) long context documents with information that is related in logically complex ways ;",3,0.64019096,200.43889812988917,27
2773,( 2 ) multi-hop questions that require compositional logical reasoning ;,0,0.37340227,194.68303900572164,11
2773,"( 3 ) a combination of extractive questions , yes / no questions , questions with multiple answers , and not-answerable questions ;",2,0.46925217,76.76595017092582,23
2773,( 4 ) questions asked without knowing the answers .,2,0.4872954,101.30607902019646,10
2773,"We show that ConditionalQA is challenging for many of the existing QA models , especially in selecting answer conditions .",3,0.93559724,53.427054740274876,20
2773,We believe that this dataset will motivate further research in answering complex questions over long documents .,3,0.9751247,32.968326403943266,17
2774,Current methods for few-shot fine-tuning of pretrained masked language models ( PLMs ) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score .,0,0.8364891,52.86995335094949,35
2774,"In this work , we propose Perfect , a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting , which is highly effective given as few as 32 data points .",1,0.68925226,32.783853189019744,37
2774,"First , we show that manually engineered task prompts can be replaced with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factors of 5 and 100 , respectively .",3,0.8640239,45.812908090602164,38
2774,"Second , instead of using handcrafted verbalizers , we learn new multi-token label embeddings during fine-tuning , which are not tied to the model vocabulary and which allow us to avoid complex auto-regressive decoding .",2,0.6971165,45.6955637002692,35
2774,These embeddings are not only learnable from limited data but also enable nearly 100x faster training and inference .,0,0.5626502,28.4084693396534,19
2774,"Experiments on a wide range of few shot NLP tasks demonstrate that Perfect , while being simple and efficient , also outperforms existing state-of-the-art few-shot learning methods .",3,0.8774693,23.116156261915155,35
2774,Our code is publicly available at https://github.com/rabeehk/perfect .,3,0.6020094,11.972628669701251,8
2775,Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks .,0,0.82101744,16.658864961947533,27
2775,"Existing work on continual sequence generation either always reuses existing parameters to learn new tasks , which is vulnerable to catastrophic forgetting on dissimilar tasks , or blindly adds new parameters for every new task , which could prevent knowledge sharing between similar tasks .",0,0.7937788,62.984282310522886,45
2775,"To get the best of both worlds , in this work , we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks .",2,0.39245707,54.47319781291198,39
2775,We also incorporate pseudo experience replay to facilitate knowledge transfer in those shared modules .,2,0.6675284,273.2452332651055,15
2775,"Experiment results on various sequences of generation tasks show that our framework can adaptively add modules or reuse modules based on task similarity , outperforming state-of-the-art baselines in terms of both performance and parameter efficiency .",3,0.9419378,16.775121678891523,42
2775,We make our code public at https://github.com/GT-SALT/Adaptive-Compositional-Modules .,3,0.4872238,12.197711591530112,8
2776,"While pretrained language models achieve excellent performance on natural language understanding benchmarks , they tend to rely on spurious correlations and generalize poorly to out-of-distribution ( OOD ) data .",0,0.871094,14.906093174052145,31
2776,Recent work has explored using counterfactually-augmented data ( CAD ) — data generated by minimally perturbing examples to flip the ground-truth label — to identify robust features that are invariant under distribution shift .,0,0.9321861,72.38748631145407,38
2776,"However , empirical results using CAD during training for OOD generalization have been mixed .",0,0.86086845,120.39846339878386,15
2776,"To explain this discrepancy , through a toy theoretical example and empirical analysis on two crowdsourced CAD datasets , we show that : ( a ) while features perturbed in CAD are indeed robust features , it may prevent the model from learning unperturbed robust features ;",3,0.53392655,119.06491016638343,47
2776,and ( b) CAD may exacerbate existing spurious correlations in the data .,3,0.73560953,208.617365152401,13
2776,"Our results thus show that the lack of perturbation diversity limits CAD ’s effectiveness on OOD generalization , calling for innovative crowdsourcing procedures to elicit diverse perturbation of examples .",3,0.98942065,108.11650754711033,30
2777,"Sentiment transfer is one popular example of a text style transfer task , where the goal is to reverse the sentiment polarity of a text .",0,0.92241925,35.70520233996804,26
2777,With a sentiment reversal comes also a reversal in meaning .,0,0.54619884,235.59940604800875,11
2777,We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning .,2,0.62276787,37.00834214469066,34
2777,Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task .,0,0.5917529,61.12009214504324,15
2777,"To facilitate rapid progress , we introduce a large-scale benchmark , Positive Psychology Frames , with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically-motivated reframing strategies .",2,0.73166937,67.34938042355084,37
2777,"Then we evaluate a set of state-of-the-art text style transfer models , and conclude by discussing key challenges and directions for future work .",3,0.44818118,12.878127242494873,30
2778,English Natural Language Understanding ( NLU ) systems have achieved great performances and even outperformed humans on benchmarks like GLUE and SuperGLUE .,0,0.93053097,23.532196355733603,23
2778,"However , these benchmarks contain only textbook Standard American English ( SAE ) .",0,0.806687,293.4368808788365,14
2778,Other dialects have been largely overlooked in the NLP community .,0,0.90938455,13.061805662335798,11
2778,This leads to biased and inequitable NLU systems that serve only a sub-population of speakers .,0,0.73915046,43.46668702443314,16
2778,"To understand disparities in current models and to facilitate more dialect-competent NLU systems , we introduce the VernAcular Language Understanding Evaluation ( VALUE ) benchmark , a challenging variant of GLUE that we created with a set of lexical and morphosyntactic transformation rules .",1,0.49677962,59.48180522156914,45
2778,"In this initial release ( V.1 ) , we construct rules for 11 features of African American Vernacular English ( AAVE ) , and we recruit fluent AAVE speakers to validate each feature transformation via linguistic acceptability judgments in a participatory design manner .",2,0.79129094,100.38210893603747,44
2778,Experiments show that these new dialectal features can lead to a drop in model performance .,3,0.8817489,24.997253880249495,16
2779,"We study the task of toxic spans detection , which concerns the detection of the spans that make a text toxic , when detecting such spans is possible .",1,0.67146534,74.98234126066271,29
2779,"We introduce a dataset for this task , ToxicSpans , which we release publicly .",2,0.6056834,80.30461285047198,15
2779,"By experimenting with several methods , we show that sequence labeling models perform best , but methods that add generic rationale extraction mechanisms on top of classifiers trained to predict if a post is toxic or not are also surprisingly promising .",3,0.8469061,83.81106382541721,42
2779,"Finally , we use ToxicSpans and systems trained on it , to provide further analysis of state-of-the-art toxic to non-toxic transfer systems , as well as of human performance on that latter task .",2,0.49564153,42.95398608761427,40
2779,Our work highlights challenges in finer toxicity detection and mitigation .,3,0.9789819,119.2213700513635,11
2780,Sequence modeling has demonstrated state-of-the-art performance on natural language and document understanding tasks .,0,0.90587795,10.451086178583688,19
2780,"However , it is challenging to correctly serialize tokens in form-like documents in practice due to their variety of layout patterns .",0,0.9338471,42.42373876926598,22
2780,"We propose FormNet , a structure-aware sequence model to mitigate the suboptimal serialization of forms .",1,0.36635864,67.74077096806425,18
2780,"First , we design Rich Attention that leverages the spatial relationship between tokens in a form for more precise attention score calculation .",2,0.8638103,69.36962032537905,23
2780,"Second , we construct Super-Tokens for each word by embedding representations from their neighboring tokens through graph convolutions .",2,0.87486017,50.78650465622923,19
2780,FormNet therefore explicitly recovers local syntactic information that may have been lost during serialization .,3,0.57532007,80.5122331701674,15
2780,"In experiments , FormNet outperforms existing methods with a more compact model size and less pre-training data , establishing new state-of-the-art performance on CORD , FUNSD and Payment benchmarks .",3,0.9086643,58.80875582521497,35
2781,Conversational agents have come increasingly closer to human competence in open-domain dialogue settings ;,0,0.9198627,66.38307966633872,14
2781,"however , such models can reflect insensitive , hurtful , or entirely incoherent viewpoints that erode a user ’s trust in the moral integrity of the system .",0,0.830969,69.664806298735,28
2781,"Moral deviations are difficult to mitigate because moral judgments are not universal , and there may be multiple competing judgments that apply to a situation simultaneously .",0,0.90603876,65.76539625502967,27
2781,"In this work , we introduce a new resource , not to authoritatively resolve moral ambiguities , but instead to facilitate systematic understanding of the intuitions , values and moral judgments reflected in the utterances of dialogue systems .",1,0.76043934,43.463194744171595,39
2781,"The Moral Integrity Corpus , MIC , is such a resource , which captures the moral assumptions of 38 k prompt-reply pairs , using 99 k distinct Rules of Thumb ( RoTs ) .",0,0.454132,273.8171679067416,34
2781,Each RoT reflects a particular moral conviction that can explain why a chatbot ’s reply may appear acceptable or problematic .,0,0.475819,134.5510503621293,21
2781,We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification .,2,0.7753331,182.65300325818137,20
2781,"Most importantly , we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions , but they still struggle with certain scenarios .",3,0.9634805,83.77386538521527,30
2781,Our findings suggest that MIC will be a useful resource for understanding and language models ’ implicit moral assumptions and flexibly benchmarking the integrity of conversational agents .,3,0.9901613,67.52167091560652,28
2781,"To download the data , see https://github.com/GT-SALT/mic .",3,0.4931295,25.192913342939754,8
2782,Transformer-based models generally allocate the same amount of computation for each token in a given sequence .,0,0.80628884,22.583336143615387,19
2782,"We develop a simple but effective “ token dropping ” method to accelerate the pretraining of transformer models , such as BERT , without degrading its performance on downstream tasks .",2,0.40552872,41.70346190203425,31
2782,"In particular , we drop unimportant tokens starting from an intermediate layer in the model to make the model focus on important tokens more efficiently if with limited computational resource .",2,0.6573177,69.73509943333711,31
2782,The dropped tokens are later picked up by the last layer of the model so that the model still produces full-length sequences .,3,0.41243014,39.2142594257742,23
2782,We leverage the already built-in masked language modeling ( MLM ) loss to identify unimportant tokens with practically no computational overhead .,2,0.7589279,69.18380581246305,24
2782,"In our experiments , this simple approach reduces the pretraining cost of BERT by 25 % while achieving similar overall fine-tuning performance on standard downstream tasks .",3,0.8886547,28.787211130351192,27
2783,Fact-checking is an essential tool to mitigate the spread of misinformation and disinformation .,0,0.90860474,11.947806040082007,16
2783,"We introduce the task of fact-checking in dialogue , which is a relatively unexplored area .",1,0.6376792,25.454394720555637,18
2783,"We construct DialFact , a testing benchmark dataset of 22,245 annotated conversational claims , paired with pieces of evidence from Wikipedia .",2,0.9164982,124.80180431114218,22
2783,There are three sub-tasks in DialFact : 1 ) Verifiable claim detection task distinguishes whether a response carries verifiable factual information ;,2,0.36981124,111.70599558316817,22
2783,2 ) Evidence retrieval task retrieves the most relevant Wikipedia snippets as evidence ;,2,0.6830195,318.0508649889154,14
2783,"3 ) Claim verification task predicts a dialogue response to be supported , refuted , or not enough information .",2,0.44118482,364.88522893226633,20
2783,"We found that existing fact-checking models trained on non-dialogue data like FEVER fail to perform well on our task , and thus , we propose a simple yet data-efficient solution to effectively improve fact-checking performance in dialogue .",3,0.9788488,30.13332017940666,42
2783,"We point out unique challenges in DialFact such as handling the colloquialisms , coreferences , and retrieval ambiguities in the error analysis to shed light on future research in this direction .",3,0.761675,64.8338776482837,32
2784,This work connects language model adaptation with concepts of machine learning theory .,1,0.42991918,88.10568701475736,13
2784,We consider a training setup with a large out-of-domain set and a small in-domain set .,2,0.8140083,14.407390421968987,21
2784,We derive how the benefit of training a model on either set depends on the size of the sets and the distance between their underlying distributions .,3,0.53709984,46.07945035782333,27
2784,We analyze how out-of-domain pre-training before in-domain fine-tuning achieves better generalization than either solution independently .,3,0.6405715,24.241377785378734,20
2784,"Finally , we present how adaptation techniques based on data selection , such as importance sampling , intelligent data selection and influence functions , can be presented in a common framework which highlights their similarity and also their subtle differences .",3,0.71356314,93.76109708038135,41
2785,"Aligning with ACL 2022 special Theme on “ Language Diversity : from Low Resource to Endangered Languages ” , we discuss the major linguistic and sociopolitical challenges facing development of NLP technologies for African languages .",1,0.6270045,139.11856793208983,36
2785,"Situating African languages in a typological framework , we discuss how the particulars of these languages can be harnessed .",1,0.5841758,41.19334602729821,20
2785,"To facilitate future research , we also highlight current efforts , communities , venues , datasets , and tools .",1,0.66032296,159.32873468608202,20
2785,Our main objective is to motivate and advocate for an Afrocentric approach to technology development .,1,0.939442,21.664118523557693,16
2785,"With this in mind , we recommend what technologies to build and how to build , evaluate , and deploy them based on the needs of local African communities .",3,0.6153338,42.064189681987905,30
2786,"In this paper , we investigate improvements to the GEC sequence tagging architecture with a focus on ensembling of recent cutting-edge Transformer-based encoders in Large configurations .",1,0.91317886,47.44800118039414,31
2786,We encourage ensembling models by majority votes on span-level edits because this approach is tolerant to the model architecture and vocabulary size .,2,0.5865803,108.32013101450497,23
2786,"Our best ensemble achieves a new SOTA result with an F0.5 score of 76.05 on BEA-2019 ( test ) , even without pre-training on synthetic datasets .",3,0.9371093,57.36931194766126,29
2786,"In addition , we perform knowledge distillation with a trained ensemble to generate new synthetic training datasets , “ Troy-Blogs ” and “ Troy-1BW ” .",2,0.8157597,91.42497268937045,29
2786,Our best single sequence tagging model that is pretrained on the generated Troy-datasets in combination with the publicly available synthetic PIE dataset achieves a near-SOTA result with an F0.5 score of 73.21 on BEA-2019 ( test ) .,3,0.9144097,75.73572697131142,41
2786,"The code , datasets , and trained models are publicly available .",3,0.4349337,51.41511050351244,12
2787,"Natural language processing ( NLP ) models trained on people-generated data can be unreliable because , without any constraints , they can learn from spurious correlations that are not relevant to the task .",0,0.9311542,40.73017674858965,36
2787,"We hypothesize that enriching models with speaker information in a controlled , educated way can guide them to pick up on relevant inductive biases .",1,0.54178005,66.19528985268764,25
2787,"For the speaker-driven task of predicting code-switching points in English –Spanish bilingual dialogues , we show that adding sociolinguistically-grounded speaker features as prepended prompts significantly improves accuracy .",3,0.9035233,68.90987766144163,33
2787,"We find that by adding influential phrases to the input , speaker-informed models learn useful and explainable linguistic information .",3,0.9731237,76.59183559505524,22
2787,"To our knowledge , we are the first to incorporate speaker characteristics in a neural model for code-switching , and more generally , take a step towards developing transparent , personalized models that use speaker information in a controlled way .",3,0.942588,48.81822584319505,41
2788,This work presents a new resource for borrowing identification and analyzes the performance and errors of several models on this task .,1,0.78568697,51.342408837690094,22
2788,"We introduce a new annotated corpus of Spanish newswire rich in unassimilated lexical borrowings — words from one language that are introduced into another without orthographic adaptation — and use it to evaluate how several sequence labeling models ( CRF , BiLSTM-CRF , and Transformer-based models ) perform .",2,0.68845636,32.965072416262664,53
2788,"The corpus contains 370,000 tokens and is larger , more borrowing-dense , OOV-rich , and topic-varied than previous corpora available for this task .",3,0.6664217,78.268527053149,28
2788,Our results show that a BiLSTM-CRF model fed with subword embeddings along with either Transformer-based embeddings pretrained on codeswitched data or a combination of contextualized word embeddings outperforms results obtained by a multilingual BERT-based model .,3,0.9856734,17.027985204828493,42
2789,"The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity , and so the need for explanations of these models becomes paramount .",0,0.94872934,21.581916899121833,35
2789,"Attention has been seen as a solution to increase performance , while providing some explanations .",0,0.91886467,87.09442327123895,16
2789,"However , a debate has started to cast doubt on the explanatory power of attention in neural networks .",0,0.9580066,33.23655840478383,19
2789,"Although the debate has created a vast literature thanks to contributions from various areas , the lack of communication is becoming more and more tangible .",0,0.8873668,45.31677671983133,26
2789,"In this paper , we provide a clear overview of the insights on the debate by critically confronting works from these different areas .",1,0.8595492,97.28541848135308,24
2789,This holistic vision can be of great interest for future works in all the communities concerned by this debate .,3,0.8947587,83.6093676605651,20
2789,"We sum up the main challenges spotted in these areas , and we conclude by discussing the most promising future avenues on attention as an explanation .",3,0.5788148,82.89248154207779,27
2790,"Knowledge-grounded conversation ( KGC ) shows great potential in building an engaging and knowledgeable chatbot , and knowledge selection is a key ingredient in it .",0,0.9211118,61.09037215547425,26
2790,"However , previous methods for knowledge selection only concentrate on the relevance between knowledge and dialogue context , ignoring the fact that age , hobby , education and life experience of an interlocutor have a major effect on his or her personal preference over external knowledge .",0,0.87476236,63.48790036733024,47
2790,"Without taking the personalization issue into account , it is difficult for existing dialogue systems to select the proper knowledge and generate persona-consistent responses .",0,0.75158536,49.01524286350935,25
2790,"In this work , we introduce personal memory into knowledge selection in KGC to address the personalization issue .",1,0.81788516,137.97934448050117,19
2790,"We propose a variational method to model the underlying relationship between one ’s personal memory and his or her selection of knowledge , and devise a learning scheme in which the forward mapping from personal memory to knowledge and its inverse mapping is included in a closed loop so that they could teach each other .",2,0.6306322,48.91011035615412,56
2790,Experiment results show that our methods outperform existing KGC methods significantly on both automatic evaluation and human evaluation .,3,0.9793108,12.58299436302964,19
2791,"In data-to-text ( D2T ) generation , training on in-domain data leads to overfitting to the data representation and repeating training data noise .",0,0.84086215,61.96317047965906,26
2791,We examine how to avoid finetuning pretrained language models ( PLMs ) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs .,1,0.6111026,48.6299421682769,27
2791,"Inspired by pipeline approaches , we propose to generate text by transforming single-item descriptions with a sequence of modules trained on general-domain text-based operations : ordering , aggregation , and paragraph compression .",2,0.6454775,107.49142526469471,39
2791,We train PLMs for performing these operations on a synthetic corpus WikiFluent which we build from English Wikipedia .,2,0.8625454,160.75434246891808,19
2791,Our experiments on two major triple-to-text datasets — WebNLG and E2E — show that our approach enables D2T generation from RDF triples in zero-shot settings .,3,0.9128592,56.59092391847728,30
2792,Languages are classified as low-resource when they lack the quantity of data necessary for training statistical and machine learning tools and models .,0,0.9224872,48.7547870536659,23
2792,"Causes of resource scarcity vary but can include poor access to technology for developing these resources , a relatively small population of speakers , or a lack of urgency for collecting such resources in bilingual populations where the second language is high-resource .",0,0.7179043,94.68618461858605,44
2792,"As a result , the languages described as low-resource in the literature are as different as Finnish on the one hand , with millions of speakers using it in every imaginable domain , and Seneca , with only a small-handful of fluent speakers using the language primarily in a restricted domain .",0,0.6573257,65.98260430634264,54
2792,"While issues stemming from the lack of resources necessary to train models unite this disparate group of languages , many other issues cut across the divide between widely-spoken low-resource languages and endangered languages .",0,0.73971385,81.91749682776971,36
2792,"In this position paper , we discuss the unique technological , cultural , practical , and ethical challenges that researchers and indigenous speech community members face when working together to develop language technology to support endangered language documentation and revitalization .",1,0.8721536,92.62872161956199,41
2792,"We report the perspectives of language teachers , Master Speakers and elders from indigenous communities , as well as the point of view of academics .",1,0.6067981,146.59053763159446,26
2792,We describe an ongoing fruitful collaboration and make recommendations for future partnerships between academic researchers and language community stakeholders .,3,0.677105,86.08310834513891,20
2793,Bragging is a speech act employed with the goal of constructing a favorable self-image through positive statements about oneself .,0,0.9445805,108.30272600460145,20
2793,"It is widespread in daily communication and especially popular in social media , where users aim to build a positive image of their persona directly or indirectly .",0,0.910391,40.922847207924825,28
2793,"In this paper , we present the first large scale study of bragging in computational linguistics , building on previous research in linguistics and pragmatics .",1,0.8856471,33.13142018284535,26
2793,"To facilitate this , we introduce a new publicly available data set of tweets annotated for bragging and their types .",2,0.5687904,58.54493943789282,21
2793,"We empirically evaluate different transformer-based models injected with linguistic information in ( a ) binary bragging classification , i.e. , if tweets contain bragging statements or not ;",2,0.7538691,323.1594291346769,30
2793,and ( b ) multi-class bragging type prediction including not bragging .,2,0.47294748,1999.6712050522608,12
2793,Our results show that our models can predict bragging with macro F1 up to 72.42 and 35.95 in the binary and multi-class classification tasks respectively .,3,0.9883624,61.24898752173128,26
2793,"Finally , we present an extensive linguistic and error analysis of bragging prediction to guide future research on this topic .",3,0.574912,86.92688705212237,21
2794,Document-level information extraction ( IE ) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts .,0,0.9578907,52.04879154947855,37
2794,"Evaluation of the approaches , however , has been limited in a number of dimensions .",0,0.87180394,44.838043096571816,16
2794,"In particular , the precision / recall / F1 scores typically reported provide few insights on the range of errors the models make .",3,0.615663,134.52891734824888,24
2794,We build on the work of Kummerfeld and Klein ( 2013 ) to propose a transformation-based framework for automating error analysis in document-level event and ( N-ary ) relation extraction .,2,0.45842028,62.02554463442262,35
2794,We employ our framework to compare two state-of-the-art document-level template-filling approaches on datasets from three domains ;,2,0.69454163,35.54046957502444,27
2794,"and then , to gauge progress in IE since its inception 30 years ago , vs .",1,0.42419538,344.2295026368725,17
2794,four systems from the MUC-4 ( 1992 ) evaluation .,2,0.63781565,285.54779083911006,12
2795,Functional Distributional Semantics is a recently proposed framework for learning distributional semantics that provides linguistic interpretability .,0,0.88141286,26.10032148892023,17
2795,It models the meaning of a word as a binary classifier rather than a numerical vector .,0,0.6010054,27.90846755960767,17
2795,"In this work , we propose a method to train a Functional Distributional Semantics model with grounded visual data .",1,0.8483375,42.2554628844692,20
2795,"We train it on the Visual Genome dataset , which is closer to the kind of data encountered in human language acquisition than a large text corpus .",2,0.75902504,35.39567345507913,28
2795,"On four external evaluation datasets , our model outperforms previous work on learning semantics from Visual Genome .",3,0.8671128,62.55462723302027,18
2796,"While large language models have shown exciting progress on several NLP benchmarks , evaluating their ability for complex analogical reasoning remains under-explored .",0,0.90064377,29.354099163747296,23
2796,"Here , we introduce a high-quality crowdsourced dataset of narratives for employing proverbs in context as a benchmark for abstract language understanding .",1,0.6647641,38.12853069856354,23
2796,"The dataset provides fine-grained annotation of aligned spans between proverbs and narratives , and contains minimal lexical overlaps between narratives and proverbs , ensuring that models need to go beyond surface-level reasoning to succeed .",3,0.8512265,56.18473150628459,38
2796,"We explore three tasks : ( 1 ) proverb recommendation and alignment prediction , ( 2 ) narrative generation for a given proverb and topic , and ( 3 ) identifying narratives with similar motifs .",2,0.6444218,101.62971707534953,36
2796,"Our experiments show that neural language models struggle on these tasks compared to humans , and these tasks pose multiple learning challenges .",3,0.95905614,48.7148515686368,23
2797,Charts are commonly used for exploring data and communicating insights .,0,0.89300704,40.52403427430446,11
2797,Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts .,0,0.7759477,43.3888250864204,28
2797,"We present Chart-to-text , a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types .",2,0.7139618,23.999977724979956,28
2797,We explain the dataset construction process and analyze the datasets .,2,0.68744564,40.16214865624767,11
2797,We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations : one assumes the underlying data table of the chart is available while the other needs to extract data from chart images .,2,0.64916015,25.110591472894992,56
2797,"Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores , they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts .",3,0.9608004,41.43708150390351,45
2798,Idioms are unlike most phrases in two important ways .,0,0.83004767,55.174474676800976,10
2798,"First , words in an idiom have non-canonical meanings .",0,0.8358339,49.631650922892824,10
2798,"Second , the non-canonical meanings of words in an idiom are contingent on the presence of other words in the idiom .",0,0.76473725,18.84760088071383,22
2798,"Linguistic theories differ on whether these properties depend on one another , as well as whether special theoretical machinery is needed to accommodate idioms .",0,0.8935441,75.43423831358929,25
2798,"We define two measures that correspond to the properties above , and we show that idioms fall at the expected intersection of the two dimensions , but that the dimensions themselves are not correlated .",2,0.52669525,64.0783567647919,35
2798,Our results suggest that introducing special machinery to handle idioms may not be warranted .,3,0.9905984,81.99565670416816,15
2799,"Graph neural networks have triggered a resurgence of graph-based text classification methods , defining today ’s state of the art .",0,0.94883645,79.62673103632105,23
2799,We show that a wide multi-layer perceptron ( MLP ) using a Bag-of-Words ( BoW ) outperforms the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting and is comparable with HyperGAT .,3,0.88878936,60.98373243516675,37
2799,"Moreover , we fine-tune a sequence-based BERT and a lightweight DistilBERT model , which both outperform all state-of-the-art models .",3,0.49961942,17.40058738606379,29
2799,These results question the importance of synthetic graphs used in modern text classifiers .,3,0.94506,78.98153238986937,14
2799,"In terms of efficiency , DistilBERT is still twice as large as our BoW-based wide MLP , while graph-based models like TextGCN require setting up an 𝒪 ( N2 ) graph , where N is the vocabulary plus corpus size .",3,0.8804591,111.59563061357754,45
2799,"Finally , since Transformers need to compute 𝒪 ( L2 ) attention weights with sequence length L , the MLP models show higher training and inference speeds on datasets with long sequences .",3,0.89219403,84.93315119222314,33
2800,"We introduce ParaBLEU , a paraphrase representation learning model and evaluation metric for text generation .",2,0.5009922,73.3064534999478,16
2800,"Unlike previous approaches , ParaBLEU learns to understand paraphrasis using generative conditioning as a pretraining objective .",0,0.39325938,89.58148506715675,17
2800,"ParaBLEU correlates more strongly with human judgements than existing metrics , obtaining new state-of-the-art results on the 2017 WMT Metrics Shared Task .",3,0.90935105,33.65862860700065,28
2800,"We show that our model is robust to data scarcity , exceeding previous state-of-the-art performance using only 50 % of the available training data and surpassing BLEU , ROUGE and METEOR with only 40 labelled examples .",3,0.9017449,19.17807190496177,43
2800,"Finally , we demonstrate that ParaBLEU can be used to conditionally generate novel paraphrases from a single demonstration , which we use to confirm our hypothesis that it learns abstract , generalized paraphrase representations .",3,0.9360383,46.43162253357109,35
2801,Research in stance detection has so far focused on models which leverage purely textual input .,0,0.924215,55.17210689603448,16
2801,"In this paper , we investigate the integration of textual and financial signals for stance detection in the financial domain .",1,0.93020105,38.30407209187283,21
2801,"Specifically , we propose a robust multi-task neural architecture that combines textual input with high-frequency intra-day time series from stock market prices .",2,0.64628,33.1576320137804,25
2801,"Moreover , we extend wt–wt , an existing stance detection dataset which collects tweets discussing Mergers and Acquisitions operations , with the relevant financial signal .",2,0.77866274,204.65561506843645,26
2801,"Importantly , the obtained dataset aligns with Stander , an existing news stance detection dataset , thus resulting in a unique multimodal , multi-genre stance detection resource .",3,0.95009434,130.41029260434482,29
2801,"We show experimentally and through detailed result analysis that our stance detection system benefits from financial information , and achieves state-of-the-art results on the wt–wt dataset : this demonstrates that the combination of multiple input signals is effective for cross-target stance detection , and opens interesting research directions for future work .",3,0.9191882,40.97941698007808,58
2802,Multilingual neural machine translation models are trained to maximize the likelihood of a mix of examples drawn from multiple language pairs .,0,0.686121,26.832081072108604,22
2802,The dominant inductive bias applied to these models is a shared vocabulary and a shared set of parameters across languages ;,0,0.46668226,68.04781039841424,21
2802,the inputs and labels corresponding to examples drawn from different language pairs might still reside in distinct sub-spaces .,3,0.64286673,74.82129801027743,19
2802,"In this paper , we introduce multilingual crossover encoder-decoder ( mXEncDec ) to fuse language pairs at an instance level .",1,0.75543755,47.257430167485296,21
2802,Our approach interpolates instances from different language pairs into joint ‘ crossover examples ’ in order to encourage sharing input and output spaces across languages .,2,0.6801748,99.28663966626613,26
2802,"To ensure better fusion of examples in multilingual settings , we propose several techniques to improve example interpolation across dissimilar languages under heavy data imbalance .",2,0.3682222,88.92043198135185,26
2802,"Experiments on a large-scale WMT multilingual dataset demonstrate that our approach significantly improves quality on English-to-Many , Many-to-English and zero-shot translation tasks ( from + 0.5 BLEU up to + 5.5 BLEU points ) .",3,0.88115674,16.64452505748233,40
2802,Results on code-switching sets demonstrate the capability of our approach to improve model generalization to out-of-distribution multilingual examples .,3,0.9783844,17.412614250870227,21
2802,We also conduct qualitative and quantitative representation comparisons to analyze the advantages of our approach at the representation level .,2,0.76330644,32.95763032345903,20
2803,Word identification from continuous input is typically viewed as a segmentation task .,0,0.93356854,68.16098472764185,13
2803,Experiments with human adults suggest that familiarity with syntactic structures in their native language also influences word identification in artificial languages ;,0,0.8027319,101.41758344194001,22
2803,"however , the relation between syntactic processing and word identification is yet unclear .",0,0.9518951,57.983292856696735,14
2803,"This work takes one step forward by exploring a radically different approach of word identification , in which segmentation of a continuous input is viewed as a process isomorphic to unsupervised constituency parsing .",1,0.35594562,41.8775362547102,34
2803,"Besides formalizing the approach , this study reports simulations of human experiments with DIORA ( Drozdov et al. , 2020 ) , a neural unsupervised constituency parser .",2,0.36502755,107.84593058879165,28
2803,"Results show that this model can reproduce human behavior in word identification experiments , suggesting that this is a viable approach to study word identification and its relation to syntactic processing .",3,0.9889508,30.903319918196978,32
2804,The social impact of natural language processing and its applications has received increasing attention .,0,0.9649786,13.922726577261317,15
2804,"In this position paper , we focus on the problem of safety for end-to-end conversational AI .",1,0.89108676,19.831721526124483,18
2804,"We survey the problem landscape therein , introducing a taxonomy of three observed phenomena : the Instigator , Yea-Sayer , and Impostor effects .",1,0.5057313,149.48234714775296,25
2804,We then empirically assess the extent to which current tools can measure these effects and current systems display them .,1,0.40075126,43.49149339518017,20
2804,We release these tools as part of a “ first aid kit ” ( SafetyKit ) to quickly assess apparent safety concerns .,2,0.5236925,79.87695930337745,23
2804,"Our results show that , while current tools are able to provide an estimate of the relative safety of systems in various settings , they still have several shortcomings .",3,0.98941225,35.70752640552702,30
2804,We suggest several future directions and discuss ethical considerations .,3,0.77275306,54.47005494735422,10
2805,Recent work in cross-lingual semantic parsing has successfully applied machine translation to localize parsers to new languages .,0,0.94712985,16.99764910477777,18
2805,"However , these advances assume access to high-quality machine translation systems and word alignment tools .",0,0.93073577,75.5322478245833,16
2805,"We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem , without parallel data ( i.e. , utterance-logical form pairs ) for new languages .",2,0.785095,53.248956520685944,29
2805,We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-logical form paired data and in-domain natural language corpora in each new language .,2,0.5113383,32.83445571522614,29
2805,Our model encourages language-agnostic encodings by jointly optimizing for logical-form generation with auxiliary objectives designed for cross-lingual latent representation alignment .,2,0.51444167,60.5478286559701,25
2805,"Our parser performs significantly above translation-based baselines and , in some cases , competes with the supervised upper-bound .",3,0.9411126,68.60722474863763,21
2806,Obtaining human-like performance in NLP is often argued to require compositional generalisation .,0,0.8464065,31.250770119385628,13
2806,Whether neural networks exhibit this ability is usually studied by training models on highly compositional synthetic data .,0,0.8993904,62.70305003801378,18
2806,"However , compositionality in natural language is much more complex than the rigid , arithmetic-like version such data adheres to , and artificial compositionality tests thus do not allow us to determine how neural models deal with more realistic forms of compositionality .",0,0.77095085,60.51010542114556,44
2806,"In this work , we re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation ( NMT ) .",1,0.46327502,28.0023919736919,23
2806,"Our results highlight that : i ) unfavourably , models trained on more data are more compositional ;",3,0.9899199,176.03173837479517,18
2806,"ii ) models are sometimes less compositional than expected , but sometimes more , exemplifying that different levels of compositionality are required , and models are not always able to modulate between them correctly ;",3,0.726336,77.73023485493141,35
2806,"iii ) some of the non-compositional behaviours are mistakes , whereas others reflect the natural variation in data .",3,0.70141727,80.67875431415324,19
2806,"Apart from an empirical study , our work is a call to action : we should rethink the evaluation of compositionality in neural networks and develop benchmarks using real data to evaluate compositionality on natural language , where composing meaning is not as straightforward as doing the math .",3,0.9499778,39.21967310666907,49
2807,Document-level neural machine translation ( DocNMT ) achieves coherent translations by incorporating cross-sentence context .,0,0.7004661,58.757573053777705,17
2807,"However , for most language pairs there ’s a shortage of parallel documents , although parallel sentences are readily available .",0,0.84184414,97.2940472742605,21
2807,"In this paper , we study whether and how contextual modeling in DocNMT is transferable via multilingual modeling .",1,0.9458121,54.69385827076494,19
2807,"We focus on the scenario of zero-shot transfer from teacher languages with document level data to student languages with no documents but sentence level data , and for the first time treat document-level translation as a transfer learning problem .",2,0.5313502,30.457811509497358,42
2807,"Using simple concatenation-based DocNMT , we explore the effect of 3 factors on the transfer : the number of teacher languages with document level data , the balance between document and sentence level data at training , and the data condition of parallel documents ( genuine vs .",2,0.86364657,76.28766647394032,50
2807,back-translated ) .,3,0.35007012,70.76807614678947,5
2807,"Our experiments on Europarl-7 and IWSLT-10 show the feasibility of multilingual transfer for DocNMT , particularly on document-specific metrics .",3,0.9772574,41.32163423739489,24
2807,We observe that more teacher languages and adequate data balance both contribute to better transfer quality .,3,0.9824708,264.44010624549367,17
2807,"Surprisingly , the transfer is less sensitive to the data condition , where multilingual DocNMT delivers decent performance with either back-translated or genuine document pairs .",3,0.9419302,113.4250788706131,28
2808,Cross-lingual retrieval aims to retrieve relevant text across languages .,0,0.8981795,14.766605836744514,10
2808,Current methods typically achieve cross-lingual retrieval by learning language-agnostic text representations in word or sentence level .,0,0.8956333,36.66552424867289,19
2808,"However , how to learn phrase representations for cross-lingual phrase retrieval is still an open problem .",0,0.9037831,21.080082515789822,17
2808,"In this paper , we propose , a cross-lingual phrase retriever that extracts phrase representations from unlabeled example sentences .",1,0.8663634,25.64365858336874,20
2808,"Moreover , we create a large-scale cross-lingual phrase retrieval dataset , which contains 65 K bilingual phrase pairs and 4.2 M example sentences in 8 English-centric language pairs .",2,0.8787382,31.93881010331318,29
2808,Experimental results show that outperforms state-of-the-art baselines which utilize word-level or sentence-level representations .,3,0.9408309,7.693343265299819,24
2808,also shows impressive zero-shot transferability that enables the model to perform retrieval in an unseen language pair during training .,3,0.9388057,48.15836210771261,20
2808,"Our dataset , code , and trained models are publicly available at github.com/cwszz/XPR / .",2,0.5063581,58.636325240890784,15
2809,Data-to-text generation focuses on generating fluent natural language responses from structured meaning representations ( MRs ) .,0,0.8601823,32.38339767480916,18
2809,"Such representations are compositional and it is costly to collect responses for all possible combinations of atomic meaning schemata , thereby necessitating few-shot generalization to novel MRs .",0,0.83636844,61.9267505472398,28
2809,"In this work , we systematically study the compositional generalization of the state-of-the-art T5 models in few-shot data-to-text tasks .",1,0.818733,11.92333051642248,29
2809,"We show that T5 models fail to generalize to unseen MRs , and we propose a template-based input representation that considerably improves the model ’s generalization capability .",3,0.82705575,34.62233245194675,30
2809,"To further improve the model ’s performance , we propose an approach based on self-training using fine-tuned BLEURT for pseudo-response selection .",2,0.5409554,31.491608009672944,24
2809,"On the commonly-used SGD and Weather benchmarks , the proposed self-training approach improves tree accuracy by 46 % + and reduces the slot error rates by 73 % + over the strong T5 baselines in few-shot settings .",3,0.8792369,67.8670574806293,40
2810,The rapid development of conversational assistants accelerates the study on conversational question answering ( QA ) .,0,0.96171665,24.437406629174845,17
2810,"However , the existing conversational QA systems usually answer users ’ questions with a single knowledge source , e.g. , paragraphs or a knowledge graph , but overlook the important visual cues , let alone multiple knowledge sources of different modalities .",0,0.9140421,73.04590683472945,42
2810,"In this paper , we hence define a novel research task , i.e. , multimodal conversational question answering ( MMCoQA ) , aiming to answer users ’ questions with multimodal knowledge sources via multi-turn conversations .",1,0.8041315,40.86820679577895,36
2810,"This new task brings a series of research challenges , including but not limited to priority , consistency , and complementarity of multimodal knowledge .",0,0.818719,66.77901175623496,25
2810,"To facilitate the data-driven approaches in this area , we construct the first multimodal conversational QA dataset , named MMConvQA .",2,0.45506135,24.95773585262005,21
2810,Questions are fully annotated with not only natural language answers but also the corresponding evidence and valuable decontextualized self-contained questions .,2,0.41089118,33.54025415637917,21
2810,"Meanwhile , we introduce an end-to-end baseline model , which divides this complex research task into question understanding , multi-modal evidence retrieval , and answer extraction .",2,0.5519461,46.58407591950429,28
2810,"Moreover , we report a set of benchmarking results , and the results indicate that there is ample room for improvement .",3,0.93621206,15.809270583375373,22
2811,"The state-of-the-art model for structured sentiment analysis casts the task as a dependency parsing problem , which has some limitations : ( 1 ) The label proportions for span prediction and span relation prediction are imbalanced .",0,0.7988456,37.92795895130201,42
2811,"The span lengths of sentiment tuple components may be very large in this task , which will further exacerbates the imbalance problem .",3,0.62139046,125.04889664629404,23
2811,"( 3 ) Two nodes in a dependency graph cannot have multiple arcs , therefore some overlapped sentiment tuples cannot be recognized .",3,0.6651971,234.6542592345211,23
2811,"In this work , we propose nichetargeting solutions for these issues .",1,0.87536544,131.94557743884945,12
2811,"First , we introduce a novel labeling strategy , which contains two sets of token pair labels , namely essential label set and whole label set .",2,0.8316746,56.63700547689715,27
2811,"The essential label set consists of the basic labels for this task , which are relatively balanced and applied in the prediction layer .",2,0.41924605,118.0979560063847,24
2811,"The whole label set includes rich labels to help our model capture various token relations , which are applied in the hidden layer to softly influence our model .",3,0.45745322,113.65002602566237,29
2811,"Moreover , we also propose an effective model to well collaborate with our labeling strategy , which is equipped with the graph attention networks to iteratively refine token representations , and the adaptive multi-label classifier to dynamically predict multiple relations between token pairs .",3,0.4934048,61.19317149317919,44
2811,We perform extensive experiments on 5 benchmark datasets in four languages .,2,0.7825766,20.34766970303663,12
2811,Experimental results show that our model outperforms previous SOTA models by a large margin .,3,0.9697347,5.456571252266684,15
2812,This paper focuses on the Data Augmentation for low-resource Natural Language Understanding ( NLU ) tasks .,1,0.890404,25.747597504097097,17
2812,"We propose Prompt-based Data Augmentation model ( PromDA ) which only trains small-scale Soft Prompt ( i.e. , a set of trainable vectors ) in the frozen Pre-trained Language Models ( PLMs ) .",2,0.7464194,78.77767153254463,35
2812,This avoids human effort in collecting unlabeled in-domain data and maintains the quality of generated synthetic data .,3,0.44244006,33.5374474602916,19
2812,"In addition , PromDA generates synthetic data via two different views and filters out the low-quality data using NLU models .",2,0.6522332,118.34351251983885,21
2812,"Experiments on four benchmarks show that synthetic data produced by PromDA successfully boost up the performance of NLU models which consistently outperform several competitive baseline models , including a state-of-the-art semi-supervised model using unlabeled in-domain data .",3,0.8981004,19.655689763697364,45
2812,The synthetic data from PromDA are also complementary with unlabeled in-domain data .,3,0.6158703,49.79918218257566,15
2812,The NLU models can be further improved when they are combined for training .,3,0.81470644,42.910575836938996,14
2813,"There is mounting evidence that existing neural network models , in particular the very popular sequence-to-sequence architecture , struggle to systematically generalize to unseen compositions of seen components .",0,0.9261928,38.36962571252898,31
2813,We demonstrate that one of the reasons hindering compositional generalization relates to representations being entangled .,3,0.9035765,67.51165844027027,16
2813,We propose an extension to sequence-to-sequence models which encourage disentanglement by adaptively re-encoding ( at each time step ) the source input .,2,0.46090505,22.82488179662564,25
2813,"Specifically , we condition the source representations on the newly decoded target context which makes it easier for the encoder to exploit specialized information for each prediction rather than capturing it all in a single forward pass .",2,0.7827704,35.81351268908928,38
2813,Experimental results on semantic parsing and machine translation empirically show that our proposal delivers more disentangled representations and better generalization .,3,0.88774604,20.67792696350302,21
2814,Pre-trained language models ( PLMs ) have shown great potentials in natural language processing ( NLP ) including rhetorical structure theory ( RST ) discourse parsing .,0,0.9334523,32.14676681869649,27
2814,"Current PLMs are obtained by sentence-level pre-training , which is different from the basic processing unit , i.e .",0,0.48626196,50.02668926194564,21
2814,element discourse unit ( EDU ) .,4,0.63445586,303.51346285161924,7
2814,"To this end , we propose a second-stage EDU-level pre-training approach in this work , which presents two novel tasks to learn effective EDU representations continually based on well pre-trained language models .",2,0.55113655,39.005065436818164,35
2814,"Concretely , the two tasks are ( 1 ) next EDU prediction ( NEP ) and ( 2 ) discourse marker prediction ( DMP ) .",2,0.5958802,86.29496913109011,26
2814,"We take a state-of-the-art transition-based neural parser as baseline , and adopt it with a light bi-gram EDU modification to effectively explore the EDU-level pre-trained EDU representation .",2,0.78142273,46.44076738738078,36
2814,"Experimental results on a benckmark dataset show that our method is highly effective , leading a 2.1-point improvement in F1-score .",3,0.944607,24.95394574332951,25
2814,All codes and pre-trained models will be released publicly to facilitate future studies .,3,0.5708074,16.748214643184685,14
2815,Knowledge graph completion ( KGC ) aims to reason over known facts and infer the missing links .,0,0.8848156,65.54871569397679,18
2815,"Text-based methods such as KGBERT ( Yao et al. , 2019 ) learn entity representations from natural language descriptions , and have the potential for inductive KGC .",0,0.6657085,83.00647440377541,30
2815,"However , the performance of text-based methods still largely lag behind graph embedding-based methods like TransE ( Bordes et al. , 2013 ) and RotatE ( Sun et al. , 2019 b ) .",0,0.7736579,43.89652146283935,38
2815,"In this paper , we identify that the key issue is efficient contrastive learning .",1,0.8022629,37.654457764288786,15
2815,"To improve the learning efficiency , we introduce three types of negatives : in-batch negatives , pre-batch negatives , and self-negatives which act as a simple form of hard negatives .",2,0.74685526,40.274903797537974,34
2815,"Combined with InfoNCE loss , our proposed model SimKGC can substantially outperform embedding-based methods on several benchmark datasets .",3,0.8329493,84.41533285118936,21
2815,"In terms of mean reciprocal rank ( MRR ) , we advance the state-of-the-art by + 19 % on WN18 RR , + 6.8 % on the Wikidata5 M transductive setting , and + 22 % on the Wikidata5 M inductive setting .",3,0.8847295,32.81741783176525,49
2815,Thorough analyses are conducted to gain insights into each component .,2,0.7174689,38.09187704861941,11
2815,Our code is available at https://github.com/intfloat/SimKGC .,3,0.5900576,20.682497580391555,7
2816,Learned self-attention functions in state-of-the-art NLP models often correlate with human attention .,0,0.79842937,13.933698316921944,17
2816,We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention .,1,0.8012344,35.056132732774316,29
2816,We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction .,2,0.79400164,57.08973502520773,17
2816,"We find the predictiveness of large-scale pre-trained self-attention for human attention depends on ‘ what is in the tail ’ , e.g. , the syntactic nature of rare contexts .",3,0.93956435,55.24602954942739,30
2816,"Further , we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading .",3,0.97891504,20.03870480606234,21
2816,"Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off , showing that lower-entropy attention vectors are more faithful .",3,0.76572543,91.30372875914921,27
2817,"Laws and their interpretations , legal arguments and agreements are typically expressed in writing , leading to the production of vast corpora of legal text .",0,0.9341967,66.7169155096798,26
2817,"Their analysis , which is at the center of legal practice , becomes increasingly elaborate as these collections grow in size .",0,0.86891544,103.66057547602578,22
2817,Natural language understanding ( NLU ) technologies can be a valuable tool to support legal practitioners in these endeavors .,0,0.9048168,43.71304375217265,20
2817,"Their usefulness , however , largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain .",0,0.8985317,23.39984206173579,28
2817,"To answer this currently open question , we introduce the Legal General Language Understanding Evaluation ( LexGLUE ) benchmark , a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way .",1,0.5330974,49.24439289286926,41
2817,We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks .,3,0.73236346,61.14831036945727,25
2818,Lexical ambiguity poses one of the greatest challenges in the field of Machine Translation .,0,0.9623719,19.2893107621895,15
2818,"Over the last few decades , multiple efforts have been undertaken to investigate incorrect translations caused by the polysemous nature of words .",0,0.9609557,30.089082758828667,23
2818,"Within this body of research , some studies have posited that models pick up semantic biases existing in the training data , thus producing translation errors .",0,0.9139059,62.229421585195205,27
2818,"In this paper , we present DiBiMT , the first entirely manually-curated evaluation benchmark which enables an extensive study of semantic biases in Machine Translation of nominal and verbal words in five different language combinations , namely , English and one or other of the following languages : Chinese , German , Italian , Russian and Spanish .",1,0.82129437,60.69930423948341,60
2818,"Furthermore , we test state-of-the-art Machine Translation systems , both commercial and non-commercial ones , against our new test bed and provide a thorough statistical and linguistic analysis of the results .",2,0.6131076,30.212999125449144,37
2818,We release DiBiMT at https://nlp.uniroma1.it/dibimt as a closed benchmark with a public leaderboard .,2,0.42692468,78.74221898210122,14
2819,"Word translation or bilingual lexicon induction ( BLI ) is a key cross-lingual task , aiming to bridge the lexical gap between different languages .",0,0.9496945,26.20639251909739,25
2819,"In this work , we propose a robust and effective two-stage contrastive learning framework for the BLI task .",1,0.80239505,17.934566779729018,20
2819,"At Stage C1 , we propose to refine standard cross-lingual linear maps between static word embeddings ( WEs ) via a contrastive learning objective ;",2,0.65233934,79.15485141139315,25
2819,we also show how to integrate it into the self-learning procedure for even more refined cross-lingual maps .,3,0.7888664,32.26349222606983,19
2819,"In Stage C2 , we conduct BLI-oriented contrastive fine-tuning of mBERT , unlocking its word translation capability .",2,0.6998633,87.53830036199666,20
2819,We also show that static WEs induced from the ‘ C2-tuned ’ mBERT complement static WEs from Stage C1 .,3,0.9719697,79.18108782076148,22
2819,Comprehensive experiments on standard BLI datasets for diverse languages and different experimental setups demonstrate substantial gains achieved by our framework .,3,0.85221994,48.15847692633215,21
2819,"While the BLI method from Stage C1 already yields substantial gains over all state-of-the-art BLI methods in our comparison , even stronger improvements are met with the full two-stage framework : e.g. , we report gains for 112/112 BLI setups , spanning 28 language pairs .",3,0.89473283,62.735528917394326,53
2820,Neural Chat Translation ( NCT ) aims to translate conversational text into different languages .,0,0.9287005,58.686199081286176,15
2820,"Existing methods mainly focus on modeling the bilingual dialogue characteristics ( e.g. , coherence ) to improve chat translation via multi-task learning on small-scale chat translation data .",0,0.742334,53.036717617515045,28
2820,"Although the NCT models have achieved impressive success , it is still far from satisfactory due to insufficient chat translation data and simple joint training manners .",0,0.57097065,107.53428374380128,27
2820,"To address the above issues , we propose a scheduled multi-task learning framework for NCT .",2,0.40828058,36.418592310352295,16
2820,"Specifically , we devise a three-stage training framework to incorporate the large-scale in-domain chat translation data into training by adding a second pre-training stage between the original pre-training and fine-tuning stages .",2,0.863069,16.374083898451712,35
2820,"Further , we investigate where and how to schedule the dialogue-related auxiliary tasks in multiple training stages to effectively enhance the main chat translation task .",3,0.60803705,75.60031379920146,26
2820,Extensive experiments on four language directions ( English-Chinese and English-German ) verify the effectiveness and superiority of the proposed approach .,3,0.70808697,16.823140287993276,25
2820,"Additionally , we will make the large-scale in-domain paired bilingual dialogue dataset publicly available for the research community .",3,0.58966243,28.64097461242406,20
2821,We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks .,2,0.46808723,11.674990143918082,29
2821,"Our benchmarks cover four jurisdictions ( European Council , USA , Switzerland , and China ) , five languages ( English , German , French , Italian and Chinese ) and fairness across five attributes ( gender , age , region , language , and legal area ) .",2,0.8692226,53.77528250503368,49
2821,"In our experiments , we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases , while none of these techniques guarantee fairness , nor consistently mitigate group disparities .",3,0.5704631,52.95862943812998,40
2821,"Furthermore , we provide a quantitative and qualitative analysis of our results , highlighting open challenges in the development of robustness methods in legal NLP .",3,0.916243,31.619340312523782,26
2822,Podcasts have shown a recent rise in popularity .,0,0.9396317,26.0672247410705,9
2822,Summarization of podcasts is of practical benefit to both content providers and consumers .,3,0.49390715,59.395728721751496,14
2822,It helps people quickly decide whether they will listen to a podcast and / or reduces the cognitive load of content providers to write summaries .,3,0.57015985,55.353113580508975,26
2822,"Nevertheless , podcast summarization faces significant challenges including factual inconsistencies of summaries with respect to the inputs .",0,0.8773384,82.29511080350629,18
2822,The problem is exacerbated by speech disfluencies and recognition errors in transcripts of spoken language .,0,0.93447214,43.43528731273446,16
2822,"In this paper , we explore a novel abstractive summarization method to alleviate these issues .",1,0.9187537,22.347889601938835,16
2822,Our approach learns to produce an abstractive summary while grounding summary segments in specific regions of the transcript to allow for full inspection of summary details .,2,0.649782,84.35827428657328,27
2822,We conduct a series of analyses of the proposed approach on a large podcast dataset and show that the approach can achieve promising results .,3,0.66938686,18.084113331560413,25
2822,"Grounded summaries bring clear benefits in locating the summary and transcript segments that contain inconsistent information , and hence improve summarization quality in terms of automatic and human evaluation .",0,0.59028494,75.84609827784993,30
2823,Publicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language ( XBRL ) word-level tags .,0,0.83496815,61.30169744023949,21
2823,Manually tagging the reports is tedious and costly .,0,0.8213262,84.4592193357503,9
2823,"We , therefore , introduce XBRL tagging as a new entity extraction task for the financial domain and release FiNER-139 , a dataset of 1.1 M sentences with gold XBRL tags .",2,0.7261868,79.19067854871268,34
2823,"Unlike typical entity extraction datasets , FiNER-139 uses a much larger label set of 139 entity types .",3,0.42443937,334.25094146365984,20
2823,"Most annotated tokens are numeric , with the correct tag per token depending mostly on context , rather than the token itself .",3,0.5547236,124.11191284107808,23
2823,"We show that subword fragmentation of numeric expressions harms BERT ’s performance , allowing word-level BILSTMs to perform better .",3,0.9415634,104.25358694595393,21
2823,"To improve BERT ’s performance , we propose two simple and effective solutions that replace numeric expressions with pseudo-tokens reflecting original token shapes and numeric magnitudes .",2,0.44202346,81.90890379109352,27
2823,"We also experiment with FIN-BERT , an existing BERT model for the financial domain , and release our own BERT ( SEC-BERT ) , pre-trained on financial filings , which performs best .",2,0.61287075,74.7304467513375,37
2823,"Through data and error analysis , we finally identify possible limitations to inspire future work on XBRL tagging .",3,0.8542793,110.39494769920589,19
2824,Contrastive learning has achieved impressive success in generation tasks to militate the “ exposure bias ” problem and discriminatively exploit the different quality of references .,0,0.87568146,80.94942503102135,26
2824,"Existing works mostly focus on contrastive learning on the instance-level without discriminating the contribution of each word , while keywords are the gist of the text and dominant the constrained mapping relationships .",0,0.8198356,113.20619082909768,35
2824,"Hence , in this work , we propose a hierarchical contrastive learning mechanism , which can unify hybrid granularities semantic meaning in the input text .",1,0.68704134,73.39494362601258,26
2824,"Concretely , we first propose a keyword graph via contrastive correlations of positive-negative pairs to iteratively polish the keyword representations .",2,0.7719569,100.6587235811164,23
2824,"Then , we construct intra-contrasts within instance-level and keyword-level , where we assume words are sampled nodes from a sentence distribution .",2,0.9189419,79.058210392066,24
2824,"Finally , to bridge the gap between independent contrast levels and tackle the common contrast vanishing problem , we propose an inter-contrast mechanism that measures the discrepancy between contrastive keyword nodes respectively to the instance distribution .",2,0.53987634,98.08318121133908,37
2824,"Experiments demonstrate that our model outperforms competitive baselines on paraphrasing , dialogue generation , and storytelling tasks .",3,0.9264212,21.41260707283683,18
2825,"In this paper , we propose a neural model EPT-X ( Expression-Pointer Transformer with Explanations ) , which utilizes natural language explanations to solve an algebraic word problem .",1,0.8195346,45.25279528281517,32
2825,"To enhance the explainability of the encoding process of a neural model , EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans .",2,0.4539327,63.14203394465592,34
2825,A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem .,0,0.4783054,53.69592767585547,23
2825,A faithful explanation is one that accurately represents the reasoning process behind the model ’s solution equation .,0,0.6973764,73.63580536913875,18
2825,The EPT-X model yields an average baseline performance of 69.59 % on our PEN dataset and produces explanations with quality that is comparable to human output .,3,0.90854865,57.546522523761745,28
2825,The contribution of this work is two-fold .,1,0.38111705,10.308607256810268,9
2825,"An explainable neural model that sets a baseline for algebraic word problem solving task , in terms of model ’s correctness , plausibility , and faithfulness .",2,0.29511973,78.83930078531888,27
2825,"We release a novel dataset PEN ( Problems with Explanations for Numbers ) , which expands the existing datasets by attaching explanations to each number / variable .",2,0.73726153,115.08399031724927,28
2826,"This paper studies the ( often implicit ) human values behind natural language arguments , such as to have freedom of thought or to be broadminded .",1,0.8656164,152.71275386167096,27
2826,Values are commonly accepted answers to why some option is desirable in the ethical sense and are thus essential both in real-world argumentation and theoretical argumentation frameworks .,0,0.90947735,68.72801465025309,28
2826,"However , their large variety has been a major obstacle to modeling them in argument mining .",0,0.9270854,128.02112161310725,17
2826,"To overcome this obstacle , we contribute an operationalization of human values , namely a multi-level taxonomy with 54 values that is in line with psychological research .",2,0.34260133,69.70534498638679,28
2826,"Moreover , we provide a dataset of 5270 arguments from four geographical cultures , manually annotated for human values .",2,0.6551062,134.99505546065367,20
2826,"First experiments with the automatic classification of human values are promising , with F1-scores up to 0.81 and 0.25 on average .",3,0.93892777,35.95170273109953,24
2827,"Intrinsic evaluations of OIE systems are carried out either manually — with human evaluators judging the correctness of extractions — or automatically , on standardized benchmarks .",0,0.62064093,45.62338908787597,27
2827,"The latter , while much more cost-effective , is less reliable , primarily because of the incompleteness of the existing OIE benchmarks : the ground truth extractions do not include all acceptable variants of the same fact , leading to unreliable assessment of the models ’ performance .",0,0.54049325,56.49577514842286,48
2827,"Moreover , the existing OIE benchmarks are available for English only .",0,0.67775905,69.56794133805103,12
2827,"In this work , we introduce BenchIE : a benchmark and evaluation framework for comprehensive evaluation of OIE systems for English , Chinese , and German .",1,0.7823398,55.0725417201629,27
2827,"In contrast to existing OIE benchmarks , BenchIE is fact-based , i.e. , it takes into account informational equivalence of extractions : our gold standard consists of fact synsets , clusters in which we exhaustively list all acceptable surface forms of the same fact .",0,0.3748619,152.79600879671486,47
2827,"Moreover , having in mind common downstream applications for OIE , we make BenchIE multi-faceted ;",3,0.5988344,333.38883537368037,16
2827,"i.e. , we create benchmark variants that focus on different facets of OIE evaluation , e.g. , compactness or minimality of extractions .",2,0.6827686,76.85909278442399,23
2827,We benchmark several state-of-the-art OIE systems using BenchIE and demonstrate that these systems are significantly less effective than indicated by existing OIE benchmarks .,3,0.72982144,24.362687686343442,28
2827,We make BenchIE ( data and evaluation code ) publicly available .,2,0.66617733,299.7146308572016,12
2828,"Training Transformer-based models demands a large amount of data , while obtaining aligned and labelled data in multimodality is rather cost-demanding , especially for audio-visual speech recognition ( AVSR ) .",0,0.7945986,62.29340041405345,33
2828,Thus it makes a lot of sense to make use of unlabelled unimodal data .,0,0.7207285,13.830734894000091,15
2828,"On the other side , although the effectiveness of large-scale self-supervised learning is well established in both audio and visual modalities , how to integrate those pre-trained models into a multimodal scenario remains underexplored .",0,0.82368755,14.166994206523135,35
2828,"In this work , we successfully leverage unimodal self-supervised learning to promote the multimodal AVSR .",2,0.34705412,52.209803844907796,16
2828,"In particular , audio and visual front-ends are trained on large-scale unimodal datasets , then we integrate components of both front-ends into a larger multimodal framework which learns to recognize parallel audio-visual data into characters through a combination of CTC and seq2seq decoding .",2,0.8033079,36.470274652409316,44
2828,"We show that both components inherited from unimodal self-supervised learning cooperate well , resulting in that the multimodal framework yields competitive results through fine-tuning .",3,0.94272536,40.79925012741161,25
2828,Our model is experimentally validated on both word-level and sentence-level tasks .,3,0.5664614,11.801525667691926,14
2828,"Especially , even without an external language model , our proposed model raises the state-of-the-art performances on the widely accepted Lip Reading Sentences 2 ( LRS2 ) dataset by a large margin , with a relative improvement of 30 % .",3,0.9037187,32.899581586558305,47
2829,"Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization , especially through fine-tuning large pre-trained language models on the downstream dataset .",0,0.9257478,11.426353342628186,24
2829,These models are typically decoded with beam search to generate a unique summary .,0,0.7454825,58.00740743904476,14
2829,"However , the search space is very large , and with the exposure bias , such decoding is not optimal .",0,0.7099201,68.96510254603783,21
2829,"In this paper , we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates .",1,0.7045763,27.737846360598002,25
2829,Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model .,3,0.5114856,42.46864102885871,20
2829,"With a base PEGASUS , we push ROUGE scores by 5.44 % on CNN-DailyMail ( 47.16 ROUGE-1 ) , 1.31 % on XSum ( 48.12 ROUGE-1 ) and 9.34 % on Reddit TIFU ( 29.83 ROUGE-1 ) , reaching a new state-of-the-art .",3,0.9231148,18.704338864271413,55
2829,Our code and checkpoints will be available at https://github.com/ntunlp/SummaReranker .,3,0.6328586,25.183058672293264,10
2830,The ability to sequence unordered events is evidence of comprehension and reasoning about real world tasks / procedures .,0,0.88889,159.87310355763057,19
2830,It is essential for applications such as task planning and multi-source instruction summarization .,0,0.81671166,30.02184626240583,14
2830,"It often requires thorough understanding of temporal common sense and multimodal information , since these procedures are often conveyed by a combination of texts and images .",0,0.9069926,53.06493598027506,27
2830,"While humans are capable of reasoning about and sequencing unordered procedural instructions , the extent to which the current machine learning methods possess such capability is still an open question .",0,0.92386955,42.064360173367106,31
2830,"In this work , we benchmark models ’ capability of reasoning over and sequencing unordered multimodal instructions by curating datasets from online instructional manuals and collecting comprehensive human annotations .",1,0.53598523,103.18605698473047,30
2830,We find current state-of-the-art models not only perform significantly worse than humans but also seem incapable of efficiently utilizing multimodal information .,3,0.9779106,13.66304624112373,28
2830,"To improve machines ’ performance on multimodal event sequencing , we propose sequence-aware pretraining techniques exploiting the sequential alignment properties of both texts and images , resulting in > 5 % improvements on perfect match ratio .",2,0.5375556,96.43388749065153,39
2831,Fake news detection is crucial for preventing the dissemination of misinformation on social media .,0,0.9056196,16.607735389484308,15
2831,"To differentiate fake news from real ones , existing methods observe the language patterns of the news post and “ zoom in ” to verify its content with knowledge sources or check its readers ’ replies .",0,0.8576821,82.82172031030277,37
2831,"However , these methods neglect the information in the external news environment where a fake news post is created and disseminated .",0,0.87496126,42.95599337606687,22
2831,"The news environment represents recent mainstream media opinion and public attention , which is an important inspiration of fake news fabrication because fake news is often designed to ride the wave of popular events and catch public attention with unexpected novel content for greater exposure and spread .",0,0.9063825,80.05151800927777,48
2831,"To capture the environmental signals of news posts , we “ zoom out ” to observe the news environment and propose the News Environment Perception Framework ( NEP ) .",2,0.5818539,97.21229005005657,30
2831,"For each post , we construct its macro and micro news environment from recent mainstream news .",2,0.90438735,279.4526878418578,17
2831,Then we design a popularity-oriented and a novelty-oriented module to perceive useful signals and further assist final prediction .,2,0.8015611,148.7040917959254,21
2831,Experiments on our newly built datasets show that the NEP can efficiently improve the performance of basic fake news detectors .,3,0.95567703,46.68911736194867,21
2832,Multi-encoder models are a broad family of context-aware neural machine translation systems that aim to improve translation quality by encoding document-level contextual information alongside the current sentence .,0,0.8833821,15.984500889315596,31
2832,"The context encoding is undertaken by contextual parameters , trained on document-level data .",2,0.6015109,187.0976741508749,15
2832,"In this work , we discuss the difficulty of training these parameters effectively , due to the sparsity of the words in need of context ( i.e. , the training signal ) , and their relevant context .",1,0.7637378,65.5395270554553,38
2832,"We propose to pre-train the contextual parameters over split sentence pairs , which makes an efficient use of the available data for two reasons .",2,0.4299747,67.88764253801907,25
2832,"Firstly , it increases the contextual training signal by breaking intra-sentential syntactic relations , and thus pushing the model to search the context for disambiguating clues more frequently .",3,0.79394305,51.568371311246175,29
2832,"Secondly , it eases the retrieval of relevant context , since context segments become shorter .",3,0.5113863,127.7546317169041,16
2832,"We propose four different splitting methods , and evaluate our approach with BLEU and contrastive test sets .",2,0.58811957,75.33337441383416,18
2832,"Results show that it consistently improves learning of contextual parameters , both in low and high resource settings .",3,0.98669666,65.41719727280721,19
2833,Event detection ( ED ) is a critical subtask of event extraction that seeks to identify event triggers of certain types in texts .,0,0.94275355,56.85629061383293,24
2833,"Despite significant advances in ED , existing methods typically follow a “ one model fits all types ” approach , which sees no differences between event types and often results in a quite skewed performance .",0,0.8838094,99.19280176061498,36
2833,"Finding the causes of skewed performance is crucial for the robustness of an ED model , but to date there has been little exploration of this problem .",0,0.90668136,29.449223455154087,28
2833,"This research examines the issue in depth and presents a new concept termed trigger salience attribution , which can explicitly quantify the underlying patterns of events .",1,0.75471616,77.79423479853816,27
2833,"On this foundation , we develop a new training mechanism for ED , which can distinguish between trigger-dependent and context-dependent types and achieve promising performance on two benchmarks .",3,0.38822898,49.49021054607501,31
2833,"Finally , by highlighting many distinct characteristics of trigger-dependent and context-dependent types , our work may promote more research into this problem .",3,0.98551416,75.73897726584879,25
2834,"In the field of sentiment analysis , several studies have highlighted that a single sentence may express multiple , sometimes contrasting , sentiments and emotions , each with its own experiencer , target and / or cause .",0,0.92104024,81.09086192732177,38
2834,"To this end , over the past few years researchers have started to collect and annotate data manually , in order to investigate the capabilities of automatic systems not only to distinguish between emotions , but also to capture their semantic constituents .",0,0.94699323,30.11066927787512,43
2834,"However , currently available gold datasets are heterogeneous in size , domain , format , splits , emotion categories and role labels , making comparisons across different works difficult and hampering progress in the area .",0,0.8999121,122.3985990878592,36
2834,"In this paper , we tackle this issue and present a unified evaluation framework focused on Semantic Role Labeling for Emotions ( SRL4E ) , in which we unify several datasets tagged with emotions and semantic roles by using a common labeling scheme .",1,0.8552361,44.54511920288402,44
2834,"We use SRL4E as a benchmark to evaluate how modern pretrained language models perform and analyze where we currently stand in this task , hoping to provide the tools to facilitate studies in this complex area .",2,0.56583416,44.82874356748212,37
2835,"In linguistics , there are two main perspectives on negation : a semantic and a pragmatic view .",0,0.921281,36.9834594866223,18
2835,"So far , research in NLP on negation has almost exclusively adhered to the semantic view .",0,0.94382286,53.77968029692245,17
2835,"In this article , we adopt the pragmatic paradigm to conduct a study of negation understanding focusing on transformer-based PLMs .",1,0.88711196,67.02986098743602,23
2835,"Our results differ from previous , semantics-based studies and therefore help to contribute a more comprehensive – and , given the results , much more optimistic – picture of the PLMs ’ negation understanding .",3,0.9903364,150.18143925705715,37
2836,"Thanks to the effectiveness and wide availability of modern pretrained language models ( PLMs ) , recently proposed approaches have achieved remarkable results in dependency-and span-based , multilingual and cross-lingual Semantic Role Labeling ( SRL ) .",0,0.94008726,35.45083140616716,39
2836,"These results have prompted researchers to investigate the inner workings of modern PLMs with the aim of understanding how , where , and to what extent they encode information about SRL .",0,0.69799316,27.824004633630732,32
2836,"In this paper , we follow this line of research and probe for predicate argument structures in PLMs .",1,0.895089,54.359261590202834,19
2836,"Our study shows that PLMs do encode semantic structures directly into the contextualized representation of a predicate , and also provides insights into the correlation between predicate senses and their structures , the degree of transferability between nominal and verbal structures , and how such structures are encoded across languages .",3,0.97708225,46.960936609064625,51
2836,"Finally , we look at the practical implications of such insights and demonstrate the benefits of embedding predicate argument structure information into an SRL model .",3,0.6917341,39.37764317371033,26
2837,We present a study on leveraging multilingual pre-trained generative language models for zero-shot cross-lingual event argument extraction ( EAE ) .,1,0.7983212,22.720804488358695,21
2837,"By formulating EAE as a language generation task , our method effectively encodes event structures and captures the dependencies between arguments .",3,0.55519617,56.136822902461056,22
2837,"We design language-agnostic templates to represent the event argument structures , which are compatible with any language , hence facilitating the cross-lingual transfer .",2,0.7272904,57.25947418900395,26
2837,Our proposed model finetunes multilingual pre-trained generative language models to generate sentences that fill in the language-agnostic template with arguments extracted from the input passage .,2,0.5132727,28.141046832404818,28
2837,The model is trained on source languages and is then directly applied to target languages for event argument extraction .,2,0.6266016,31.929384364957347,20
2837,Experiments demonstrate that the proposed model outperforms the current state-of-the-art models on zero-shot cross-lingual EAE .,3,0.9435339,6.8549227962583625,22
2837,Comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language models for zero-shot cross-lingual transfer EAE .,3,0.8315475,27.532456714683637,27
2838,"Identifying changes in individuals ’ behaviour and mood , as observed via content shared on online platforms , is increasingly gaining importance .",0,0.9397959,136.68302074967204,23
2838,Most research to-date on this topic focuses on either : ( a ) identifying individuals at risk or with a certain mental health condition given a batch of posts or ( b ) providing equivalent labels at the post level .,0,0.9156371,52.397979254260335,42
2838,A disadvantage of such work is the lack of a strong temporal component and the inability to make longitudinal assessments following an individual ’s trajectory and allowing timely interventions .,0,0.85319525,67.50061746409592,30
2838,"Here we define a new task , that of identifying moments of change in individuals on the basis of their shared content online .",1,0.79588515,58.10551766391969,24
2838,The changes we consider are sudden shifts in mood ( switches ) or gradual mood progression ( escalations ) .,2,0.5605409,226.98541170893938,20
2838,We have created detailed guidelines for capturing moments of change and a corpus of 500 manually annotated user timelines ( 18.7 K posts ) .,2,0.726919,86.81703120719058,25
2838,We have developed a variety of baseline models drawing inspiration from related tasks and show that the best performance is obtained through context aware sequential modelling .,3,0.55794924,47.91913689007986,27
2838,We also introduce new metrics for capturing rare events in temporal windows .,3,0.37749866,82.71697357879592,13
2839,Pre-trained language models have been recently shown to benefit task-oriented dialogue ( TOD ) systems .,0,0.92478704,18.24386500365237,18
2839,"Despite their success , existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead .",0,0.8999845,41.35880225204036,30
2839,"In this study , we present PPTOD , a unified plug-and-play model for task-oriented dialogue .",1,0.9039375,27.863138140956078,20
2839,"In addition , we introduce a new dialogue multi-task pre-training strategy that allows the model to learn the primary TOD task completion skills from heterogeneous dialog corpora .",2,0.5497731,46.81172515076517,28
2839,"We extensively test our model on three benchmark TOD tasks , including end-to-end dialogue modelling , dialogue state tracking , and intent classification .",2,0.6645681,58.86208821581064,26
2839,Experimental results show that PPTOD achieves new state of the art on all evaluated tasks in both high-resource and low-resource scenarios .,3,0.98102933,12.45502073653367,23
2839,"Furthermore , comparisons against previous SOTA methods show that the responses generated by PPTOD are more factually correct and semantically coherent as judged by human annotators .",3,0.9665617,41.094192266088776,27
2840,The impression section of a radiology report summarizes the most prominent observation from the findings section and is the most important section for radiologists to communicate to physicians .,0,0.58665615,34.63134765664476,29
2840,"Summarizing findings is time-consuming and can be prone to error for inexperienced radiologists , and thus automatic impression generation has attracted substantial attention .",0,0.9047063,73.30610394837156,26
2840,"With the encoder-decoder framework , most previous studies explore incorporating extra knowledge ( e.g. , static pre-defined clinical ontologies or extra background information ) .",0,0.7985565,87.29807423800774,25
2840,"Yet , they encode such knowledge by a separate encoder to treat it as an extra input to their models , which is limited in leveraging their relations with the original findings .",0,0.7768106,72.83308656054089,33
2840,"To address the limitation , we propose a unified framework for exploiting both extra knowledge and the original findings in an integrated way so that the critical information ( i.e. , key words and their relations ) can be extracted in an appropriate way to facilitate impression generation .",2,0.33888328,40.51998623146307,49
2840,"In detail , for each input findings , it is encoded by a text encoder and a graph is constructed through its entities and dependency tree .",2,0.6361801,80.3648691130795,27
2840,"Then , a graph encoder ( e.g. , graph neural networks ( GNNs ) ) is adopted to model relation information in the constructed graph .",2,0.78087056,34.11225311513391,26
2840,"Finally , to emphasize the key words in the findings , contrastive learning is introduced to map positive samples ( constructed by masking non-key words ) closer and push apart negative ones ( constructed by masking key words ) .",2,0.6029636,60.724866893388885,40
2840,"The experimental results on two datasets , OpenI and MIMIC-CXR , confirm the effectiveness of our proposed method , where the state-of-the-art results are achieved .",3,0.9324025,16.278604943912764,34
2841,Formality style transfer ( FST ) is a task that involves paraphrasing an informal sentence into a formal one without altering its meaning .,0,0.93089193,31.62084807610927,24
2841,"To address the data-scarcity problem of existing parallel datasets , previous studies tend to adopt a cycle-reconstruction scheme to utilize additional unlabeled data , where the FST model mainly benefits from target-side unlabeled sentences .",0,0.87573385,55.98255641873687,36
2841,"In this work , we propose a simple yet effective semi-supervised framework to better utilize source-side unlabeled sentences based on consistency training .",1,0.74047023,22.28493043050705,23
2841,"Specifically , our approach augments pseudo-parallel data obtained from a source-side informal sentence by enforcing the model to generate similar outputs for its perturbed version .",2,0.73604834,64.20393825222804,26
2841,"Moreover , we empirically examined the effects of various data perturbation methods and propose effective data filtering strategies to improve our framework .",3,0.5064887,28.765716108453013,23
2841,"Experimental results on the GYAFC benchmark demonstrate that our approach can achieve state-of-the-art results , even with less than 40 % of the parallel data .",3,0.9410731,14.923335949696314,32
2842,"Multilingual pre-trained language models , such as mBERT and XLM-R , have shown impressive cross-lingual ability .",0,0.8809949,14.479222781925253,19
2842,"Surprisingly , both of them use multilingual masked language model ( MLM ) without any cross-lingual supervision or aligned data .",3,0.46525437,37.686673910620414,21
2842,"Despite the encouraging results , we still lack a clear understanding of why cross-lingual ability could emerge from multilingual MLM .",3,0.6485232,31.12532074683821,21
2842,"In our work , we argue that cross-language ability comes from the commonality between languages .",1,0.40510312,38.33848930499987,16
2842,"Specifically , we study three language properties : constituent order , composition and word co-occurrence .",2,0.7150025,99.4307632762284,16
2842,"First , we create an artificial language by modifying property in source language .",2,0.86505955,101.32206973453157,14
2842,Then we study the contribution of modified property through the change of cross-language transfer results on target language .,1,0.41798088,83.67693142686474,19
2842,"We conduct experiments on six languages and two cross-lingual NLP tasks ( textual entailment , sentence retrieval ) .",2,0.8704293,36.030372685001566,19
2842,"Our main conclusion is that the contribution of constituent order and word co-occurrence is limited , while the composition is more crucial to the success of cross-linguistic transfer .",3,0.98500603,25.616148461408155,29
2843,Word sense disambiguation ( WSD ) is a crucial problem in the natural language processing ( NLP ) community .,0,0.9591337,19.14425221856847,20
2843,Current methods achieve decent performance by utilizing supervised learning and large pre-trained language models .,0,0.79602283,24.749152746173785,15
2843,"However , the imbalanced training dataset leads to poor performance on rare senses and zero-shot senses .",3,0.6307523,46.96439641167911,17
2843,There are more training instances and senses for words with top frequency ranks than those with low frequency ranks in the training dataset .,3,0.89996856,52.051397588491746,24
2843,We investigate the statistical relation between word frequency rank and word sense number distribution .,1,0.6763567,71.04508844244576,15
2843,"Based on the relation , we propose a Z-reweighting method on the word level to adjust the training on the imbalanced dataset .",2,0.6590573,48.99013577135784,23
2843,The experiments show that the Z-reweighting strategy achieves performance gain on the standard English all words WSD benchmark .,3,0.96264356,84.02025745880043,19
2843,"Moreover , the strategy can help models generalize better on rare and zero-shot senses .",3,0.8498464,46.383768761910346,15
2844,"With state-of-the-art systems having finally attained estimated human performance , Word Sense Disambiguation ( WSD ) has now joined the array of Natural Language Processing tasks that have seemingly been solved , thanks to the vast amounts of knowledge encoded into Transformer-based pre-trained language models .",0,0.9534664,22.127181050799646,53
2844,"And yet , if we look below the surface of raw figures , it is easy to realize that current approaches still make trivial mistakes that a human would never make .",0,0.8431722,48.72372587447643,32
2844,"In this work , we provide evidence showing why the F1 score metric should not simply be taken at face value and present an exhaustive analysis of the errors that seven of the most representative state-of-the-art systems for English all-words WSD make on traditional evaluation benchmarks .",1,0.73086834,40.03839570197209,55
2844,"In addition , we produce and release a collection of test sets featuring ( a ) an amended version of the standard evaluation benchmark that fixes its lexical and semantic inaccuracies , ( b) 42D , a challenge set devised to assess the resilience of systems with respect to least frequent word senses and senses not seen at training time , and ( c ) hardEN , a challenge set made up solely of instances which none of the investigated state-of-the-art systems can solve .",3,0.3739213,47.61597144863973,92
2844,We make all of the test sets and model predictions available to the research community at https://github.com/SapienzaNLP/wsd-hard-benchmark .,2,0.44350636,14.487717477468948,18
2845,"We present a word-sense induction method based on pre-trained masked language models ( MLMs ) , which can cheaply scale to large vocabularies and large corpora .",2,0.4247334,22.80544081629245,27
2845,The result is a corpus which is sense-tagged according to a corpus-derived sense inventory and where each sense is associated with indicative words .,2,0.5075193,47.79657997086764,24
2845,"Evaluation on English Wikipedia that was sense-tagged using our method shows that both the induced senses , and the per-instance sense assignment , are of high quality even compared to WSD methods , such as Babelfy .",3,0.8749649,108.91891791318776,37
2845,"Furthermore , by training a static word embeddings algorithm on the sense-tagged corpus , we obtain high-quality static senseful embeddings .",3,0.5956282,32.15456244480521,21
2845,These outperform existing senseful embeddings methods on the WiC dataset and on a new outlier detection dataset we developed .,3,0.8556851,72.13161603294287,20
2845,"The data driven nature of the algorithm allows to induce corpora-specific senses , which may not appear in standard sense inventories , as we demonstrate using a case study on the scientific domain .",3,0.55506015,62.852933907420464,34
2846,Synthetic translations have been used for a wide range of NLP tasks primarily as a means of data augmentation .,0,0.9367985,14.658176078022713,20
2846,"This work explores , instead , how synthetic translations can be used to revise potentially imperfect reference translations in mined bitext .",1,0.79136133,231.1542400695216,22
2846,We find that synthetic samples can improve bitext quality without any additional bilingual supervision when they replace the originals based on a semantic equivalence classifier that helps mitigate NMT noise .,3,0.9770031,110.87723680064363,31
2846,The improved quality of the revised bitext is confirmed intrinsically via human evaluation and extrinsically through bilingual induction and MT tasks .,3,0.924824,166.29547789734968,22
2847,Recent work has identified properties of pretrained self-attention models that mirror those of dependency parse structures .,0,0.89524907,41.752240347938084,17
2847,"In particular , some self-attention heads correspond well to individual dependency types .",3,0.8330081,129.76746447118833,13
2847,"Inspired by these developments , we propose a new competitive mechanism that encourages these attention heads to model different dependency relations .",2,0.34569907,82.82938220459201,22
2847,"We introduce a new model , the Unsupervised Dependency Graph Network ( UDGN ) , that can induce dependency structures from raw corpora and the masked language modeling task .",2,0.57127154,45.054205652577274,30
2847,Experiment results show that UDGN achieves very strong unsupervised dependency parsing performance without gold POS tags and any other external information .,3,0.9824782,79.94162131082929,22
2847,The competitive gated heads show a strong correlation with human-annotated dependency types .,3,0.94602257,59.43666824516793,13
2847,"Furthermore , the UDGN can also achieve competitive performance on masked language modeling and sentence textual similarity tasks .",3,0.87451285,64.95003725102983,19
2848,"Multimodal Entity Linking ( MEL ) which aims at linking mentions with multimodal contexts to the referent entities from a knowledge base ( e.g. , Wikipedia ) , is an essential task for many multimodal applications .",0,0.94499755,34.14763371240911,37
2848,"Although much attention has been paid to MEL , the shortcomings of existing MEL datasets including limited contextual topics and entity types , simplified mention ambiguity , and restricted availability , have caused great obstacles to the research and application of MEL .",0,0.9201627,65.17490044874918,43
2848,"In this paper , we present WikiDiverse , a high-quality human-annotated MEL dataset with diversified contextual topics and entity types from Wikinews , which uses Wikipedia as the corresponding knowledge base .",1,0.7571419,54.57888795598586,32
2848,A well-tailored annotation procedure is adopted to ensure the quality of the dataset .,2,0.5110261,19.872331440863913,16
2848,"Based on WikiDiverse , a sequence of well-designed MEL models with intra-modality and inter-modality attentions are implemented , which utilize the visual information of images more adequately than existing MEL models do .",2,0.5269364,43.04818379251003,35
2848,"Extensive experimental analyses are conducted to investigate the contributions of different modalities in terms of MEL , facilitating the future research on this task .",3,0.7681526,27.777024252743054,25
2849,Knowledge probing is crucial for understanding the knowledge transfer mechanism behind the pre-trained language models ( PLMs ) .,0,0.8844389,33.757825123022236,19
2849,"Despite the growing progress of probing knowledge for PLMs in the general domain , specialised areas such as the biomedical domain are vastly under-explored .",0,0.92335427,44.063316175413306,25
2849,"To facilitate this , we release a well-curated biomedical knowledge probing benchmark , MedLAMA , constructed based on the Unified Medical Language System ( UMLS ) Metathesaurus .",2,0.5555551,133.03663257114556,28
2849,"We test a wide spectrum of state-of-the-art PLMs and probing approaches on our benchmark , reaching at most 3 % of acc@10 .",3,0.5387273,69.52980332937089,27
2849,"While highlighting various sources of domain-specific challenges that amount to this underwhelming performance , we illustrate that the underlying PLMs have a higher potential for probing tasks .",3,0.8518819,92.484358034854,28
2849,"To achieve this , we propose Contrastive-Probe , a novel self-supervised contrastive probing approach , that adjusts the underlying PLMs without using any probing data .",2,0.46000308,37.453182925728015,28
2849,"While Contrastive-Probe pushes the acc@10 to 28 % , the performance gap still remains notable .",3,0.9357797,237.8954572018935,18
2849,Our human expert evaluation suggests that the probing performance of our Contrastive-Probe is still under-estimated as UMLS still does not include the full spectrum of factual knowledge .,3,0.9725375,41.67529329727349,30
2849,We hope MedLAMA and Contrastive-Probe facilitate further developments of more suited probing techniques for this domain .,3,0.93467236,149.96704261371772,19
2849,Our code and dataset are publicly available at https://github.com/cambridgeltl/medlama .,3,0.5695809,13.640405682874652,10
2850,"Transformer-based pre-trained models , such as BERT , have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications .",0,0.86223555,8.28564783303995,31
2850,"However , deploying these models can be prohibitively costly , as the standard self-attention mechanism of the Transformer suffers from quadratic computational cost in the input sequence length .",0,0.56781304,26.514104077685765,29
2850,"To confront this , we propose FCA , a fine-and coarse-granularity hybrid self-attention that reduces the computation cost through progressively shortening the computational sequence length in self-attention .",1,0.42530608,60.267014060614855,31
2850,"Specifically , FCA conducts an attention-based scoring strategy to determine the informativeness of tokens at each layer .",2,0.5998826,64.98321522534398,20
2850,"Then , the informative tokens serve as the fine-granularity computing units in self-attention and the uninformative tokens are replaced with one or several clusters as the coarse-granularity computing units in self-attention .",2,0.6606235,24.815742582565402,34
2850,Experiments on the standard GLUE benchmark show that BERT with FCA achieves 2x reduction in FLOPs over original BERT with < 1 % loss in accuracy .,3,0.91466326,35.146381734630616,27
2850,We show that FCA offers a significantly better trade-off between accuracy and FLOPs compared to prior methods .,3,0.972073,26.6705329829596,19
2851,The increasing size of generative Pre-trained Language Models ( PLMs ) have greatly increased the demand for model compression .,0,0.9416743,26.666641711504894,20
2851,"Despite various methods to compress BERT or its variants , there are few attempts to compress generative PLMs , and the underlying difficulty remains unclear .",0,0.85686517,53.13060076982298,26
2851,"In this paper , we compress generative PLMs by quantization .",1,0.6309252,118.61802662363655,11
2851,We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights .,3,0.9616043,33.66369266347361,27
2851,"Correspondingly , we propose a token-level contrastive distillation to learn distinguishable word embeddings , and a module-wise dynamic scaling to make quantizers adaptive to different modules .",2,0.58326584,54.030550513655804,30
2851,Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin .,3,0.94256425,9.089396347965554,29
2851,"With comparable performance with the full-precision models , we achieve 14.4x and 13.4x compression rate on GPT-2 and BART , respectively .",3,0.93631613,26.580752626221816,24
2852,Vision-language navigation ( VLN ) is a challenging task due to its large searching space in the environment .,0,0.9624198,32.69960053041406,21
2852,"To address this problem , previous works have proposed some methods of fine-tuning a large model that pretrained on large-scale datasets .",0,0.91186714,22.37572520502775,22
2852,"However , the conventional fine-tuning methods require extra human-labeled navigation data and lack self-exploration capabilities in environments , which hinders their generalization of unseen scenes .",0,0.84510356,58.182982249390555,26
2852,"To improve the ability of fast cross-domain adaptation , we propose Prompt-based Environmental Self-exploration ( ProbES ) , which can self-explore the environments by sampling trajectories and automatically generates structured instructions via a large-scale cross-modal pretrained model ( CLIP ) .",2,0.5954567,49.505918020547114,42
2852,Our method fully utilizes the knowledge learned from CLIP to build an in-domain dataset by self-exploration without human labeling .,2,0.4955309,45.660637843193,20
2852,"Unlike the conventional approach of fine-tuning , we introduce prompt tuning to achieve fast adaptation for language embeddings , which substantially improves the learning efficiency by leveraging prior knowledge .",2,0.5368452,34.83394778999549,30
2852,"By automatically synthesizing trajectory-instruction pairs in any environment without human supervision and instruction prompt tuning , our model can adapt to diverse vision-language navigation tasks , including VLN and REVERIE .",3,0.71154535,104.95918976300024,35
2852,Both qualitative and quantitative results show that our ProbES significantly improves the generalization ability of the navigation model .,3,0.98750067,51.42502843890129,19
2853,Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses .,0,0.915436,23.610767815001907,23
2853,"In this paper , we propose a new dialog pre-training framework called DialogVED , which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and diversity of responses .",1,0.8389805,20.730567395141467,34
2853,"With the help of a large dialog corpus ( Reddit ) , we pre-train the model using the following 4 tasks , used in training language models ( LMs ) and Variational Autoencoders ( VAEs ) literature : 1 ) masked language model ;",2,0.8235258,57.28454423810794,44
2853,2 ) response generation ; 3 ) bag-of-words prediction ;,2,0.695941,189.861884213356,10
2853,and 4 ) KL divergence reduction .,2,0.63699627,1518.5909212860197,7
2853,We also add additional parameters to model the turn structure in dialogs to improve the performance of the pre-trained model .,2,0.57903093,25.862764736518802,21
2853,"We conduct experiments on PersonaChat , DailyDialog , and DSTC7-AVSD benchmarks for response generation .",2,0.84067714,158.6027918507207,17
2853,Experimental results show that our model achieves the new state-of-the-art results on all these datasets .,3,0.97500867,5.2140514019946,22
2854,We study the problem of coarse-grained response selection in retrieval-based dialogue systems .,1,0.7317638,21.671697092118784,15
2854,"The problem is equally important with fine-grained response selection , but is less explored in existing literature .",0,0.737356,47.66824447387644,19
2854,"In this paper , we propose a Contextual Fine-to-Coarse ( CFC ) distilled model for coarse-grained response selection in open-domain conversations .",1,0.91242397,28.931372981708872,24
2854,"In our CFC model , dense representations of query , candidate contexts and responses is learned based on the multi-tower architecture using contextual matching , and richer knowledge learned from the one-tower architecture ( fine-grained ) is distilled into the multi-tower architecture ( coarse-grained ) to enhance the performance of the retriever .",2,0.7294021,46.297800376852194,55
2854,"To evaluate the performance of the proposed model , we construct two new datasets based on the Reddit comments dump and Twitter corpus .",2,0.8031001,39.27918866219262,24
2854,Extensive experimental results on the two datasets show that the proposed method achieves huge improvement over all evaluation metrics compared with traditional baseline methods .,3,0.9407108,14.189723905151011,25
2855,Summarizing biomedical discovery from genomics data using natural languages is an essential step in biomedical research but is mostly done manually .,0,0.9582183,47.08503279178305,22
2855,"Here , we introduce Textomics , a novel dataset of genomics data description , which contains 22,273 pairs of genomics data matrices and their summaries .",1,0.51652455,52.12021937838067,26
2855,Each summary is written by the researchers who generated the data and associated with a scientific paper .,2,0.5148742,36.288296379536646,18
2855,"Based on this dataset , we study two novel tasks : generating textual summary from a genomics data matrix and vice versa .",2,0.675179,83.79899549707764,23
2855,"Inspired by the successful applications of k nearest neighbors in modeling genomics data , we propose a kNN-Vec2 Text model to address these tasks and observe substantial improvement on our dataset .",2,0.450772,70.70028149549724,34
2855,"We further illustrate how Textomics can be used to advance other applications , including evaluating scientific paper embeddings and generating masked templates for scientific paper understanding .",3,0.7227834,81.40537007078288,27
2855,Textomics serves as the first benchmark for generating textual summaries for genomics data and we envision it will be broadly applied to other biomedical and natural language processing applications .,3,0.7709312,29.341182659781772,30
2856,Learning high-quality sentence representations is a fundamental problem of natural language processing which could benefit a wide range of downstream tasks .,0,0.940063,12.277026709309796,22
2856,"Though the BERT-like pre-trained language models have achieved great success , using their sentence representations directly often results in poor performance on the semantic textual similarity task .",0,0.79162633,21.438004995621586,30
2856,"Recently , several contrastive learning methods have been proposed for learning sentence representations and have shown promising results .",0,0.90931743,13.490123134374686,19
2856,"However , most of them focus on the constitution of positive and negative representation pairs and pay little attention to the training objective like NT-Xent , which is not sufficient enough to acquire the discriminating power and is unable to model the partial order of semantics between sentences .",0,0.71200556,70.64245447070711,51
2856,"So in this paper , we propose a new method ArcCSE , with training objectives designed to enhance the pairwise discriminative power and model the entailment relation of triplet sentences .",1,0.86226994,46.17007661037623,31
2856,"We conduct extensive experiments which demonstrate that our approach outperforms the previous state-of-the-art on diverse sentence related tasks , including STS and SentEval .",3,0.7355884,15.421153715881797,30
2857,Recent entity and relation extraction works focus on investigating how to obtain a better span representation from the pre-trained encoder .,0,0.88948107,37.676414205303786,21
2857,"However , a major limitation of existing works is that they ignore the interrelation between spans ( pairs ) .",0,0.82509595,42.09414659277725,20
2857,"In this work , we propose a novel span representation approach , named Packed Levitated Markers ( PL-Marker ) , to consider the interrelation between the spans ( pairs ) by strategically packing the markers in the encoder .",1,0.7427136,58.64971960029695,41
2857,"In particular , we propose a neighborhood-oriented packing strategy , which considers the neighbor spans integrally to better model the entity boundary information .",2,0.5952003,73.49566530118567,25
2857,"Furthermore , for those more complicated span pair classification tasks , we design a subject-oriented packing strategy , which packs each subject and all its objects to model the interrelation between the same-subject span pairs .",2,0.80157423,97.54080148264579,36
2857,"The experimental results show that , with the enhanced marker feature , our model advances baselines on six NER benchmarks , and obtains a 4.1 %-4.3 % strict relation F1 improvement with higher speed over previous state-of-the-art models on ACE04 and ACE05 .",3,0.9599942,37.590108067214544,49
2857,Our code and models are publicly available at https://github.com/thunlp/PL-Marker .,3,0.5236908,6.081597773774882,10
2858,We study the interpretability issue of task-oriented dialogue systems in this paper .,1,0.782971,29.42512237756218,15
2858,"Previously , most neural-based task-oriented dialogue systems employ an implicit reasoning strategy that makes the model predictions uninterpretable to humans .",0,0.9404574,35.14529240996101,25
2858,"To obtain a transparent reasoning process , we introduce neuro-symbolic to perform explicit reasoning that justifies model decisions by reasoning chains .",2,0.68161076,126.66177051144666,22
2858,"Since deriving reasoning chains requires multi-hop reasoning for task-oriented dialogues , existing neuro-symbolic approaches would induce error propagation due to the one-phase design .",0,0.6777895,68.7589584493708,27
2858,"To overcome this , we propose a two-phase approach that consists of a hypothesis generator and a reasoner .",2,0.5648241,18.56622890654844,21
2858,"We first obtain multiple hypotheses , i.e. , potential operations to perform the desired task , through the hypothesis generator .",2,0.78454757,106.06650623689113,21
2858,"Each hypothesis is then verified by the reasoner , and the valid one is selected to conduct the final prediction .",2,0.5552957,67.20630111564208,21
2858,The whole system is trained by exploiting raw textual dialogues without using any reasoning chain annotations .,2,0.631053,105.83410952894769,17
2858,"Experimental studies on two public benchmark datasets demonstrate that the proposed approach not only achieves better results , but also introduces an interpretable decision process .",3,0.90222555,21.10260043966133,26
2859,"There has been a growing interest in developing machine learning ( ML ) models for code summarization tasks , e.g. , comment generation and method naming .",0,0.9606083,38.61817426256402,27
2859,"Despite substantial increase in the effectiveness of ML models , the evaluation methodologies , i.e. , the way people split datasets into training , validation , and test sets , were not well studied .",0,0.862789,74.80089322014939,35
2859,"Specifically , no prior work on code summarization considered the timestamps of code and comments during evaluation .",0,0.5964886,48.95567974603532,18
2859,This may lead to evaluations that are inconsistent with the intended use cases .,3,0.5044071,16.278899911846537,14
2859,"In this paper , we introduce the time-segmented evaluation methodology , which is novel to the code summarization research community , and compare it with the mixed-project and cross-project methodologies that have been commonly used .",1,0.8656755,30.901699017069802,38
2859,"Each methodology can be mapped to some use cases , and the time-segmented methodology should be adopted in the evaluation of ML models for code summarization .",3,0.81636465,54.214184000771795,28
2859,"To assess the impact of methodologies , we collect a dataset of ( code , comment ) pairs with timestamps to train and evaluate several recent ML models for code summarization .",2,0.8192433,70.20553637161152,32
2859,Our experiments show that different methodologies lead to conflicting evaluation results .,3,0.97749364,36.83351333503044,12
2859,We invite the community to expand the set of methodologies used in evaluations .,3,0.58774084,42.277098345918674,14
2860,"Current Open-Domain Question Answering ( ODQA ) models typically include a retrieving module and a reading module , where the retriever selects potentially relevant passages from open-source documents for a given question , and the reader produces an answer based on the retrieved passages .",0,0.9323827,19.399719327842423,45
2860,"The recently proposed Fusion-in-Decoder ( FiD ) framework is a representative example , which is built on top of a dense passage retriever and a generative reader , achieving the state-of-the-art performance .",0,0.5651138,26.715858475414475,41
2860,"In this paper we further improve the FiD approach by introducing a knowledge-enhanced version , namely KG-FiD .",1,0.6303179,33.0136877100679,22
2860,"Our new model uses a knowledge graph to establish the structural relationship among the retrieved passages , and a graph neural network ( GNN ) to re-rank the passages and select only a top few for further processing .",2,0.7766199,37.52644094442363,39
2860,"Our experiments on common ODQA benchmark datasets ( Natural Questions and TriviaQA ) demonstrate that KG-FiD can achieve comparable or better performance in answer prediction than FiD , with less than 40 % of the computation cost .",3,0.94104534,42.19072350002719,40
2861,Social media is a breeding ground for threat narratives and related conspiracy theories .,0,0.90333855,44.13156025460401,14
2861,Insiders – agents with whom the authors identify and Outsiders – agents who threaten the insiders .,0,0.50627786,188.43818236463832,17
2861,Inferring the members of these groups constitutes a challenging new NLP task : ( i ) Information is distributed over many poorly-constructed posts ;,0,0.83686674,122.345032477001,26
2861,"( ii ) Threats and threat agents are highly contextual , with the same post potentially having multiple agents assigned to membership in either group ;",3,0.66583234,410.7498493950124,26
2861,An agent ’s identity is often implicit and transitive ;,0,0.8843353,148.21478500780728,10
2861,and ( iv ) Phrases used to imply Outsider status often do not follow common negative sentiment patterns .,3,0.8164582,199.98847404400897,19
2861,"To address these challenges , we define a novel Insider-Outsider classification task .",1,0.5869547,60.44153076369836,15
2861,"Because we are not aware of any appropriate existing datasets or attendant models , we introduce a labeled dataset ( CT5 K ) and design a model ( NP2IO ) to address this task .",2,0.7849465,102.26600439142165,35
2861,NP2IO leverages pretrained language modeling to classify Insiders and Outsiders .,2,0.43777716,102.41293894660002,11
2861,"NP2IO is shown to be robust , generalizing to noun phrases not seen during training , and exceeding the performance of non-trivial baseline models by 20 % .",3,0.87064093,47.60382578841922,28
2862,Most low resource language technology development is premised on the need to collect data for training statistical models .,0,0.9178564,52.36482175930962,19
2862,"When we follow the typical process of recording and transcribing text for small Indigenous languages , we hit up against the so-called “ transcription bottleneck .",0,0.82445145,76.45948538800383,26
2862,Therefore it is worth exploring new ways of engaging with speakers which generate data while avoiding the transcription bottleneck .,3,0.6373938,72.77056554124454,20
2862,We have deployed a prototype app for speakers to use for confirming system guesses in an approach to transcription based on word spotting .,2,0.6094394,122.46801379764703,24
2862,"However , in the process of testing the app we encountered many new problems for engagement with speakers .",3,0.859,86.86378166519198,19
2862,This paper presents a close-up study of the process of deploying data capture technology on the ground in an Australian Aboriginal community .,1,0.85193443,23.805874201615072,25
2862,We reflect on our interactions with participants and draw lessons that apply to anyone seeking to develop methods for language data collection in an Indigenous community .,3,0.6700564,39.491152150231365,27
2863,Multi-hop reading comprehension requires an ability to reason across multiple documents .,0,0.8848595,25.57673134413194,12
2863,"On the one hand , deep learning approaches only implicitly encode query-related information into distributed embeddings which fail to uncover the discrete relational reasoning process to infer the correct answer .",0,0.84789056,51.19567813387759,32
2863,"On the other hand , logic-based approaches provide interpretable rules to infer the target answer , but mostly work on structured data where entities and relations are well-defined .",0,0.88620925,46.93251765408493,32
2863,"In this paper , we propose a deep-learning based inductive logic reasoning method that firstly extracts query-related ( candidate-related ) information , and then conducts logic reasoning among the filtered information by inducing feasible rules that entail the target relation .",1,0.7443022,56.86925124061895,44
2863,The reasoning process is accomplished via attentive memories with novel differentiable logic operators .,2,0.5399153,246.22508319440297,14
2863,"To demonstrate the effectiveness of our model , we evaluate it on two reading comprehension datasets , namely WikiHop and MedHop .",2,0.6435724,30.521279518911633,22
2864,This paper addresses the problem of dialogue reasoning with contextualized commonsense inference .,1,0.88182265,35.40944856027789,13
2864,"We curate CICERO , a dataset of dyadic conversations with five types of utterance-level reasoning-based inferences : cause , subsequent event , prerequisite , motivation , and emotional reaction .",2,0.87367517,90.11943319625972,34
2864,"The dataset contains 53,105 of such inferences from 5,672 dialogues .",3,0.48298222,67.70715364165598,11
2864,We use this dataset to solve relevant generative and discriminative tasks : generation of cause and subsequent event ;,2,0.7269894,164.35125867056783,19
2864,"generation of prerequisite , motivation , and listener ’s emotional reaction ;",0,0.40526932,859.6702465990877,12
2864,and selection of plausible alternatives .,0,0.60359126,187.608770685957,6
2864,Our results ascertain the value of such dialogue-centric commonsense knowledge datasets .,3,0.98917496,92.55741644039841,12
2864,It is our hope that CICERO will open new research avenues into commonsense-based dialogue reasoning .,3,0.86572665,36.07641177576508,18
2865,Interpretable methods to reveal the internal reasoning processes behind machine learning models have attracted increasing attention in recent years .,0,0.95674926,18.11802123238161,20
2865,"To quantify the extent to which the identified interpretations truly reflect the intrinsic decision-making mechanisms , various faithfulness evaluation metrics have been proposed .",0,0.8151285,47.509886859211676,24
2865,"However , we find that different faithfulness metrics show conflicting preferences when comparing different interpretations .",3,0.98189753,55.32326953814264,16
2865,"Motivated by this observation , we aim to conduct a comprehensive and comparative study of the widely adopted faithfulness metrics .",1,0.91909385,28.84738364021238,21
2865,"In particular , we introduce two assessment dimensions , namely diagnosticity and complexity .",2,0.5559596,190.01105096237498,14
2865,"Diagnosticity refers to the degree to which the faithfulness metric favors relatively faithful interpretations over randomly generated ones , and complexity is measured by the average number of model forward passes .",2,0.45739213,74.21333814852399,32
2865,"According to the experimental results , we find that sufficiency and comprehensiveness metrics have higher diagnosticity and lower complexity than the other faithfulness metrics .",3,0.98246,40.03152324571942,25
2866,There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks .,0,0.9555427,9.93572508350632,19
2866,Building on the Prompt Tuning approach of Lester et al .,3,0.42110187,57.40507714772597,11
2866,"( 2021 ) , which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks , we propose a novel prompt-based transfer learning approach called SPoT : Soft Prompt Transfer .",2,0.559432,72.5422533615401,37
2866,SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task .,2,0.61271447,23.889534031352472,24
2866,We show that SPoT significantly boosts the performance of Prompt Tuning across many tasks .,3,0.96785396,61.94819229481107,15
2866,"More remarkably , across all model sizes , SPoT matches or outperforms standard Model Tuning ( which fine-tunes all model parameters ) on the SuperGLUE benchmark , while using up to 27,000 × fewer task-specific parameters .",3,0.94427884,71.54950279235655,39
2866,"To understand where SPoT is most effective , we conduct a large-scale study on task transferability with 26 NLP tasks in 160 combinations , and demonstrate that many tasks can benefit each other via prompt transfer .",2,0.45839748,56.62390877642717,37
2866,"Finally , we propose an efficient retrieval approach that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task .",3,0.61929244,43.54456410475894,32
2867,Selecting an appropriate pre-trained model ( PTM ) for a specific downstream task typically requires significant efforts of fine-tuning .,0,0.8604842,23.212451671670852,20
2867,"To accelerate this process , researchers propose feature-based model selection ( FMS ) methods , which assess PTMs ’ transferability to a specific task in a fast way without fine-tuning .",0,0.8655422,92.88906444926381,33
2867,"In this work , we argue that current FMS methods are vulnerable , as the assessment mainly relies on the static features extracted from PTMs .",1,0.6377958,85.97591769399006,26
2867,"However , such features are derived without training PTMs on downstream tasks , and are not necessarily reliable indicators for the PTM ’s transferability .",0,0.7377441,98.10806585081141,25
2867,"To validate our viewpoints , we design two methods to evaluate the robustness of FMS : ( 1 ) model disguise attack , which post-trains an inferior PTM with a contrastive objective , and ( 2 ) evaluation data selection , which selects a subset of the data points for FMS evaluation based on K-means clustering .",2,0.8163911,63.010596929732564,57
2867,Experimental results prove that both methods can successfully make FMS mistakenly judge the transferability of PTMs .,3,0.9766198,120.44566404762246,17
2867,"Moreover , we find that these two methods can further be combined with the backdoor attack to misguide the FMS to select poisoned models .",3,0.9749307,80.79058892774187,25
2867,"To the best of our knowledge , this is the first work to demonstrate the defects of current FMS algorithms and evaluate their potential security risks .",3,0.9250087,24.423433255798898,27
2867,"By identifying previously unseen risks of FMS , our study indicates new directions for improving the robustness of FMS .",3,0.9880074,42.917533262302015,20
2868,Generating educational questions of fairytales or storybooks is vital for improving children ’s literacy ability .,0,0.8542863,66.9079717380206,16
2868,"However , it is challenging to generate questions that capture the interesting aspects of a fairytale story with educational meaningfulness .",0,0.87694055,43.4206570608215,21
2868,"In this paper , we propose a novel question generation method that first learns the question type distribution of an input story paragraph , and then summarizes salient events which can be used to generate high-cognitive-demand questions .",1,0.882136,34.30867524742456,41
2868,"To train the event-centric summarizer , we finetune a pre-trained transformer-based sequence-to-sequence model using silver samples composed by educational question-answer pairs .",2,0.8572643,29.256533736700025,28
2868,"On a newly proposed educational question-answering dataset FairytaleQA , we show good performance of our method on both automatic and human evaluation metrics .",3,0.8210393,21.488327690669,26
2868,Our work indicates the necessity of decomposing question type distribution learning and event-centric summary generation for educational question generation .,3,0.98686314,88.83846708647715,20
2869,"Recently , various response generation models for two-party conversations have achieved impressive improvements , but less effort has been paid to multi-party conversations ( MPCs ) which are more practical and complicated .",0,0.9282542,55.120803639776504,34
2869,"Compared with a two-party conversation where a dialogue context is a sequence of utterances , building a response generation model for MPCs is more challenging , since there exist complicated context structures and the generated responses heavily rely on both interlocutors ( i.e. , speaker and addressee ) and history utterances .",0,0.65448546,53.1338183671231,53
2869,"To address these challenges , we present HeterMPC , a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph .",1,0.48630312,25.78663239930055,38
2869,"Besides , we also design six types of meta relations with node-edge-type-dependent parameters to characterize the heterogeneous interactions within the graph .",2,0.8204806,72.98706615198644,22
2869,"Through multi-hop updating , HeterMPC can adequately utilize the structural knowledge of conversations for response generation .",3,0.7382046,134.69214564054508,17
2869,Experimental results on the Ubuntu Internet Relay Chat ( IRC ) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs .,3,0.8483208,49.89134286066067,26
2870,"Although multi-document summarisation ( MDS ) of the biomedical literature is a highly valuable task that has recently attracted substantial interest , evaluation of the quality of biomedical summaries lacks consistency and transparency .",0,0.96950793,37.568381061844775,34
2870,"In this paper , we examine the summaries generated by two current models in order to understand the deficiencies of existing evaluation approaches in the context of the challenges that arise in the MDS task .",1,0.90804267,30.776682318034144,36
2870,"Based on this analysis , we propose a new approach to human evaluation and identify several challenges that must be overcome to develop effective biomedical MDS systems .",3,0.85777164,27.717709811521402,28
2871,"Multi-document summarization ( MDS ) has made significant progress in recent years , in part facilitated by the availability of new , dedicated datasets and capacious language models .",0,0.9663115,34.73126617062445,29
2871,"However , a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives .",0,0.8408347,104.74005651422377,23
2871,"As for many other generative tasks , reinforcement learning ( RL ) offers the potential to improve the training of MDS models ;",0,0.8711714,76.79351857768064,23
2871,"yet , it requires a carefully-designed reward that can ensure appropriate leverage of both the reference summaries and the input documents .",0,0.71729267,67.74083557072849,24
2871,"For this reason , in this paper we propose fine-tuning an MDS baseline with a reward that balances a reference-based metric such as ROUGE with coverage of the input documents .",1,0.60698754,35.13412459021395,33
2871,"To implement the approach , we utilize RELAX ( Grathwohl et al. , 2018 ) , a contemporary gradient estimator which is both low-variance and unbiased , and we fine-tune the baseline in a few-shot style for both stability and computational efficiency .",2,0.68303,51.01048269449516,45
2871,"Experimental results over the Multi-News and WCEP MDS datasets show significant improvements of up to + 0.95 pp average ROUGE score and + 3.17 pp METEOR score over the baseline , and competitive results with the literature .",3,0.92463046,33.318929487387635,38
2871,"In addition , they show that the coverage of the input documents is increased , and evenly across all documents .",3,0.9550724,76.61324037971966,21
2872,The Out-of-Domain ( OOD ) intent classification is a basic and challenging task for dialogue systems .,0,0.9564763,22.01927552666238,17
2872,"Previous methods commonly restrict the region ( in feature space ) of In-domain ( IND ) intent features to be compact or simply-connected implicitly , which assumes no OOD intents reside , to learn discriminative semantic features .",0,0.7810033,206.97760668488723,40
2872,Then the distribution of the IND intent features is often assumed to obey a hypothetical distribution ( Gaussian mostly ) and samples outside this distribution are regarded as OOD samples .,0,0.76174605,101.65676181377357,31
2872,"In this paper , we start from the nature of OOD intent classification and explore its optimization objective .",1,0.83376795,65.37460103033996,19
2872,"We further propose a simple yet effective method , named KNN-contrastive learning .",3,0.46023175,37.55267376830431,15
2872,Our approach utilizes k-nearest neighbors ( KNN ) of IND intents to learn discriminative semantic features that are more conducive to OOD detection .,2,0.78325254,40.4087029480731,24
2872,"Notably , the density-based novelty detection algorithm is so well-grounded in the essence of our method that it is reasonable to use it as the OOD detection algorithm without making any requirements for the feature distribution .",3,0.8925154,38.060240081803066,41
2872,Extensive experiments on four public datasets show that our approach can not only enhance the OOD detection performance substantially but also improve the IND intent classification while requiring no restrictions on feature distribution .,3,0.9134229,41.96405031231612,34
2873,Program understanding is a fundamental task in program language processing .,0,0.9535052,34.14180494582941,11
2873,"Despite the success , existing works fail to take human behaviors as reference in understanding programs .",0,0.91382456,86.28381856799278,17
2873,"In this paper , we consider human behaviors and propose the PGNN-EK model that consists of two main components .",1,0.7429234,53.108538876049565,22
2873,"On the one hand , inspired by the “ divide-and-conquer ” reading behaviors of humans , we present a partitioning-based graph neural network model PGNN on the upgraded AST of codes .",2,0.71976715,61.561700982300415,37
2873,"On the other hand , to characterize human behaviors of resorting to other resources to help code comprehension , we transform raw codes with external knowledge and apply pre-training techniques for information extraction .",2,0.60088843,86.5549255657427,34
2873,"Finally , we combine the two embeddings generated from the two components to output code embeddings .",2,0.74771184,30.59077958286571,17
2873,We conduct extensive experiments to show the superior performance of PGNN-EK on the code summarization and code clone detection tasks .,3,0.6165893,52.965650138031336,23
2873,"In particular , to show the generalization ability of our model , we release a new dataset that is more challenging for code clone detection and could advance the development of the community .",3,0.65196306,39.02452491086001,34
2873,Our codes and data are publicly available at https://github.com/RecklessRonan/PGNN-EK .,3,0.52829456,17.321698147076457,10
2874,"Despite significant interest in developing general purpose fact checking models , it is challenging to construct a large-scale fact verification dataset with realistic real-world claims .",0,0.940316,28.24313987047506,26
2874,"Existing claims are either authored by crowdworkers , thereby introducing subtle biases thatare difficult to control for , or manually verified by professional fact checkers , causing them to be expensive and limited in scale .",0,0.8762689,75.88375667737584,36
2874,"In this paper , we construct a large-scale challenging fact verification dataset called FAVIQ , consisting of 188 k claims derived from an existing corpus of ambiguous information-seeking questions .",1,0.58625454,53.12210167638706,31
2874,"The ambiguities in the questions enable automatically constructing true and false claims that reflect user confusions ( e.g. , the year of the movie being filmed vs .",3,0.65435237,60.001262086222034,28
2874,being released ) .,0,0.4705126,997.7995328407378,4
2874,"Claims in FAVIQ are verified to be natural , contain little lexical bias , and require a complete understanding of the evidence for verification .",3,0.5784886,89.02123281751857,25
2874,Our experiments show that the state-of-the-art models are far from solving our new task .,3,0.97420293,9.660707474679576,21
2874,"Moreover , training on our data helps in professional fact-checking , outperforming models trained on the widely used dataset FEVER or in-domain data by up to 17 % absolute .",3,0.96039295,56.15647414357052,32
2874,"Altogether , our data will serve as a challenging benchmark for natural language understanding and support future progress in professional fact checking .",3,0.9892266,38.53305473681262,23
2875,We study learning from user feedback for extractive question answering by simulating feedback using supervised data .,2,0.5036608,65.62799772217974,17
2875,"We cast the problem as contextual bandit learning , and analyze the characteristics of several learning scenarios with focus on reducing data annotation .",2,0.6396009,130.2932517934926,24
2875,"We show that systems initially trained on few examples can dramatically improve given feedback from users on model-predicted answers , and that one can use existing datasets to deploy systems in new domains without any annotation effort , but instead improving the system on-the-fly via user feedback .",3,0.8868758,55.78001830978108,53
2876,"Despite recent improvements in open-domain dialogue models , state of the art models are trained and evaluated on short conversations with little context .",0,0.90613955,29.30795894839722,24
2876,"In contrast , the long-term conversation setting has hardly been studied .",0,0.9324599,68.87236313814886,12
2876,In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each other ’s interests and discuss the things they have learnt from past sessions .,2,0.51955456,41.08776553066321,36
2876,"We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations , and we study long-context models that can perform much better .",3,0.6055205,39.81491270849567,34
2876,"In particular , we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art .",3,0.95938116,50.094732987479865,32
2877,"Training a referring expression comprehension ( ReC ) model for a new visual domain requires collecting referring expressions , and potentially corresponding bounding boxes , for images in the domain .",0,0.77856815,154.68250841037354,31
2877,"While large-scale pre-trained models are useful for image classification across domains , it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC .",0,0.87975883,18.34227119934695,31
2877,"We present ReCLIP , a simple but strong zero-shot baseline that repurposes CLIP , a state-of-the-art large-scale model , for ReC .",1,0.3334335,45.02509415120944,28
2877,"Motivated by the close connection between ReC and CLIP ’s contrastive pre-training objective , the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring , and passes them to CLIP .",0,0.4026471,61.15005986167657,38
2877,"However , through controlled experiments on a synthetic dataset , we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf .",3,0.91346467,34.01041766695506,24
2877,"We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29 % on RefCOCOg , and on RefGTA ( video game imagery ) , ReCLIP ’s relative improvement over supervised ReC models trained on real images is 8 % .",3,0.8766536,107.5756717147049,47
2878,We consider event extraction in a generative manner with template-based conditional generation .,2,0.7813254,48.87229644092085,15
2878,"Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts , these generation-based methods have two significant challenges , including using suboptimal prompts and static event type information .",0,0.8695563,61.96402732950505,41
2878,"In this paper , we propose a generative template-based event extraction method with dynamic prefix ( GTEE-DynPref ) by integrating context information with type-specific prefixes to learn a context-specific prefix for each context .",1,0.82687026,56.138589626386235,38
2878,Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE .,3,0.9633364,16.21934742127509,34
2878,"Additionally , our model is proven to be portable to new types of events effectively .",3,0.9084607,60.88233004280514,16
2879,Building huge and highly capable language models has been a trend in the past years .,0,0.94354194,43.95429897568525,16
2879,"Despite their great performance , they incur high computational cost .",0,0.87645674,68.53702271426312,11
2879,"A common solution is to apply model compression or choose light-weight architectures , which often need a separate fixed-size model for each desirable computational budget , and may lose performance in case of heavy compression .",0,0.79965407,92.56796526236722,40
2879,"This paper proposes an effective dynamic inference approach , called E-LANG , which distributes the inference between large accurate Super-models and light-weight Swift models .",1,0.7670241,102.55035308988884,27
2879,"To this end , a decision making module routes the inputs to Super or Swift models based on the energy characteristics of the representations in the latent space .",2,0.6266923,92.56266863244691,29
2879,This method is easily adoptable and architecture agnostic .,3,0.6333689,50.215280372442635,9
2879,"As such , it can be applied to black-box pre-trained models without a need for architectural manipulations , reassembling of modules , or re-training .",3,0.6631131,36.56956081894395,27
2879,"Unlike existing methods that are only applicable to encoder-only backbones and classification tasks , our method also works for encoder-decoder structures and sequence-to-sequence tasks such as translation .",3,0.6167048,32.82361524831457,30
2879,"The E-LANG performance is verified through a set of experiments with T5 and BERT backbones on GLUE , SuperGLUE , and WMT .",3,0.54057735,38.95128615245055,23
2879,"In particular , we outperform T5-11B with an average computations speed-up of 3.3X on GLUE and 2.9X on SuperGLUE .",3,0.9370899,25.62843331867954,24
2879,We also achieve BERT-based SOTA on GLUE with 3.2X less computations .,3,0.8615613,28.37024057688963,14
2879,Code and demo are available in supplementary materials .,3,0.4541963,64.51595488008304,9
2880,"We introduce PRIMERA , a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data .",2,0.3624229,26.100943776494795,31
2880,PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents .,2,0.6166838,64.47151678280188,20
2880,It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents .,2,0.3457211,20.522176277568352,15
2880,"With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot , few-shot and full-supervised settings , PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins .",3,0.7870417,20.870745390922306,44
2881,"Extracting informative arguments of events from news articles is a challenging problem in information extraction , which requires a global contextual understanding of each document .",0,0.93044055,38.86552506688609,26
2881,"While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models , they are still restricted by certain input sequence length constraints and usually ignore the global context between events .",0,0.9015248,23.214897952211647,42
2881,"To tackle this issue , we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events .",2,0.45257574,60.51991639510629,50
2881,Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design .,3,0.95148826,24.602667493378352,25
2882,There is a growing interest in the combined use of NLP and machine learning methods to predict gaze patterns during naturalistic reading .,0,0.9467241,19.11225098304125,23
2882,"While promising results have been obtained through the use of transformer-based language models , little work has been undertaken to relate the performance of such models to general text characteristics .",0,0.87500167,18.697089184557477,33
2882,In this paper we report on experiments with two eye-tracking corpora of naturalistic reading and two language models ( BERT and GPT-2 ) .,1,0.76769626,20.31805006842448,26
2882,"In all experiments , we test effects of a broad spectrum of features for predicting human reading behavior that fall into five categories ( syntactic complexity , lexical richness , register-based multiword combinations , readability and psycholinguistic word properties ) .",2,0.8498448,68.09529798145115,43
2882,Our experiments show that both the features included and the architecture of the transformer-based language models play a role in predicting multiple eye-tracking measures during naturalistic reading .,3,0.9852384,43.335448248846006,30
2882,We also report the results of experiments aimed at determining the relative importance of features from different groups using SP-LIME .,3,0.7099169,31.868937040824473,23
2883,"Recent work in multilingual machine translation ( MMT ) has focused on the potential of positive transfer between languages , particularly cases where higher-resourced languages can benefit lower-resourced ones .",0,0.9539312,29.091012492536997,34
2883,"While training an MMT model , the supervision signals learned from one language pair can be transferred to the other via the tokens shared by multiple source languages .",2,0.3690343,46.71413666274015,29
2883,"However , the transfer is inhibited when the token overlap among source languages is small , which manifests naturally when languages use different writing systems .",3,0.65170604,84.06241533960163,26
2883,"In this paper , we tackle inhibited transfer by augmenting the training data with alternative signals that unify different writing systems , such as phonetic , romanized , and transliterated input .",1,0.7146609,59.388761896109266,32
2883,"We test these signals on Indic and Turkic languages , two language families where the writing systems differ but languages still share common features .",2,0.6738076,79.82761211300483,25
2883,"Our results indicate that a straightforward multi-source self-ensemble – training a model on a mixture of various signals and ensembling the outputs of the same model fed with different signals during inference , outperforms strong ensemble baselines by 1.3 BLEU points on both language families .",3,0.9864807,34.24914510393318,46
2883,"Further , we find that incorporating alternative inputs via self-ensemble can be particularly effective when training set is small , leading to + 5 BLEU when only 5 % of the total training data is accessible .",3,0.97227716,48.80600624214154,37
2883,"Finally , our analysis demonstrates that including alternative signals yields more consistency and translates named entities more accurately , which is crucial for increased factuality of automated systems .",3,0.987689,92.02369416598228,29
2884,Multi-modal techniques offer significant untapped potential to unlock improved NLP technology for local languages .,0,0.68024427,59.76816581909463,15
2884,"However , many advances in language model pre-training are focused on text , a fact that only increases systematic inequalities in the performance of NLP tasks across the world ’s languages .",0,0.87126124,60.46436112249824,32
2884,"In this work , we propose a multi-modal approach to train language models using whatever text and / or audio data might be available in a language .",1,0.7875395,23.51125041785114,28
2884,"Initial experiments using Swahili and Kinyarwanda data suggest the viability of the approach for downstream Named Entity Recognition ( NER ) tasks , with models pre-trained on phone data showing an improvement of up to 6 % F1-score above models that are trained from scratch .",3,0.6727143,24.033858932012592,48
2884,Preprocessing and training code will be uploaded to https://github.com/sil-ai/phone-it-in .,3,0.604655,33.480253274667106,10
2885,We introduce a noisy channel approach for language model prompting in few-shot text classification .,2,0.5236594,77.43269468792549,15
2885,"Instead of computing the likelihood of the label given the input ( referred as direct models ) , channel models compute the conditional probability of the input given the label , and are thereby required to explain every word in the input .",0,0.7271245,46.94120157469652,43
2885,"We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters , via either in-context demonstration or prompt tuning .",2,0.8160955,103.67120330185772,30
2885,"Our experiments show that , for both methods , channel models significantly outperform their direct counterparts , which we attribute to their stability , i.e. , lower variance and higher worst-case accuracy .",3,0.9637975,54.06765019785004,34
2885,"We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models ( e.g. , direct head tuning ) : channel prompt tuning is preferred when the number of training examples is small , labels in the training data are imbalanced , or generalization to unseen labels is required .",3,0.66667485,62.7544378134528,58
2886,"We show that unsupervised sequence-segmentation performance can be transferred to extremely low-resource languages by pre-training a Masked Segmental Language Model ( Downey et al. , 2021 ) multilingually .",3,0.8647601,39.345501159879184,29
2886,"Further , we show that this transfer can be achieved by training over a collection of low-resource languages that are typologically similar ( but phylogenetically unrelated ) to the target language .",3,0.8692769,19.84901081506601,32
2886,"In our experiments , we transfer from a collection of 10 Indigenous American languages ( AmericasNLP , Mager et al. , 2021 ) to K’iche ’ , a Mayan language .",2,0.8531811,113.79367280903921,31
2886,"We compare our multilingual model to a monolingual ( from-scratch ) baseline , as well as a model pre-trained on Quechua only .",2,0.6665671,40.33953096304901,23
2886,"We show that the multilingual pre-trained approach yields consistent segmentation quality across target dataset sizes , exceeding the monolingual baseline in 6/10 experimental settings .",3,0.95467013,51.2738561691722,25
2886,"Our model yields especially strong results at small target sizes , including a zero-shot performance of 20.6 F1 .",3,0.9535366,37.35675222104817,19
2886,"These results have promising implications for low-resource NLP pipelines involving human-like linguistic units , such as the sparse transcription framework proposed by Bird ( 2020 ) .",3,0.984982,65.55187263239098,27
2887,Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks .,0,0.9035199,6.392583057495892,17
2887,"However , the unsupervised sub-word tokenization methods commonly used in these models ( e.g. , byte-pair encoding-BPE ) are sub-optimal at handling morphologically rich languages .",0,0.8581371,39.43190770650838,28
2887,"Even given a morphological analyzer , naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities .",0,0.54420406,62.20945465790694,27
2887,We address these challenges by proposing a simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality .,1,0.42664537,24.469256617104097,25
2887,"Despite the success of BERT , most of its evaluations have been conducted on high-resource languages , obscuring its applicability on low-resource languages .",0,0.8352666,12.038582991040622,25
2887,"We evaluate our proposed method on the low-resource morphologically rich Kinyarwanda language , naming the proposed model architecture KinyaBERT .",2,0.55022395,33.63826771784468,20
2887,A robust set of experimental results reveal that KinyaBERT outperforms solid baselines by 2 % in F1 score on a named entity recognition task and by 4.3 % in average score of a machine-translated GLUE benchmark .,3,0.91864425,21.890664897690325,39
2887,KinyaBERT fine-tuning has better convergence and achieves more robust results on multiple tasks even in the presence of translation noise .,3,0.8425703,44.4655162926674,21
2888,A well-calibrated neural model produces confidence ( probability outputs ) closely approximated by the expected accuracy .,0,0.6157792,103.0807672317009,19
2888,"While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks , little is known about using mixup for model calibration on natural language understanding ( NLU ) tasks .",0,0.9309708,21.815133614043106,40
2888,"In this paper , we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further .",1,0.9029835,24.700343492494653,30
2888,"Our proposed mixup is guided by both the Area Under the Margin ( AUM ) statistic ( Pleiss et al. , 2020 ) and the saliency map of each sample ( Simonyan et al. , 2013 ) .",2,0.5591841,35.577670785456355,38
2888,"Moreover , we combine our mixup strategy with model miscalibration correction techniques ( i.e. , label smoothing and temperature scaling ) and provide detailed analyses of their impact on our proposed mixup .",3,0.5129855,50.908388998808846,33
2888,"We focus on systematically designing experiments on three NLU tasks : natural language inference , paraphrase detection , and commonsense reasoning .",2,0.49880025,63.450160671435185,22
2888,Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy .,3,0.9051679,17.10307774478356,29
2889,Natural language inference ( NLI ) has been widely used as a task to train and evaluate models for language understanding .,0,0.96251565,16.642854460076286,22
2889,"However , the ability of NLI models to perform inferences requiring understanding of figurative language such as idioms and metaphors remains understudied .",0,0.94096076,24.401694030606354,23
2889,"We introduce the IMPLI ( Idiomatic and Metaphoric Paired Language Inference ) dataset , an English dataset consisting of paired sentences spanning idioms and metaphors .",2,0.6289196,51.42788525720596,26
2889,We develop novel methods to generate 24 k semiautomatic pairs as well as manually creating 1.8 k gold pairs .,2,0.68979037,68.10039602011221,20
2889,We use IMPLI to evaluate NLI models based on RoBERTa fine-tuned on the widely used MNLI dataset .,2,0.8597913,22.245822872346903,18
2889,"We then show that while they can reliably detect entailment relationship between figurative phrases with their literal counterparts , they perform poorly on similarly structured examples where pairs are designed to be non-entailing .",3,0.8272438,60.82899451914604,34
2889,This suggests the limits of current NLI models with regard to understanding figurative language and this dataset serves as a benchmark for future improvements in this direction .,3,0.9819602,24.068310152339542,28
2890,"This paper introduces QAConv , a new question answering ( QA ) dataset that uses conversations as a knowledge source .",1,0.75622016,53.6665932628196,21
2890,"We focus on informative conversations , including business emails , panel discussions , and work channels .",2,0.5287553,268.707812890122,17
2890,"Unlike open-domain and task-oriented dialogues , these conversations are usually long , complex , asynchronous , and involve strong domain knowledge .",0,0.8161934,53.65487421910905,24
2890,"In total , we collect 34,608 QA pairs from 10,259 selected conversations with both human-written and machine-generated questions .",2,0.61201125,42.92736766503371,21
2890,We use a question generator and a dialogue summarizer as auxiliary tools to collect and recommend questions .,2,0.7812615,39.931719474159515,18
2890,"The dataset has two testing scenarios : chunk mode and full mode , depending on whether the grounded partial conversation is provided or retrieved .",2,0.81926566,209.4629334128206,25
2890,Experimental results show that state-of-the-art pretrained QA systems have limited zero-shot performance and tend to predict our questions as unanswerable .,3,0.95003104,13.286134112322749,25
2890,Our dataset provides a new training and evaluation testbed to facilitate QA on conversations research .,3,0.93249744,94.73852780499102,16
2891,Knowledge bases ( KBs ) contain plenty of structured world and commonsense knowledge .,0,0.94957495,100.34080914568845,14
2891,"As such , they often complement distributional text-based information and facilitate various downstream tasks .",0,0.9137054,104.73696003504031,17
2891,"Since their manual construction is resource-and time-intensive , recent efforts have tried leveraging large pretrained language models ( PLMs ) to generate additional monolingual knowledge facts for KBs .",0,0.93758756,55.26260200786828,33
2891,"However , such methods have not been attempted for building and enriching multilingual KBs .",0,0.9254758,52.83829862098519,15
2891,"Besides wider application , such multilingual KBs can provide richer combined knowledge than monolingual ( e.g. , English ) KBs .",3,0.6047149,73.62277986240905,21
2891,Knowledge expressed in different languages may be complementary and unequally distributed : this implies that the knowledge available in high-resource languages can be transferred to low-resource ones .,0,0.7318406,21.027993387204837,29
2891,"To achieve this , it is crucial to represent multilingual knowledge in a shared / unified space .",0,0.921446,66.87145139145579,18
2891,"To this end , we propose a unified representation model , Prix-LM , for multilingual KB construction and completion .",1,0.40580058,114.71739796136568,21
2891,"We leverage two types of knowledge , monolingual triples and cross-lingual links , extracted from existing multilingual KBs , and tune a multilingual language encoder XLM-R via a causal language modeling objective .",2,0.88550466,47.85379727506994,35
2891,Prix-LM integrates useful multilingual and KB-based factual knowledge into a single model .,3,0.52799004,98.77697325773894,16
2891,"Experiments on standard entity-related tasks , such as link prediction in multiple languages , cross-lingual entity linking and bilingual lexicon induction , demonstrate its effectiveness , with gains reported over strong task-specialised baselines .",3,0.7488825,57.0000529341083,37
2892,We introduce a data-driven approach to generating derivation trees from meaning representation graphs with probabilistic synchronous hyperedge replacement grammar ( PSHRG ) .,2,0.5957339,85.53302913035621,23
2892,"SHRG has been used to produce meaning representation graphs from texts and syntax trees , but little is known about its viability on the reverse .",0,0.90613544,76.74289257419412,26
2892,"In particular , we experiment on Dependency Minimal Recursion Semantics ( DMRS ) and adapt PSHRG as a formalism that approximates the semantic composition of DMRS graphs and simultaneously recovers the derivations that license the DMRS graphs .",2,0.79048675,62.90549435890765,38
2892,Consistent results are obtained as evaluated on a collection of annotated corpora .,3,0.86890817,29.884880854386896,13
2892,"This work reveals the ability of PSHRG in formalizing a syntax –semantics interface , modelling compositional graph-to-tree translations , and channelling explainability to surface realization .",3,0.9171586,243.09389809326404,30
2893,AI systems embodied in the physical world face a fundamental challenge of partial observability ;,0,0.9513823,171.18168326031704,15
2893,operating with only a limited view and knowledge of the environment .,0,0.7096852,34.93234366770933,12
2893,This creates challenges when AI systems try to reason about language and its relationship with the environment : objects referred to through language ( e.g .,0,0.8847778,66.55968982044558,26
2893,giving many instructions ) are not immediately visible .,0,0.7669296,337.0653192415346,9
2893,Actions by the AI system may be required to bring these objects in view .,0,0.64281714,74.68068227562658,15
2893,"A good benchmark to study this challenge is Dynamic Referring Expression Recognition ( dRER ) task , where the goal is to find a target location by dynamically adjusting the field of view ( FoV ) in a partially observed 360 scenes .",0,0.89072275,67.23502088003582,43
2893,"In this paper , we introduce HOLM , Hallucinating Objects with Language Models , to address the challenge of partial observability .",1,0.8402885,87.24526559764858,22
2893,HOLM uses large pre-trained language models ( LMs ) to infer object hallucinations for the unobserved part of the environment .,0,0.4520842,46.54925892430507,21
2893,"Our core intuition is that if a pair of objects co-appear in an environment frequently , our usage of language should reflect this fact about the world .",0,0.6766736,42.20815943958397,28
2893,"Based on this intuition , we prompt language models to extract knowledge about object affinities which gives us a proxy for spatial relationships of objects .",3,0.40373087,70.3050323354976,26
2893,Our experiments show that HOLM performs better than the state-of-the-art approaches on two datasets for dRER ;,3,0.9786444,35.629136630348064,23
2893,allowing to study generalization for both indoor and outdoor settings .,3,0.46106422,53.74071531151122,11
2894,"Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages , though the performance varies from language to language depending on the pivot language ( s ) used for fine-tuning .",0,0.80179906,24.512483303402398,39
2894,"In this work , we build upon some of the existing techniques for predicting the zero-shot performance on a task , by modeling it as a multi-task learning problem .",1,0.59684616,20.301446092582676,30
2894,We jointly train predictive models for different tasks which helps us build more accurate predictors for tasks where we have test data in very few languages to measure the actual performance of the model .,2,0.65041727,29.226847975016852,35
2894,"Our approach also lends us the ability to perform a much more robust feature selection , and identify a common set of features that influence zero-shot performance across a variety of tasks .",3,0.8584427,38.1219497082875,33
2895,"Transformers are unable to model long-term memories effectively , since the amount of computation they need to perform grows with the context length .",0,0.7548352,33.18684740817519,24
2895,"While variations of efficient transformers have been proposed , they all have a finite memory capacity and are forced to drop old information .",0,0.791535,77.34129093707102,24
2895,"In this paper , we propose the ∞-former , which extends the vanilla transformer with an unbounded long-term memory .",1,0.8384125,40.555640295519666,20
2895,"By making use of a continuous-space attention mechanism to attend over the long-term memory , the ∞-former ’s attention complexity becomes independent of the context length , trading off memory length with precision .",3,0.55883986,67.80469311297233,35
2895,"In order to control where precision is more important , ∞-former maintains “ sticky memories , ” being able to model arbitrarily long contexts while keeping the computation budget fixed .",3,0.42106575,188.72521576383977,31
2895,"Experiments on a synthetic sorting task , language modeling , and document grounded dialogue generation demonstrate the ∞-former ’s ability to retain information from long sequences .",3,0.7569655,116.34221844402781,27
2896,"Natural language processing ( NLP ) systems have become a central technology in communication , education , medicine , artificial intelligence , and many other domains of research and development .",0,0.9696951,39.89375088596018,31
2896,"While the performance of NLP methods has grown enormously over the last decade , this progress has been restricted to a minuscule subset of the world ’s ≈ 6,500 languages .",0,0.94062346,19.048013794947163,31
2896,We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP .,1,0.50290686,53.60861156534042,24
2896,"Our analyses involve the field at large , but also more in-depth studies on both user-facing technologies ( machine translation , language understanding , question answering , text-to-speech synthesis ) as well as foundational NLP tasks ( dependency parsing , morphological inflection ) .",2,0.5324299,41.11166513423776,49
2896,"In the process , we ( 1 ) quantify disparities in the current state of NLP research , ( 2 ) explore some of its associated societal and academic factors , and ( 3 ) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies .",1,0.5815091,48.60278435230629,54
2896,Data and code to reproduce the findings discussed in this paper areavailable on GitHub ( https://github.com/neubig/globalutility) .,3,0.7339236,20.698756846240116,17
2897,"We introduce CaMEL ( Case Marker Extraction without Labels ) , a novel and challenging task in computational morphology that is especially relevant for low-resource languages .",1,0.51657325,49.708827227454755,27
2897,We propose a first model for CaMEL that uses a massively multilingual corpus to extract case markers in 83 languages based only on a noun phrase chunker and an alignment system .,2,0.32830787,121.61408283008899,32
2897,"To evaluate CaMEL , we automatically construct a silver standard from UniMorph .",2,0.8197275,284.02293845857224,13
2897,The case markers extracted by our model can be used to detect and visualise similarities and differences between the case systems of different languages as well as to annotate fine-grained deep cases in languages in which they are not overtly marked .,3,0.92132854,33.0716381510029,44
2898,"Robustness of machine learning models on ever-changing real-world data is critical , especially for applications affecting human well-being such as content moderation .",0,0.8423809,23.53918248099079,25
2898,"New kinds of abusive language continually emerge in online discussions in response to current events ( e.g. , COVID-19 ) , and the deployed abuse detection systems should be updated regularly to remain accurate .",0,0.7270276,53.83783418200683,36
2898,"In this paper , we show that general abusive language classifiers tend to be fairly reliable in detecting out-of-domain explicitly abusive utterances but fail to detect new types of more subtle , implicit abuse .",1,0.646859,41.23541230143698,38
2898,"Next , we propose an interpretability technique , based on the Testing Concept Activation Vector ( TCAV ) method from computer vision , to quantify the sensitivity of a trained model to the human-defined concepts of explicit and implicit abusive language , and use that to explain the generalizability of the model on new data , in this case , COVID-related anti-Asian hate speech .",2,0.5175602,51.010105678637515,67
2898,"Extending this technique , we introduce a novel metric , Degree of Explicitness , for a single instance and show that the new metric is beneficial in suggesting out-of-domain unlabeled examples to effectively enrich the training data with informative , implicitly abusive texts .",3,0.61620545,55.56491066925976,47
2899,"Reports of personal experiences or stories can play a crucial role in argumentation , as they represent an immediate and ( often ) relatable way to back up one ’s position with respect to a given topic .",0,0.86600864,43.42147489954094,38
2899,They are easy to understand and increase empathy : this makes them powerful in argumentation .,0,0.65846014,60.988705199855254,16
2899,"The impact of personal reports and stories in argumentation has been studied in the Social Sciences , but it is still largely underexplored in NLP .",0,0.9294459,22.038603284452957,26
2899,Our work is the first step towards filling this gap : our goal is to develop robust classifiers to identify documents containing personal experiences and reports .,1,0.5271943,51.27474857565325,27
2899,The main challenge is the scarcity of annotated data : our solution is to leverage existing annotations to be able to scale-up the analysis .,0,0.81195235,28.159577441775028,27
2899,Our contribution is two-fold .,3,0.4668849,15.42545236498724,6
2899,"First , we conduct a set of in-domain and cross-domain experiments involving three datasets ( two from Argument Mining , one from the Social Sciences ) , modeling architectures , training setups and fine-tuning options tailored to the involved domains .",2,0.9197366,58.7387481123457,41
2899,"We show that despite the differences among datasets and annotations , robust cross-domain classification is possible .",3,0.96664864,70.31535850172207,17
2899,"Second , we employ linear regression for performance mining , identifying performance trends both for overall classification performance and individual classifier predictions .",2,0.8943156,118.95459153469294,23
2900,"In recent years , neural models have often outperformed rule-based and classic Machine Learning approaches in NLG .",0,0.9325579,51.48585284385467,18
2900,"These classic approaches are now often disregarded , for example when new neural models are evaluated .",0,0.88481396,68.6649901001493,17
2900,"We argue that they should not be overlooked , since , for some tasks , well-designed non-neural approaches achieve better performance than neural ones .",3,0.78749996,39.188314056048895,27
2900,"In this paper , the task of generating referring expressions in linguistic context is used as an example .",1,0.60910845,46.86685838783282,19
2900,"We examined two very different English datasets ( WEBNLG and WSJ ) , and evaluated each algorithm using both automatic and human evaluations .",2,0.8013726,109.3813012422717,24
2900,"Overall , the results of these evaluations suggest that rule-based systems with simple rule sets achieve on-par or better performance on both datasets compared to state-of-the-art neural REG systems .",3,0.989703,18.354642604508058,39
2900,"In the case of the more realistic dataset , WSJ , a machine learning-based system with well-designed linguistic features performed best .",3,0.95492345,68.62947419800183,26
2900,We hope that our work can encourage researchers to consider non-neural models in future .,3,0.9361603,13.213055159235907,15
2901,"Text-to-SQL parsers map natural language questions to programs that are executable over tables to generate answers , and are typically evaluated on large-scale datasets like Spider ( Yu et al. , 2018 ) .",0,0.7926936,44.62338576790807,34
2901,We argue that existing benchmarks fail to capture a certain out-of-domain generalization problem that is of significant practical importance : matching domain specific phrases to composite operation over columns .,3,0.8087177,82.59959709279667,33
2901,"To study this problem , we first propose a synthetic dataset along with a re-purposed train / test split of the Squall dataset ( Shi et al. , 2020 ) as new benchmarks to quantify domain generalization over column operations , and find existing state-of-the-art parsers struggle in these benchmarks .",2,0.542073,59.311388828701915,57
2901,"We propose to address this problem by incorporating prior domain knowledge by preprocessing table schemas , and design a method that consists of two components : schema expansion and schema pruning .",2,0.47555438,47.49418989862082,32
2901,"This method can be easily applied to multiple existing base parsers , and we show that it significantly outperforms baseline parsers on this domain generalization problem , boosting the underlying parsers ’ overall performance by up to 13.8 % relative accuracy gain ( 5.1 % absolute ) on the new Squall data split .",3,0.8835751,46.593284125588774,54
2902,Paraphrase identification involves identifying whether a pair of sentences express the same or similar meanings .,0,0.84589297,25.516268395850698,16
2902,"While cross-encoders have achieved high performances across several benchmarks , bi-encoders such as SBERT have been widely applied to sentence pair tasks .",0,0.8478605,31.853789937133264,23
2902,They exhibit substantially lower computation complexity and are better suited to symmetric tasks .,0,0.6153106,78.09003630089428,14
2902,"In this work , we adopt a bi-encoder approach to the paraphrase identification task , and investigate the impact of explicitly incorporating predicate-argument information into SBERT through weighted aggregation .",1,0.55553627,52.21088681363002,31
2902,"Experiments on six paraphrase identification datasets demonstrate that , with a minimal increase in parameters , the proposed model is able to outperform SBERT / SRoBERTa significantly .",3,0.94618976,39.43856438829761,28
2902,"Further , ablation studies reveal that the predicate-argument based component plays a significant role in the performance gain .",3,0.9674173,37.18596006779231,21
2903,NER model has achieved promising performance on standard NER benchmarks .,3,0.6493683,27.784959211169678,11
2903,"However , recent studies show that previous approaches may over-rely on entity mention information , resulting in poor performance on out-of-vocabulary ( OOV ) entity recognition .",0,0.9381417,27.13810543016854,28
2903,"In this work , we propose MINER , a novel NER learning framework , to remedy this issue from an information-theoretic perspective .",1,0.8082411,39.709362090839285,25
2903,"The proposed approach contains two mutual information based training objectives : i ) generalizing information maximization , which enhances representation via deep understanding of context and entity surface forms ;",2,0.5545923,276.5737552803685,30
2903,"ii ) superfluous information minimization , which discourages representation from rotate memorizing entity names or exploiting biased cues in data .",3,0.42047194,413.8188406700971,21
2903,Experiments on various settings and datasets demonstrate that it achieves better performance in predicting OOV entities .,3,0.8719073,47.83361859510717,17
2904,"Detecting biased language is useful for a variety of applications , such as identifying hyperpartisan news sources or flagging one-sided rhetoric .",0,0.86334836,41.6840579156105,22
2904,"In this work we introduce WikiEvolve , a dataset for document-level promotional tone detection .",1,0.6977207,99.33498920866356,16
2904,"Unlike previously proposed datasets , WikiEvolve contains seven versions of the same article from Wikipedia , from different points in its revision history ;",3,0.49864143,145.48763424175766,24
2904,"one with promotional tone , and six without it .",3,0.62612146,441.49347063943173,10
2904,This allows for obtaining more precise training signal for learning models from promotional tone detection .,3,0.7163728,249.47676572436922,16
2904,We adapt the previously proposed gradient reversal layer framework to encode two article versions simultaneously and thus leverage this additional training signal .,2,0.6697295,179.27893862037243,23
2904,"In our experiments , our proposed adaptation of gradient reversal improves the accuracy of four different architectures on both in-domain and out-of-domain evaluation .",3,0.892333,28.093799052971246,27
2905,Informal social interaction is the primordial home of human language .,0,0.95290583,33.582767117330654,11
2905,Linguistically diverse conversational corpora are an important and largely untapped resource for computational linguistics and language technology .,0,0.9410303,22.78757560703324,18
2905,"Through the efforts of a worldwide language documentation movement , such corpora are increasingly becoming available .",0,0.9315488,75.93522805093679,17
2905,"We show how interactional data from 63 languages ( 26 families ) harbours insights about turn-taking , timing , sequential structure and social action , with implications for language technology , natural language understanding , and the design of conversational interfaces .",3,0.76298505,113.2362081986355,42
2905,"Harnessing linguistically diverse conversational corpora will provide the empirical foundations for flexible , localizable , humane language technologies of the future .",0,0.5231571,112.58324392078151,22
2906,"Adversarial robustness has attracted much attention recently , and the mainstream solution is adversarial training .",0,0.9347131,34.173663639635656,16
2906,"However , the tradition of generating adversarial perturbations for each input embedding ( in the settings of NLP ) scales up the training computational complexity by the number of gradient steps it takes to obtain the adversarial samples .",0,0.82613224,46.325404242081305,39
2906,"To address this problem , we leverage Flooding method which primarily aims at better generalization and we find promising in defending adversarial attacks .",2,0.528543,140.96822194182195,24
2906,We further propose an effective criterion to bring hyper-parameter-dependent flooding into effect with a narrowed-down search space by measuring how the gradient steps taken within one epoch affect the loss of each batch .,3,0.52291185,91.21178138473178,38
2906,"Our approach requires zero adversarial sample for training , and its time consumption is equivalent to fine-tuning , which can be 2-15 times faster than standard adversarial training .",3,0.5464616,32.611140063451586,31
2906,"We experimentally show that our method improves BERT ’s resistance to textual adversarial attacks by a large margin , and achieves state-of-the-art robust accuracy on various text classification and GLUE tasks .",3,0.8605721,17.579443090388317,38
2907,Evaluating Natural Language Generation ( NLG ) systems is a challenging task .,0,0.9581867,19.10142729248918,13
2907,"Firstly , the metric should ensure that the generated hypothesis reflects the reference ’s semantics .",3,0.5234707,91.33394844442299,16
2907,"Secondly , it should consider the grammatical quality of the generated sentence .",3,0.6341906,31.203977133822544,13
2907,"Thirdly , it should be robust enough to handle various surface forms of the generated sentence .",3,0.6229281,75.0313049682038,17
2907,"Thus , an effective evaluation metric has to be multifaceted .",0,0.83336574,48.42991274787757,11
2907,"In this paper , we propose an automatic evaluation metric incorporating several core aspects of natural language understanding ( language competence , syntactic and semantic variation ) .",1,0.8780021,47.733114505680554,28
2907,"Our proposed metric , RoMe , is trained on language features such as semantic similarity combined with tree edit distance and grammatical acceptability , using a self-supervised neural network to assess the overall quality of the generated sentence .",2,0.6949424,50.27020317920684,39
2907,"Moreover , we perform an extensive robustness analysis of the state-of-the-art methods and RoMe .",2,0.5143429,24.092762122105242,21
2907,Empirical results suggest that RoMe has a stronger correlation to human judgment over state-of-the-art metrics in evaluating system-generated sentences across several NLG tasks .,3,0.9724363,24.30964404900032,31
2908,"In this work , we investigate the knowledge learned in the embeddings of multimodal-BERT models .",1,0.8429303,23.022088056655377,18
2908,"More specifically , we probe their capabilities of storing the grammatical structure of linguistic data and the structure learned over objects in visual data .",2,0.4711112,86.28571118196835,25
2908,"To reach that goal , we first make the inherent structure of language and visuals explicit by a dependency parse of the sentences that describe the image and by the dependencies between the object regions in the image , respectively .",2,0.69882625,55.94068831313625,41
2908,"We call this explicit visual structure the scene tree , that is based on the dependency tree of the language description .",2,0.45150015,79.96075940733762,22
2908,Extensive probing experiments show that the multimodal-BERT models do not encode these scene trees .,3,0.8758525,69.0196476395483,17
2909,Hyperbolic neural networks have shown great potential for modeling complex data .,0,0.940866,15.846864218295938,12
2909,"However , existing hyperbolic networks are not completely hyperbolic , as they encode features in the hyperbolic space yet formalize most of their operations in the tangent space ( a Euclidean subspace ) at the origin of the hyperbolic model .",0,0.8801695,27.815806514695904,41
2909,This hybrid method greatly limits the modeling ability of networks .,0,0.5682615,104.85094079434184,11
2909,"In this paper , we propose a fully hyperbolic framework to build hyperbolic networks based on the Lorentz model by adapting the Lorentz transformations ( including boost and rotation ) to formalize essential operations of neural networks .",1,0.8619384,27.804408777580388,38
2909,"Moreover , we also prove that linear transformation in tangent spaces used by existing hyperbolic networks is a relaxation of the Lorentz rotation and does not include the boost , implicitly limiting the capabilities of existing hyperbolic networks .",3,0.7969949,45.03354324638994,39
2909,The experimental results on four NLP tasks show that our method has better performance for building both shallow and deep networks .,3,0.9619306,18.554170519164376,22
2909,Our code will be released to facilitate follow-up research .,3,0.8650659,14.416499457642972,12
2910,"Multimodal machine translation ( MMT ) aims to improve neural machine translation ( NMT ) with additional visual information , but most existing MMT methods require paired input of source sentence and image , which makes them suffer from shortage of sentence-image pairs .",0,0.9478923,23.82521938104332,45
2910,"In this paper , we propose a phrase-level retrieval-based method for MMT to get visual information for the source input from existing sentence-image data sets so that MMT can break the limitation of paired sentence-image input .",1,0.8912819,42.74967346067597,42
2910,"Our method performs retrieval at the phrase level and hence learns visual information from pairs of source phrase and grounded region , which can mitigate data sparsity .",3,0.5316895,116.49654480354808,28
2910,"Furthermore , our method employs the conditional variational auto-encoder to learn visual representations which can filter redundant visual information and only retain visual information related to the phrase .",2,0.5293856,32.1173949873738,29
2910,"Experiments show that the proposed method significantly outperforms strong baselines on multiple MMT datasets , especially when the textual context is limited .",3,0.94605356,14.686843809483621,23
2911,"The emotional state of a speaker can be influenced by many different factors in dialogues , such as dialogue scene , dialogue topic , and interlocutor stimulus .",0,0.8568407,37.56292664481718,28
2911,The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity .,0,0.9043617,37.882916751208526,21
2911,"In this work , we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset , M3ED , which contains 990 dyadic emotional dialogues from 56 different TV series , a total of 9,082 turns and 24,449 utterances .",2,0.5312846,37.71155316173832,37
2911,"M3ED is annotated with 7 emotion categories ( happy , surprise , sad , disgust , anger , fear , and neutral ) at utterance level , and encompasses acoustic , visual , and textual modalities .",0,0.43049985,49.628609907384,37
2911,"To the best of our knowledge , M3ED is the first multimodal emotional dialogue dataset in Chinese .",3,0.7711097,25.668211623788757,18
2911,It is valuable for cross-culture emotion analysis and recognition .,3,0.5964105,60.203681059890805,10
2911,We apply several state-of-the-art methods on the M3ED dataset to verify the validity and quality of the dataset .,2,0.8018128,13.995919347088869,24
2911,"We also propose a general Multimodal Dialogue-aware Interaction framework , MDI , to model the dialogue context for emotion recognition , which achieves comparable performance to the state-of-the-art methods on the M3ED .",2,0.519551,24.878365145775867,41
2911,The full dataset and codes are available .,3,0.51480985,23.924514180803644,8
2912,Few-shot NER needs to effectively capture information from limited instances and transfer useful knowledge from external resources .,0,0.7737219,63.027424821025384,20
2912,"In this paper , we propose a self-describing mechanism for few-shot NER , which can effectively leverage illustrative instances and precisely transfer knowledge from external resources by describing both entity types and mentions using a universal concept set .",1,0.81228524,63.12622897732693,39
2912,"Specifically , we design Self-describing Networks ( SDNet ) , a Seq2Seq generation model which can universally describe mentions using concepts , automatically map novel entity types to concepts , and adaptively recognize entities on-demand .",2,0.86317784,82.79439602683037,36
2912,"We pre-train SDNet with large-scale corpus , and conduct experiments on 8 benchmarks from different domains .",2,0.87946916,46.79969537052434,17
2912,"Experiments show that SDNet achieves competitive performances on all benchmarks and achieves the new state-of-the-art on 6 benchmarks , which demonstrates its effectiveness and robustness .",3,0.93635803,17.17767413277529,32
2913,"Motivated by the success of T5 ( Text-To-Text Transfer Transformer ) in pre-trained natural language processing models , we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech / text representation learning .",2,0.509937,22.8685946553817,41
2913,The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific ( speech / text ) pre/ post-nets .,2,0.53072006,68.14655550466777,21
2913,"After preprocessing the input speech / text through the pre-nets , the shared encoder-decoder network models the sequence-to-sequence transformation , and then the post-nets generate the output in the speech / text modality based on the output of the decoder .",2,0.68013495,25.45385460303082,43
2913,"Leveraging large-scale unlabeled speech and text data , we pre-train SpeechT5 to learn a unified-modal representation , hoping to improve the modeling capability for both speech and text .",2,0.6827858,34.106820707132854,30
2913,"To align the textual and speech information into this unified semantic space , we propose a cross-modal vector quantization approach that randomly mixes up speech / text states with latent units as the interface between encoder and decoder .",2,0.6632099,48.58349448298163,39
2913,"Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks , including automatic speech recognition , speech synthesis , speech translation , voice conversion , speech enhancement , and speaker identification .",3,0.8254781,34.15818663638264,41
2914,"In recent years , machine learning models have rapidly become better at generating clinical consultation notes ;",0,0.94966376,167.9065946782475,17
2914,"yet , there is little work on how to properly evaluate the generated consultation notes to understand the impact they may have on both the clinician using them and the patient ’s clinical safety .",0,0.87088364,41.145191098961405,35
2914,"To address this we present an extensive human evaluation study of consultation notes where 5 clinicians ( i ) listen to 57 mock consultations , ( ii ) write their own notes , ( iii ) post-edit a number of automatically generated notes , and ( iv ) extract all the errors , both quantitative and qualitative .",1,0.45357376,63.97449312024529,59
2914,We then carry out a correlation study with 18 automatic quality metrics and the human judgements .,2,0.7671512,56.59424312842076,17
2914,"We find that a simple , character-based Levenshtein distance metric performs on par if not better than common model-based metrics like BertScore .",3,0.97930604,40.89788725592825,27
2914,All our findings and annotations are open-sourced .,3,0.91570073,20.484565057761554,8
2915,"Information extraction suffers from its varying targets , heterogeneous structures , and demand-specific schemas .",0,0.92861795,173.12834257567093,15
2915,"In this paper , we propose a unified text-to-structure generation framework , namely UIE , which can universally model different IE tasks , adaptively generate targeted structures , and collaboratively learn general IE abilities from different knowledge sources .",1,0.86750233,81.21484611812592,41
2915,"Specifically , UIE uniformly encodes different extraction structures via a structured extraction language , adaptively generates target extractions via a schema-based prompt mechanism – structural schema instructor , and captures the common IE abilities via a large-scale pretrained text-to-structure model .",2,0.6270188,132.73609176765956,43
2915,"Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks , 13 datasets , and on all supervised , low-resource , and few-shot settings for a wide range of entity , relation , event and sentiment extraction tasks and their unification .",3,0.92125195,47.800716750019554,48
2915,"These results verified the effectiveness , universality , and transferability of UIE .",3,0.9883223,65.36671472719014,13
2916,Recent works on knowledge base question answering ( KBQA ) retrieve subgraphs for easier reasoning .,0,0.87598497,56.435032799236716,16
2916,The desired subgraph is crucial as a small one may exclude the answer but a large one might introduce more noises .,0,0.57760745,116.11631667561055,22
2916,"However , the existing retrieval is either heuristic or interwoven with the reasoning , causing reasoning on the partial subgraphs , which increases the reasoning bias when the intermediate supervision is missing .",0,0.52865654,92.15085934576213,33
2916,"This paper proposes a trainable subgraph retriever ( SR ) decoupled from the subsequent reasoning process , which enables a plug-and-play framework to enhance any subgraph-oriented KBQA model .",1,0.7970134,58.97674373945725,32
2916,Extensive experiments demonstrate SR achieves significantly better retrieval and QA performance than existing retrieval methods .,3,0.8500812,65.3459592835791,16
2916,"Via weakly supervised pre-training as well as the end-to-end fine-tuning , SR achieves new state-of-the-art performance when combined with NSM ( He et al. , 2021 ) , a subgraph-oriented reasoner , for embedding-based KBQA methods .",3,0.6968545,40.554412323407746,47
2916,Codes and datasets are available online ( https://github.com/RUCKBReasoning/SubgraphRetrievalKBQA) .,3,0.46301296,37.03982887170141,9
2917,"Low-shot relation extraction ( RE ) aims to recognize novel relations with very few or even no samples , which is critical in real scenario application .",0,0.8961919,84.94428922514776,29
2917,"Few-shot and zero-shot RE are two representative low-shot RE tasks , which seem to be with similar target but require totally different underlying abilities .",0,0.85335374,111.72027166795884,27
2917,"In this paper , we propose Multi-Choice Matching Networks to unify low-shot relation extraction .",1,0.8682302,43.07030728954306,15
2917,"To fill in the gap between zero-shot and few-shot RE , we propose the triplet-paraphrase meta-training , which leverages triplet paraphrase to pre-train zero-shot label matching ability and uses meta-learning paradigm to learn few-shot instance summarizing ability .",2,0.7697835,32.994039481192665,40
2917,"Experimental results on three different low-shot RE tasks show that the proposed method outperforms strong baselines by a large margin , and achieve the best performance on few-shot RE leaderboard .",3,0.9302587,13.859968442360094,31
2918,Prompt-based probing has been widely used in evaluating the abilities of pretrained language models ( PLMs ) .,0,0.92199165,18.07493628871985,20
2918,"Unfortunately , recent studies have discovered such an evaluation may be inaccurate , inconsistent and unreliable .",0,0.9100982,123.2570982697858,17
2918,"Furthermore , the lack of understanding its inner workings , combined with its wide applicability , has the potential to lead to unforeseen risks for evaluating and applying PLMs in real-world applications .",0,0.5530746,33.89467664076836,33
2918,"To discover , understand and quantify the risks , this paper investigates the prompt-based probing from a causal view , highlights three critical biases which could induce biased results and conclusions , and proposes to conduct debiasing via causal intervention .",1,0.81970084,123.45379384751179,42
2918,"This paper provides valuable insights for the design of unbiased datasets , better probing frameworks and more reliable evaluations of pretrained language models .",3,0.9118538,55.236599431562986,24
2918,"Furthermore , our conclusions also echo that we need to rethink the criteria for identifying better pretrained language models .",3,0.9887919,48.114762314436355,20
2919,"Several natural language processing ( NLP ) tasks are defined as a classification problem in its most complex form : Multi-label Hierarchical Extreme classification , in which items may be associated with multiple classes from a set of thousands of possible classes organized in a hierarchy and with a highly unbalanced distribution both in terms of class frequency and the number of labels per item .",0,0.9293224,33.13190993352474,66
2919,We analyze the state of the art of evaluation metrics based on a set of formal properties and we define an information theoretic based metric inspired by the Information Contrast Model ( ICM ) .,2,0.6394092,37.61525539042197,35
2919,Experiments on synthetic data and a case study on real data show the suitability of the ICM for such scenarios .,3,0.7808873,24.022143695893888,21
2920,We present a complete pipeline to extract characters in a novel and link them to their direct-speech utterances .,3,0.4377302,52.70253229620065,19
2920,"Our model is divided into three independent components : extracting direct-speech , compiling a list of characters , and attributing those characters to their utterances .",2,0.76726836,76.99279553743362,26
2920,"Although we find that existing systems can perform the first two tasks accurately , attributing characters to direct speech is a challenging problem due to the narrator ’s lack of explicit character mentions , and the frequent use of nominal and pronominal coreference when such explicit mentions are made .",3,0.94625723,47.501131709710606,50
2920,We adapt the progress made on Dialogue State Tracking to tackle a new problem : attributing speakers to dialogues .,2,0.48101345,102.80661439328755,20
2920,"This is the first application of deep learning to speaker attribution , and it shows that is possible to overcome the need for the hand-crafted features and rules used in the past .",3,0.9553566,31.557079768732766,33
2920,Our full pipeline improves the performance of state-of-the-art models by a relative 50 % in F1-score .,3,0.8964411,14.406878617616412,25
2921,"With the rapid growth in language processing applications , fairness has emerged as an important consideration in data-driven solutions .",0,0.95052946,26.392103399692893,20
2921,"Although various fairness definitions have been explored in the recent literature , there is lack of consensus on which metrics most accurately reflect the fairness of a system .",0,0.90604335,20.381180681487336,29
2921,"In this work , we propose a new formulation – accumulated prediction sensitivity , which measures fairness in machine learning models based on the model ’s prediction sensitivity to perturbations in input features .",1,0.77337176,44.94534075484998,34
2921,"The metric attempts to quantify the extent to which a single prediction depends on a protected attribute , where the protected attribute encodes the membership status of an individual in a protected group .",2,0.43251237,33.055415021647114,34
2921,We show that the metric can be theoretically linked with a specific notion of group fairness ( statistical parity ) and individual fairness .,3,0.8426039,136.24359817442743,24
2921,It also correlates well with humans ’ perception of fairness .,3,0.55817515,50.36624953108949,11
2921,"We conduct experiments on two text classification datasets – Jigsaw Toxicity , and Bias in Bios , and evaluate the correlations between metrics and manual annotations on whether the model produced a fair outcome .",2,0.8841345,68.95842718905025,35
2921,We observe that the proposed fairness metric based on prediction sensitivity is statistically significantly more correlated with human annotation than the existing counterfactual fairness metric .,3,0.9760134,33.38285930956023,26
2922,"Temporal factors are tied to the growth of facts in realistic applications , such as the progress of diseases and the development of political situation , therefore , research on Temporal Knowledge Graph ( TKG ) attracks much attention .",0,0.94454384,77.2291857325879,40
2922,"In TKG , relation patterns inherent with temporality are required to be studied for representation learning and reasoning across temporal facts .",0,0.52285486,146.76658108938526,22
2922,"However , existing methods can hardly model temporal relation patterns , nor can capture the intrinsic connections between relations when evolving over time , lacking of interpretability .",0,0.9128285,124.25094805084281,28
2922,"In this paper , we propose a novel temporal modeling method which represents temporal entities as Rotations in Quaternion Vector Space ( Rotate QVS ) and relations as complex vectors in Hamilton ’s quaternion space .",1,0.8587532,47.499432967237475,36
2922,"We demonstrate our method can model key patterns of relations in TKG , such as symmetry , asymmetry , inverse , and can capture time-evolved relations by theory .",3,0.87211406,102.68109667919441,31
2922,"And empirically , we show that our method can boost the performance of link prediction tasks over four temporal knowledge graph benchmarks .",3,0.76090354,47.3543471670959,23
2923,Machine Reading Comprehension ( MRC ) reveals the ability to understand a given text passage and answer questions based on it .,0,0.9478498,28.0243789494107,22
2923,Existing research works in MRC rely heavily on large-size models and corpus to improve the performance evaluated by metrics such as Exact Match ( EM ) and F1 .,0,0.8574028,54.99961174900926,29
2923,"However , such a paradigm lacks sufficient interpretation to model capability and can not efficiently train a model with a large corpus .",0,0.87594134,67.52311978902662,23
2923,"In this paper , we argue that a deep understanding of model capabilities and data properties can help us feed a model with appropriate training data based on its learning status .",1,0.81214285,35.02089626882016,32
2923,"Specifically , we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi-dimensional manner .",2,0.7590624,37.969985699586296,20
2923,"Based on it , we further uncover and disentangle the connections between various data properties and model performance .",3,0.74719745,43.66393162349634,19
2923,"Finally , to verify the effectiveness of the proposed MRC capability assessment framework , we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum ( CBBC ) strategy , which performs a model capability-based training to maximize the data value and improve training efficiency .",2,0.5498647,64.02927378172375,52
2923,"Extensive experiments demonstrate that our approach significantly improves performance , achieving up to an 11.22 % / 8.71 % improvement of EM / F1 on MRC tasks .",3,0.94177294,48.31480323810093,28
2924,Simile interpretation ( SI ) and simile generation ( SG ) are challenging tasks for NLP because models require adequate world knowledge to produce predictions .,0,0.92962235,79.09048637457771,26
2924,"Previous works have employed many hand-crafted resources to bring knowledge-related into models , which is time-consuming and labor-intensive .",0,0.9118542,44.1305186102922,22
2924,"In recent years , pre-trained language models ( PLMs ) based approaches have become the de-facto standard in NLP since they learn generic knowledge from a large corpus .",0,0.95092696,17.90954865255271,29
2924,The knowledge embedded in PLMs may be useful for SI and SG tasks .,3,0.7914831,86.74449156016082,14
2924,"Nevertheless , there are few works to explore it .",0,0.89492315,59.63263273475864,10
2924,"In this paper , we probe simile knowledge from PLMs to solve the SI and SG tasks in the unified framework of simile triple completion for the first time .",1,0.87394613,69.92889652930658,30
2924,The backbone of our framework is to construct masked sentences with manual patterns and then predict the candidate words in the masked position .,2,0.68721247,53.92380085355464,24
2924,"In this framework , we adopt a secondary training process ( Adjective-Noun mask Training ) with the masked language model ( MLM ) loss to enhance the prediction diversity of candidate words in the masked position .",2,0.81625694,83.65303457795835,39
2924,"Moreover , pattern ensemble ( PE ) and pattern search ( PS ) are applied to improve the quality of predicted words .",2,0.5571765,86.22253677758071,23
2924,"Finally , automatic and human evaluations demonstrate the effectiveness of our framework in both SI and SG tasks .",3,0.9451236,33.70863641309053,19
2925,"Entity alignment ( EA ) aims to discover the equivalent entity pairs between KGs , which is a crucial step for integrating multi-source KGs .",0,0.90496486,62.158868828132405,25
2925,"For a long time , most researchers have regarded EA as a pure graph representation learning task and focused on improving graph encoders while paying little attention to the decoding process .",0,0.9446237,29.159285639760295,32
2925,"In this paper , we propose an effective and efficient EA Decoding Algorithm via Third-order Tensor Isomorphism ( DATTI ) .",1,0.87404233,66.3526355793514,22
2925,"Specifically , we derive two sets of isomorphism equations : ( 1 ) Adjacency tensor isomorphism equations and ( 2 ) Gramian tensor isomorphism equations .",2,0.71607995,21.53262763659214,26
2925,"By combining these equations , DATTI could effectively utilize the adjacency and inner correlation isomorphisms of KGs to enhance the decoding process of EA .",3,0.75287765,119.67845870930977,25
2925,"Extensive experiments on public datasets indicate that our decoding algorithm can deliver significant performance improvements even on the most advanced EA methods , while the extra required time is less than 3 seconds .",3,0.9109428,50.645480010078046,34
2926,Typed entailment graphs try to learn the entailment relations between predicates from text and model them as edges between predicate nodes .,0,0.7769133,39.94754562038664,22
2926,The construction of entailment graphs usually suffers from severe sparsity and unreliability of distributional similarity .,0,0.89644367,60.603518415391925,16
2926,"We propose a two-stage method , Entailment Graph with Textual Entailment and Transitivity ( EGT2 ) .",2,0.6752127,52.630833460639295,17
2926,EGT2 learns the local entailment relations by recognizing the textual entailment between template sentences formed by typed CCG-parsed predicates .,2,0.5815077,95.05645858136297,22
2926,"Based on the generated local graph , EGT2 then uses three novel soft transitivity constraints to consider the logical transitivity in entailment structures .",2,0.6712127,201.3981960683696,24
2926,"Experiments on benchmark datasets show that EGT2 can well model the transitivity in entailment graph to alleviate the sparsity , and leads to signifcant improvement over current state-of-the-art methods .",3,0.932957,30.233831160923838,36
2927,"Modern deep learning models are notoriously opaque , which has motivated the development of methods for interpreting how deep models predict .",0,0.96489644,34.98058241625476,22
2927,"This goal is usually approached with attribution method , which assesses the influence of features on model predictions .",0,0.83502734,58.3925192212259,19
2927,"As an explanation method , the evaluation criteria of attribution methods is how accurately it reflects the actual reasoning process of the model ( faithfulness ) .",0,0.5243505,100.64422928321622,27
2927,"Meanwhile , since the reasoning process of deep models is inaccessible , researchers design various evaluation methods to demonstrate their arguments .",0,0.88368475,162.221710911067,22
2927,"However , some crucial logic traps in these evaluation methods are ignored in most works , causing inaccurate evaluation and unfair comparison .",0,0.7458572,234.86527022787075,23
2927,This paper systematically reviews existing methods for evaluating attribution scores and summarizes the logic traps in these methods .,1,0.883362,99.5134372363258,19
2927,We further conduct experiments to demonstrate the existence of each logic trap .,3,0.5323316,111.09331686645238,13
2927,"Through both theoretical and experimental analysis , we hope to increase attention on the inaccurate evaluation of attribution scores .",3,0.8164507,80.56888014481329,20
2927,"Moreover , with this paper , we suggest stopping focusing on improving performance under unreliable evaluation systems and starting efforts on reducing the impact of proposed logic traps .",3,0.61951196,146.68654150143,29
2928,"In this paper , we study how to continually pre-train language models for improving the understanding of math problems .",1,0.93393034,23.291793573468787,20
2928,"Specifically , we focus on solving a fundamental challenge in modeling math problems , how to fuse the semantics of textual description and formulas , which are highly different in essence .",1,0.6905697,76.89054429265326,32
2928,"To address this issue , we propose a new approach called COMUS to continually pre-train language models for math problem understanding with syntax-aware memory network .",1,0.5606794,51.30743602154986,28
2928,"In this approach , we first construct the math syntax graph to model the structural semantic information , by combining the parsing trees of the text and formulas , and then design the syntax-aware memory networks to deeply fuse the features from the graph and text .",2,0.86718434,65.74257058505006,49
2928,"With the help of syntax relations , we can model the interaction between the token from the text and its semantic-related nodes within the formulas , which is helpful to capture fine-grained semantic correlations between texts and formulas .",3,0.5496229,33.944768079566394,41
2928,"Besides , we devise three continual pre-training tasks to further align and fuse the representations of the text and math syntax graph .",2,0.67328715,76.16938817167951,23
2928,Experimental results on four tasks in the math domain demonstrate the effectiveness of our approach .,3,0.89454937,12.100970499727177,16
2928,Our code and data are publicly available at the link : bluehttps://github.com/RUCAIBox/COMUS .,3,0.6053517,34.89051791575022,13
2929,The definition generation task can help language learners by providing explanations for unfamiliar words .,3,0.55428374,75.72767406470302,15
2929,This task has attracted much attention in recent years .,0,0.9610951,8.262791824039153,10
2929,We propose a novel task of Simple Definition Generation ( SDG ) to help language learners and low literacy readers .,1,0.77758586,66.46245180108225,21
2929,"A significant challenge of this task is the lack of learner ’s dictionaries in many languages , and therefore the lack of data for supervised training .",0,0.91532016,29.82253001750041,27
2929,We explore this task and propose a multitasking framework SimpDefiner that only requires a standard dictionary with complex definitions and a corpus containing arbitrary simple texts .,2,0.43976477,90.57458099251555,27
2929,We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders .,2,0.6908225,33.52112370316634,19
2929,"By jointly training these components , the framework can generate both complex and simple definitions simultaneously .",3,0.5349416,99.58824922971091,17
2929,"We demonstrate that the framework can generate relevant , simple definitions for the target words through automatic and manual evaluations on English and Chinese datasets .",3,0.8833571,60.21022669200054,26
2929,"Our method outperforms the baseline model by a 1.77 SARI score on the English dataset , and raises the proportion of the low level ( HSK level 1-3 ) words in Chinese definitions by 3.87 % .",3,0.856333,50.15048048129815,39
2930,Solving math word problems requires deductive reasoning over the quantities in the text .,0,0.77653176,49.01190074011298,14
2930,Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context .,0,0.91811514,36.01311888415699,33
2930,"While empirically effective , such approaches typically do not provide explanations for the generated expressions .",0,0.8425159,106.9071129158213,16
2930,"In this work , we view the task as a complex relation extraction problem , proposing a novel approach that presents explainable deductive reasoning steps to iteratively construct target expressions , where each step involves a primitive operation over two quantities defining their relation .",1,0.51837444,62.68663755620605,45
2930,"Through extensive experiments on four benchmark datasets , we show that the proposed model significantly outperforms existing strong baselines .",3,0.85370487,8.995238014053808,20
2930,We further demonstrate that the deductive procedure not only presents more explainable steps but also enables us to make more accurate predictions on questions that require more complex reasoning .,3,0.96632105,25.450068033475226,30
2931,Indirect speech such as sarcasm achieves a constellation of discourse goals in human communication .,0,0.8190669,170.62565551047274,15
2931,"While the indirectness of figurative language warrants speakers to achieve certain pragmatic goals , it is challenging for AI agents to comprehend such idiosyncrasies of human communication .",0,0.89519316,65.71426898261599,28
2931,"Though sarcasm identification has been a well-explored topic in dialogue analysis , for conversational systems to truly grasp a conversation ’s innate meaning and generate appropriate responses , simply detecting sarcasm is not enough ;",0,0.84593624,78.45726434774899,37
2931,it is vital to explain its underlying sarcastic connotation to capture its true essence .,0,0.87321144,153.81210997793468,15
2931,"In this work , we study the discourse structure of sarcastic conversations and propose a novel task – Sarcasm Explanation in Dialogue ( SED ) .",1,0.88584936,32.40754190323726,26
2931,"Set in a multimodal and code-mixed setting , the task aims to generate natural language explanations of satirical conversations .",2,0.40805596,49.52696747621147,20
2931,"To this end , we curate WITS , a new dataset to support our task .",2,0.4636017,53.60721842294205,16
2931,"We propose MAF ( Modality Aware Fusion ) , a multimodal context-aware attention and global information fusion module to capture multimodality and use it to benchmark WITS .",1,0.35376593,83.61164017048469,30
2931,The proposed attention module surpasses the traditional multimodal fusion baselines and reports the best performance on almost all metrics .,3,0.8401214,40.70519844303561,20
2931,"Lastly , we carry out detailed analysis both quantitatively and qualitatively .",2,0.56752,24.314443503732605,12
2932,"Recently , finetuning a pretrained language model to capture the similarity between sentence embeddings has shown the state-of-the-art performance on the semantic textual similarity ( STS ) task .",0,0.90751004,12.86787317139814,35
2932,"However , the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output .",0,0.63351786,35.87649754181548,21
2932,"In this work , we explicitly describe the sentence distance as the weighted sum of contextualized token distances on the basis of a transportation problem , and then present the optimal transport-based distance measure , named RCMD ;",2,0.48540735,106.29013784541766,40
2932,it identifies and leverages semantically-aligned token pairs .,0,0.40560955,71.14204258113045,10
2932,"In the end , we propose CLRCMD , a contrastive learning framework that optimizes RCMD of sentence pairs , which enhances the quality of sentence similarity and their interpretation .",2,0.3700752,71.8937203955903,30
2932,"Extensive experiments demonstrate that our learning framework outperforms other baselines on both STS and interpretable-STS benchmarks , indicating that it computes effective sentence similarity and also provides interpretation consistent with human judgement .",3,0.94894594,33.140260506561944,35
2933,Recent years have witnessed growing interests in incorporating external knowledge such as pre-trained word embeddings ( PWEs ) or pre-trained language models ( PLMs ) into neural topic modeling .,0,0.94752055,14.123771996406955,30
2933,"However , we found that employing PWEs and PLMs for topic modeling only achieved limited performance improvements but with huge computational overhead .",3,0.98548824,66.44496024800753,23
2933,"In this paper , we propose a novel strategy to incorporate external knowledge into neural topic modeling where the neural topic model is pre-trained on a large corpus and then fine-tuned on the target dataset .",1,0.7883017,10.073396105014018,36
2933,Experiments have been conducted on three datasets and results show that the proposed approach significantly outperforms both current state-of-the-art neural topic models and some topic modeling approaches enhanced with PWEs or PLMs .,3,0.9447986,17.603634942255763,39
2933,"Moreover , further study shows that the proposed approach greatly reduces the need for the huge size of training data .",3,0.9800442,25.767986074948475,21
2934,"Dense retrieval has achieved impressive advances in first-stage retrieval from a large-scale document collection , which is built on bi-encoder architecture to produce single vector representation of query and document .",0,0.909105,51.669866753042385,31
2934,"However , a document can usually answer multiple potential queries from different views .",0,0.9117091,163.3497133175784,14
2934,"So the single vector representation of a document is hard to match with multi-view queries , and faces a semantic mismatch problem .",0,0.86170954,46.49273679907526,23
2934,"This paper proposes a multi-view document representation learning framework , aiming to produce multi-view embeddings to represent documents and enforce them to align with different queries .",1,0.7709907,35.76427027362411,27
2934,"First , we propose a simple yet effective method of generating multiple embeddings through viewers .",2,0.43283767,24.217975949612878,16
2934,"Second , to prevent multi-view embeddings from collapsing to the same one , we further propose a global-local loss with annealed temperature to encourage the multiple viewers to better align with different potential queries .",2,0.6939199,78.83264701517153,37
2934,Experiments show our method outperforms recent works and achieves state-of-the-art results .,3,0.93186843,6.164682098688248,17
2935,Abstract meaning representation ( AMR ) highlights the core semantic information of text in a graph structure .,0,0.9425293,63.84887329232476,18
2935,"Recently , pre-trained language models ( PLMs ) have advanced tasks of AMR parsing and AMR-to-text generation , respectively .",0,0.9273219,25.005622853776337,24
2935,"However , PLMs are typically pre-trained on textual data , thus are sub-optimal for modeling structural knowledge .",0,0.8684583,32.372227598815435,18
2935,"To this end , we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs .",1,0.5480285,51.33422027234898,20
2935,"In particular , we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training .",2,0.6225338,21.402618598572822,28
2935,We further design a unified framework to bridge the gap between pre-training and fine-tuning tasks .,3,0.5042583,8.531435039065553,16
2935,Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model .,3,0.9220632,11.063732944582938,19
2935,"To our knowledge , we are the first to consider pre-training on semantic graphs .",3,0.76310414,27.319005081182542,15
2936,"Models pre-trained with a language modeling objective possess ample world knowledge and language skills , but are known to struggle in tasks that require reasoning .",0,0.8169043,67.8706173428203,26
2936,"In this work , we propose to leverage semi-structured tables , and automatically generate at scale question-paragraph pairs , where answering the question requires reasoning over multiple facts in the paragraph .",1,0.57234097,54.16566983442031,34
2936,"We add a pre-training step over this synthetic data , which includes examples that require 16 different reasoning skills such as number comparison , conjunction , and fact composition .",2,0.7376616,166.46105073564277,30
2936,"To improve data efficiency , we sample examples from reasoning skills where the model currently errs .",2,0.77818745,206.18060700140234,17
2936,"We evaluate our approach on three reasoning-focused reading comprehension datasets , and show that our model , PReasM , substantially outperforms T5 , a popular pre-trained encoder-decoder model .",3,0.7261951,45.68870057275343,30
2936,"Moreover , sampling examples based on model errors leads to faster training and higher performance .",3,0.7237961,83.32133981824397,16
2937,"Existing KBQA approaches , despite achieving strong performance on i.i.d .",0,0.5845194,78.16692971439564,11
2937,"test data , often struggle in generalizing to questions involving unseen KB schema items .",0,0.8402446,485.574755384337,15
2937,"Prior ranking-based approaches have shown some success in generalization , but suffer from the coverage issue .",0,0.89549255,35.35960673794308,19
2937,"We present RnG-KBQA , a Rank-and-Generate approach for KBQA , which remedies the coverage issue with a generation model while preserving a strong generalization capability .",1,0.43917054,41.72054726896902,31
2937,Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph .,2,0.8176235,38.69200434886437,23
2937,It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form .,2,0.68293697,49.01824629467526,22
2937,We achieve new state-of-the-art results on GrailQA and WebQSP datasets .,3,0.90696675,12.42411390089593,15
2937,"In particular , our method surpasses the prior state-of-the-art by a large margin on the GrailQA leaderboard .",3,0.9145286,12.307720230920195,24
2937,"In addition , RnG-KBQA outperforms all prior approaches on the popular WebQSP benchmark , even including the ones that use the oracle entity linking .",3,0.9257354,82.36176962644713,27
2937,"The experimental results demonstrate the effectiveness of the interplay between ranking and generation , which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization .",3,0.9701642,29.88902795660841,35
2938,"Given the claims of improved text generation quality across various pre-trained neural models , we consider the coherence evaluation of machine generated text to be one of the principal applications of coherence models that needs to be investigated .",3,0.46926126,26.607002188372622,39
2938,Prior work in neural coherence modeling has primarily focused on devising new architectures for solving the permuted document task .,0,0.9276534,53.763424363200144,20
2938,We instead use a basic model architecture and show significant improvements over state of the art within the same training regime .,3,0.70920235,33.76667157543091,22
2938,"We then design a harder self-supervision objective by increasing the ratio of negative samples within a contrastive learning setup , and enhance the model further through automatic hard negative mining coupled with a large global negative queue encoded by a momentum encoder .",2,0.76659596,129.79308447081473,43
2938,"We show empirically that increasing the density of negative samples improves the basic model , and using a global negative queue further improves and stabilizes the model while training with hard negative samples .",3,0.95443916,57.992279344297515,34
2938,We evaluate the coherence model on task-independent test sets that resemble real-world applications and show significant improvements in coherence evaluations of downstream tasks .,3,0.5665653,27.327753922076568,26
2939,Word and sentence embeddings are useful feature representations in natural language processing .,0,0.8563561,14.04787075543539,13
2939,"However , intrinsic evaluation for embeddings lags far behind , and there has been no significant update since the past decade .",0,0.902086,54.796239986589065,22
2939,Word and sentence similarity tasks have become the de facto evaluation method .,0,0.92975307,37.346146033320345,13
2939,"It leads models to overfit to such evaluations , negatively impacting embedding models ’ development .",3,0.51278937,260.1342275796486,16
2939,This paper first points out the problems using semantic similarity as the gold standard for word and sentence embedding evaluations .,1,0.8002031,40.519338968557435,21
2939,"Further , we propose a new intrinsic evaluation method called EvalRank , which shows a much stronger correlation with downstream tasks .",3,0.5086693,46.02326826506862,22
2939,Extensive experiments are conducted based on 60 + models and popular datasets to certify our judgments .,2,0.7659038,152.92254406393917,17
2939,"Finally , the practical evaluation toolkit is released for future benchmarking purposes .",3,0.68501925,66.99969533920547,13
2940,"Multimodal pre-training with text , layout , and image has made significant progress for Visually Rich Document Understanding ( VRDU ) , especially the fixed-layout documents such as scanned document images .",0,0.93942744,72.82471722663612,34
2940,"While , there are still a large number of digital documents where the layout information is not fixed and needs to be interactively and dynamically rendered for visualization , making existing layout-based pre-training approaches not easy to apply .",0,0.8995508,38.82107244407251,41
2940,"In this paper , we propose MarkupLM for document understanding tasks with markup languages as the backbone , such as HTML / XML-based documents , where text and markup information is jointly pre-trained .",1,0.7712551,60.47554883920434,36
2940,Experiment results show that the pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding tasks .,3,0.9765761,27.000123915576523,20
2940,The pre-trained model and code will be publicly available at https://aka.ms/markuplm .,3,0.6083598,20.976723131299607,12
2941,CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks .,0,0.84792626,19.678140491972247,15
2941,"Previously , CLIP is only regarded as a powerful visual encoder .",0,0.94073516,100.69492044380432,12
2941,"However , after being pre-trained by language supervision from a large amount of image-caption pairs , CLIP itself should also have acquired some few-shot abilities for vision-language tasks .",3,0.7284605,81.32028865153919,31
2941,"In this work , we empirically show that CLIP can be a strong vision-language few-shot learner by leveraging the power of language .",1,0.5420417,30.28986273046183,26
2941,We first evaluate CLIP ’s zero-shot performance on a typical visual question answering task and demonstrate a zero-shot cross-modality transfer capability of CLIP on the visual entailment task .,2,0.49115592,21.19672812604312,29
2941,Then we propose a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task .,2,0.501856,18.24656200233231,19
2941,We achieve competitive zero / few-shot results on the visual question answering and visual entailment tasks without introducing any additional pre-training procedure .,3,0.9103203,36.94351995931903,23
2942,"Complex question answering over knowledge base ( Complex KBQA ) is challenging because it requires various compositional reasoning capabilities , such as multi-hop inference , attribute comparison , set operation , etc .",0,0.9312221,82.13845160174905,33
2942,Existing benchmarks have some shortcomings that limit the development of Complex KBQA : 1 ) they only provide QA pairs without explicit reasoning processes ;,0,0.8003099,134.8279240340627,25
2942,2 ) questions are poor in diversity or scale .,0,0.5212882,317.6107535794051,10
2942,"To this end , we introduce KQA Pro , a dataset for Complex KBQA including around 120K diverse natural language questions .",2,0.58232653,73.24642498719331,22
2942,We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions .,2,0.5523785,54.042751038001995,18
2942,"For each question , we provide the corresponding KoPL program and SPARQL query , so that KQA Pro can serve for both KBQA and semantic parsing tasks .",2,0.73762774,76.21124076036064,28
2942,"Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets , which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts .",3,0.97919023,26.7818967232033,37
2942,"We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills , conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA .",3,0.537289,83.30449569964433,30
2942,Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro_Baselines .,3,0.71686846,26.181268500413495,10
2943,"Recently , contrastive learning has been shown to be effective in improving pre-trained language models ( PLM ) to derive high-quality sentence representations .",0,0.9322699,12.188929200143281,24
2943,It aims to pull close positive examples to enhance the alignment while push apart irrelevant negatives for the uniformity of the whole representation space .,0,0.61933476,273.6675799166024,25
2943,"However , previous works mostly adopt in-batch negatives or sample from training data at random .",0,0.85067487,144.55905737449456,18
2943,"Such a way may cause the sampling bias that improper negatives ( false negatives and anisotropy representations ) are used to learn sentence representations , which will hurt the uniformity of the representation space .",3,0.62906873,66.8975079763913,35
2943,"To address it , we present a new framework DCLR ( Debiased Contrastive Learning of unsupervised sentence Representations ) to alleviate the influence of these improper negatives .",1,0.4063155,86.65478027878544,28
2943,"In DCLR , we design an instance weighting method to punish false negatives and generate noise-based negatives to guarantee the uniformity of the representation space .",2,0.7686549,56.66301883250818,28
2943,Experiments on seven semantic textual similarity tasks show that our approach is more effective than competitive baselines .,3,0.89609253,14.312916429524321,18
2943,Our code and data are publicly available at the link : bluehttps://github.com/RUCAIBox/DCLR .,3,0.5744348,28.684348167876404,13
2944,Prompting has recently been shown as a promising approach for applying pre-trained language models to perform downstream tasks .,0,0.88539785,11.564801119584674,19
2944,"We present Multi-Stage Prompting , a simple and automatic approach for leveraging pre-trained language models to translation tasks .",1,0.36808163,30.575481834564904,19
2944,"To better mitigate the discrepancy between pre-training and translation , MSP divides the translation process via pre-trained language models into three separate stages : the encoding stage , the re-encoding stage , and the decoding stage .",2,0.60116917,28.45424497093685,37
2944,"During each stage , we independently apply different continuous prompts for allowing pre-trained language models better shift to translation tasks .",2,0.79781884,148.8388063814753,21
2944,We conduct extensive experiments on three translation tasks .,2,0.823167,28.361962658042216,9
2944,Experiments show that our method can significantly improve the translation performance of pre-trained language models .,3,0.95512503,5.9234736064550235,16
2945,"Dialogue systems are usually categorized into two types , open-domain and task-oriented .",0,0.8894746,23.098179814450724,14
2945,"The first one focuses on chatting with users and making them engage in the conversations , where selecting a proper topic to fit the dialogue context is essential for a successful dialogue .",0,0.4101703,45.119035803839864,33
2945,"The other one focuses on a specific task instead of casual talks , e.g. , finding a movie on Friday night , playing a song .",0,0.3996783,127.82568203613685,26
2945,These two directions have been studied separately due to their different purposes .,0,0.8484175,27.494056403749564,13
2945,"However , how to smoothly transition from social chatting to task-oriented dialogues is important for triggering the business opportunities , and there is no any public data focusing on such scenarios .",0,0.8788821,95.41421190050832,34
2945,"Hence , this paper focuses on investigating the conversations starting from open-domain social chatting and then gradually transitioning to task-oriented purposes , and releases a large-scale dataset with detailed annotations for encouraging this research direction .",1,0.73512876,78.03996962695388,38
2945,"To achieve this goal , this paper proposes a framework to automatically generate many dialogues without human involvement , in which any powerful open-domain dialogue generation model can be easily leveraged .",1,0.7578922,33.89364227310923,32
2945,"The human evaluation shows that our generated dialogue data has a natural flow at a reasonable quality , showing that our released data has a great potential of guiding future research directions and commercial activities .",3,0.980212,48.42305455847549,36
2945,"Furthermore , the released models allow researchers to automatically generate unlimited dialogues in the target scenarios , which can greatly benefit semi-supervised and unsupervised approaches .",3,0.80653113,56.42903211677484,26
2946,High-quality phrase representations are essential to finding topics and related terms in documents ( a.k.a .,0,0.65666217,37.28316767191501,16
2946,topic mining ) .,2,0.4062256,3323.3272144358693,4
2946,Existing phrase representation learning methods either simply combine unigram representations in a context-free manner or rely on extensive annotations to learn context-aware knowledge .,0,0.7559552,35.2453828431472,27
2946,"In this paper , we propose UCTopic , a novel unsupervised contrastive learning framework for context-aware phrase representations and topic mining .",1,0.87673485,37.6237581818938,24
2946,UCTopic is pretrained in a large scale to distinguish if the contexts of two phrase mentions have the same semantics .,2,0.40614325,79.46241916154955,21
2946,The key to the pretraining is positive pair construction from our phrase-oriented assumptions .,2,0.46297175,180.39965479279948,14
2946,"However , we find traditional in-batch negatives cause performance decay when finetuning on a dataset with small topic numbers .",3,0.95832634,143.5784068492745,22
2946,"Hence , we propose cluster-assisted contrastive learning ( CCL ) which largely reduces noisy negatives by selecting negatives from clusters and further improves phrase representations for topics accordingly .",2,0.39313525,135.8127765338559,29
2946,UCTopic outperforms the state-of-the-art phrase representation model by 38.2 % NMI in average on four entity clustering tasks .,3,0.9062092,32.531845060922215,23
2946,Comprehensive evaluation on topic mining shows that UCTopic can extract coherent and diverse topical phrases .,3,0.91610736,228.90098448603453,16
2947,"In this paper , we introduce ELECTRA-style tasks to cross-lingual language model pre-training .",1,0.823617,35.89801635602691,16
2947,"Specifically , we present two pre-training tasks , namely multilingual replaced token detection , and translation replaced token detection .",2,0.7541367,77.76886583150865,20
2947,"Besides , we pretrain the model , named as XLM-E , on both multilingual and parallel corpora .",2,0.7623485,47.688182843994355,19
2947,Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost .,3,0.90379035,16.217576434824018,17
2947,"Moreover , analysis shows that XLM-E tends to obtain better cross-lingual transferability .",3,0.978044,32.27146234609413,14
2948,Nested named entity recognition ( NER ) has been receiving increasing attention .,0,0.9691495,38.091813475994044,13
2948,"Recently , Fu et al .",0,0.71202666,70.77064080173267,6
2948,( 2020 ) adapt a span-based constituency parser to tackle nested NER .,0,0.428258,139.65880049849602,13
2948,They treat nested entities as partially-observed constituency trees and propose the masked inside algorithm for partial marginalization .,2,0.38006717,127.42208663840537,20
2948,"However , their method cannot leverage entity heads , which have been shown useful in entity mention detection and entity typing .",3,0.5328031,100.6365030435615,22
2948,"In this work , we resort to more expressive structures , lexicalized constituency trees in which constituents are annotated by headwords , to model nested entities .",2,0.66411513,115.55436102049114,27
2948,We leverage the Eisner-Satta algorithm to perform partial marginalization and inference efficiently .,2,0.726269,207.13804758390575,13
2948,"In addition , we propose to use ( 1 ) a two-stage strategy ( 2 ) a head regularization loss and ( 3 ) a head-aware labeling loss in order to enhance the performance .",2,0.6094141,29.867913658792,38
2948,We make a thorough ablation study to investigate the functionality of each component .,1,0.39105433,32.077585969223286,14
2948,"Experimentally , our method achieves the state-of-the-art performance on ACE2004 , ACE2005 and NNE , and competitive performance on GENIA , and meanwhile has a fast inference speed .",3,0.8727494,32.47521213043266,34
2949,NLP practitioners often want to take existing trained models and apply them to data from new domains .,0,0.9075577,40.967733421707436,18
2949,"While fine-tuning or few-shot learning can be used to adapt a base model , there is no single recipe for making these techniques work ;",0,0.7803111,45.97849907515769,26
2949,"moreover , one may not have access to the original model weights if it is deployed as a black box .",0,0.57497865,69.33843477141053,21
2949,We study how to improve a black box model ’s performance on a new domain by leveraging explanations of the model ’s behavior .,1,0.74962753,38.48823909007569,24
2949,"Our approach first extracts a set of features combining human intuition about the task with model attributions generated by black box interpretation techniques , then uses a simple calibrator , in the form of a classifier , to predict whether the base model was correct or not .",2,0.8273707,55.10521964029303,48
2949,"We experiment with our method on two tasks , extractive question answering and natural language inference , covering adaptation from several pairs of domains with limited target-domain data .",2,0.717757,55.16192659655203,30
2949,"The experimental results across all the domain pairs show that explanations are useful for calibrating these models , boosting accuracy when predictions do not have to be returned on every example .",3,0.9788087,86.76021092307855,32
2949,We further show that the calibration model transfers to some extent between tasks .,3,0.9588531,114.12498381335419,14
2950,"Different Open Information Extraction ( OIE ) tasks require different types of information , so the OIE field requires strong adaptability of OIE algorithms to meet different task requirements .",0,0.9303584,40.336318781266236,30
2950,This paper discusses the adaptability problem in existing OIE systems and designs a new adaptable and efficient OIE system-OIE@OIA as a solution .,1,0.84899056,65.7441380287619,25
2950,OIE@OIA follows the methodology of Open Information eXpression ( OIX ) : parsing a sentence to an Open Information Annotation ( OIA ) Graph and then adapting the OIA graph to different OIE tasks with simple rules .,0,0.46402526,94.32900197620253,38
2950,"As the core of our OIE@OIA system , we implement an end-to-end OIA generator by annotating a dataset ( we make it open available ) and designing an efficient learning algorithm for the complex OIA graph .",2,0.70558393,66.99321022038544,38
2950,We easily adapt the OIE@OIA system to accomplish three popular OIE tasks .,2,0.47261047,110.90505011181621,13
2950,"The experimental show that our OIE@OIA achieves new SOTA performances on these tasks , showing the great adaptability of our OIE@OIA system .",3,0.9783925,49.93946953126419,23
2950,"Furthermore , compared to other end-to-end OIE baselines that need millions of samples for training , our OIE@OIA needs much fewer training samples ( 12 K ) , showing a significant advantage in terms of efficiency .",3,0.9385151,51.177592036063345,38
2951,"Code completion , which aims to predict the following code token ( s ) according to the code context , can improve the productivity of software development .",0,0.8794861,63.94875172973184,28
2951,Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets .,0,0.89391345,29.537570595262068,28
2951,"However , current approaches focus only on code context within the file or project , i.e .",0,0.90752447,68.51771096502692,17
2951,internal context .,0,0.534192,157.21470503949422,3
2951,"Our distinction is utilizing ” external ” context , inspired by human behaviors of copying from the related code snippets when writing code .",2,0.6161048,230.2909368662302,24
2951,"Specifically , we propose a retrieval-augmented code completion framework , leveraging both lexical copying and referring to code with similar semantics by retrieval .",2,0.59035784,78.53759017258228,26
2951,We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language .,2,0.84624743,32.19554929274458,21
2951,"We evaluate our approach in the code completion task in Python and Java programming languages , achieving a state-of-the-art performance on CodeXGLUE benchmark .",2,0.46301574,25.115620914998477,29
2952,DocRED is a widely used dataset for document-level relation extraction .,0,0.7411697,24.656732615349423,12
2952,"In the large-scale annotation , a recommend-revise scheme is adopted to reduce the workload .",2,0.44474012,86.75086169556097,17
2952,"Within this scheme , annotators are provided with candidate relation instances from distant supervision , and they then manually supplement and remove relational facts based on the recommendations .",2,0.6967761,137.54959288249307,29
2952,"However , when comparing DocRED with a subset relabeled from scratch , we find that this scheme results in a considerable amount of false negative samples and an obvious bias towards popular entities and relations .",3,0.9721763,85.60524963116549,36
2952,"Furthermore , we observe that the models trained on DocRED have low recall on our relabeled dataset and inherit the same bias in the training data .",3,0.96973,50.27448213140418,27
2952,"Through the analysis of annotators ’ behaviors , we figure out the underlying reason for the problems above : the scheme actually discourages annotators from supplementing adequate instances in the revision phase .",3,0.76441234,95.37454672718603,33
2952,We appeal to future research to take into consideration the issues with the recommend-revise scheme when designing new models and annotation schemes .,3,0.9395074,75.57472331378958,25
2952,"The relabeled dataset is released at https://github.com/AndrewZhe/Revisit-DocRED, to serve as a more reliable test set of document RE models .",3,0.56178457,26.347559606068707,20
2953,Recent parameter-efficient language model tuning ( PELT ) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when training data is limited .,0,0.89962673,31.52646526252852,33
2953,"However , different PELT methods may perform rather differently on the same task , making it nontrivial to select the most appropriate method for a specific task , especially considering the fast-growing number of new PELT methods and tasks .",0,0.7658417,34.76638528925085,41
2953,"In light of model diversity and the difficulty of model selection , we propose a unified framework , UniPELT , which incorporates different PELT methods as submodules and learns to activate the ones that best suit the current data or task setup via gating mechanism .",2,0.6355453,44.435460869690566,46
2953,"On the GLUE benchmark , UniPELT consistently achieves 1 4 % gains compared to the best individual PELT method that it incorporates and even outperforms fine-tuning under different setups .",3,0.94228816,58.10637658323541,30
2953,"Moreover , UniPELT generally surpasses the upper bound that takes the best performance of all its submodules used individually on each task , indicating that a mixture of multiple PELT methods may be inherently more effective than single methods .",3,0.97366357,65.09206859878637,40
2954,A recent study by Feldman ( 2020 ) proposed a long-tail theory to explain the memorization behavior of deep learning models .,0,0.85645616,39.77354641142757,22
2954,"However , memorization has not been empirically verified in the context of NLP , a gap addressed by this work .",0,0.536867,39.95260331041406,21
2954,"In this paper , we use three different NLP tasks to check if the long-tail theory holds .",1,0.58827597,38.57006853619017,18
2954,"Our experiments demonstrate that top-ranked memorized training instances are likely atypical , and removing the top-memorized training instances leads to a more serious drop in test accuracy compared with removing training instances randomly .",3,0.97726554,41.04496913932488,34
2954,"Furthermore , we develop an attribution method to better understand why a training instance is memorized .",2,0.5679804,52.50746665019265,17
2954,"We empirically show that our memorization attribution method is faithful , and share our interesting finding that the top-memorized parts of a training instance tend to be features negatively correlated with the class label .",3,0.92108417,75.64560495317475,35
2955,"Pretrained multilingual models are able to perform cross-lingual transfer in a zero-shot setting , even for languages unseen during pretraining .",3,0.63272,13.830352388806785,21
2955,"However , prior work evaluating performance on unseen languages has largely been limited to low-level , syntactic tasks , and it remains unclear if zero-shot learning of high-level , semantic tasks is possible for unseen languages .",0,0.94018745,27.291018498567592,38
2955,"To explore this question , we present AmericasNLI , an extension of XNLI ( Conneau et al. , 2018 ) to 10 Indigenous languages of the Americas .",1,0.6700109,48.118318596368795,28
2955,"We conduct experiments with XLM-R , testing multiple zero-shot and translation-based approaches .",2,0.73021036,47.08671671388602,16
2955,"Additionally , we explore model adaptation via continued pretraining and provide an analysis of the dataset by considering hypothesis-only models .",2,0.4646187,59.73651105756451,21
2955,"We find that XLM-R’s zero-shot performance is poor for all 10 languages , with an average performance of 38.48 % .",3,0.9791646,27.424954372419602,24
2955,"Continued pretraining offers improvements , with an average accuracy of 43.85 % .",3,0.9497574,84.97195836788997,13
2955,"Surprisingly , training on poorly translated data by far outperforms all other methods with an accuracy of 49.12 % .",3,0.95987445,53.619297787884435,20
2956,Understanding the functional ( dis )-similarity of source code is significant for code modeling tasks such as software vulnerability and code clone detection .,0,0.9471119,99.93470103347427,26
2956,"We present DISCO ( DIS-similarity of COde ) , a novel self-supervised model focusing on identifying ( dis ) similar functionalities of source code .",1,0.43877405,107.33331468127744,27
2956,"Different from existing works , our approach does not require a huge amount of randomly collected datasets .",2,0.45811555,31.999855346575544,18
2956,"Rather , we design structure-guided code transformation algorithms to generate synthetic code clones and inject real-world security bugs , augmenting the collected datasets in a targeted way .",2,0.7679802,92.38267532350342,30
2956,We propose to pre-train the Transformer model with such automatically generated program contrasts to better identify similar code in the wild and differentiate vulnerable programs from benign ones .,3,0.6498597,61.209805885125064,29
2956,"To better capture the structural features of source code , we propose a new cloze objective to encode the local tree-based context ( e.g. , parents or sibling nodes ) .",2,0.52714455,53.9603512262124,33
2956,"We pre-train our model with a much smaller dataset , the size of which is only 5 % of the state-of-the-art models ’ training datasets , to illustrate the effectiveness of our data augmentation and the pre-training approach .",2,0.5931358,17.42993042022449,45
2956,"The evaluation shows that , even with much less data , DISCO can still outperform the state-of-the-art models in vulnerability and code clone detection tasks .",3,0.97224563,32.11199698555583,32
2957,"Most works on financial forecasting use information directly associated with individual companies ( e.g. , stock prices , news on the company ) to predict stock returns for trading .",0,0.8893749,57.39243227288193,30
2957,We refer to such company-specific information as local information .,2,0.5154331,54.78260240680609,11
2957,"Stock returns may also be influenced by global information ( e.g. , news on the economy in general ) , and inter-company relationships .",0,0.7789878,60.81626242638308,24
2957,"Capturing such diverse information is challenging due to the low signal-to-noise ratios , different time-scales , sparsity and distributions of global and local information from different modalities .",0,0.936214,25.190991349506067,31
2957,"In this paper , we propose a model that captures both global and local multimodal information for investment and risk management-related forecasting tasks .",1,0.889271,27.573415244202774,26
2957,Our proposed Guided Attention Multimodal Multitask Network ( GAME ) model addresses these challenges by using novel attention modules to guide learning with global and local information from different modalities and dynamic inter-company relationship networks .,3,0.41070518,51.225090835682835,36
2957,Our extensive experiments show that GAME outperforms other state-of-the-art models in several forecasting tasks and important real-world application case studies .,3,0.9637148,21.5785005376566,27
2958,Previous work on multimodal machine translation ( MMT ) has focused on the way of incorporating vision features into translation but little attention is on the quality of vision models .,0,0.95402527,27.5406501165947,31
2958,"In this work , we investigate the impact of vision models on MMT .",1,0.9167596,48.711251198725115,14
2958,"Given the fact that Transformer is becoming popular in computer vision , we experiment with various strong models ( such as Vision Transformer ) and enhanced features ( such as object-detection and image captioning ) .",2,0.5244961,31.563475641792433,36
2958,We develop a selective attention model to study the patch-level contribution of an image in MMT .,2,0.5579727,55.500617029330435,18
2958,"On detailed probing tasks , we find that stronger vision models are helpful for learning translation from the visual modality .",3,0.9696689,74.13407672038562,21
2958,"Our results also suggest the need of carefully examining MMT models , especially when current benchmarks are small-scale and biased .",3,0.9909622,69.76270429354322,21
2959,Named Entity Recognition ( NER ) in Few-Shot setting is imperative for entity tagging in low resource domains .,0,0.9363776,37.64803938291611,21
2959,Existing approaches only learn class-specific semantic features and intermediate representations from source domains .,0,0.7488917,50.72064112833901,14
2959,"This affects generalizability to unseen target domains , resulting in suboptimal performances .",0,0.6882961,52.085263135724496,13
2959,"To this end , we present CONTaiNER , a novel contrastive learning technique that optimizes the inter-token distribution distance for Few-Shot NER .",1,0.6299071,58.646307802931005,25
2959,"Instead of optimizing class-specific attributes , CONTaiNER optimizes a generalized objective of differentiating between token categories based on their Gaussian-distributed embeddings .",2,0.5526324,93.80040433072055,24
2959,This effectively alleviates overfitting issues originating from training domains .,3,0.6779046,97.83566138574488,10
2959,"Our experiments in several traditional test domains ( OntoNotes , CoNLL ’03 , WNUT ‘ 17 , GUM ) and a new large scale Few-Shot NER dataset ( Few-NERD ) demonstrate that on average , CONTaiNER outperforms previous methods by 3 %-13 % absolute F1 points while showing consistent performance trends , even in challenging scenarios where previous approaches could not achieve appreciable performance .",3,0.91315687,77.26093604994726,70
2960,Plains Cree ( nêhiyawêwin ) is an Indigenous language that is spoken in Canada and the USA .,0,0.9692523,69.9540763072509,18
2960,"It is the most widely spoken dialect of Cree and a morphologically complex language that is polysynthetic , highly inflective , and agglutinative .",0,0.930453,26.312028892787836,24
2960,"It is an extremely low resource language , with no existing corpus that is both available and prepared for supporting the development of language technologies .",0,0.7955051,52.07339279029482,26
2960,"To support nêhiyaw êwin revitalization and preservation , we developed a corpus covering diverse genres , time periods , and texts for a variety of intended audiences .",2,0.69049585,177.95699495195436,28
2960,The data has been verified and cleaned ;,2,0.5291813,318.7076166728413,8
2960,it is ready for use in developing language technologies for nêhiyawêwin .,3,0.85781264,143.51283377595024,12
2960,The corpus includes the corresponding English phrases or audio files where available .,2,0.61278296,168.9851447359947,13
2960,We demonstrate the utility of the corpus through its community use and its use to build language technologies that can provide the types of support that community members have expressed are desirable .,3,0.8929383,44.76491752767554,33
2960,The corpus is available for public use .,3,0.637289,26.345712836963283,8
2961,Visual storytelling ( VIST ) is a typical vision and language task that has seen extensive development in the natural language generation research domain .,0,0.96074986,66.7799670444935,25
2961,"However , it remains unclear whether conventional automatic evaluation metrics for text generation are applicable on VIST .",0,0.9006001,60.56555832436675,18
2961,"In this paper , we present the VHED ( VIST Human Evaluation Data ) dataset , which first re-purposes human evaluation results for automatic evaluation ;",1,0.85083824,93.38843196106463,26
2961,"hence we develop Vrank ( VIST Ranker ) , a novel reference-free VIST metric for story evaluation .",1,0.41762996,231.923882570539,20
2961,"We first show that the results from commonly adopted automatic metrics for text generation have little correlation with those obtained from human evaluation , which motivates us to directly utilize human evaluation results to learn the automatic evaluation model .",3,0.6741816,31.786785571783565,40
2961,"In the experiments , we evaluate the generated texts to predict story ranks using our model as well as other reference-based and reference-free metrics .",2,0.6248357,52.94931203283414,28
2961,Results show that Vrank prediction is significantly more aligned to human evaluation than other metrics with almost 30 % higher accuracy when ranking story pairs .,3,0.98759407,110.14912786204643,26
2961,"Moreover , we demonstrate that only Vrank shows human-like behavior in its strong ability to find better stories when the quality gap between two stories is high .",3,0.9785928,76.80933918446513,28
2961,"Finally , we show the superiority of Vrank by its generalizability to pure textual stories , and conclude that this reuse of human evaluation results puts Vrank in a strong position for continued future advances .",3,0.97623175,76.75610409573882,36
2962,Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation ( NMT ) .,0,0.93804646,8.06022507153401,13
2962,"Different from prior works where pre-trained models usually adopt an unidirectional decoder , this paper demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT .",3,0.67015463,13.639326019726662,40
2962,"Specifically , we propose CeMAT , a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages .",2,0.5952951,28.339940554806628,22
2962,"We also introduce two simple but effective methods to enhance the CeMAT , aligned code-switching & masking and dynamic dual-masking .",2,0.43762258,108.08578574775186,21
2962,"We conduct extensive experiments and show that our CeMAT can achieve significant performance improvement for all scenarios from low-to extremely high-resource languages , i.e. , up to + 14.4 BLEU on low resource and + 7.9 BLEU improvements on average for Autoregressive NMT .",3,0.8321429,32.73193952431842,46
2962,"For Non-autoregressive NMT , we demonstrate it can also produce consistent performance gains , i.e. , up to + 5.3 BLEU .",3,0.93519944,29.111619225976096,22
2962,"To the best of our knowledge , this is the first work to pre-train a unified model for fine-tuning on both NMT tasks .",3,0.9244274,8.578262900631605,24
2962,"Code , data , and pre-trained models are available at https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT .",3,0.5720791,25.378683945272332,12
2963,"We introduce CARETS , a systematic test suite to measure consistency and robustness of modern VQA models through a series of six fine-grained capability tests .",1,0.38270333,37.36650615965898,28
2963,"In contrast to existing VQA test sets , CARETS features balanced question generation to create pairs of instances to test models , with each pair focusing on a specific capability such as rephrasing , logical symmetry or image obfuscation .",0,0.41285738,94.69273157913965,40
2963,"We evaluate six modern VQA systems on CARETS and identify several actionable weaknesses in model comprehension , especially with concepts such as negation , disjunction , or hypernym invariance .",3,0.5825352,100.48607965405989,30
2963,"Interestingly , even the most sophisticated models are sensitive to aspects such as swapping the order of terms in a conjunction or varying the number of answer choices mentioned in the question .",3,0.85193837,53.7233311746187,33
2963,We release CARETS to be used as an extensible tool for evaluating multi-modal model robustness .,3,0.6387683,37.62801029836196,16
2964,Recent studies have achieved inspiring success in unsupervised grammar induction using masked language modeling ( MLM ) as the proxy task .,0,0.941787,50.31960708328227,22
2964,"Despite their high accuracy in identifying low-level structures , prior arts tend to struggle in capturing high-level structures like clauses , since the MLM task usually only requires information from local context .",0,0.8594141,59.753433299365184,35
2964,"In this work , we revisit LM-based constituency parsing from a phrase-centered perspective .",1,0.7996649,53.13034742398016,16
2964,"Inspired by the natural reading process of human , we propose to regularize the parser with phrases extracted by an unsupervised phrase tagger to help the LM model quickly manage low-level structures .",2,0.61566824,51.28391803496677,33
2964,"For a better understanding of high-level structures , we propose a phrase-guided masking strategy for LM to emphasize more on reconstructing non-phrase words .",1,0.31450045,61.60413343919857,27
2964,"We show that the initial phrase regularization serves as an effective bootstrap , and phrase-guided masking improves the identification of high-level structures .",3,0.9642231,52.339646079678225,26
2964,Experiments on the public benchmark with two different backbone models demonstrate the effectiveness and generality of our method .,3,0.80896986,16.3795736819923,19
2965,Evaluation of open-domain dialogue systems is highly challenging and development of better techniques is highlighted time and again as desperately needed .,0,0.76237255,41.58303891118723,22
2965,"Despite substantial efforts to carry out reliable live evaluation of systems in recent competitions , annotations have been abandoned and reported as too unreliable to yield sensible results .",0,0.8824384,121.99160331795808,29
2965,This is a serious problem since automatic metrics are not known to provide a good indication of what may or may not be a high-quality conversation .,0,0.8476907,17.887153867825845,27
2965,"Answering the distress call of competitions that have emphasized the urgent need for better evaluation techniques in dialogue , we present the successful development of human evaluation that is highly reliable while still remaining feasible and low cost .",1,0.6098282,76.54992000404476,39
2965,Self-replication experiments reveal almost perfectly repeatable results with a correlation of r=0.969 .,3,0.9069582,42.44647238131393,13
2965,"Furthermore , due to the lack of appropriate methods of statistical significance testing , the likelihood of potential improvements to systems occurring due to chance is rarely taken into account in dialogue evaluation , and the evaluation we propose facilitates application of standard tests .",3,0.70291704,90.08879915818545,45
2965,"Since we have developed a highly reliable evaluation method , new insights into system performance can be revealed .",3,0.7126985,68.30502168491671,19
2965,"We therefore include a comparison of state-of-the-art models ( i ) with and without personas , to measure the contribution of personas to conversation quality , as well as ( ii ) prescribed versus freely chosen topics .",2,0.82996356,42.25994626920218,44
2965,"Interestingly with respect to personas , results indicate that personas do not positively contribute to conversation quality as expected .",3,0.9878429,31.781329476717193,20
2966,"We propose the task of updated headline generation , in which a system generates a headline for an updated article , considering both the previous article and headline .",1,0.486966,54.23155888313474,29
2966,"The system must identify the novel information in the article update , and modify the existing headline accordingly .",3,0.5618343,103.03884847002564,19
2966,We create data for this task using the NewsEdits corpus by automatically identifying contiguous article versions that are likely to require a substantive headline update .,2,0.8835776,99.4003292494138,26
2966,We find that models conditioned on the prior headline and body revisions produce headlines judged by humans to be as factual as gold headlines while making fewer unnecessary edits compared to a standard headline generation model .,3,0.977861,81.54146001874255,37
2966,Our experiments establish benchmarks for this new contextual summarization task .,3,0.92269826,128.07021138561996,11
2967,Current open-domain conversational models can easily be made to talk in inadequate ways .,0,0.716576,42.584317981582444,14
2967,"Online learning from conversational feedback given by the conversation partner is a promising avenue for a model to improve and adapt , so as to generate fewer of these safety failures .",3,0.59360814,78.35680445137042,32
2967,"However , current state-of-the-art models tend to react to feedback with defensive or oblivious responses .",0,0.9116547,27.125950593565932,22
2967,This makes for an unpleasant experience and may discourage conversation partners from giving feedback in the future .,0,0.5763018,30.495537643367314,18
2967,"This work proposes SaFeRDialogues , a task and dataset of graceful responses to conversational feedback about safety failures .",1,0.738078,226.60355520454593,19
2967,"We collect a dataset of 8 k dialogues demonstrating safety failures , feedback signaling them , and a response acknowledging the feedback .",2,0.90663826,222.96756019339415,23
2967,"We show how fine-tuning on this dataset results in conversations that human raters deem considerably more likely to lead to a civil conversation , without sacrificing engagingness or general conversational ability .",3,0.93669033,53.42407413084011,33
2968,Compositionality — the ability to combine familiar units like words into novel phrases and sentences — has been the focus of intense interest in artificial intelligence in recent years .,0,0.96781015,22.05114387925673,30
2968,"To test compositional generalization in semantic parsing , Keysers et al .",0,0.6329102,75.64582137729613,12
2968,( 2020 ) introduced Compositional Freebase Queries ( CFQ ) .,0,0.8802473,159.12047461323624,11
2968,"This dataset maximizes the similarity between the test and train distributions over primitive units , like words , while maximizing the compound divergence : the dissimilarity between test and train distributions over larger structures , like phrases .",2,0.66045445,127.68075936031325,38
2968,"Dependency parsing , however , lacks a compositional generalization benchmark .",0,0.8962411,120.56765597637013,11
2968,"In this work , we introduce a gold-standard set of dependency parses for CFQ , and use this to analyze the behaviour of a state-of-the art dependency parser ( Qi et al. , 2020 ) on the CFQ dataset .",2,0.4067887,33.250373178353136,44
2968,"We find that increasing compound divergence degrades dependency parsing performance , although not as dramatically as semantic parsing performance .",3,0.98073214,82.24391674984963,20
2968,"Additionally , we find the performance of the dependency parser does not uniformly degrade relative to compound divergence , and the parser performs differently on different splits with the same compound divergence .",3,0.96928453,58.34573267668038,33
2968,"We explore a number of hypotheses for what causes the non-uniform degradation in dependency parsing performance , and identify a number of syntactic structures that drive the dependency parser ’s lower performance on the most challenging splits .",3,0.54012626,43.394401239093774,38
2969,Generic summaries try to cover an entire document and query-based summaries try to answer document-specific questions .,0,0.8659897,26.39723217771664,19
2969,"But real users ’ needs often fall in between these extremes and correspond to aspects , high-level topics discussed among similar types of documents .",0,0.8503337,130.01161673974488,27
2969,"In this paper , we collect a dataset of realistic aspect-oriented summaries , AspectNews , which covers different subtopics about articles in news sub-domains .",1,0.47549686,78.78740124069083,25
2969,"We annotate data across two domains of articles , earthquakes and fraud investigations , where each article is annotated with two distinct summaries focusing on different aspects for each domain .",2,0.8814821,54.39381165267346,31
2969,A system producing a single generic summary cannot concisely satisfy both aspects .,0,0.63655806,288.73800341421133,13
2969,"Our focus in evaluation is how well existing techniques can generalize to these domains without seeing in-domain training data , so we turn to techniques to construct synthetic training data that have been used in query-focused summarization work .",1,0.46411297,46.82471810731444,42
2969,We compare several training schemes that differ in how strongly keywords are used and how oracle summaries are extracted .,2,0.7432593,59.34494061774078,20
2969,"Our evaluation shows that our final approach yields ( a ) focused summaries , better than those from a generic summarization system or from keyword matching ;",3,0.97443974,223.71602508753188,27
2969,( b ) a system sensitive to the choice of keywords .,3,0.47659382,169.05848698778235,12
2970,"We introduce MemSum ( Multi-step Episodic Markov decision process extractive SUMmarizer ) , a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history .",2,0.42267343,76.63476077311167,33
2970,"When MemSum iteratively selects sentences into the summary , it considers a broad information set that would intuitively also be used by humans in this task : 1 ) the text content of the sentence , 2 ) the global text context of the rest of the document , and 3 ) the extraction history consisting of the set of sentences that have already been extracted .",3,0.4122098,45.390154460300046,67
2970,"With a lightweight architecture , MemSum obtains state-of-the-art test-set performance ( ROUGE ) in summarizing long documents taken from PubMed , arXiv , and GovReport .",3,0.55223763,45.938384232692115,31
2970,"Ablation studies demonstrate the importance of local , global , and history information .",0,0.8255125,66.44907922105418,14
2970,"A human evaluation confirms the high quality and low redundancy of the generated summaries , stemming from MemSum ’s awareness of extraction history .",3,0.9260328,126.31303579800469,24
2971,Supervised learning has traditionally focused on inductive learning by observing labeled examples of a task .,0,0.93318206,83.1177703560798,16
2971,"In contrast , a hallmark of human intelligence is the ability to learn new concepts purely from language .",0,0.94118416,46.433482357944776,19
2971,"Here , we explore training zero-shot classifiers for structured data purely from language .",2,0.4029636,68.65330219430587,14
2971,"For this , we introduce CLUES , a benchmark for Classifier Learning Using natural language ExplanationS , consisting of a range of classification tasks over structured data along with natural language supervision in the form of explanations .",2,0.59497577,61.763758773320426,38
2971,CLUES consists of 36 real-world and 144 synthetic classification tasks .,2,0.6778309,94.60797217736587,11
2971,It contains crowdsourced explanations describing real-world tasks from multiple teachers and programmatically generated explanations for the synthetic tasks .,2,0.47656175,55.792573971777585,19
2971,"To model the influence of explanations in classifying an example , we develop ExEnt , an entailment-based model that learns classifiers using explanations .",2,0.6875743,53.75520852529862,26
2971,ExEnt generalizes up to 18 % better ( relative ) on novel tasks than a baseline that does not use explanations .,3,0.9231057,106.95131935821097,22
2971,"We delineate key challenges for automated learning from explanations , addressing which can lead to progress on CLUES in the future .",3,0.5413053,108.69758162785368,22
2971,Code and datasets are available at : https://clues-benchmark.github.io .,3,0.4640754,20.67434316267379,9
2972,"We present substructure distribution projection ( SubDP ) , a technique that projects a distribution over structures in one domain to another , by projecting substructure distributions separately .",1,0.52489865,125.1118796284919,29
2972,"Models for the target domain can then be trained , using the projected distributions as soft silver labels .",2,0.40951994,128.98700295947503,19
2972,"We evaluate SubDP on zero shot cross-lingual dependency parsing , taking dependency arcs as substructures : we project the predicted dependency arc distributions in the source language ( s ) to target language ( s ) , and train a target language parser on the resulting distributions .",2,0.8570398,96.97159183531735,48
2972,"Given an English tree bank as the only source of human supervision , SubDP achieves better unlabeled attachment score than all prior work on the Universal Dependencies v2.2 ( Nivre et al. , 2020 ) test set across eight diverse target languages , as well as the best labeled attachment score on six languages .",3,0.7339723,78.85509163991532,55
2972,"In addition , SubDP improves zero shot cross-lingual dependency parsing with very few ( e.g. , 50 ) supervised bitext pairs , across a broader range of target languages .",3,0.7279372,190.64879544585312,30
2973,"Detecting disclosures of individuals ’ employment status on social media can provide valuable information to match job seekers with suitable vacancies , offer social protection , or measure labor market flows .",0,0.6016794,109.73408562054355,32
2973,"However , identifying such personal disclosures is a challenging task due to their rarity in a sea of social media content and the variety of linguistic forms used to describe them .",0,0.94632906,27.796024246172806,32
2973,"Here , we examine three Active Learning ( AL ) strategies in real-world settings of extreme class imbalance , and identify five types of disclosures about individuals ’ employment status ( e.g .",1,0.5656285,85.60271884321006,33
2973,job loss ) in three languages using BERT-based classification models .,2,0.6636298,63.42698927141847,13
2973,"Our findings show that , even under extreme imbalance settings , a small number of AL iterations is sufficient to obtain large and significant gains in precision , recall , and diversity of results compared to a supervised baseline with the same number of labels .",3,0.9885005,58.011031023657054,46
2973,We also find that no AL strategy consistently outperforms the rest .,3,0.980775,67.90910813635129,12
2973,"Qualitative analysis suggests that AL helps focus the attention mechanism of BERT on core terms and adjust the boundaries of semantic expansion , highlighting the importance of interpretable models to provide greater control and visibility into this dynamic learning process .",3,0.9847418,69.89955935218013,41
2974,"Numerical reasoning over hybrid data containing both textual and tabular content ( e.g. , financial reports ) has recently attracted much attention in the NLP community .",0,0.9559395,32.56512840123923,27
2974,"However , existing question answering ( QA ) benchmarks over hybrid data only include a single flat table in each document and thus lack examples of multi-step numerical reasoning across multiple hierarchical tables .",0,0.90994567,68.24313346099252,34
2974,"To facilitate data analytical progress , we construct a new large-scale benchmark , MultiHiertt , with QA pairs over Multi Hierarchical Tabular and Textual data .",2,0.73945355,134.98572203782078,26
2974,MultiHiertt is built from a wealth of financial reports and has the following unique characteristics : 1 ) each document contain multiple tables and longer unstructured texts ;,0,0.7854909,139.51216983198188,28
2974,2 ) most of tables contained are hierarchical ;,3,0.64046234,2770.3507127570633,9
2974,3 ) the reasoning process required for each question is more complex and challenging than existing benchmarks ;,3,0.76041996,101.97952642753502,18
2974,and 4 ) fine-grained annotations of reasoning processes and supporting facts are provided to reveal complex numerical reasoning .,2,0.45661876,92.29567057027432,21
2974,"We further introduce a novel QA model termed MT2 Net , which first applies facts retrieving to extract relevant supporting facts from both tables and text and then uses a reasoning module to perform symbolic reasoning over retrieved facts .",2,0.68780243,68.96562871002119,40
2974,We conduct comprehensive experiments on various baselines .,2,0.7087825,44.56135010598476,8
2974,The experimental results show that MultiHiertt presents a strong challenge for existing baselines whose results lag far behind the performance of human experts .,3,0.9814295,51.805628329385996,24
2974,The dataset and code are publicly available at https://github.com/psunlpgroup/MultiHiertt .,3,0.57161117,28.749117000819002,10
2975,Representation of linguistic phenomena in computational language models is typically assessed against the predictions of existing linguistic theories of these phenomena .,0,0.9234112,36.32292876309373,22
2975,"Using the notion of polarity as a case study , we show that this is not always the most adequate set-up .",3,0.707234,25.87300876254301,23
2975,"We probe polarity via so-called ‘ negative polarity items ’ ( in particular , English ‘ any ’ ) in two pre-trained Transformer-based models ( BERT and GPT-2 ) .",2,0.87921494,47.64711026937945,34
2975,We show that – at least for polarity – metrics derived from language models are more consistent with data from psycholinguistic experiments than linguistic theory predictions .,3,0.9585729,34.21821133479693,27
2975,Establishing this allows us to more adequately evaluate the performance of language models and also to use language models to discover new insights into natural language grammar beyond existing linguistic theories .,3,0.6577689,31.71101463596317,32
2975,This work contributes to establishing closer ties between psycholinguistic experiments and experiments with language models .,3,0.9523849,23.999994891107857,16
2976,"Back-translation is a critical component of Unsupervised Neural Machine Translation ( UNMT ) , which generates pseudo parallel data from target monolingual data .",0,0.9368524,24.342662344500003,26
2976,"A UNMT model is trained on the pseudo parallel data with \text { \bf translated source} , and translates \text { \bf natural source } sentences in inference .",2,0.7664309,109.13263863107125,30
2976,The source discrepancy between training and inference hinders the translation performance of UNMT models .,0,0.56877506,61.798167420396005,15
2976,"By carefully designing experiments , we identify two representative characteristics of the data gap in source : ( 1 ) \text{\textit {style gap} } ( i.e. , translated vs .",3,0.5944854,199.73963896475647,32
2976,natural text style ) that leads to poor generalization capability ;,0,0.58382046,385.35132286060355,11
2976,( 2 ) \text { \textit {content gap} } that induces the model to produce hallucination content biased towards the target language .,2,0.46290818,74.25928557137084,24
2976,"To narrow the data gap , we propose an online self-training approach , which simultaneously uses the pseudo parallel data { natural source , translated target} to mimic the inference scenario .",2,0.72807026,149.66428984357765,32
2976,Experimental results on several widely-used language pairs show that our approach outperforms two strong baselines ( XLM and MASS ) by remedying the style and content gaps .,3,0.9241631,32.7756393082121,30
2977,BERT based ranking models have achieved superior performance on various information retrieval tasks .,0,0.84870607,25.757488858313405,14
2977,"However , the large number of parameters and complex self-attention operations come at a significant latency overhead .",3,0.49736223,36.55531697195537,18
2977,"To remedy this , recent works propose late-interaction architectures , which allow pre-computation of intermediate document representations , thus reducing latency .",0,0.8893328,90.35746974490749,22
2977,"Nonetheless , having solved the immediate latency issue , these methods now introduce storage costs and network fetching latency , which limit their adoption in real-life production systems .",0,0.8124768,106.15074946584755,29
2977,"In this work , we propose the Succinct Document Representation ( SDR ) scheme that computes highly compressed intermediate document representations , mitigating the storage / network issue .",1,0.7795144,83.34057170148789,29
2977,Our approach first reduces the dimension of token representations by encoding them using a novel autoencoder architecture that uses the document ’s textual content in both the encoding and decoding phases .,2,0.76852536,32.36443321539611,32
2977,"After this token encoding step , we further reduce the size of the document representations using modern quantization techniques .",2,0.57971257,56.00132584233906,20
2977,"Evaluation on MSMARCO ’s passage re-reranking task show that compared to existing approaches using compressed document representations , our method is highly efficient , achieving 4x – 11.6x higher compression rates for the same ranking quality .",3,0.9226975,87.43329999358535,38
2977,"Similarly , on the TREC CAR dataset , we achieve 7.7x higher compression rate for the same ranking quality .",3,0.9533542,99.32726875062545,20
2978,"Task-oriented dialogue systems are increasingly prevalent in healthcare settings , and have been characterized by a diverse range of architectures and objectives .",0,0.9432446,25.11676465489456,25
2978,"Although these systems have been surveyed in the medical community from a non-technical perspective , a systematic review from a rigorous computational perspective has to date remained noticeably absent .",0,0.9208236,47.68613633312518,30
2978,"As a result , many important implementation details of healthcare-oriented dialogue systems remain limited or underspecified , slowing the pace of innovation in this area .",0,0.79080546,61.87359169116543,28
2978,"To fill this gap , we investigated an initial pool of 4070 papers from well-known computer science , natural language processing , and artificial intelligence venues , identifying 70 papers discussing the system-level implementation of task-oriented dialogue systems for healthcare applications .",2,0.65219545,60.003865728358946,46
2978,"We conducted a comprehensive technical review of these papers , and present our key findings including identified gaps and corresponding recommendations .",3,0.34010956,80.5701863754207,22
2979,"Even though several methods have proposed to defend textual neural network ( NN ) models against black-box adversarial attacks , they often defend against a specific text perturbation strategy and / or require re-training the models from scratch .",0,0.9215554,40.87483307596644,41
2979,This leads to a lack of generalization in practice and redundant computation .,0,0.76184464,40.81608182610081,13
2979,"In particular , the state-of-the-art transformer models ( e.g. , BERT , RoBERTa ) require great time and computation resources .",0,0.82479376,20.29366931368684,25
2979,"By borrowing an idea from software engineering , in order to address these limitations , we propose a novel algorithm , SHIELD , which modifies and re-trains only the last layer of a textual NN , and thus it “ patches ” and “ transforms ” the NN into a stochastic weighted ensemble of multi-expert prediction heads .",2,0.64498955,48.39146632187574,58
2979,"Considering that most of current black-box attacks rely on iterative search mechanisms to optimize their adversarial perturbations , SHIELD confuses the attackers by automatically utilizing different weighted ensembles of predictors depending on the input .",0,0.40629646,48.12688916850786,37
2979,"In other words , SHIELD breaks a fundamental assumption of the attack , which is a victim NN model remains constant during an attack .",3,0.513179,95.33535263659624,25
2979,"By conducting comprehensive experiments , we demonstrate that all of CNN , RNN , BERT , and RoBERTa-based textual NNs , once patched by SHIELD , exhibit a relative enhancement of 15 % –70 % in accuracy on average against 14 different black-box attacks , outperforming 6 defensive baselines across 3 public datasets .",3,0.81727886,95.93461066932028,59
2979,All codes are to be released .,0,0.5639331,71.85255998422815,7
2980,Online alignment in machine translation refers to the task of aligning a target word to a source word when the target sequence has only been partially decoded .,0,0.93542355,15.667183743684493,28
2980,Good online alignments facilitate important applications such as lexically constrained translation where user-defined dictionaries are used to inject lexical constraints into the translation model .,0,0.78158104,48.27553891098182,25
2980,We propose a novel posterior alignment technique that is truly online in its execution and superior in terms of alignment error rates compared to existing methods .,1,0.41001767,43.93811109659658,27
2980,Our proposed inference technique jointly considers alignment and token probabilities in a principled manner and can be seamlessly integrated within existing constrained beam-search decoding algorithms .,3,0.7520434,110.45708086524023,26
2980,"On five language pairs , including two distant language pairs , we achieve consistent drop in alignment error rates .",3,0.90509063,93.94632943028695,20
2980,"When deployed on seven lexically constrained translation tasks , we achieve significant improvements in BLEU specifically around the constrained positions .",3,0.9155859,104.42881965758136,21
2981,Identifying sections is one of the critical components of understanding medical information from unstructured clinical notes and developing assistive technologies for clinical note-writing tasks .,0,0.90704256,33.59184800573139,25
2981,Most state-of-the-art text classification systems require thousands of in-domain text data to achieve high performance .,0,0.9061614,12.605463547493777,22
2981,"However , collecting in-domain and recent clinical note data with section labels is challenging given the high level of privacy and sensitivity .",0,0.8883685,78.02932761842446,24
2981,The present paper proposes an algorithmic way to improve the task transferability of meta-learning-based text classification in order to address the issue of low-resource target data .,1,0.80389905,17.735806389848396,29
2981,"Specifically , we explore how to make the best use of the source dataset and propose a unique task transferability measure named Normalized Negative Conditional Entropy ( NNCE ) .",1,0.5087209,46.5575056233853,30
2981,"Leveraging the NNCE , we develop strategies for selecting clinical categories and sections from source task data to boost cross-domain meta-learning accuracy .",2,0.48240894,98.18542627239283,23
2981,Experimental results show that our task selection strategies improve section classification accuracy significantly compared to meta-learning algorithms .,3,0.97590625,37.18900116968791,18
2982,"As large Pre-trained Language Models ( PLMs ) trained on large amounts of data in an unsupervised manner become more ubiquitous , identifying various types of bias in the text has come into sharp focus .",0,0.95291054,23.94783798945448,36
2982,Existing ‘ Stereotype Detection ’ datasets mainly adopt a diagnostic approach toward large PLMs .,0,0.8487589,143.83756949650723,15
2982,Blodgett et .,4,0.9255054,75.70012732245833,3
2982,al .,4,0.9227275,207.69517318273193,2
2982,( 2021 ) show that there are significant reliability issues with the existing benchmark datasets .,0,0.5190037,58.15846189152004,16
2982,Annotating a reliable dataset requires a precise understanding of the subtle nuances of how stereotypes manifest in text .,0,0.90948147,46.36622184192884,19
2982,"In this paper , we annotate a focused evaluation set for ‘ Stereotype Detection ’ that addresses those pitfalls by de-constructing various ways in which stereotypes manifest in text .",1,0.8720352,50.616544927264705,30
2982,"Further , we present a multi-task model that leverages the abundance of data-rich neighboring tasks such as hate speech detection , offensive language detection , misogyny detection , etc. , to improve the empirical performance on ‘ Stereotype Detection ’ .",2,0.5216146,28.18449653452831,42
2982,We then propose a reinforcement-learning agent that guides the multi-task learning model by learning to identify the training examples from the neighboring tasks that help the target task the most .,2,0.5036612,29.218752020746408,33
2982,We show that the proposed models achieve significant empirical gains over existing baselines on all the tasks .,3,0.94785297,17.223399447933158,18
2983,"While a great deal of work has been done on NLP approaches to lexical semantic change detection , other aspects of language change have received less attention from the NLP community .",0,0.8903998,14.599591618337202,32
2983,"In this paper , we address the detection of sound change through historical spelling .",1,0.91084826,152.4680594248475,15
2983,We propose that a sound change can be captured by comparing the relative distance through time between the distributions of the characters involved before and after the change has taken place .,1,0.3390526,41.35349753196967,32
2983,We model these distributions using PPMI character embeddings .,2,0.83918726,91.4027420322582,9
2983,We verify this hypothesis in synthetic data and then test the method ’s ability to trace the well-known historical change of lenition of plosives in Danish historical sources .,2,0.6439801,102.94426223037189,31
2983,We show that the models are able to identify several of the changes under consideration and to uncover meaningful contexts in which they appeared .,3,0.9487375,41.33499549935239,25
2983,The methodology has the potential to contribute to the study of open questions such as the relative chronology of sound shifts and their geographical distribution .,3,0.952882,43.424270160823646,26
2984,"Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content , which undermines their potential merits in real applications .",0,0.8019933,44.84230869824722,26
2984,Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level .,0,0.87833565,77.03825968138294,21
2984,"However ground-truth references may not be readily available for many free-form text generation applications , and sentence-or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time .",0,0.8193792,39.422836496145486,42
2984,"As a first step to addressing these issues , we propose a novel token-level , reference-free hallucination detection task and an associated annotated dataset named HaDeS ( HAllucination DEtection dataSet ) .",1,0.40128756,87.0534014360004,35
2984,"To create this dataset , we first perturb a large number of text segments extracted from English language Wikipedia , and then verify these with crowd-sourced annotations .",2,0.9115937,29.01236256387112,28
2984,"To mitigate label imbalance during annotation , we utilize an iterative model-in-loop strategy .",2,0.7894265,83.52588579399841,17
2984,We conduct comprehensive data analyses and create multiple baseline models .,2,0.7453084,114.17680249336205,11
2985,Classifiers in natural language processing ( NLP ) often have a large number of output classes .,0,0.9413833,27.558437134785756,17
2985,"For example , neural language models ( LMs ) and machine translation ( MT ) models both predict tokens from a vocabulary of thousands .",0,0.8329501,52.59122125881676,25
2985,"The Softmax output layer of these models typically receives as input a dense feature representation , which has much lower dimensionality than the output .",0,0.62098765,71.61223817421096,25
2985,"In theory , the result is some words may be impossible to be predicted via argmax , irrespective of input features , and empirically , there is evidence this happens in small language models ( Demeter et al. , 2020 ) .",0,0.6714751,114.07101289839018,42
2985,In this paper we ask whether it can happen in practical large language models and translation models .,1,0.90804195,67.05447649902142,18
2985,"To do so , we develop algorithms to detect such unargmaxable tokens in public models .",2,0.48997694,135.2880663805967,16
2985,We find that 13 out of 150 models do indeed have such tokens ;,3,0.9774075,177.20649871725516,14
2985,"however , they are very infrequent and unlikely to impact model quality .",0,0.52774,68.00693854494064,13
2985,We release our algorithms and code to the public .,2,0.5460887,27.73472509736809,10
2986,"In this paper , we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction ( EAE ) , which also generalizes well when there is a lack of training data .",1,0.8666708,32.97597532558736,40
2986,"On the one hand , PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models ( PLMs ) .",0,0.5715323,81.69444849274858,25
2986,It introduces two span selectors based on the prompt to select start / end tokens among input texts for each role .,2,0.6846855,122.69310840602787,22
2986,"On the other hand , it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss .",2,0.6149456,151.68149201589793,26
2986,"Also , with a flexible prompt design , PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning .",3,0.8154511,271.1564449427815,24
2986,"We have conducted extensive experiments on three benchmarks , including both sentence-and document-level EAE .",2,0.5546651,51.523219781760275,18
2986,"The results present promising improvements from PAIE ( 3.5 % and 2.3 % F1 gains in average on three benchmarks , for PAIE-base and PAIE-large respectively ) .",3,0.98035985,49.74095519206832,32
2986,"Further analysis demonstrates the efficiency , generalization to few-shot settings , and effectiveness of different extractive prompt tuning strategies .",3,0.9594641,63.34281467365421,20
2986,Our code is available at https://github.com/mayubo2333/PAIE .,3,0.5521576,16.391672536167682,7
2987,"Simultaneous machine translation ( SiMT ) starts translating while receiving the streaming source inputs , and hence the source sentence is always incomplete during translating .",0,0.90104926,137.92527267445234,26
2987,"Different from the full-sentence MT using the conventional seq-to-seq architecture , SiMT often applies prefix-to-prefix architecture , which forces each target word to only align with a partial source prefix to adapt to the incomplete source in streaming inputs .",0,0.7166819,72.39366512588707,41
2987,"However , the source words in the front positions are always illusoryly considered more important since they appear in more prefixes , resulting in position bias , which makes the model pay more attention on the front source positions in testing .",3,0.80273825,84.61663305989569,42
2987,"In this paper , we first analyze the phenomenon of position bias in SiMT , and develop a Length-Aware Framework to reduce the position bias by bridging the structural gap between SiMT and full-sentence MT .",1,0.84396213,24.559270060451805,38
2987,"Specifically , given the streaming inputs , we first predict the full-sentence length and then fill the future source position with positional encoding , thereby turning the streaming inputs into a pseudo full-sentence .",2,0.7805417,52.667398971716466,34
2987,The proposed framework can be integrated into most existing SiMT methods to further improve performance .,3,0.91995513,39.38733315507091,16
2987,"Experiments on two representative SiMT methods , including the state-of-the-art adaptive policy , show that our method successfully reduces the position bias and thereby achieves better SiMT performance .",3,0.9003135,38.085783640724614,35
2988,Statutory article retrieval is the task of automatically retrieving law articles relevant to a legal question .,0,0.9197654,45.321098670520726,17
2988,"While recent advances in natural language processing have sparked considerable interest in many legal tasks , statutory article retrieval remains primarily untouched due to the scarcity of large-scale and high-quality annotated datasets .",0,0.9615266,33.52786967940267,33
2988,Belgian law articles .,4,0.7254297,1451.5910866724128,4
2988,"Using BSARD , we benchmark several state-of-the-art retrieval approaches , including lexical and dense architectures , both in zero-shot and supervised setups .",2,0.7335784,55.27351250726476,28
2988,We find that fine-tuned dense retrieval models significantly outperform other systems .,3,0.9809373,40.05610725799771,12
2988,"Our best performing baseline achieves 74.8 % R@100 , which is promising for the feasibility of the task and indicates there is still room for improvement .",3,0.96968395,40.702393830618654,27
2988,"By the specificity of the domain and addressed task , BSARD presents a unique challenge problem for future research on legal information retrieval .",0,0.50895697,132.410972113455,24
2988,Our dataset and source code are publicly available .,3,0.49962038,14.303545456653563,9
2989,We present a novel pipeline for the collection of parallel data for the detoxification task .,1,0.46783698,27.344986187556486,16
2989,"We collect non-toxic paraphrases for over 10,000 English toxic sentences .",2,0.9260286,46.495563491724006,11
2989,We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxic-neutral sentence pairs .,3,0.93371433,41.633056208582445,24
2989,We release two parallel corpora which can be used for the training of detoxification models .,2,0.544247,27.265387692958086,16
2989,"To the best of our knowledge , these are the first parallel datasets for this task .",3,0.7080777,9.612429431558446,17
2989,"We describe our pipeline in detail to make it fast to set up for a new language or domain , thus contributing to faster and easier development of new parallel resources .",3,0.60553676,51.4516900710855,32
2989,We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches .,2,0.8123788,12.007206502544221,26
2989,We conduct both automatic and manual evaluations .,2,0.8146979,42.89074313590858,8
2989,All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin .,3,0.8986039,5.4635956850052585,22
2989,This suggests that our novel datasets can boost the performance of detoxification systems .,3,0.9844636,40.76206016809307,14
2990,"Character-level information is included in many NLP models , but evaluating the information encoded in character representations is an open issue .",0,0.91110235,34.68034485237895,24
2990,"We leverage perceptual representations in the form of shape , sound , and color embeddings and perform a representational similarity analysis to evaluate their correlation with textual representations in five languages .",2,0.81911904,44.66800693593217,32
2990,"This cross-lingual analysis shows that textual character representations correlate strongly with sound representations for languages using an alphabetic script , while shape correlates with featural scripts .",3,0.9654255,88.14447268570004,27
2990,We further develop a set of probing classifiers to intrinsically evaluate what phonological information is encoded in character embeddings .,3,0.46250537,39.01684972400348,20
2990,Our results suggest that information on features such as voicing are embedded in both LSTM and transformer-based representations .,3,0.99054784,36.1577769510852,21
2991,The introduction of immensely large Causal Language Models ( CLMs ) has rejuvenated the interest in open-ended text generation .,0,0.9731154,43.82170288233247,20
2991,"However , controlling the generative process for these Transformer-based models is at large an unsolved problem .",0,0.80425274,38.34246567573822,19
2991,"Earlier work has explored either plug-and-play decoding strategies , or more powerful but blunt approaches such as prompting .",0,0.89499515,100.73170671267908,21
2991,"There hence currently exists a trade-off between fine-grained control , and the capability for more expressive high-level instructions .",0,0.91936964,32.454886179739425,23
2991,"To alleviate this trade-off , we propose an encoder-decoder architecture that enables intermediate text prompts at arbitrary time steps .",2,0.43500456,30.08066907226257,21
2991,"We propose a resource-efficient method for converting a pre-trained CLM into this architecture , and demonstrate its potential on various experiments , including the novel task of contextualized word inclusion .",3,0.4439241,64.81264238291789,31
2991,"Our method provides strong results on multiple experimental settings , proving itself to be both expressive and versatile .",3,0.92566,72.74437197572361,19
2992,"While neural text-to-speech systems perform remarkably well in high-resource scenarios , they cannot be applied to the majority of the over 6,000 spoken languages in the world due to a lack of appropriate training data .",0,0.87377083,13.844994052363687,39
2992,"In this work , we use embeddings derived from articulatory vectors rather than embeddings derived from phoneme identities to learn phoneme representations that hold across languages .",2,0.59386665,24.31351599901198,27
2992,"In conjunction with language agnostic meta learning , this enables us to fine-tune a high-quality text-to-speech model on just 30 minutes of data in a previously unseen language spoken by a previously unseen speaker .",3,0.44600907,24.097317673296086,39
2993,Modern Irish is a minority language lacking sufficient computational resources for the task of accurate automatic syntactic parsing of user-generated content such as tweets .,0,0.95147663,41.82397437081858,25
2993,"Although language technology for the Irish language has been developing in recent years , these tools tend to perform poorly on user-generated content .",0,0.89932436,34.721719975055215,24
2993,"As with other languages , the linguistic style observed in Irish tweets differs , in terms of orthography , lexicon , and syntax , from that of standard texts more commonly used for the development of language models and parsers .",3,0.62385744,50.82991987402897,41
2993,"We release the first Universal Dependencies treebank of Irish tweets , facilitating natural language processing of user-generated content in Irish .",2,0.3911118,56.200835185254775,21
2993,"In this paper , we explore the differences between Irish tweets and standard Irish text , and the challenges associated with dependency parsing of Irish tweets .",1,0.93380296,49.32391862438505,27
2993,We describe our bootstrapping method of treebank development and report on preliminary parsing experiments .,3,0.38615477,67.4423523256838,15
2994,"Previous length-controllable summarization models mostly control lengths at the decoding stage , whereas the encoding or the selection of information from the source document is not sensitive to the designed length .",0,0.8382256,48.82012306510243,34
2994,They also tend to generate summaries as long as those in the training data .,3,0.5690417,23.318858905727566,15
2994,"In this paper , we propose a length-aware attention mechanism ( LAAM ) to adapt the encoding of the source based on the desired length .",1,0.8427231,29.000280930455972,27
2994,"Our approach works by training LAAM on a summary length balanced dataset built from the original training data , and then fine-tuning as usual .",2,0.75132114,89.03065691254916,26
2994,Results show that this approach is effective in generating high-quality summaries with desired lengths and even those short lengths never seen in the original training set .,3,0.9869277,25.71285747187022,27
2995,Multi-hop question generation focuses on generating complex questions that require reasoning over multiple pieces of information of the input passage .,0,0.83764637,16.477660087252485,21
2995,Current models with state-of-the-art performance have been able to generate the correct questions corresponding to the answers .,0,0.751187,13.030191486391233,23
2995,"However , most models can not ensure the complexity of generated questions , so they may generate shallow questions that can be answered without multi-hop reasoning .",0,0.83276594,43.06662096193979,27
2995,"To address this challenge , we propose the CQG , which is a simple and effective controlled framework .",1,0.6302067,44.57418605289462,19
2995,"CQG employs a simple method to generate the multi-hop questions that contain key entities in multi-hop reasoning chains , which ensure the complexity and quality of the questions .",2,0.44628546,45.15213713787804,29
2995,"In addition , we introduce a novel controlled Transformer-based decoder to guarantee that key entities appear in the questions .",2,0.6108229,49.310232187873936,22
2995,"Experiment results show that our model greatly improves performance , which also outperforms the state-of-the-art model about 25 % by 5 BLEU points on HotpotQA .",3,0.96376723,13.625467097264423,32
2996,"Recent studies have shown that language models pretrained and / or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE , putting into question the importance of word order information .",0,0.8225686,31.158145642150043,32
2996,"Somewhat counter-intuitively , some of these studies also report that position embeddings appear to be crucial for models ’ good performance with shuffled text .",0,0.60236907,45.252730548218565,26
2996,"We probe these language models for word order information and investigate what position embeddings learned from shuffled text encode , showing that these models retain a notion of word order information .",2,0.42671168,55.89581297230218,32
2996,We show this is in part due to a subtlety in how shuffling is implemented in previous work – before rather than after subword segmentation .,3,0.92383647,39.83391196262031,26
2996,"Surprisingly , we find even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities .",3,0.9695041,69.55855413145892,34
2996,"Finally , we show that beyond GLUE , a variety of language understanding tasks do require word order information , often to an extent that cannot be learned through fine-tuning .",3,0.96012574,34.407334650095976,31
2997,"Recent work in Natural Language Processing has focused on developing approaches that extract faithful explanations , either via identifying the most important tokens in the input ( i.e .",0,0.93450266,45.05340002780307,29
2997,post-hoc explanations ) or by designing inherently faithful models that first select the most important tokens and then use them to predict the correct label ( i.e .,3,0.3887747,41.273486785987515,29
2997,select-then-predict models ) .,2,0.6014294,65.1589284095985,8
2997,"Currently , these approaches are largely evaluated on in-domain settings .",0,0.8657426,35.71068499350869,12
2997,"Yet , little is known about how post-hoc explanations and inherently faithful models perform in out-of-domain settings .",0,0.94894874,22.63790710738162,21
2997,"In this paper , we conduct an extensive empirical study that examines : ( 1 ) the out-of-domain faithfulness of post-hoc explanations , generated by five feature attribution methods ;",1,0.89529777,59.92626278795072,31
2997,and ( 2 ) the out-of-domain performance of two inherently faithful models over six datasets .,2,0.625561,59.07463425921287,17
2997,"Contrary to our expectations , results show that in many cases out-of-domain post-hoc explanation faithfulness measured by sufficiency and comprehensiveness is higher compared to in-domain .",3,0.9874795,33.48236067199406,30
2997,We find this misleading and suggest using a random baseline as a yardstick for evaluating post-hoc explanation faithfulness .,3,0.96213293,73.22540218118071,19
2997,Our findings also show that select-then predict models demonstrate comparable predictive performance in out-of-domain settings to full-text trained models .,3,0.9901894,38.548253024941694,27
2998,"Open Information Extraction ( OpenIE ) is the task of extracting ( subject , predicate , object ) triples from natural language sentences .",0,0.91787755,32.600132377683025,24
2998,Current OpenIE systems extract all triple slots independently .,0,0.7402553,1195.6837362473798,9
2998,"In contrast , we explore the hypothesis that it may be beneficial to extract triple slots iteratively : first extract easy slots , followed by the difficult ones by conditioning on the easy slots , and therefore achieve a better overall extraction .",2,0.47816956,78.18482275239585,43
2998,"Based on this hypothesis , we propose a neural OpenIE system , MILIE , that operates in an iterative fashion .",1,0.45877293,59.74767805976503,21
2998,"Due to the iterative nature , the system is also modularit is possible to seamlessly integrate rule based extraction systems with a neural end-to-end system , thereby allowing rule based systems to supply extraction slots which MILIE can leverage for extracting the remaining slots .",3,0.72736895,92.24322555001511,47
2998,We confirm our hypothesis empirically : MILIE outperforms SOTA systems on multiple languages ranging from Chinese to Arabic .,3,0.95924026,70.4249158688092,19
2998,"Additionally , we are the first to provide an OpenIE test dataset for Arabic and Galician .",3,0.90164804,67.28867892147304,17
2999,"For a natural language understanding benchmark to be useful in research , it has to consist of examples that are diverse and difficult enough to discriminate among current and near-future state-of-the-art systems .",0,0.7253217,24.917347582275788,39
2999,"However , we do not yet know how best to select text sources to collect a variety of challenging examples .",0,0.9147027,43.16751823714999,21
2999,"In this study , we crowdsource multiple-choice reading comprehension questions for passages taken from seven qualitatively distinct sources , analyzing what attributes of passages contribute to the difficulty and question types of the collected examples .",1,0.57954466,75.27459345020469,37
2999,"To our surprise , we find that passage source , length , and readability measures do not significantly affect question difficulty .",3,0.976854,97.748787900881,22
2999,"Through our manual annotation of seven reasoning types , we observe several trends between passage sources and reasoning types , e.g. , logical reasoning is more often required in questions written for technical passages .",3,0.9083965,96.27363118348185,35
2999,"These results suggest that when creating a new benchmark dataset , selecting a diverse set of passages can help ensure a diverse range of question types , but that passage difficulty need not be a priority .",3,0.9903009,56.907391049528364,37
3000,Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications .,0,0.95862836,33.104566075832246,19
3000,"Simultaneous translation systems need to find a trade-off between translation quality and response time , and with this purpose multiple latency measures have been proposed .",0,0.94100386,36.62281076994828,27
3000,"However , latency evaluations for simultaneous translation are estimated at the sentence level , not taking into account the sequential nature of a streaming scenario .",0,0.47522786,101.62598566029023,26
3000,"Indeed , these sentence-level latency measures are not well suited for continuous stream translation , resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed .",3,0.53893435,68.06105035789582,35
3000,"This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation , that is successfully evaluated on streaming conditions for a reference IWSLT task .",1,0.5454678,66.11926340390004,35
3001,We present a novel rational-centric framework with human-in-the-loop – Rationales-centric Double-robustness Learning ( RDL ) – to boost model out-of-distribution performance in few-shot learning scenarios .,1,0.47783366,28.569882285146242,32
3001,"By using static semi-factual generation and dynamic human-intervened correction , RDL , acting like a sensible “ inductive bias ” , exploits rationales ( i.e .",3,0.41504988,154.4498284226852,26
3001,"phrases that cause the prediction ) , human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions , which enables fast and accurate generalisation .",3,0.35575125,173.27518587474808,32
3001,"Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests , especially for few-shot learning scenarios , compared to many state-of-the-art benchmarks .",3,0.97254646,19.324828766675722,38
3001,We also perform extensive ablation studies to support in-depth analyses of each component in our framework .,3,0.56623566,19.13183666235315,19
3002,Various efforts in the Natural Language Processing ( NLP ) community have been made to accommodate linguistic diversity and serve speakers of many different languages .,0,0.9526029,20.41350573754855,26
3002,"However , it is important to acknowledge that speakers and the content they produce and require , vary not just by language , but also by culture .",0,0.71994466,65.05315818372762,28
3002,"Although language and culture are tightly linked , there are important differences .",0,0.81208277,60.071600094299605,13
3002,"Analogous to cross-lingual and multilingual NLP , cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems .",0,0.77923065,32.15042293893495,24
3002,"We propose a principled framework to frame these efforts , and survey existing and potential strategies .",1,0.62000906,116.37750671690168,17
3003,Prompt-based tuning for pre-trained language models ( PLMs ) has shown its effectiveness in few-shot learning .,0,0.8472403,14.549376931011905,19
3003,"Typically , prompt-based tuning wraps the input text into a cloze question .",0,0.50035167,96.55582015390334,14
3003,"To make predictions , the model maps the output words to labels via a verbalizer , which is either manually designed or automatically built .",2,0.5842833,61.62946001449247,25
3003,"However , manual verbalizers heavily depend on domain-specific prior knowledge and human efforts , while finding appropriate label words automatically still remains challenging .",0,0.91923594,158.39170426699005,24
3003,"In this work , we propose the prototypical verbalizer ( ProtoVerb ) which is built directly from training data .",1,0.65556836,55.241129901799454,20
3003,"Specifically , ProtoVerb learns prototype vectors as verbalizers by contrastive learning .",2,0.4334008,303.434741847728,12
3003,"In this way , the prototypes summarize training instances and are able to enclose rich class-level semantics .",3,0.56607944,136.78349317107745,18
3003,"We conduct experiments on both topic classification and entity typing tasks , and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers , especially when training data is extremely scarce .",3,0.8795341,59.899236868810576,32
3003,"More surprisingly , ProtoVerb consistently boosts prompt-based tuning even on untuned PLMs , indicating an elegant non-tuning way to utilize PLMs .",3,0.97558063,144.53445105548184,23
3003,Our codes are avaliable at https://github.com/thunlp/OpenPrompt .,3,0.51268166,8.613371165766496,7
3004,We introduce and study the task of clickbait spoiling : generating a short text that satisfies the curiosity induced by a clickbait post .,1,0.60998565,44.180713816799404,24
3004,Clickbait links to a web page and advertises its contents by arousing curiosity instead of providing an informative summary .,0,0.8258419,35.33980930266673,20
3004,"Our contributions are approaches to classify the type of spoiler needed ( i.e. , a phrase or a passage ) , and to generate appropriate spoilers .",2,0.32361385,86.89170315008136,27
3004,"A large-scale evaluation and error analysis on a new corpus of 5,000 manually spoiled clickbait posts — the Webis Clickbait Spoiling Corpus 2022 — shows that our spoiler type classifier achieves an accuracy of 80 % , while the question answering model DeBERTa-large outperforms all others in generating spoilers for both types .",3,0.8450169,64.87587408838517,55
3005,"We present Knowledge Distillation with Meta Learning ( MetaDistil ) , a simple yet effective alternative to traditional knowledge distillation ( KD ) methods where the teacher model is fixed during training .",1,0.547745,43.551416653737505,33
3005,"We show the teacher network can learn to better transfer knowledge to the student network ( i.e. , learning to teach ) with the feedback from the performance of the distilled student network in a meta learning framework .",3,0.86636484,58.24635585145508,39
3005,"Moreover , we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner .",2,0.43528533,36.62313383903799,28
3005,"Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters , facilitating the use of KD on different tasks and models .",3,0.9483917,52.52408166706486,41
3006,"Existing techniques often attempt to transfer powerful machine translation ( MT ) capabilities to ST , but neglect the representation discrepancy across modalities .",0,0.915564,162.66764588681144,24
3006,"In this paper , we propose the Speech-TExt Manifold Mixup ( STEMM ) method to calibrate such discrepancy .",1,0.8505693,113.56551707473936,21
3006,"Specifically , we mix up the representation sequences of different modalities , and take both unimodal speech sequences and multimodal mixed sequences as input to the translation model in parallel , and regularize their output predictions with a self-learning framework .",2,0.8767391,50.932135496714466,41
3006,"Experiments on MuST-C speech translation benchmark and further analysis show that our method effectively alleviates the cross-modal representation discrepancy , and achieves significant improvements over a strong baseline on eight translation directions .",3,0.92554206,33.350147553063735,35
3007,"Lexically constrained neural machine translation ( NMT ) , which controls the generation of NMT models with pre-specified constraints , is important in many practical scenarios .",0,0.9270689,31.900971720823456,27
3007,"Due to the representation gap between discrete constraints and continuous vectors in NMT models , most existing works choose to construct synthetic data or modify the decoding algorithm to impose lexical constraints , treating the NMT model as a black box .",0,0.8020854,49.15025150791146,42
3007,"In this work , we propose to open this black box by directly integrating the constraints into NMT models .",1,0.8143594,42.3781464057547,20
3007,"Specifically , we vectorize source and target constraints into continuous keys and values , which can be utilized by the attention modules of NMT models .",2,0.81261915,69.43150379517702,26
3007,The proposed integration method is based on the assumption that the correspondence between keys and values in attention modules is naturally suitable for modeling constraint pairs .,2,0.5155562,51.69372195097011,27
3007,"Experimental results show that our method consistently outperforms several representative baselines on four language pairs , demonstrating the superiority of integrating vectorized lexical constraints .",3,0.96568674,23.71656940644713,25
3008,"In order to better understand the rationale behind model behavior , recent works have exploited providing interpretation to support the inference prediction .",0,0.9184617,82.20173012909919,23
3008,"However , existing methods tend to provide human-unfriendly interpretation , and are prone to sub-optimal performance due to one-side promotion , i.e .",0,0.87981856,46.19598524001227,23
3008,either inference promotion with interpretation or vice versa .,3,0.47606733,644.9322664361146,9
3008,"In this paper , we propose a multi-level Mutual Promotion mechanism for self-evolved Inference and sentence-level Interpretation ( MPII ) .",1,0.9058954,74.20898559186266,23
3008,"Specifically , from the model-level , we propose a Step-wise Integration Mechanism to jointly perform and deeply integrate inference and interpretation in an autoregressive manner .",2,0.6106461,53.87439050838655,27
3008,"From the optimization-level , we propose an Adversarial Fidelity Regularization to improve the fidelity between inference and interpretation with the Adversarial Mutual Information training strategy .",2,0.69353795,36.95699869050786,28
3008,Extensive experiments on NLI and CQA tasks reveal that the proposed MPII approach can significantly outperform baseline models for both the inference performance and the interpretation quality .,3,0.9369696,21.812715221183694,28
3009,The Mixture-of-Experts ( MoE ) technique can scale up the model size of Transformers with an affordable computational overhead .,0,0.50339675,68.97585681983897,20
3009,"We point out that existing learning-to-route MoE methods suffer from the routing fluctuation issue , i.e. , the target expert of the same input may change along with training , but only one expert will be activated for the input during inference .",3,0.7331471,78.15470516030172,46
3009,The routing fluctuation tends to harm sample efficiency because the same input updates different experts but only one is finally used .,3,0.68763745,350.99856897170224,22
3009,"In this paper , we propose StableMoE with two training stages to address the routing fluctuation problem .",1,0.808152,76.33599015511138,18
3009,"In the first training stage , we learn a balanced and cohesive routing strategy and distill it into a lightweight router decoupled from the backbone model .",2,0.7637158,44.51841694349881,27
3009,"In the second training stage , we utilize the distilled router to determine the token-to-expert assignment and freeze it for a stable routing strategy .",2,0.76977307,123.58460750291607,29
3009,We validate our method on language modeling and multilingual machine translation .,3,0.5553754,23.408044565969913,12
3009,The results show that StableMoE outperforms existing MoE methods in terms of both convergence speed and performance .,3,0.9846149,23.220621706031526,18
3010,"Neural named entity recognition ( NER ) models may easily encounter the over-confidence issue , which degrades the performance and calibration .",0,0.9323169,69.11683675559408,22
3010,"Inspired by label smoothing and driven by the ambiguity of boundary annotation in NER engineering , we propose boundary smoothing as a regularization technique for span-based neural NER models .",2,0.5414624,46.79929368724076,30
3010,It re-assigns entity probabilities from annotated spans to the surrounding ones .,2,0.48385665,50.11971310366083,12
3010,"Built on a simple but strong baseline , our model achieves results better than or competitive with previous state-of-the-art systems on eight well-known NER benchmarks .",3,0.86164623,17.61914981447997,34
3010,"Further empirical analysis suggests that boundary smoothing effectively mitigates over-confidence , improves model calibration , and brings flatter neural minima and more smoothed loss landscapes .",3,0.9706293,107.62908418702298,26
3011,Hierarchical text classification is a challenging subtask of multi-label classification due to its complex label hierarchy .,0,0.92763186,17.267142662632246,17
3011,"Existing methods encode text and label hierarchy separately and mix their representations for classification , where the hierarchy remains unchanged for all input text .",0,0.7256804,83.52592562225392,25
3011,"Instead of modeling them separately , in this work , we propose Hierarchy-guided Contrastive Learning ( HGCLR ) to directly embed the hierarchy into a text encoder .",2,0.61261916,41.51086697919625,30
3011,"During training , HGCLR constructs positive samples for input text under the guidance of the label hierarchy .",2,0.6757491,166.6445870389237,18
3011,"By pulling together the input text and its positive sample , the text encoder can learn to generate the hierarchy-aware text representation independently .",2,0.40569103,69.60908749978155,26
3011,"Therefore , after training , the HGCLR enhanced text encoder can dispense with the redundant hierarchy .",3,0.85063225,161.5645366717452,17
3011,Extensive experiments on three benchmark datasets verify the effectiveness of HGCLR .,3,0.73686254,22.144618272493666,12
3012,"Natural language processing models learn word representations based on the distributional hypothesis , which asserts that word context ( e.g. , co-occurrence ) correlates with meaning .",0,0.8919434,37.380486790363264,27
3012,"We propose that n-grams composed of random character sequences , or garble , provide a novel context for studying word meaning both within and beyond extant language .",3,0.48260543,91.70917221144857,28
3012,"In particular , randomly generated character n-grams lack meaning but contain primitive information based on the distribution of characters they contain .",0,0.88073426,78.33875998981243,22
3012,"By studying the embeddings of a large corpus of garble , extant language , and pseudowords using CharacterBERT , we identify an axis in the model ’s high-dimensional embedding space that separates these classes of n-grams .",2,0.5553408,54.77999023104065,38
3012,"Furthermore , we show that this axis relates to structure within extant language , including word part-of-speech , morphology , and concept concreteness .",3,0.94926625,85.24032539608457,25
3012,"Thus , in contrast to studies that are mainly limited to extant language , our work reveals that meaning and primitive information are intrinsically linked .",3,0.98317933,73.40803383440583,26
3013,"To alleviate the data scarcity problem in training question answering systems , recent works propose additional intermediate pre-training for dense passage retrieval ( DPR ) .",0,0.8790281,97.16307410568963,26
3013,"However , there still remains a large discrepancy between the provided upstream signals and the downstream question-passage relevance , which leads to less improvement .",0,0.6215911,73.12874706641631,27
3013,"To bridge this gap , we propose the HyperLink-induced Pre-training ( HLP ) , a method to pre-train the dense retriever with the text relevance induced by hyperlink-based topology within Web documents .",0,0.3573373,61.69702885143337,36
3013,We demonstrate that the hyperlink-based structures of dual-link and co-mention can provide effective relevance signals for large-scale pre-training that better facilitate downstream passage retrieval .,3,0.9667253,65.93432633710212,27
3013,"We investigate the effectiveness of our approach across a wide range of open-domain QA datasets under zero-shot , few-shot , multi-hop , and out-of-domain scenarios .",2,0.46506035,16.278480748487,28
3013,The experiments show our HLP outperforms the BM25 by up to 7 points as well as other pre-training methods by more than 10 points in terms of top-20 retrieval accuracy under the zero-shot scenario .,3,0.9443353,21.508938022226978,37
3013,"Furthermore , HLP significantly outperforms other pre-training methods under the other scenarios .",3,0.97915846,46.45834251851836,13
3014,Recent machine reading comprehension datasets such as ReClor and LogiQA require performing logical reasoning over text .,0,0.8639968,56.135083001423084,17
3014,"Conventional neural models are insufficient for logical reasoning , while symbolic reasoners cannot directly apply to text .",0,0.88944733,113.92375658491206,18
3014,"To meet the challenge , we present a neural-symbolic approach which , to predict an answer , passes messages over a graph representing logical relations between text units .",1,0.3229866,68.67720395928995,29
3014,"It incorporates an adaptive logic graph network ( AdaLoGN ) which adaptively infers logical relations to extend the graph and , essentially , realizes mutual and iterative reinforcement between neural and symbolic reasoning .",2,0.4054441,192.3627139018314,34
3014,We also implement a novel subgraph-to-node message passing mechanism to enhance context-option interaction for answering multiple-choice questions .,2,0.4935811,66.22967238209732,23
3014,Our approach shows promising results on ReClor and LogiQA .,3,0.9607034,80.41742432043849,10
3015,Model ensemble is a popular approach to produce a low-variance and well-generalized model .,0,0.83266777,26.89630961218696,16
3015,"However , it induces large memory and inference costs , which is often not affordable for real-world deployment .",0,0.77825785,53.84848906634574,20
3015,Existing work has resorted to sharing weights among models .,0,0.85915434,124.76830460046948,10
3015,"However , when increasing the proportion of the shared weights , the resulting models tend to be similar , and the benefits of using model ensemble diminish .",3,0.9256256,83.65375258079477,28
3015,"To retain ensemble benefits while maintaining a low memory cost , we propose a consistency-regularized ensemble learning approach based on perturbed models , named CAMERO .",2,0.63785785,94.866911290007,28
3015,"Specifically , we share the weights of bottom layers across all models and apply different perturbations to the hidden representations for different models , which can effectively promote the model diversity .",2,0.7554169,51.30599258962703,32
3015,"Meanwhile , we apply a prediction consistency regularizer across the perturbed models to control the variance due to the model diversity .",2,0.78591347,62.38825741377674,22
3015,Our experiments using large language models demonstrate that CAMERO significantly improves the generalization performance of the ensemble model .,3,0.9650564,48.55497341732171,19
3015,"Specifically , CAMERO outperforms the standard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a significantly smaller model size ( 114.2 M vs .",3,0.9547869,43.67874798300293,30
3015,880.6 M ) .,3,0.6388547,110.74297450395963,4
3016,Grammatical Error Correction ( GEC ) should not focus only on high accuracy of corrections but also on interpretability for language learning .,0,0.88378996,44.970626447188714,23
3016,"However , existing neural-based GEC models mainly aim at improving accuracy , and their interpretability has not been explored .",0,0.90391695,47.01218790153292,22
3016,"A promising approach for improving interpretability is an example-based method , which uses similar retrieved examples to generate corrections .",0,0.6805478,52.55910685649218,22
3016,"In addition , examples are beneficial in language learning , helping learners understand the basis of grammatically incorrect / correct texts and improve their confidence in writing .",0,0.5859865,99.229465115269,28
3016,"Therefore , we hypothesize that incorporating an example-based method into GEC can improve interpretability as well as support language learners .",3,0.5760106,39.227958700837135,23
3016,"In this study , we introduce an Example-Based GEC ( EB-GEC ) that presents examples to language learners as a basis for a correction result .",1,0.87639177,72.59924685523309,30
3016,The examples consist of pairs of correct and incorrect sentences similar to a given input and its predicted correction .,2,0.50034606,72.34763010283567,20
3016,Experiments demonstrate that the examples presented by EB-GEC help language learners decide to accept or refuse suggestions from the GEC output .,3,0.9541691,78.97407579864516,24
3016,"Furthermore , the experiments also show that retrieved examples improve the accuracy of corrections .",3,0.97487736,75.36696874045312,15
3017,Negative sampling is highly effective in handling missing annotations for named entity recognition ( NER ) .,0,0.8299314,50.69818987467497,17
3017,One of our contributions is an analysis on how it makes sense through introducing two insightful concepts : missampling and uncertainty .,1,0.3032362,121.57720670976917,22
3017,Empirical studies show low missampling rate and high uncertainty are both essential for achieving promising performances with negative sampling .,0,0.5840716,62.870229331102436,20
3017,"Based on the sparsity of named entities , we also theoretically derive a lower bound for the probability of zero missampling rate , which is only relevant to sentence length .",2,0.5723724,59.717230103665166,31
3017,The other contribution is an adaptive and weighted sampling distribution that further improves negative sampling via our former analysis .,3,0.47386643,169.4119425175535,20
3017,"Experiments on synthetic datasets and well-annotated datasets ( e.g. , CoNLL-2003 ) show that our proposed approach benefits negative sampling in terms of F1 score and loss convergence .",3,0.8653777,32.63823198976422,32
3017,"Besides , models with improved negative sampling have achieved new state-of-the-art results on real-world datasets ( e.g. , EC ) .",0,0.68968207,26.21612885286795,27
3018,"In this paper , we study the named entity recognition ( NER ) problem under distant supervision .",1,0.84738714,31.48265201939325,18
3018,"Due to the incompleteness of the external dictionaries and / or knowledge bases , such distantly annotated training data usually suffer from a high false negative rate .",0,0.86645854,27.030376996452663,28
3018,"To this end , we formulate the Distantly Supervised NER ( DS-NER ) problem via Multi-class Positive and Unlabeled ( MPU ) learning and propose a theoretically and practically novel CONFidence-based MPU ( Conf-MPU ) approach .",1,0.3949963,54.992321443541954,43
3018,"To handle the incomplete annotations , Conf-MPU consists of two steps .",2,0.42232698,216.020099402903,12
3018,"First , a confidence score is estimated for each token of being an entity token .",2,0.8442525,64.84055567581181,16
3018,"Then , the proposed Conf-MPU risk estimation is applied to train a multi-class classifier for the NER task .",2,0.72563255,66.36719130066298,19
3018,Thorough experiments on two benchmark datasets labeled by various external knowledge demonstrate the superiority of the proposed Conf-MPU over existing DS-NER methods .,3,0.8474813,73.41811558598287,27
3018,Our code is available at Github .,3,0.5101858,15.567963447816794,7
3019,Pre-trained models for programming languages have recently demonstrated great success on code intelligence .,0,0.9426535,24.172081424438677,14
3019,"To support both code-related understanding and generation tasks , recent works attempt to pre-train unified encoder-decoder models .",0,0.8745642,37.96107883838088,18
3019,"However , such encoder-decoder framework is sub-optimal for auto-regressive tasks , especially code completion that requires a decoder-only manner for efficient inference .",0,0.6388158,40.0378706810567,23
3019,"In this paper , we present UniXcoder , a unified cross-modal pre-trained model for programming language .",1,0.8199723,27.808028495024327,17
3019,The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation .,2,0.61089075,87.52957682018835,30
3019,"To encode AST that is represented as a tree in parallel , we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree .",2,0.6541109,28.65647959464224,37
3019,"Furthermore , we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning , and then align representations among programming languages using a cross-modal generation task .",2,0.47982314,46.00276466629482,31
3019,We evaluate UniXcoder on five code-related tasks over nine datasets .,2,0.7659923,152.58486425623053,11
3019,"To further evaluate the performance of code fragment representation , we also construct a dataset for a new task , called zero-shot code-to-code search .",2,0.7986477,38.31858622849894,26
3019,Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder .,3,0.9852765,50.31479646442967,29
3020,NLP research is impeded by a lack of resources and awareness of the challenges presented by underrepresented languages and dialects .,0,0.9317787,24.24972495712267,21
3020,"Focusing on the languages spoken in Indonesia , the second most linguistically diverse and the fourth most populous nation of the world , we provide an overview of the current state of NLP research for Indonesia ’s 700 + languages .",1,0.71105176,24.716531824159755,41
3020,We highlight challenges in Indonesian NLP and how these affect the performance of current NLP systems .,1,0.6226042,30.13008021473653,17
3020,"Finally , we provide general recommendations to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages .",3,0.7803948,46.2631863143385,23
3021,Modern neural language models can produce remarkably fluent and grammatical text .,0,0.90666395,34.47968209968864,12
3021,"So much , in fact , that recent work by Clark et al .",0,0.6893462,83.95073516700963,14
3021,( 2021 ) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored ( GPT-3 ) and human-authored writing .,0,0.8566855,53.47704930070481,26
3021,"As errors in machine generations become ever subtler and harder to spot , it poses a new challenge to the research community for robust machine text evaluation .",0,0.9338134,59.447353993330964,28
3021,We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation .,1,0.40430978,121.54393503120507,15
3021,"To support the broad range of real machine errors that can be identified by laypeople , the ten error categories of Scarecrow — such as redundancy , commonsense errors , and incoherence — are identified through several rounds of crowd annotation experiments without a predefined ontology .",2,0.48208117,75.71853884421841,47
3021,We then use Scarecrow to collect over 41 k error spans in human-written and machine-generated paragraphs of English language news text .,2,0.8746491,102.0713767271643,24
3021,"We isolate factors for detailed analysis , including parameter count , training data , and various decoding-time configurations .",2,0.69015616,210.38173855707754,20
3021,"Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes , including fourteen configurations of GPT-3 .",3,0.81065035,221.1240992827869,25
3021,"In addition , our analysis unveils new insights , with detailed rationales provided by laypeople , e.g. , that the commonsense capabilities have been improving with larger models while math capabilities have not , and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text .",3,0.97333884,90.47926914478718,54
3021,"We release our training material , annotation toolkit and dataset at https://yao-dou.github.io/scarecrow/ .",2,0.49288154,38.711482998759756,13
3022,Transformer architecture has become the de-facto model for many machine learning tasks from natural language processing and computer vision .,0,0.93895257,15.966995117088336,20
3022,"As such , improving its computational efficiency becomes paramount .",0,0.8608039,189.15234664428752,10
3022,One of the major computational inefficiency of Transformer based models is that they spend the identical amount of computation throughout all layers .,0,0.82287806,43.32046947293406,23
3022,Prior works have proposed to augment the Transformer model with the capability of skimming tokens to improve its computational efficiency .,0,0.8917104,31.380667736897,21
3022,"However , they suffer from not having effectual and end-to-end optimization of the discrete skimming predictor .",0,0.7416351,89.56961086714722,19
3022,"To address the above limitations , we propose the Transkimmer architecture , which learns to identify hidden state tokens that are not required by each layer .",2,0.505341,62.59466971701183,27
3022,"The skimmed tokens are then forwarded directly to the final output , thus reducing the computation of the successive layers .",3,0.45871326,75.37642096695866,21
3022,The key idea in Transkimmer is to add a parameterized predictor before each layer that learns to make the skimming decision .,2,0.40029112,115.77469777013921,22
3022,We also propose to adopt reparameterization trick and add skim loss for the end-to-end training of Transkimmer .,3,0.6858612,98.53846198877535,20
3022,Transkimmer achieves 10.97x average speedup on GLUE benchmark compared with vanilla BERT-base baseline with less than 1 % accuracy degradation .,3,0.9166486,53.06215268661044,23
3023,"In this paper , we propose SkipBERT to accelerate BERT inference by skipping the computation of shallow layers .",1,0.83765405,46.72801607174948,19
3023,"To achieve this , our approach encodes small text chunks into independent representations , which are then materialized to approximate the shallow representation of BERT .",2,0.6680084,65.25276577215377,26
3023,"Since the use of such approximation is inexpensive compared with transformer calculations , we leverage it to replace the shallow layers of BERT to skip their runtime overhead .",2,0.5473158,119.22080156092596,29
3023,"With off-the-shelf early exit mechanisms , we also skip redundant computation from the highest few layers to further improve inference efficiency .",3,0.47520605,85.27459665476141,24
3023,Results on GLUE show that our approach can reduce latency by 65 % without sacrificing performance .,3,0.98205435,25.950911931664415,17
3023,"By using only two-layer transformer calculations , we can still maintain 95 % accuracy of BERT .",3,0.891936,163.1923710025394,18
3024,We investigate what kind of structural knowledge learned in neural network encoders is transferable to processing natural language .,1,0.79850173,26.491798729067337,19
3024,"We design artificial languages with structural properties that mimic natural language , pretrain encoders on the data , and see how much performance the encoder exhibits on downstream tasks in natural language .",2,0.7021139,50.81464036764372,33
3024,Our experimental results show that pretraining with an artificial language with a nesting dependency structure provides some knowledge transferable to natural language .,3,0.98820347,48.17466909249456,23
3024,A follow-up probing analysis indicates that its success in the transfer is related to the amount of encoded contextual information and what is transferred is the knowledge of position-aware context dependence of language .,3,0.9435286,49.676127825355174,38
3024,Our results provide insights into how neural network encoders process human languages and the source of cross-lingual transferability of recent multilingual language models .,3,0.9865363,22.301540184381125,24
3025,Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities .,0,0.90667814,17.81366362419018,21
3025,"However , existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks .",0,0.86620426,32.29332873494532,20
3025,"In this study , we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks .",1,0.93773997,25.212213477054206,17
3025,We train a multilingual language model with 24 languages with entity representations and showthe model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks .,3,0.5455657,22.604085943539943,28
3025,We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features .,3,0.8148317,30.840858908838385,27
3025,We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset .,3,0.49630368,90.70869666652223,16
3025,We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations .,3,0.9769579,64.78804661036537,19
3026,Automated simplification models aim to make input texts more readable .,0,0.88697606,53.671430033446924,11
3026,"Such methods have the potential to make complex information accessible to a wider audience , e.g. , providing access to recent medical literature which might otherwise be impenetrable for a lay reader .",3,0.5008087,24.237875603908005,33
3026,"However , such models risk introducing errors into automatically simplified texts , for instance by inserting statements unsupported by the corresponding original text , or by omitting key information .",0,0.7631599,99.15478080304301,30
3026,Providing more readable but inaccurate versions of texts may in many cases be worse than providing no such access at all .,0,0.69476646,47.57384949082928,22
3026,"The problem of factual accuracy ( and the lack thereof ) has received heightened attention in the context of summarization models , but the factuality of automatically simplified texts has not been investigated .",0,0.9501986,36.9974820390669,34
3026,We introduce a taxonomy of errors that we use to analyze both references drawn from standard simplification datasets and state-of-the-art model outputs .,2,0.52377546,23.030696250643327,29
3026,"We find that errors often appear in both that are not captured by existing evaluation metrics , motivating a need for research into ensuring the factual accuracy of automated simplification models .",3,0.97955084,56.757150187247646,32
3027,This paper describes the motivation and development of speech synthesis systems for the purposes of language revitalization .,1,0.91198236,29.155107717708404,18
3027,"By building speech synthesis systems for three Indigenous languages spoken in Canada , Kanien’kéha , Gitksan & SENĆOŦEN , we re-evaluate the question of how much data is required to build low-resource speech synthesis systems featuring state-of-the-art neural models .",2,0.35166562,46.98849893576323,46
3027,"For example , preliminary results with English data show that a FastSpeech2 model trained with 1 hour of training data can produce speech with comparable naturalness to a Tacotron2 model trained with 10 hours of data .",3,0.8879952,33.52945246283726,37
3027,"Finally , we motivate future research in evaluation and classroom integration in the field of speech synthesis for language revitalization .",3,0.84653354,62.16799851331315,21
3028,"The allure of superhuman-level capabilities has led to considerable interest in language models like GPT-3 and T5 , wherein the research has , by and large , revolved around new model architectures , training tasks , and loss objectives , along with substantial engineering efforts to scale up model capacity and dataset size .",0,0.9203625,61.89663831547258,56
3028,Comparatively little work has been done to improve the generalization of these models through better optimization .,0,0.9114346,22.79211259809607,17
3028,"In this work , we show that Sharpness-Aware Minimization ( SAM ) , a recently proposed optimization procedure that encourages convergence to flatter minima , can substantially improve the generalization of language models without much computational overhead .",1,0.74289453,43.74499886185036,40
3028,"We show that SAM is able to boost performance on SuperGLUE , GLUE , Web Questions , Natural Questions , Trivia QA , and TyDiQA , with particularly large gains when training data for these tasks is limited .",3,0.95047575,48.90613408984895,39
3029,Recent advances in natural language processing have enabled powerful privacy-invasive authorship attribution .,0,0.94317776,37.29084855887451,15
3029,"To counter authorship attribution , researchers have proposed a variety of rule-based and learning-based text obfuscation approaches .",0,0.9222168,46.70206515311606,20
3029,"However , existing authorship obfuscation approaches do not consider the adversarial threat model .",0,0.8551193,57.93144729709314,14
3029,"Specifically , they are not evaluated against adversarially trained authorship attributors that are aware of potential obfuscation .",0,0.6042778,59.32578604972278,18
3029,"To fill this gap , we investigate the problem of adversarial authorship attribution for deobfuscation .",1,0.7797091,32.93553399183034,16
3029,We show that adversarially trained authorship attributors are able to degrade the effectiveness of existing obfuscators from 20-30 % to 5-10 % .,3,0.94323474,32.65239750373432,27
3029,We also evaluate the effectiveness of adversarial training when the attributor makes incorrect assumptions about whether and which obfuscator was used .,2,0.5531609,41.819088566070945,22
3029,"While there is a a clear degradation in attribution accuracy , it is noteworthy that this degradation is still at or above the attribution accuracy of the attributor that is not adversarially trained at all .",3,0.8754846,26.851170906071644,36
3029,Our results motivate the need to develop authorship obfuscation approaches that are resistant to deobfuscation .,3,0.9879809,30.046135224543406,16
3030,Word and morpheme segmentation are fundamental steps of language documentation as they allow to discover lexical units in a language for which the lexicon is unknown .,0,0.8993924,32.81920181657848,27
3030,"However , in most language documentation scenarios , linguists do not start from a blank page : they may already have a pre-existing dictionary or have initiated manual segmentation of a small part of their data .",0,0.827803,59.703591947367066,37
3030,This paper studies how such a weak supervision can be taken advantage of in Bayesian non-parametric models of segmentation .,1,0.86412185,27.691229325118044,20
3030,"Our experiments on two very low resource languages ( Mboshi and Japhug ) , whose documentation is still in progress , show that weak supervision can be beneficial to the segmentation quality .",3,0.9391475,103.96749006035755,33
3030,"In addition , we investigate an incremental learning scenario where manual segmentations are provided in a sequential manner .",2,0.704053,56.88273017168768,19
3030,This work opens the way for interactive annotation tools for documentary linguists .,3,0.96379757,75.93917490598558,13
3031,"Existing Natural Language Inference ( NLI ) datasets , while being instrumental in the advancement of Natural Language Understanding ( NLU ) research , are not related to scientific text .",0,0.9625426,39.04992425772,31
3031,"In this paper , we introduce SciNLI , a large dataset for NLI that captures the formality in scientific text and contains 107,412 sentence pairs extracted from scholarly papers on NLP and computational linguistics .",1,0.6945326,27.60051349079033,35
3031,"Given that the text used in scientific literature differs vastly from the text used in everyday language both in terms of vocabulary and sentence structure , our dataset is well suited to serve as a benchmark for the evaluation of scientific NLU models .",3,0.89132917,17.380544493024466,44
3031,Our experiments show that SciNLI is harder to classify than the existing NLI datasets .,3,0.9789202,34.82761160529452,15
3031,Our best performing model with XLNet achieves a Macro F1 score of only 78.18 % and an accuracy of 78.23 % showing that there is substantial room for improvement .,3,0.9732396,27.058405951725035,30
3032,"In lexicalist linguistic theories , argument structure is assumed to be predictable from the meaning of verbs .",0,0.85054445,74.6157569178265,18
3032,"As a result , the verb is the primary determinant of the meaning of a clause .",0,0.81390405,23.96351599729552,17
3032,"In contrast , construction grammarians propose that argument structure is encoded in constructions ( or form-meaning pairs ) that are distinct from verbs .",0,0.90717274,107.06878204664876,24
3032,Two decades of psycholinguistic research have produced substantial empirical evidence in favor of the construction view .,0,0.9399762,20.95738225350088,17
3032,Here we adapt several psycholinguistic studies to probe for the existence of argument structure constructions ( ASCs ) in Transformer-based language models ( LMs ) .,1,0.5378064,32.995258794484165,28
3032,"First , using a sentence sorting experiment , we find that sentences sharing the same construction are closer in embedding space than sentences sharing the same verb .",3,0.7669544,31.79794317024896,28
3032,"Furthermore , LMs increasingly prefer grouping by construction with more input data , mirroring the behavior of non-native language learners .",3,0.8820949,117.75438130087943,21
3032,"Second , in a “ Jabberwocky ” priming-based experiment , we find that LMs associate ASCs with meaning , even in semantically nonsensical sentences .",3,0.9146379,81.68502193906991,27
3032,Our work offers the first evidence for ASCs in LMs and highlights the potential to devise novel probing methods grounded in psycholinguistic research .,3,0.9864942,32.66793993831629,24
3033,"Social media platforms are deploying machine learning based offensive language classification systems to combat hateful , racist , and other forms of offensive speech at scale .",0,0.9104876,43.213743480579744,27
3033,"However , despite their real-world deployment , we do not yet comprehensively understand the extent to which offensive language classifiers are robust against adversarial attacks .",0,0.90979415,32.160504333687314,26
3033,Prior work in this space is limited to studying robustness of offensive language classifiers against primitive attacks such as misspellings and extraneous spaces .,0,0.8863424,34.17461692638188,24
3033,"To address this gap , we systematically analyze the robustness of state-of-the-art offensive language classifiers against more crafty adversarial attacks that leverage greedy-and attention-based word selection and context-aware embeddings for word replacement .",1,0.74426883,33.5552271696559,45
3033,Our results on multiple datasets show that these crafty adversarial attacks can degrade the accuracy of offensive language classifiers by more than 50 % while also being able to preserve the readability and meaning of the modified text .,3,0.9800764,30.48989609992533,39
3034,Style transfer is the task of rewriting a sentence into a target style while approximately preserving content .,0,0.9225557,58.84134989072484,18
3034,"While most prior literature assumes access to a large style-labelled corpus , recent work ( Riley et al .",0,0.7943768,82.7416288316677,21
3034,2021 ) has attempted “ few-shot ” style transfer using only 3-10 sentences at inference for style extraction .,0,0.8621536,136.99733217784862,22
3034,In this work we study a relevant low-resource setting : style transfer for languages where no style-labelled corpora are available .,1,0.6159788,50.81724519259092,23
3034,"We notice that existing few-shot methods perform this task poorly , often copying inputs verbatim .",3,0.93258536,68.1449632775855,16
3034,We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases .,2,0.5789013,16.944760606676788,25
3034,"When compared to prior work , our model achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages .",3,0.90847445,40.92136420619992,24
3034,"Moreover , our method is better at controlling the style transfer magnitude using an input scalar knob .",3,0.9176582,125.70371018854819,18
3034,"We report promising qualitative results for several attribute transfer tasks ( sentiment transfer , simplification , gender neutralization , text anonymization ) all without retraining the model .",3,0.9416796,124.92719561078033,28
3034,"Finally , we find model evaluation to be difficult due to the lack of datasets and metrics for many languages .",3,0.884477,30.407217686203953,21
3034,"To facilitate future research we crowdsource formality annotations for 4000 sentence pairs in four Indic languages , and use this data to design our automatic evaluations .",2,0.6847848,92.24238983884769,27
3035,Transformer architectures have achieved state-of-the-art results on a variety of natural language processing ( NLP ) tasks .,0,0.93318087,6.551741031454332,23
3035,"However , their attention mechanism comes with a quadratic complexity in sequence lengths , making the computational overhead prohibitive , especially for long sequences .",0,0.82828254,57.14753042024652,25
3035,Attention context can be seen as a random-access memory with each token taking a slot .,0,0.6827422,35.990389811026866,16
3035,"Under this perspective , the memory size grows linearly with the sequence length , and so does the overhead of reading from it .",3,0.5398218,29.66785805758042,24
3035,One way to improve the efficiency is to bound the memory size .,0,0.46903992,35.006829964003515,13
3035,"We show that disparate approaches can be subsumed into one abstraction , attention with bounded-memory control ( ABC ) , and they vary in their organization of the memory .",3,0.73976725,154.33674740631855,31
3035,"ABC reveals new , unexplored possibilities .",0,0.5824558,545.178946153185,7
3035,"First , it connects several efficient attention variants that would otherwise seem apart .",3,0.35589644,333.226246487605,14
3035,"Second , this abstraction gives new insights — an established approach ( Wang et al. , 2020 b ) previously thought to not be applicable in causal attention , actually is .",3,0.54008913,148.81560038810514,32
3035,"Last , we present a new instance of ABC , which draws inspiration from existing ABC approaches , but replaces their heuristic memory-organizing functions with a learned , contextualized one .",2,0.43245474,91.20508368298965,33
3035,"Our experiments on language modeling , machine translation , and masked language model finetuning show that our approach outperforms previous efficient attention models ;",3,0.8957937,44.39149535215586,24
3035,"compared to the strong transformer baselines , it significantly improves the inference time and space efficiency with no or negligible accuracy loss .",3,0.92149293,52.48236003079793,23
3036,"Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field ’s successes , often in response to the field ’s widespread hype .",0,0.8926383,38.683149465272564,30
3036,"Though well-meaning , this has yielded many misleading or false claims about the limits of our best technology .",0,0.92594784,80.02331421198807,21
3036,"It harms our credibility in ways that can make it harder to mitigate present-day harms , like those involving biased systems for content moderation or resume screening .",0,0.6805605,78.34809925438759,30
3036,It also limits our ability to prepare for the potentially enormous impacts of more distant future advances .,0,0.74212486,60.49458424702837,18
3036,This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them .,1,0.57455987,43.09808297285066,29
3037,"Humanities scholars commonly provide evidence for claims that they make about a work of literature ( e.g. , a novel ) in the form of quotations from the work .",0,0.9435721,43.55962036513294,30
3037,"We collect a large-scale dataset ( RELiC ) of 78 K literary quotations and surrounding critical analysis and use it to formulate the novel task of literary evidence retrieval , in which models are given an excerpt of literary analysis surrounding a masked quotation and asked to retrieve the quoted passage from the set of all passages in the work .",2,0.8882341,63.08780178838028,61
3037,"Solving this retrieval task requires a deep understanding of complex literary and linguistic phenomena , which proves challenging to methods that overwhelmingly rely on lexical and semantic similarity matching .",0,0.8273237,47.55192950987966,30
3037,We implement a RoBERTa-based dense passage retriever for this task that outperforms existing pretrained information retrieval baselines ;,2,0.58606285,55.988803295011415,20
3037,"however , experiments and analysis by human domain experts indicate that there is substantial room for improvement .",0,0.72555953,32.84088341494149,18
3038,Vision and language navigation ( VLN ) is a challenging visually-grounded language understanding task .,0,0.9539219,35.661915557306024,17
3038,"Given a natural language navigation instruction , a visual agent interacts with a graph-based environment equipped with panorama images and tries to follow the described route .",2,0.4417626,69.4001580506783,29
3038,"Most prior work has been conducted in indoor scenarios where best results were obtained for navigation on routes that are similar to the training routes , with sharp drops in performance when testing on unseen environments .",0,0.8484698,51.93725146366417,37
3038,"We focus on VLN in outdoor scenarios and find that in contrast to indoor VLN , most of the gain in outdoor VLN on unseen data is due to features like junction type embedding or heading delta that are specific to the respective environment graph , while image information plays a very minor role in generalizing VLN to unseen outdoor areas .",3,0.88258123,46.470860685893186,62
3038,"These findings show a bias to specifics of graph representations of urban environments , demanding that VLN tasks grow in scale and diversity of geographical environments .",3,0.9881622,145.2059716715847,27
3039,"Neural coreference resolution models trained on one dataset may not transfer to new , low-resource domains .",0,0.6085998,63.28632780244559,17
3039,Active learning mitigates this problem by sampling a small subset of data for annotators to label .,0,0.5377729,31.400515570123122,17
3039,"While active learning is well-defined for classification tasks , its application to coreference resolution is neither well-defined nor fully understood .",0,0.89069366,41.299798467736494,25
3039,"This paper explores how to actively label coreference , examining sources of model uncertainty and document reading costs .",1,0.8945523,206.40862617815776,19
3039,We compare uncertainty sampling strategies and their advantages through thorough error analysis .,2,0.54542387,149.3499699026221,13
3039,"In both synthetic and human experiments , labeling spans within the same document is more effective than annotating spans across documents .",3,0.80225676,40.96047683514842,22
3039,The findings contribute to a more realistic development of coreference resolution models .,3,0.9908935,28.714044274903376,13
3040,"We propose a framework for training non-autoregressive sequence-to-sequence models for editing tasks , where the original input sequence is iteratively edited to produce the output .",1,0.3472764,15.387768797188542,28
3040,We show that the imitation learning algorithms designed to train such models for machine translation introduces mismatches between training and inference that lead to undertraining and poor generalization in editing scenarios .,3,0.9055604,45.51028424545967,32
3040,"We address this issue with two complementary strategies : 1 ) a roll-in policy that exposes the model to intermediate training sequences that it is more likely to encounter during inference , 2 ) a curriculum that presents easy-to-learn edit operations first , gradually increasing the difficulty of training samples as the model becomes competent .",2,0.5792378,47.33367940268303,62
3040,We show the efficacy of these strategies on two challenging English editing tasks : controllable text simplification and abstractive summarization .,3,0.7638061,29.702077514098317,21
3040,Our approach significantly improves output quality on both tasks and controls output complexity better on the simplification task .,3,0.9212627,72.83166266405699,19
3041,State-of-the-art pre-trained language models have been shown to memorise facts and perform well with limited amounts of training data .,0,0.8891219,7.9843369931045345,24
3041,"To gain a better understanding of how these models learn , we study their generalisation and memorisation capabilities in noisy and low-resource scenarios .",1,0.514942,23.8511473494956,24
3041,We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal results even on extremely noisy datasets .,3,0.9695421,15.90581507870093,29
3041,"However , our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition .",3,0.9774828,28.88340426540873,32
3041,"To mitigate such limitations , we propose an extension based on prototypical networks that improves performance in low-resource named entity recognition tasks .",1,0.36907902,29.771365970562847,23
3042,"Existing automatic evaluation systems of chatbots mostly rely on static chat scripts as ground truth , which is hard to obtain , and requires access to the models of the bots as a form of “ white-box testing ” .",0,0.9010347,50.9955136578435,42
3042,Interactive evaluation mitigates this problem but requires human involvement .,0,0.6472684,60.8166684200907,10
3042,"In our work , we propose an interactive chatbot evaluation framework in which chatbots compete with each other like in a sports tournament , using flexible scoring metrics .",2,0.42104912,53.12514143439454,29
3042,This framework can efficiently rank chatbots independently from their model architectures and the domains for which they are trained .,3,0.6764467,62.568081274815896,20
3043,Self-supervised models for speech processing form representational spaces without using any external labels .,0,0.55186874,41.42551437630076,16
3043,"Increasingly , they appear to be a feasible way of at least partially eliminating costly manual annotations , a problem of particular concern for low-resource languages .",0,0.70316124,39.6954379141928,27
3043,Human perception specializes to the sounds of listeners ’ native languages .,0,0.9431678,492.63829290063865,12
3043,"We examine the representational spaces of three kinds of state of the art self-supervised models : wav2vec , HuBERT and contrastive predictive coding ( CPC ) , and compare them with the perceptual spaces of French-speaking and English-speaking human listeners , both globally and taking account of the behavioural differences between the two language groups .",2,0.6989862,52.60850245946171,56
3043,"We show that the CPC model shows a small native language effect , but that wav2vec and HuBERT seem to develop a universal speech perception space which is not language specific .",3,0.96682054,109.05289276507386,32
3043,"A comparison against the predictions of supervised phone recognisers suggests that all three self-supervised models capture relatively fine-grained perceptual phenomena , while supervised models are better at capturing coarser , phone-level effects , and effects of listeners ’ native language , on perception .",3,0.94148505,129.79859281243105,45
3044,"A long-term goal of AI research is to build intelligent agents that can communicate with humans in natural language , perceive the environment , and perform real-world tasks .",0,0.93897194,16.237760888906056,30
3044,"Vision-and-Language Navigation ( VLN ) is a fundamental and interdisciplinary research topic towards this goal , and receives increasing attention from natural language processing , computer vision , robotics , and machine learning communities .",0,0.957193,38.69011329207483,37
3044,"In this paper , we review contemporary studies in the emerging field of VLN , covering tasks , evaluation metrics , methods , etc .",1,0.90528566,77.6966615216854,25
3044,"Through structured analysis of current progress and challenges , we also highlight the limitations of current VLN and opportunities for future work .",3,0.5256266,42.21266799817393,23
3044,This paper serves as a thorough reference for the VLN research community .,3,0.4542823,35.63773425142924,13
3045,Table fact verification aims to check the correctness of textual statements based on given semi-structured data .,0,0.8452071,21.88575423668046,17
3045,"Most existing methods are devoted to better comprehending logical operations and tables , but they hardly study generating latent programs from statements , with which we can not only retrieve evidences efficiently but also explain reasons behind verifications naturally .",0,0.82934475,143.27229092733637,40
3045,"However , it is challenging to get correct programs with existing weakly supervised semantic parsers due to the huge search space with lots of spurious programs .",0,0.8559993,37.27848346327198,27
3045,"In this paper , we address the challenge by leveraging both lexical features and structure features for program generation .",1,0.9017293,29.75355520294959,20
3045,"Through analyzing the connection between the program tree and the dependency tree , we define a unified concept , operation-oriented tree , to mine structure features , and introduce Structure-Aware Semantic Parsing to integrate structure features into program generation .",2,0.64338297,62.82431848661114,44
3045,"Moreover , we design a refined objective function with lexical features and violation punishments to further avoid spurious programs .",2,0.6132077,162.9739316602311,20
3045,"Experimental results show that our proposed method generates programs more accurately than existing semantic parsers , and achieves comparable performance to the SOTA on the large-scale benchmark TABFACT .",3,0.96463525,26.021690616812382,29
3046,"In real-world scenarios , a text classification task often begins with a cold start , when labeled data is scarce .",0,0.92049086,41.1797556471442,21
3046,"In such cases , the common practice of fine-tuning pre-trained models , such as BERT , for a target classification task , is prone to produce poor performance .",0,0.7931542,23.31706876739909,29
3046,"We suggest a method to boost the performance of such models by adding an intermediate unsupervised classification task , between the pre-training and fine-tuning phases .",3,0.6103186,18.448116742833744,26
3046,"As such an intermediate task , we perform clustering and train the pre-trained model on predicting the cluster labels .",2,0.7084909,43.4426198234569,20
3046,"We test this hypothesis on various data sets , and show that this additional classification phase can significantly improve performance , mainly for topical classification tasks , when the number of labeled instances available for fine-tuning is only a couple of dozen to a few hundred .",3,0.66276735,38.51875323844248,48
3047,"Although transformers are remarkably effective for many tasks , there are some surprisingly easy-looking regular languages that they struggle with .",0,0.81896657,72.77181474151473,23
3047,"Hahn shows that for languages where acceptance depends on a single input symbol , a transformer ’s classification decisions get closer and closer to random guessing ( that is , a cross-entropy of 1 ) as input strings get longer and longer .",3,0.5911716,77.07305530249428,43
3047,"We examine this limitation using two languages : PARITY , the language of bit strings with an odd number of 1s , and FIRST , the language of bit strings starting with a 1 .",2,0.72471786,67.10936645016837,35
3047,We demonstrate three ways of overcoming the limitation implied by Hahn ’s lemma .,3,0.5982682,70.19666563084472,14
3047,"First , we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy , and similarly for FIRST .",2,0.49304324,137.98006821286288,23
3047,"Second , we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero .",2,0.80871147,34.90576082382485,18
3047,"Third , when transformers need to focus on a single position , as for FIRST , we find that they can fail to generalize to longer strings ;",3,0.93104917,83.28602670154615,28
3047,we offer a simple remedy to this problem that also improves length generalization in machine translation .,3,0.50828236,51.552931281733656,17
3048,Regularization methods applying input perturbation have drawn considerable attention and have been frequently explored for NMT tasks in recent years .,0,0.8867342,39.69422652404948,21
3048,"Despite their simplicity and effectiveness , we argue that these methods are limited by the under-fitting of training data .",3,0.4728584,21.84962876748269,20
3048,"In this paper , we utilize prediction difference for ground-truth tokens to analyze the fitting of token-level samples and find that under-fitting is almost as common as over-fitting .",1,0.620891,36.678027071235526,33
3048,"We introduce prediction difference regularization ( PD-R ) , a simple and effective method that can reduce over-fitting and under-fitting at the same time .",2,0.52797776,32.90618678932692,27
3048,"For all token-level samples , PD-R minimizes the prediction difference between the original pass and the input-perturbed pass , making the model less sensitive to small input changes , thus more robust to both perturbations and under-fitted training data .",3,0.85282874,55.51334803713599,43
3048,Experiments on three widely used WMT translation tasks show that our approach can significantly improve over existing perturbation regularization methods .,3,0.8946272,16.663512596154565,21
3048,"On WMT16 En-De task , our model achieves 1.80 SacreBLEU improvement over vanilla transformer .",3,0.9024889,173.04927387898215,16
3049,Cross-lingual transfer learning with large multilingual pre-trained models can be an effective approach for low-resource languages with no labeled training data .,3,0.6020353,9.324741657287174,22
3049,"Existing evaluations of zero-shot cross-lingual generalisability of large pre-trained models use datasets with English training data , and test data in a selection of target languages .",0,0.8675675,37.426555593685755,27
3049,We explore a more extensive transfer learning setup with 65 different source languages and 105 target languages for part-of-speech tagging .,2,0.68999565,37.34129366036696,22
3049,"Through our analysis , we show that pre-training of both source and target language , as well as matching language families , writing systems , word order systems , and lexical-phonetic distance significantly impact cross-lingual performance .",3,0.963533,50.215783209703645,39
3049,The findings described in this paper can be used as indicators of which factors are important for effective zero-shot cross-lingual transfer to zero-and low-resource languages .,3,0.9793858,26.225581179727257,26
3050,Previous sarcasm generation research has focused on how to generate text that people perceive as sarcastic to create more human-like interactions .,0,0.95011777,24.437936831117135,22
3050,"In this paper , we argue that we should first turn our attention to the question of when sarcasm should be generated , finding that humans consider sarcastic responses inappropriate to many input utterances .",1,0.8496707,44.131875909249565,35
3050,"Next , we use a theory-driven framework for generating sarcastic responses , which allows us to control the linguistic devices included during generation .",2,0.8133981,58.559848694953125,26
3050,"For each device , we investigate how much humans associate it with sarcasm , finding that pragmatic insincerity and emotional markers are devices crucial for making sarcasm recognisable .",3,0.63922477,103.62202785793785,29
3051,"With the rapid development of deep learning , Seq2Seq paradigm has become prevalent for end-to-end data-to-text generation , and the BLEU scores have been increasing in recent years .",0,0.92444,13.605344120006627,34
3051,"However , it is widely recognized that there is still a gap between the quality of the texts generated by models and the texts written by human .",0,0.95160645,16.253215060117796,28
3051,"In order to better understand the ability of Seq2Seq models , evaluate their performance and analyze the results , we choose to use Multidimensional Quality Metric ( MQM ) to evaluate several representative Seq2Seq models on end-to-end data-to-text generation .",2,0.52980185,15.112551551777191,44
3051,We annotate the outputs of five models on four datasets with eight error types and find that 1 ) copy mechanism is helpful for the improvement in Omission and Inaccuracy Extrinsic errors but it increases other types of errors such as Addition ;,3,0.89962894,76.14019217131904,43
3051,"2 ) pre-training techniques are highly effective , and pre-training strategy and model size are very significant ;",3,0.83877516,70.42216226286996,18
3051,3 ) the structure of the dataset also influences the model ’s performance greatly ;,3,0.8492459,116.91079288797248,15
3051,4 ) some specific types of errors are generally challenging for seq2seq models .,3,0.5439609,70.41974455130456,14
3052,Probing has become an important tool for analyzing representations in Natural Language Processing ( NLP ) .,0,0.95938706,22.74160468718081,17
3052,"For graphical NLP tasks such as dependency parsing , linear probes are currently limited to extracting undirected or unlabeled parse trees which do not capture the full task .",0,0.86885625,44.89581817889527,29
3052,"This work introduces DepProbe , a linear probe which can extract labeled and directed dependency parse trees from embeddings while using fewer parameters and compute than prior methods .",1,0.68382806,108.7956383378797,29
3052,"Leveraging its full task coverage and lightweight parametrization , we investigate its predictive power for selecting the best transfer language for training a full biaffine attention parser .",2,0.50448567,66.97759100882989,28
3052,"Across 13 languages , our proposed method identifies the best source treebank 94 % of the time , outperforming competitive baselines and prior work .",3,0.8992232,48.982077124828415,25
3052,"Finally , we analyze the informativeness of task-specific subspaces in contextual embeddings as well as which benefits a full parser ’s non-linear parametrization provides .",3,0.58375317,45.77196646396974,26
3053,"Natural language processing ( NLP ) algorithms have become very successful , but they still struggle when applied to out-of-distribution examples .",0,0.9548016,22.761620748370866,23
3053,In this paper we propose a controllable generation approach in order to deal with this domain adaptation ( DA ) challenge .,1,0.9047953,39.55453109839508,22
3053,"Given an input text example , our DoCoGen algorithm generates a domain-counterfactual textual example ( D-con )-that is similar to the original in all aspects , including the task label , but its domain is changed to a desired one .",2,0.64067876,85.24560950794924,43
3053,"Importantly , DoCoGen is trained using only unlabeled examples from multiple domains-no NLP task labels or parallel pairs of textual examples and their domain-counterfactuals are required .",3,0.522499,86.04112686522505,29
3053,We show that DoCoGen can generate coherent counterfactuals consisting of multiple sentences .,3,0.9228859,44.578554092999795,13
3053,"We use the D-cons generated by DoCoGen to augment a sentiment classifier and a multi-label intent classifier in 20 and 78 DA setups , respectively , where source-domain labeled data is scarce .",2,0.7101303,126.60585519925239,33
3053,Our model outperforms strong baselines and improves the accuracy of a state-of-the-art unsupervised DA algorithm .,3,0.8648103,10.054335177442999,22
3054,"Structured document understanding has attracted considerable attention and made significant progress recently , owing to its crucial role in intelligent document processing .",0,0.96144867,47.21701002326638,23
3054,"However , most existing related models can only deal with the document data of specific language ( s ) ( typically English ) included in the pre-training collection , which is extremely limited .",0,0.80604446,100.50170132012549,34
3054,"To address this issue , we propose a simple yet effective Language-independent Layout Transformer ( LiLT ) for structured document understanding .",1,0.5749514,48.73170716080493,24
3054,LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual / multilingual pre-trained textual models .,3,0.46881327,19.17392517868498,32
3054,"Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks , which enables language-independent benefit from the pre-training of document layout structure .",3,0.9061982,60.13037843709919,38
3054,Code and model are publicly available at https://github.com/jpWang/LiLT .,3,0.5558058,20.516168715015553,9
3055,Various models have been proposed to incorporate knowledge of syntactic structures into neural language models .,0,0.8936683,10.989800874480933,16
3055,"However , previous works have relied heavily on elaborate components for a specific language model , usually recurrent neural network ( RNN ) , which makes themselves unwieldy in practice to fit into other neural language models , such as Transformer and GPT-2 .",0,0.89531076,42.72263160938346,46
3055,"In this paper , we introduce the Dependency-based Mixture Language Models .",1,0.74681246,30.139657436560814,14
3055,"In detail , we first train neural language models with a novel dependency modeling objective to learn the probability distribution of future dependent tokens given context .",2,0.83052516,53.40363468837688,27
3055,We then formulate the next-token probability by mixing the previous dependency modeling probability distributions with self-attention .,2,0.82386816,111.60696554612919,17
3055,Extensive experiments and human evaluations show that our method can be easily and effectively applied to different neural language models while improving neural text generation on various tasks .,3,0.94066006,15.277600547586644,29
3056,Identifying argument components from unstructured texts and predicting the relationships expressed among them are two primary steps of argument mining .,0,0.856727,49.73276118206582,21
3056,The intrinsic complexity of these tasks demands powerful learning models .,0,0.90930295,74.01998448408304,11
3056,"While pretrained Transformer-based Language Models ( LM ) have been shown to provide state-of-the-art results over different NLP tasks , the scarcity of manually annotated data and the highly domain-dependent nature of argumentation restrict the capabilities of such models .",0,0.93371576,14.786387430116223,48
3056,"In this work , we propose a novel transfer learning strategy to overcome these challenges .",1,0.84303474,19.676221707100353,16
3056,"We utilize argumentation-rich social discussions from the ChangeMyView subreddit as a source of unsupervised , argumentative discourse-aware knowledge by finetuning pretrained LMs on a selectively masked language modeling task .",2,0.83131534,62.34694967921137,34
3056,"Furthermore , we introduce a novel prompt-based strategy for inter-component relation prediction that compliments our proposed finetuning method while leveraging on the discourse context .",3,0.45771012,46.52683484847803,26
3056,"Exhaustive experiments show the generalization capability of our method on these two tasks over within-domain as well as out-of-domain datasets , outperforming several existing and employed strong baselines .",3,0.9089303,31.723060792005036,31
3057,"In this paper , we propose an entity-based neural local coherence model which is linguistically more sound than previously proposed neural coherence models .",1,0.90167695,21.93868138996664,26
3057,Recent neural coherence models encode the input document using large-scale pretrained language models .,0,0.815667,29.72722048967083,14
3057,Hence their basis for computing local coherence are words and even sub-words .,0,0.5535686,149.37026773215348,13
3057,"The analysis of their output shows that these models frequently compute coherence on the basis of connections between ( sub-) words which , from a linguistic perspective , should not play a role .",3,0.90797377,71.56854286885263,36
3057,"Still , these models achieve state-of-the-art performance in several end applications .",0,0.7518705,11.742823551256974,18
3057,"In contrast to these models , we compute coherence on the basis of entities by constraining the input to noun phrases and proper names .",2,0.47887114,71.9327777403488,25
3057,This provides us with an explicit representation of the most important items in sentences leading to the notion of focus .,3,0.40124846,45.84743666503634,21
3057,This brings our model linguistically in line with pre-neural models of computing coherence .,3,0.8597548,51.702349991545105,14
3057,It also gives us better insight into the behaviour of the model thus leading to better explainability .,3,0.68359435,26.92048948829524,18
3057,"Our approach is also in accord with a recent study ( O’ Connor and Andreas , 2021 ) , which shows that most usable information is captured by nouns and verbs in transformer-based language models .",3,0.8919687,58.121700729120235,38
3057,We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications .,3,0.7655701,24.214067256635516,30
3058,Adversarial attacks are a major challenge faced by current machine learning research .,0,0.93492967,26.92538713087755,13
3058,"These purposely crafted inputs fool even the most advanced models , precluding their deployment in safety-critical applications .",0,0.6113165,139.87439996963593,19
3058,Extensive research in computer vision has been carried to develop reliable defense strategies .,0,0.9546382,64.98966071860679,14
3058,"However , the same issue remains less explored in natural language processing .",0,0.92204046,36.176324801868134,13
3058,Our work presents a model-agnostic detector of adversarial text examples .,1,0.36866748,37.26008120102735,11
3058,The approach identifies patterns in the logits of the target classifier when perturbing the input text .,2,0.4786194,46.21762176431286,17
3058,"The proposed detector improves the current state-of-the-art performance in recognizing adversarial inputs and exhibits strong generalization capabilities across different NLP models , datasets , and word-level attacks .",3,0.86554235,36.318616308702815,34
3059,"In one view , languages exist on a resource continuum and the challenge is to scale existing solutions , bringing under-resourced languages into the high-resource world .",0,0.90126455,71.34492047656845,28
3059,"In another view , presented here , the world ’s language ecology includes standardised languages , local languages , and contact languages .",0,0.44689932,299.2790567011474,23
3059,These are often subsumed under the label of “ under-resourced languages ” even though they have distinct functions and prospects .,0,0.8833985,44.30730740875565,21
3059,I explore this position and propose some ecologically-aware language technology agendas .,1,0.60681355,183.6869581043889,14
3060,The evolution of language follows the rule of gradual change .,0,0.88301545,45.67293018033355,11
3060,"Grammar , vocabulary , and lexical semantic shifts take place over time , resulting in a diachronic linguistic gap .",0,0.8334574,81.27512653314747,20
3060,"As such , a considerable amount of texts are written in languages of different eras , which creates obstacles for natural language processing tasks , such as word segmentation and machine translation .",0,0.9392994,42.55626463091632,33
3060,"Although the Chinese language has a long history , previous Chinese natural language processing research has primarily focused on tasks within a specific era .",0,0.95774865,27.274594061098096,25
3060,"Therefore , we propose a cross-era learning framework for Chinese word segmentation ( CWS ) , CROSSWISE , which uses the Switch-memory ( SM ) module to incorporate era-specific linguistic knowledge .",1,0.5036577,87.05763558921701,32
3060,Experiments on four corpora from different eras show that the performance of each corpus significantly improves .,3,0.8909444,21.706990131795646,17
3060,Further analyses also demonstrate that the SM can effectively integrate the knowledge of the eras into the neural network .,3,0.9847653,56.14488068576003,20
3061,"Although much work in NLP has focused on measuring and mitigating stereotypical bias in semantic spaces , research addressing bias in computational argumentation is still in its infancy .",0,0.9039174,33.967168974090534,29
3061,"In this paper , we address this research gap and conduct a thorough investigation of bias in argumentative language models .",1,0.94496614,28.27850713121619,21
3061,"To this end , we introduce ABBA , a novel resource for bias measurement specifically tailored to argumentation .",1,0.7062953,71.9739497842249,19
3061,We employ our resource to assess the effect of argumentative fine-tuning and debiasing on the intrinsic bias found in transformer-based language models using a lightweight adapter-based approach that is more sustainable and parameter-efficient than full fine-tuning .,2,0.44551802,26.396823097498224,42
3061,"Finally , we analyze the potential impact of language model debiasing on the performance in argument quality prediction , a downstream task of computational argumentation .",3,0.5303142,36.18168139044199,26
3061,Our results show that we are able to successfully and sustainably remove bias in general and argumentative language models while preserving ( and sometimes improving ) model performance in downstream tasks .,3,0.9875443,30.505667414345325,32
3061,We make all experimental code and data available at https://github.com/umanlp/FairArgumentativeLM .,3,0.55301875,29.227921101146567,11
3062,End-to-end simultaneous speech-to-text translation aims to directly perform translation from streaming source speech to target text with high translation quality and low latency .,0,0.9113768,15.526821839993485,31
3062,"A typical simultaneous translation ( ST ) system consists of a speech translation model and a policy module , which determines when to wait and when to translate .",0,0.88566303,58.310910662557696,29
3062,Thus the policy is crucial to balance translation quality and latency .,0,0.56547654,57.54698901109697,12
3062,"Conventional methods usually adopt fixed policies , e.g .",0,0.80279845,95.17288282205055,9
3062,segmenting the source speech with a fixed length and generating translation .,0,0.39506027,81.88363767459654,12
3062,"However , this method ignores contextual information and suffers from low translation quality .",0,0.8226702,28.00576370337372,14
3062,This paper proposes an adaptive segmentation policy for end-to-end ST .,1,0.8515184,28.509262226697103,12
3062,"Inspired by human interpreters , the policy learns to segment the source streaming speech into meaningful units by considering both acoustic features and translation history , maintaining consistency between the segmentation and translation .",2,0.49859682,82.09197392395205,34
3062,Experimental results on English-German and Chinese-English show that our method achieves a good accuracy-latency trade-off over recently proposed state-of-the-art methods .,3,0.93785286,9.275426518107247,34
3063,Simile interpretation is a crucial task in natural language processing .,0,0.933359,16.027936622653066,11
3063,"Nowadays , pre-trained language models ( PLMs ) have achieved state-of-the-art performance on many tasks .",0,0.9528204,7.758509309464813,22
3063,"However , it remains under-explored whether PLMs can interpret similes or not .",0,0.91731757,39.507406516907615,13
3063,"In this paper , we investigate the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing , i.e. , to let the PLMs infer the shared properties of similes .",1,0.8969938,40.05652746548956,36
3063,"We construct our simile property probing datasets from both general textual corpora and human-designed questions , containing 1,633 examples covering seven main categories .",2,0.9047396,122.7463006446768,24
3063,Our empirical study based on the constructed datasets shows that PLMs can infer similes ’ shared properties while still underperforming humans .,3,0.9760572,109.55287595242997,22
3063,"To bridge the gap with human performance , we additionally design a knowledge-enhanced training objective by incorporating the simile knowledge into PLMs via knowledge embedding methods .",2,0.82409,45.683973240113815,29
3063,Our method results in a gain of 8.58 % in the probing task and 1.37 % in the downstream task of sentiment classification .,3,0.9118426,22.97637260061809,24
3063,The datasets and code are publicly available at https://github.com/Abbey4799/PLMs-Interpret-Simile .,3,0.52343684,18.21158420022088,10
3064,"Artificial Intelligence ( AI ) , along with the recent progress in biomedical language understanding , is gradually offering great promise for medical practice .",0,0.96527785,65.0123799814125,25
3064,"With the development of biomedical language understanding benchmarks , AI applications are widely used in the medical field .",0,0.9359172,38.71073541349993,19
3064,"However , most benchmarks are limited to English , which makes it challenging to replicate many of the successes in English for other languages .",0,0.85995024,26.441696039449656,25
3064,"To facilitate research in this direction , we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation ( CBLUE ) benchmark : a collection of natural language understanding tasks including named entity recognition , information extraction , clinical diagnosis normalization , single-sentence / sentence-pair classification , and an associated online platform for model evaluation , comparison , and analysis .",1,0.60676974,64.33433656663958,66
3064,"To establish evaluation on these tasks , we report empirical results with the current 11 pre-trained Chinese models , and experimental results show that state-of-the-art neural models perform by far worse than the human ceiling .",3,0.593871,31.894195686713104,42
3065,Text summarization aims to generate a short summary for an input text .,0,0.92300946,15.796701451863779,13
3065,"In this work , we propose a Non-Autoregressive Unsupervised Summarization ( NAUS ) approach , which does not require parallel data for training .",1,0.72198033,19.39355022074604,24
3065,"Our NAUS first performs edit-based search towards a heuristically defined score , and generates a summary as pseudo-groundtruth .",2,0.7039929,195.7579278201765,19
3065,"Then , we train an encoder-only non-autoregressive Transformer based on the search result .",2,0.85293454,28.606688563367136,14
3065,"We also propose a dynamic programming approach for length-control decoding , which is important for the summarization task .",3,0.42211795,50.84444026675927,21
3065,"Experiments on two datasets show that NAUS achieves state-of-the-art performance for unsupervised summarization , yet largely improving inference efficiency .",3,0.86614835,25.196691688704092,26
3065,"Further , our algorithm is able to perform explicit length-transfer summary generation .",3,0.85019773,138.90347113968187,14
3066,"The principal task in supervised neural machine translation ( NMT ) is to learn to generate target sentences conditioned on the source inputs from a set of parallel sentence pairs , and thus produce a model capable of generalizing to unseen instances .",0,0.9105454,22.598697351883104,43
3066,"However , it is commonly observed that the generalization performance of the model is highly influenced by the amount of parallel data used in training .",0,0.8899939,13.84122163334817,26
3066,"Although data augmentation is widely used to enrich the training data , conventional methods with discrete manipulations fail to generate diverse and faithful training samples .",0,0.90157634,36.67132922496989,26
3066,"In this paper , we present a novel data augmentation paradigm termed Continuous Semantic Augmentation ( CsaNMT ) , which augments each training instance with an adjacency semantic region that could cover adequate variants of literal expression under the same meaning .",1,0.79435414,63.00446788586686,42
3066,"We conduct extensive experiments on both rich-resource and low-resource settings involving various language pairs , including WMT14 English → { German , French} , NIST Chinese →English and multiple low-resource IWSLT translation tasks .",2,0.87255794,45.760847582618425,36
3066,"The provided empirical evidences show that CsaNMT sets a new level of performance among existing augmentation techniques , improving on the state-of-the-art by a large margin .",3,0.9783754,20.63178440510183,33
3066,The core codes are contained in Appendix E .,3,0.3941924,101.8873215267459,9
3067,"We propose knowledge internalization ( KI ) , which aims to complement the lexical knowledge into neural dialog models .",1,0.43126553,80.63710138011834,20
3067,"Instead of further conditioning the knowledge-grounded dialog ( KGD ) models on externally retrieved knowledge , we seek to integrate knowledge about each input token internally into the model ’s parameters .",2,0.41506338,87.79191279957033,33
3067,"To tackle the challenge due to the large scale of lexical knowledge , we adopt the contrastive learning approach and create an effective token-level lexical knowledge retriever that requires only weak supervision mined from Wikipedia .",2,0.69627625,33.243627554474955,38
3067,We demonstrate the effectiveness and general applicability of our approach on various datasets and diversified model structures .,3,0.78539306,27.408952504144995,18
3068,"In this paper , we propose a mixture model-based end-to-end method to model the syntactic-semantic dependency correlation in Semantic Role Labeling ( SRL ) .",1,0.8775042,21.34183399268202,30
3068,Semantic dependencies in SRL are modeled as a distribution over semantic dependency labels conditioned on a predicate and an argument word .,2,0.49136695,70.41541302566367,22
3068,The semantic label distribution varies depending on Shortest Syntactic Dependency Path ( SSDP ) hop patterns .,0,0.6297798,298.11611633575745,17
3068,"We target the variation of semantic label distributions using a mixture model , separately estimating semantic label distributions for different hop patterns and probabilistically clustering hop patterns with similar semantic label distributions .",2,0.8526858,57.60173147405375,33
3068,Experiments show that the proposed method successfully learns a cluster assignment reflecting the variation of semantic label distributions .,3,0.95048165,84.59190313284519,19
3068,"Modeling the variation improves performance in predicting short distance semantic dependencies , in addition to the improvement on long distance semantic dependencies that previous syntax-aware methods have achieved .",3,0.80618864,57.85048312602241,31
3068,"The proposed method achieves a small but statistically significant improvement over baseline methods in English , German , and Spanish and obtains competitive performance with state-of-the-art methods in English .",3,0.8954202,10.038509739073161,36
3069,"We are interested in a novel task , singing voice beautification ( SVB ) .",0,0.4678025,151.2068284482072,15
3069,"Given the singing voice of an amateur singer , SVB aims to improve the intonation and vocal tone of the voice , while keeping the content and vocal timbre .",0,0.9007042,47.68606811760877,30
3069,"Current automatic pitch correction techniques are immature , and most of them are restricted to intonation but ignore the overall aesthetic quality .",0,0.9010834,88.06502871964972,23
3069,"Hence , we introduce Neural Singing Voice Beautifier ( NSVB ) , the first generative model to solve the SVB task , which adopts a conditional variational autoencoder as the backbone and learns the latent representations of vocal tone .",2,0.5655754,46.66810571923021,40
3069,"In NSVB , we propose a novel time-warping approach for pitch correction : Shape-Aware Dynamic Time Warping ( SADTW ) , which ameliorates the robustness of existing time-warping approaches , to synchronize the amateur recording with the template pitch curve .",1,0.33686957,58.03922526251741,47
3069,"Furthermore , we propose a latent-mapping algorithm in the latent space to convert the amateur vocal tone to the professional one .",2,0.514269,45.71851363938098,22
3069,"To achieve this , we also propose a new dataset containing parallel singing recordings of both amateur and professional versions .",2,0.60633665,63.59452185658696,21
3069,Extensive experiments on both Chinese and English songs demonstrate the effectiveness of our methods in terms of both objective and subjective metrics .,3,0.8261727,17.491037622598583,23
3069,Audio samples are available at https://neuralsvb.github.io .,3,0.50573206,15.78919714426976,7
3069,Codes : https://github.com/MoonInTheRiver/NeuralSVB .,4,0.40966654,46.702766640705285,4
3070,"Towards building intelligent dialogue agents , there has been a growing interest in introducing explicit personas in generation models .",0,0.95630574,44.87563497832114,20
3070,"However , with limited persona-based dialogue data at hand , it may be difficult to train a dialogue generation model well .",0,0.7291654,37.29151537717083,23
3070,"We point out that the data challenges of this generation task lie in two aspects : first , it is expensive to scale up current persona-based dialogue datasets ;",3,0.7129971,60.40902967915775,30
3070,"second , each data sample in this task is more complex to learn with than conventional dialogue data .",0,0.5285999,86.8536343876259,19
3070,"To alleviate the above data issues , we propose a data manipulation method , which is model-agnostic to be packed with any persona-based dialogue generation model to improve their performance .",2,0.54477936,46.676907645050605,34
3070,The original training samples will first be distilled and thus expected to be fitted more easily .,3,0.41361296,128.5926782179223,17
3070,"Next , we show various effective ways that can diversify such easier distilled data .",3,0.4551635,188.5986406166846,15
3070,"A given base model will then be trained via the constructed data curricula , i.e .",2,0.554749,86.31385842751048,16
3070,first on augmented distilled samples and then on original ones .,2,0.67945457,280.41737159279637,11
3070,Experiments illustrate the superiority of our method with two strong base dialogue models ( Transformer encoder-decoder and GPT2 ) .,3,0.8169091,38.87286464032488,20
3071,"Language model ( LM ) pretraining captures various knowledge from text corpora , helping downstream tasks .",0,0.372444,148.35995042338402,17
3071,"However , existing methods such as BERT model a single document , and do not capture dependencies or knowledge that span across documents .",0,0.849272,56.727658181342235,24
3071,"In this work , we propose LinkBERT , an LM pretraining method that leverages links between documents , e.g. , hyperlinks .",1,0.5652962,62.806766154896295,22
3071,"Given a text corpus , we view it as a graph of documents and create LM inputs by placing linked documents in the same context .",2,0.63023615,65.94045740981696,26
3071,"We then pretrain the LM with two joint self-supervised objectives : masked language modeling and our new proposal , document relation prediction .",2,0.8162883,138.99676039847375,23
3071,We show that LinkBERT outperforms BERT on various downstream tasks across two domains : the general domain ( pretrained on Wikipedia with hyperlinks ) and biomedical domain ( pretrained on PubMed with citation links ) .,3,0.896337,35.41002263931954,36
3071,"LinkBERT is especially effective for multi-hop reasoning and few-shot QA ( + 5 % absolute improvement on HotpotQA and TriviaQA ) , and our biomedical LinkBERT sets new states of the art on various BioNLP tasks ( + 7 % on BioASQ and USMLE ) .",3,0.94690067,39.33512747514315,46
3071,"We release our pretrained models , LinkBERT and BioLinkBERT , as well as code and data .",2,0.570226,54.70924767317382,17
3072,"Question answering over temporal knowledge graphs ( KGs ) efficiently uses facts contained in a temporal KG , which records entity relations and when they occur in time , to answer natural language questions ( e.g. , “ Who was the president of the US before Obama ? ” ) .",0,0.8564417,59.70023271272261,51
3072,"These questions often involve three time-related challenges that previous work fail to adequately address : 1 ) questions often do not specify exact timestamps of interest ( e.g. , “ Obama ” instead of 2000 ) ;",0,0.8379706,77.29070922037879,38
3072,"2 ) subtle lexical differences in time relations ( e.g. , “ before ” vs “ after ” ) ;",3,0.61609834,51.897295144343175,20
3072,"3 ) off-the-shelf temporal KG embeddings that previous work builds on ignore the temporal order of timestamps , which is crucial for answering temporal-order related questions .",3,0.38765678,38.09768985125857,30
3072,"In this paper , we propose a time-sensitive question answering ( TSQA ) framework to tackle these problems .",1,0.87593454,19.859240107976884,20
3072,TSQA features a timestamp estimation module to infer the unwritten timestamp from the question .,2,0.4138102,60.86224393449542,15
3072,We also employ a time-sensitive KG encoder to inject ordering information into the temporal KG embeddings that TSQA is based on .,2,0.7627721,33.85471483183303,22
3072,"With the help of techniques to reduce the search space for potential answers , TSQA significantly outperforms the previous state of the art on a new benchmark for question answering over temporal KGs , especially achieving a 32 % ( absolute ) error reduction on complex questions that require multiple steps of reasoning over facts in the temporal KG .",3,0.91505647,38.240827679976114,60
3073,Phonemes are defined by their relationship to words : changing a phoneme changes the word .,0,0.866771,59.52030645083268,16
3073,Learning a phoneme inventory with little supervision has been a longstanding challenge with important applications to under-resourced speech technology .,0,0.93175584,64.88812559512431,20
3073,"In this paper , we bridge the gap between the linguistic and statistical definition of phonemes and propose a novel neural discrete representation learning model for self-supervised learning of phoneme inventory with raw speech and word labels .",1,0.87626624,29.548523410690667,38
3073,"Under mild assumptions , we prove that the phoneme inventory learned by our approach converges to the true one with an exponentially low error rate .",3,0.6489521,63.61999931276409,26
3073,"Moreover , in experiments on TIMIT and Mboshi benchmarks , our approach consistently learns a better phoneme-level representation and achieves a lower error rate in a zero-resource phoneme recognition task than previous state-of-the-art self-supervised representation learning algorithms .",3,0.9134946,36.30582916870752,47
3074,Neural language models ( LMs ) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary .,0,0.70191306,27.443955639043516,26
3074,The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary .,2,0.5636055,39.32178452423861,25
3074,"However , we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them .",3,0.96579146,47.03380300946068,52
3074,"In this work , we demonstrate the importance of this limitation both theoretically and practically .",1,0.79882544,37.32301166649187,16
3074,Our work not only deepens our understanding of softmax bottleneck and mixture of softmax ( MoS ) but also inspires us to propose multi-facet softmax ( MFS ) to address the limitations of MoS .,3,0.8792935,38.621489029301,35
3074,"Extensive empirical analyses confirm our findings and show that against MoS , the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT .",3,0.9794572,28.123500649362185,29
3075,Conversational question answering aims to provide natural-language answers to users in information-seeking conversations .,0,0.9239252,24.188367590546648,17
3075,"Existing conversational QA benchmarks compare models with pre-collected human-human conversations , using ground-truth answers provided in conversational history .",0,0.8383723,62.230578854677105,21
3075,It remains unclear whether we can rely on this static evaluation for model development and whether current systems can well generalize to real-world human-machine conversations .,0,0.73032165,30.086593554461412,26
3075,"In this work , we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems , where human evaluators converse with models and judge the correctness of their answers .",1,0.8065415,11.046908675500031,37
3075,"We find that the distribution of human machine conversations differs drastically from that of human-human conversations , and there is a disagreement between human and gold-history evaluation in terms of model ranking .",3,0.9695179,45.48426134524537,34
3075,"We further investigate how to improve automatic evaluations , and propose a question rewriting mechanism based on predicted history , which better correlates with human judgments .",3,0.6804143,78.14237073437222,27
3075,"Finally , we analyze the impact of various modeling strategies and discuss future directions towards building better conversational question answering systems .",3,0.7012137,23.896153361805833,22
3076,"When primed with only a handful of training samples , very large , pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised , fine-tuned , large , pretrained language models .",0,0.5430339,26.517302926876447,40
3076,We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance : essentially some permutations are “ fantastic ” and some not .,3,0.8792988,53.959656512570064,40
3076,"We analyse this phenomenon in detail , establishing that : it is present across model sizes ( even for the largest current models ) , it is not related to a specific subset of samples , and that a given good permutation for one model is not transferable to another .",3,0.7503677,57.79514599263232,51
3076,"While one could use a development set to determine which permutations are performant , this would deviate from the true few-shot setting as it requires additional annotated data .",3,0.5680128,38.648164838769254,30
3076,"Instead , we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set , we identify performant prompts .",2,0.78183866,66.43944756057586,34
3076,Our method yields a 13 % relative improvement for GPT-family models across eleven different established text classification tasks .,3,0.89758503,85.78733587806023,21
3077,We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums .,2,0.66263616,89.2221988970884,18
3077,Our agents operate in LIGHT ( Urbanek et al .,0,0.4572311,369.09713814952545,10
3077,2019 ) — a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language .,0,0.65407825,29.569827871238704,24
3077,"Goals in this environment take the form of character-based quests , consisting of personas and motivations .",0,0.81087124,73.84572437786998,19
3077,We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals .,2,0.4153069,135.63014508672512,30
3077,"In particular , we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution — an easier environment is one that is more likely to have been found in the unaugmented dataset .",2,0.6657744,51.445655041943894,39
3077,An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zero-shot performance on never-before-seen quests .,3,0.93225414,31.309104374182755,31
3078,Translation quality evaluation plays a crucial role in machine translation .,0,0.94213605,19.842159521731425,11
3078,"According to the input format , it is mainly separated into three tasks , i.e. , reference-only , source-only and source-reference-combined .",3,0.3751431,39.14637610163654,23
3078,"Recent methods , despite their promising results , are specifically designed and optimized on one of them .",0,0.82576686,108.31723859356997,18
3078,"This limits the convenience of these methods , and overlooks the commonalities among tasks .",0,0.5475762,62.614282573157176,15
3078,"In this paper , we propose , which is the first unified framework engaged with abilities to handle all three evaluation tasks .",1,0.8592544,83.67637282494664,23
3078,"Concretely , we propose monotonic regional attention to control the interaction among input segments , and unified pretraining to better adapt multi-task training .",3,0.7970542,69.2493538114795,24
3078,We testify our framework on WMT 2019 Metrics and WMT 2020 Quality Estimation benchmarks .,2,0.6392299,54.08818903454692,15
3078,Extensive analyses show that our single model can universally surpass various state-of-the-art or winner methods across tasks .,3,0.9513507,34.670333218677555,24
3078,Both source code and associated models are available at https://github.com/NLP2CT/UniTE .,3,0.5833159,12.263540323650467,11
3079,"Program induction for answering complex questions over knowledge bases ( KBs ) aims to decompose a question into a multi-step program , whose execution against the KB produces the final answer .",0,0.916226,64.04038822570627,32
3079,Learning to induce programs relies on a large number of parallel question-program pairs for the given KB .,0,0.44434538,66.7664669053043,20
3079,"However , for most KBs , the gold program annotations are usually lacking , making learning difficult .",0,0.8508662,192.92627492667117,18
3079,"In this paper , we propose the approach of program transfer , which aims to leverage the valuable program annotations on the rich-resourced KBs as external supervision signals to aid program induction for the low-resourced KBs that lack program annotations .",1,0.89214057,50.38621119666481,41
3079,"For program transfer , we design a novel two-stage parsing framework with an efficient ontology-guided pruning strategy .",2,0.8013698,39.53666427082665,21
3079,"First , a sketch parser translates the question into a high-level program sketch , which is the composition of functions .",2,0.6297574,82.9456613532094,22
3079,"Second , given the question and sketch , an argument parser searches the detailed arguments from the KB for functions .",2,0.67401665,255.04001635965972,21
3079,"During the searching , we incorporate the KB ontology to prune the search space .",2,0.79674643,104.37062489049808,15
3079,"The experiments on ComplexWebQuestions and WebQuestionSP show that our method outperforms SOTA methods significantly , demonstrating the effectiveness of program transfer and our framework .",3,0.9511874,59.63348579251405,25
3079,Our codes and datasets can be obtained from https://github.com/THU-KEG/ProgramTransfer .,3,0.74532723,15.83767455528218,10
3080,"Complete Multi-lingual Neural Machine Translation ( C-MNMT ) achieves superior performance against the conventional MNMT by constructing multi-way aligned corpus , i.e. , aligning bilingual training examples from different language pairs when either their source or target sides are identical .",0,0.8776631,55.84695237026254,42
3080,"However , since exactly identical sentences from different language pairs are scarce , the power of the multi-way aligned corpus is limited by its scale .",0,0.81605136,80.0699188052632,26
3080,"To handle this problem , this paper proposes “ Extract and Generate ” ( EAG ) , a two-step approach to construct large-scale and high-quality multi-way aligned corpus from bilingual data .",1,0.49333772,37.39607748300488,32
3080,"Specifically , we first extract candidate aligned examples by pairing the bilingual examples from different language pairs with highly similar source or target sentences ;",2,0.8849683,116.08891246554585,25
3080,and then generate the final aligned examples from the candidates with a well-trained generation model .,2,0.7074305,59.81155807959491,18
3080,"With this two-step pipeline , EAG can construct a large-scale and multi-way aligned corpus whose diversity is almost identical to the original bilingual corpus .",3,0.6550893,52.03967144038515,26
3080,"Experiments on two publicly available datasets i.e. , WMT-5 and OPUS-100 , show that the proposed method achieves significant improvements over strong baselines , with + 1.1 and + 1.4 BLEU points improvements on the two datasets respectively .",3,0.86700183,12.590565689535481,43
3081,"Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks , traditional static embeddings ( e.g. , Skip-gram , Word2Vec ) still play an important role in low-resource and lightweight settings due to their low computational cost , ease of deployment , and stability .",0,0.74273795,30.58272142611027,48
3081,"In this paper , we aim to improve word embeddings by 1 ) incorporating more contextual information from existing pre-trained models into the Skip-gram framework , which we call Context-to-Vec ;",1,0.8245421,36.047239410680156,33
3081,2 ) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution .,2,0.7179371,114.00592269880092,23
3081,"Through extrinsic and intrinsic tasks , our methods are well proven to outperform the baselines by a large margin .",3,0.77681595,17.662259978932262,20
3082,Sarcasm is important to sentiment analysis on social media .,0,0.7464936,32.85090718662296,10
3082,Sarcasm Target Identification ( STI ) deserves further study to understand sarcasm in depth .,0,0.50639635,83.71209094952805,15
3082,"However , text lacking context or missing sarcasm target makes target identification very difficult .",0,0.8314652,278.061796894305,15
3082,"In this paper , we introduce multimodality to STI and present Multimodal Sarcasm Target Identification ( MSTI ) task .",1,0.8825962,40.5681251774451,20
3082,We propose a novel multi-scale cross-modality model that can simultaneously perform textual target labeling and visual target detection .,1,0.5677835,24.444824675169215,19
3082,"In the model , we extract multi-scale visual features to enrich spatial information for different sized visual sarcasm targets .",2,0.79065496,99.50997332217716,20
3082,"We design a set of convolution networks to unify multi-scale visual features with textual features for cross-modal attention learning , and correspondingly a set of transposed convolution networks to restore multi-scale visual information .",2,0.8560567,25.911628355895363,34
3082,"The results show that visual clues can improve the performance of TSTI by a large margin , and VSTI achieves good accuracy .",3,0.98780835,32.656484852099155,23
3083,The dominant paradigm for high-performance models in novel NLP tasks today is direct specialization for the task via training from scratch or fine-tuning large pre-trained models .,0,0.9309805,26.46911456623205,28
3083,We hypothesize that human performance is better characterized by flexible inference through composition of basic computational motifs available to the human language user .,3,0.64164835,102.0424213369414,24
3083,"To test this hypothesis , we formulate a set of novel fragmentary text completion tasks , and compare the behavior of three direct-specialization models against a new model we introduce , GibbsComplete , which composes two basic computational motifs central to contemporary models : masked and autoregressive word prediction .",2,0.7294563,112.74322806136965,50
3083,"We conduct three types of evaluation : human judgments of completion quality , satisfaction of syntactic constraints imposed by the input fragment , and similarity to human behavior in the structural statistics of the completions .",2,0.9061709,85.22211804288122,36
3083,"With no task-specific parameter tuning , GibbsComplete performs comparably to direct-specialization models in the first two evaluations , and outperforms all direct-specialization models in the third evaluation .",3,0.91300416,30.803403258794674,28
3083,These results support our hypothesis that human behavior in novel language tasks and environments may be better characterized by flexible composition of basic computational motifs rather than by direct specialization .,3,0.99117494,56.47266596961101,31
3084,Non-autoregressive text to speech ( NAR-TTS ) models have attracted much attention from both academia and industry due to their fast generation speed .,0,0.96415895,22.700277684532008,26
3084,"One limitation of NAR-TTS models is that they ignore the correlation in time and frequency domains while generating speech mel-spectrograms , and thus cause blurry and over-smoothed results .",0,0.7082728,100.9349453548218,31
3084,"In this work , we revisit this over-smoothing problem from a novel perspective : the degree of over-smoothness is determined by the gap between the complexity of data distributions and the capability of modeling methods .",1,0.6851622,26.49913277913717,36
3084,Both simplifying data distributions and improving modeling methods can alleviate the problem .,0,0.6445909,51.38273422644758,13
3084,"Accordingly , we first study methods reducing the complexity of data distributions .",1,0.53569424,137.23695916291283,13
3084,Then we conduct a comprehensive study on NAR-TTS models that use some advanced modeling methods .,2,0.50531447,59.242788422990614,18
3084,"Based on these studies , we find that 1 ) methods that provide additional condition inputs reduce the complexity of data distributions to model , thus alleviating the over-smoothing problem and achieving better voice quality .",3,0.9687362,68.57337362186388,36
3084,"2 ) Among advanced modeling methods , Laplacian mixture loss performs well at modeling multimodal distributions and enjoys its simplicity , while GAN and Glow achieve the best voice quality while suffering from increased training or model complexity .",3,0.7480636,138.08432531453178,39
3084,3 ) The two categories of methods can be combined to further alleviate the over-smoothness and improve the voice quality .,3,0.84240663,42.81919055814823,21
3084,4 ) Our experiments on the multi-speaker dataset lead to similar conclusions as above and providing more variance information can reduce the difficulty of modeling the target data distribution and alleviate the requirements for model capacity .,3,0.9835118,58.34150397509764,37
3085,Long-range semantic coherence remains a challenge in automatic language generation and understanding .,0,0.95306075,26.829726981256943,13
3085,We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction .,3,0.9523491,57.216296426846924,18
3085,"We present coherence boosting , an inference procedure that increases a LM ’s focus on a long context .",1,0.33927742,175.00915976296395,19
3085,We show the benefits of coherence boosting with pretrained models by distributional analyses of generated ordinary text and dialog responses .,3,0.75268567,95.10501560880573,21
3085,It is also found that coherence boosting with state-of-the-art models for various zero-shot NLP tasks yields performance gains with no additional training .,3,0.9721263,19.199267578397453,29
3086,"Uncertainty estimation ( UE ) of model predictions is a crucial step for a variety of tasks such as active learning , misclassification detection , adversarial attack detection , out-of-distribution detection , etc .",0,0.9253759,32.30274638794753,36
3086,Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks .,0,0.77334946,26.74415428259489,20
3086,Little attention has been paid to UE in natural language processing .,0,0.94464123,34.57957524575049,12
3086,"To fill this gap , we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications , one of which approaches or even outperforms computationally intensive methods .",1,0.56990916,33.72542135639875,53
3087,"We propose VALSE ( Vision And Language Structured Evaluation ) , a novel benchmark designed for testing general-purpose pretrained vision and language ( V&L ) models for their visio-linguistic grounding capabilities on specific linguistic phenomena .",1,0.5200051,72.27315270128311,38
3087,VALSE offers a suite of six tests covering various linguistic constructs .,0,0.63226944,158.7017437433596,12
3087,"Solving these requires models to ground linguistic phenomena in the visual modality , allowing more fine-grained evaluations than hitherto possible .",0,0.72360694,67.7870422495419,22
3087,"We build VALSE using methods that support the construction of valid foils , and report results from evaluating five widely-used V&L models .",2,0.5875809,139.82385254315074,25
3087,Our experiments suggest that current models have considerable difficulty addressing most phenomena .,3,0.98520595,94.51666326008082,13
3087,"Hence , we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective , complementing the canonical task-centred V&L evaluations .",3,0.9662397,57.119742111224454,33
3088,"The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation , beyond what can be gleaned from inspecting the behavior of an adult speaker .",0,0.88078547,62.55507466018062,28
3088,"To apply a similar approach to analyze neural language models ( NLM ) , it is first necessary to establish that different models are similar enough in the generalizations they make .",0,0.7109939,33.17021188381077,32
3088,"In this paper , we show that NLMs with different initialization , architecture , and training data acquire linguistic phenomena in a similar order , despite their different end performance .",1,0.73865074,112.24152886559406,31
3088,These findings suggest that there is some mutual inductive bias that underlies these models ’ learning of linguistic phenomena .,3,0.99006224,40.351949316632655,20
3088,"Taking inspiration from psycholinguistics , we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs .",3,0.49364173,33.7081943939028,24
3088,"Leveraging these findings , we compare the relative performance on different phenomena at varying learning stages with simpler reference models .",3,0.5824499,78.01065179776556,21
3088,Results suggest that NLMs exhibit consistent “ developmental ” stages .,3,0.9882536,296.53517681705506,11
3088,"Moreover , we find the learning trajectory to be approximately one-dimensional : given an NLM with a certain overall performance , it is possible to predict what linguistic generalizations it has already acquired .",3,0.91459143,75.94851782072759,34
3088,"Initial analysis of these stages presents phenomena clusters ( notably morphological ones ) , whose performance progresses in unison , suggesting a potential link between the generalizations behind them .",3,0.84804726,258.09606277509664,30
3089,Unfamiliar terminology and complex language can present barriers to understanding science .,0,0.8841543,90.3978932639647,12
3089,Natural language processing stands to help address these issues by automatically defining unfamiliar terms .,0,0.9076822,88.00986762349984,15
3089,We introduce a new task and dataset for defining scientific terms and controlling the complexity of generated definitions as a way of adapting to a specific reader ’s background knowledge .,2,0.38437685,58.90611471784708,31
3089,"We test four definition generation methods for this new task , finding that a sequence-to-sequence approach is most successful .",3,0.52568483,35.03953760773148,22
3089,We then explore the version of the task in which definitions are generated at a target complexity level .,2,0.48807293,51.037513477455924,19
3089,"We introduce a novel reranking approach and find in human evaluations that it offers superior fluency while also controlling complexity , compared to several controllable generation baselines .",3,0.6026439,72.75665228789626,28
3090,"In text classification tasks , useful information is encoded in the label names .",0,0.793636,57.93932063407815,14
3090,Label semantic aware systems have leveraged this information for improved text classification performance during fine-tuning and prediction .,0,0.91017956,55.934980240223645,18
3090,"However , use of label-semantics during pre-training has not been extensively explored .",0,0.9047222,31.780920307342694,14
3090,We therefore propose Label Semantic Aware Pre-training ( LSAP ) to improve the generalization and data efficiency of text classification systems .,1,0.37328368,41.040957123297794,22
3090,LSAP incorporates label semantics into pre-trained generative models ( T5 in our case ) by performing secondary pre-training on labeled sentences from a variety of domains .,2,0.58389884,58.209400350303625,27
3090,"As domain-general pre-training requires large amounts of data , we develop a filtering and labeling pipeline to automatically create sentence-label pairs from unlabeled text .",2,0.6071283,33.85160741114473,27
3090,"We perform experiments on intent ( ATIS , Snips , TOPv2 ) and topic classification ( AG News , Yahoo ! Answers ) .",2,0.76879317,153.0359024194803,24
3090,LSAP obtains significant accuracy improvements over state-of-the-art models for few-shot text classification while maintaining performance comparable to state of the art in high-resource settings .,3,0.89864427,8.79733616362779,33
3091,Residual networks are an Euler discretization of solutions to Ordinary Differential Equations ( ODE ) .,0,0.8791811,48.28003944684076,16
3091,This paper explores a deeper relationship between Transformer and numerical ODE methods .,1,0.8987458,78.5825051308536,13
3091,We first show that a residual block of layers in Transformer can be described as a higher-order solution to ODE .,3,0.4718789,49.79369713545004,22
3091,"Inspired by this , we design a new architecture , ODE Transformer , which is analogous to the Runge-Kutta method that is well motivated in ODE .",2,0.7320354,60.00011766388406,28
3091,"As a natural extension to Transformer , ODE Transformer is easy to implement and efficient to use .",3,0.5657899,31.470614618377926,18
3091,"Experimental results on the large-scale machine translation , abstractive summarization , and grammar error correction tasks demonstrate the high genericity of ODE Transformer .",3,0.94147646,38.650228928568154,24
3091,"It can gain large improvements in model performance over strong baselines ( e.g. , 30.77 and 44.11 BLEU scores on the WMT ’14 English-German and English-French benchmarks ) at a slight cost in inference efficiency .",3,0.83919406,28.365201850023748,41
3092,"Data sharing restrictions are common in NLP , especially in the clinical domain , but there is limited research on adapting models to new domains without access to the original training data , a setting known as source-free domain adaptation .",0,0.91469616,43.36585558525054,42
3092,"We take algorithms that traditionally assume access to the source-domain training data — active learning , self-training , and data augmentation — and adapt them for source free domain adaptation .",2,0.81759197,56.377527851149864,31
3092,Then we systematically compare these different strategies across multiple tasks and domains .,2,0.6808574,45.15187877601611,13
3092,"Task 10 tasks and domains , but though the shared task saw successful self-trained and data augmented models , our systematic comparison finds these strategies to be unreliable for source-free domain adaptation .",3,0.95247084,198.45296884475502,34
3093,"Several high-profile events , such as the mass testing of emotion recognition systems on vulnerable sub-populations and using question answering systems to make moral judgments , have highlighted how technology will often lead to more adverse outcomes for those that are already marginalized .",0,0.8576545,38.933552573840544,46
3093,"At issue here are not just individual systems and datasets , but also the AI tasks themselves .",0,0.73610866,75.5067163837045,18
3093,"In this position paper , I make a case for thinking about ethical considerations not just at the level of individual models and datasets , but also at the level of AI tasks .",1,0.76379514,27.617134193079792,34
3093,"I will present a new form of such an effort , Ethics Sheets for AI Tasks , dedicated to fleshing out the assumptions and ethical considerations hidden in how a task is commonly framed and in the choices we make regarding the data , method , and evaluation .",1,0.6724121,73.33896908316908,49
3093,"I will also present a template for ethics sheets with 50 ethical considerations , using the task of emotion recognition as a running example .",2,0.34517616,108.0623379013691,25
3093,Ethics sheets are a mechanism to engage with and document ethical considerations before building datasets and systems .,0,0.8899605,99.57443136148795,18
3093,"Similar to survey articles , a small number of carefully created ethics sheets can serve numerous researchers and developers .",0,0.58509326,214.37397454695918,20
3094,Negation and uncertainty modeling are long-standing tasks in natural language processing .,0,0.9048109,19.58023119745517,12
3094,Linguistic theory postulates that expressions of negation and uncertainty are semantically independent from each other and the content they modify .,0,0.91900784,39.404559413428466,21
3094,"However , previous works on representation learning do not explicitly model this independence .",0,0.8894363,51.35757765739679,14
3094,"We therefore attempt to disentangle the representations of negation , uncertainty , and content using a Variational Autoencoder .",2,0.47334662,28.932614608181893,19
3094,"We find that simply supervising the latent representations results in good disentanglement , but auxiliary objectives based on adversarial learning and mutual information minimization can provide additional disentanglement gains .",3,0.97743416,46.84715172000503,30
3095,"Recently , parallel text generation has received widespread attention due to its success in generation efficiency .",0,0.95496756,42.3240955978136,17
3095,"Although many advanced techniques are proposed to improve its generation quality , they still need the help of an autoregressive model for training to overcome the one-to-many multi-modal phenomenon in the dataset , limiting their applications .",0,0.8414265,32.73196293603433,38
3095,"In this paper , we propose GLAT , which employs the discrete latent variables to capture word categorical information and invoke an advanced curriculum learning technique , alleviating the multi-modality problem .",1,0.7656126,67.01167688952988,32
3095,"Experiment results show that our method outperforms strong baselines without the help of an autoregressive model , which further broadens the application scenarios of the parallel decoding paradigm .",3,0.96530795,21.322888527481872,29
3096,Prompts for pre-trained language models ( PLMs ) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks .,0,0.9175872,13.374777752930013,24
3096,"Among these methods , prompt tuning , which freezes PLMs and only tunes soft prompts , provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks .",3,0.5765538,82.34437345747253,30
3096,"However , prompt tuning is yet to be fully explored .",0,0.78295165,50.74690126942885,11
3096,"In our pilot experiments , we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient , whereas it is much worse under few-shot learning settings , which may hinder the application of prompt tuning .",3,0.9634074,54.705908587595616,41
3096,We attribute this low performance to the manner of initializing soft prompts .,3,0.9341117,106.88422656541361,13
3096,"Therefore , in this work , we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization .",1,0.5746086,33.81087413447628,25
3096,We name this Pre-trained Prompt Tuning framework “ PPT ” .,2,0.55966413,68.88681465096359,11
3096,"To ensure the generalization of PPT , we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task .",2,0.7334496,65.18058793691395,26
3096,Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings .,3,0.921392,21.813500519361426,27
3096,Our approach is effective and efficient for using large-scale PLMs in practice .,3,0.9167951,33.798212454041476,13
3097,We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings .,3,0.9718066,54.85668327656221,16
3097,"As a result , over 1 % of the unprompted output of language models trained on these datasets is copied verbatim from the training data .",3,0.63492,31.235432760774227,26
3097,"We develop two tools that allow us to deduplicate training datasets — for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times .",2,0.7691902,61.0818958671705,30
3097,Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy .,3,0.6309119,35.95392283210019,27
3097,"We can also reduce train-test overlap , which affects over 4 % of the validation set of standard datasets , thus allowing for more accurate evaluation .",3,0.862065,91.11975280857013,27
3097,Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets .,4,0.40768576,9.590405468397327,8
3098,"Automated methods have been widely used to identify and analyze mental health conditions ( e.g. , depression ) from various sources of information , including social media .",0,0.91799843,30.726423241109654,28
3098,"Yet , deployment of such models in real-world healthcare applications faces challenges including poor out-of-domain generalization and lack of trust in black box models .",0,0.9310435,30.87364879900501,28
3098,"In this work , we propose approaches for depression detection that are constrained to different degrees by the presence of symptoms described in PHQ9 , a questionnaire used by clinicians in the depression screening process .",1,0.87890494,58.78184147388615,36
3098,"In dataset-transfer experiments on three social media datasets , we find that grounding the model in PHQ9 ’s symptoms substantially improves its ability to generalize to out-of-distribution data compared to a standard BERT-based approach .",3,0.91807127,31.868800274477582,42
3098,"Furthermore , this approach can still perform competitively on in-domain data .",3,0.8475233,26.906635847615615,13
3098,These results and our qualitative analyses suggest that grounding model predictions in clinically-relevant symptoms can improve generalizability while producing a model that is easier to inspect .,3,0.9911112,46.99847060240627,29
3099,The largest store of continually updating knowledge on our planet can be accessed via internet search .,0,0.89535636,80.26705696391355,17
3099,In this work we study giving access to this information to conversational agents .,1,0.7577723,38.797958676686015,14
3099,"Large language models , even though they store an impressive amount of knowledge within their weights , are known to hallucinate facts when generating dialogue ( Shuster et al. , 2021 ) ;",0,0.6676454,115.0558422065029,33
3099,"moreover , those facts are frozen in time at the point of model training .",3,0.4555414,124.94411461617337,15
3099,"In contrast , we propose an approach that learns to generate an internet search query based on the context , and then conditions on the search results to finally generate a response , a method that can employ up-to-the-minute relevant information .",2,0.60010546,32.7352485462346,46
3099,We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses .,2,0.8430928,85.04755765122235,35
3099,"We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval ( Lewis et al. , 2020 b ) .",3,0.9312738,107.27385939861294,39
3100,Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years .,0,0.9383579,23.723338773868573,22
3100,"In speech , a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks .",3,0.53684145,41.660153333733845,16
3100,"However , the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models .",0,0.84792554,30.145421042726827,22
3100,SUPERB was a step towards introducing a common benchmark to evaluate pre-trained models across various speech tasks .,0,0.46644047,49.90741562524197,18
3100,"In this paper , we introduce SUPERB-SG , a new benchmark focusing on evaluating the semantic and generative capabilities of pre-trained models by increasing task diversity and difficulty over SUPERB .",1,0.87073356,45.63202661150909,33
3100,We use a lightweight methodology to test the robustness of representations learned by pre-trained models under shifts in data domain and quality across different types of tasks .,2,0.84930545,35.87459013446637,28
3100,"It entails freezing pre-trained model parameters , only using simple task-specific trainable heads .",2,0.3642265,143.6882643412749,15
3100,"The goal is to be inclusive of all researchers , and encourage efficient use of computational resources .",0,0.39923176,35.34104789743804,18
3100,We also show that the task diversity of SUPERB-SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation .,3,0.9571081,59.064043671887035,28
3101,Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus .,3,0.7105816,17.39949218397502,17
3101,"In this paper , we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons .",1,0.89991355,43.063273762301186,25
3101,"Specifically , we examine the fill-in-the-blank cloze task for BERT .",2,0.7116012,18.477829668325118,16
3101,"Given a relational fact , we propose a knowledge attribution method to identify the neurons that express the fact .",2,0.5176513,58.25146649658197,20
3101,We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts .,3,0.9812517,49.325576775397714,20
3101,"In our case studies , we attempt to leverage knowledge neurons to edit ( such as update , and erase ) specific factual knowledge without fine-tuning .",2,0.44007742,103.8250579710222,27
3101,Our results shed light on understanding the storage of knowledge within pretrained Transformers .,3,0.9906212,49.529175649338804,14
3102,"Meta-learning , or learning to learn , is a technique that can help to overcome resource scarcity in cross-lingual NLP problems , by enabling fast adaptation to new tasks .",0,0.9094157,36.43315639308785,30
3102,We apply model-agnostic meta-learning ( MAML ) to the task of cross-lingual dependency parsing .,2,0.77327114,12.495760576556647,16
3102,We train our model on a diverse set of languages to learn a parameter initialization that can adapt quickly to new languages .,2,0.7122897,29.158910227942645,23
3102,"We find that meta-learning with pre-training can significantly improve upon the performance of language transfer and standard supervised learning baselines for a variety of unseen , typologically diverse , and low-resource languages , in a few-shot learning setup .",3,0.9662162,29.415183098951566,39
3103,This paper contains explicit statements of offensive stereotypes which may be upsetting .,1,0.5780564,188.29312348743656,13
3103,Much work on biases in natural language processing has addressed biases linked to the social and cultural experience of English speaking individuals in the United States .,0,0.94709295,25.386278772415928,27
3103,We seek to widen the scope of bias studies by creating material to measure social bias in language models ( LMs ) against specific demographic groups in France .,1,0.79484034,72.25013540076414,29
3103,We build on the US-centered CrowS-pairs dataset to create a multilingual stereotypes dataset that allows for comparability across languages while also characterizing biases that are specific to each country and language .,2,0.86764663,51.51685701457499,36
3103,"We introduce 1,679 sentence pairs in French that cover stereotypes in ten types of bias like gender and age .",2,0.8949057,106.66031133812008,20
3103,"1,467 sentence pairs are translated from CrowS-pairs and 212 are newly crowdsourced .",2,0.43999442,411.8341924786654,15
3103,The sentence pairs contrast stereotypes concerning underadvantaged groups with the same sentence concerning advantaged groups .,2,0.73400456,189.4911510484488,16
3103,"We find that four widely used language models ( three French , one multilingual ) favor sentences that express stereotypes in most bias categories .",3,0.9703938,159.07533565672745,25
3103,"We report on the translation process from English into French , which led to a characterization of stereotypes in CrowS-pairs including the identification of US-centric cultural traits .",1,0.5827926,110.13137645196815,32
3103,We offer guidelines to further extend the dataset to other languages and cultural environments .,3,0.8412929,39.702205343877445,15
3104,"We study the problem of building text classifiers with little or no training data , commonly known as zero and few-shot text classification .",1,0.5647275,21.52056663625758,24
3104,"In recent years , an approach based on neural textual entailment models has been found to give strong results on a diverse range of tasks .",0,0.9321379,21.424357276505393,26
3104,"In this work , we show that with proper pre-training , Siamese Networks that embed texts and labels offer a competitive alternative .",1,0.53781486,74.34896133814264,23
3104,These models allow for a large reduction in inference cost : constant in the number of labels rather than linear .,3,0.4673485,110.154222729992,21
3104,"Furthermore , we introduce label tuning , a simple and computationally efficient approach that allows to adapt the models in a few-shot setup by only changing the label embeddings .",2,0.66028386,33.79216940300635,30
3104,"While giving lower performance than model fine-tuning , this approach has the architectural advantage that a single encoder can be shared by many different tasks .",3,0.6849272,38.46307669702061,26
3105,"In classic instruction following , language like “ I ’d like the JetBlue flight ” maps to actions ( e.g. , selecting that flight ) .",0,0.75812835,162.02419347236264,26
3105,"However , language also conveys information about a user ’s underlying reward function ( e.g. , a general preference for JetBlue ) , which can allow a model to carry out desirable actions in new contexts .",0,0.48732325,64.07679848172104,37
3105,"We present a model that infers rewards from language pragmatically : reasoning about how speakers choose utterances not only to elicit desired actions , but also to reveal information about their preferences .",1,0.3595851,59.19867965915481,33
3105,"On a new interactive flight –booking task with natural language , our model more accurately infers rewards and predicts optimal actions in unseen environments , in comparison to past work that first maps language to actions ( instruction following ) and then maps actions to rewards ( inverse reinforcement learning ) .",3,0.7885016,124.18028592093144,52
3106,"Generating factual , long-form text such as Wikipedia articles raises three key challenges : how to gather relevant evidence , how to structure information into well-formed text , and how to ensure that the generated text is factually correct .",0,0.9361145,29.589010080443053,43
3106,"We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoder-decoder to generate long-form biographies section by section , including citation information .",2,0.7487436,39.79939530775907,43
3106,"To assess the impact of available web evidence on the output text , we compare the performance of our approach when generating biographies about women ( for which less information is available on the web ) vs .",2,0.77679056,58.65925690634358,38
3106,biographies generally .,4,0.6453909,4005.7437308406793,3
3106,"To this end , we curate a dataset of 1,500 biographies about women .",2,0.8296296,29.927826224206136,14
3106,We analyze our generated text to understand how differences in available web evidence data affect generation .,2,0.67731345,199.38498248953488,17
3106,"We evaluate the factuality , fluency , and quality of the generated texts using automatic metrics and human evaluation .",2,0.8218544,29.021626020937546,20
3106,"We hope that these techniques can be used as a starting point for human writers , to aid in reducing the complexity inherent in the creation of long-form , factual text .",3,0.8186503,31.645789372591853,32
3107,Handing in a paper or exercise and merely receiving “ bad ” or “ incorrect ” as feedback is not very helpful when the goal is to improve .,0,0.5087161,43.23098376964465,29
3107,"Unfortunately , this is currently the kind of feedback given by Automatic Short Answer Grading ( ASAG ) systems .",0,0.92042947,81.63163831851244,20
3107,One of the reasons for this is a lack of content-focused elaborated feedback datasets .,0,0.8741317,43.83174447237212,15
3107,"To encourage research on explainable and understandable feedback systems , we present the Short Answer Feedback dataset ( SAF ) .",1,0.48728004,104.6620232015584,21
3107,"Similar to other ASAG datasets , SAF contains learner responses and reference answers to German and English questions .",2,0.38196877,191.1718715462093,19
3107,"However , instead of only assigning a label or score to the learners ’ answers , SAF also contains elaborated feedback explaining the given score .",0,0.5003011,275.59009650503515,26
3107,"Thus , SAF enables supervised training of models that grade answers and explain where and why mistakes were made .",3,0.62771267,166.59929966484407,20
3107,"This paper discusses the need for enhanced feedback models in real-world pedagogical scenarios , describes the dataset annotation process , gives a comprehensive analysis of SAF , and provides T5-based baselines for future comparison .",1,0.7678999,67.50470531180255,37
3108,"To effectively characterize the nature of paraphrase pairs without expert human annotation , we proposes two new metrics : word position deviation ( WPD ) and lexical deviation ( LD ) .",2,0.59485316,71.0539647546752,32
3108,"WPD measures the degree of structural alteration , while LD measures the difference in vocabulary used .",2,0.4150272,194.48383046104595,17
3108,"We apply these metrics to better understand the commonly-used MRPC dataset and study how it differs from PAWS , another paraphrase identification dataset .",2,0.53311497,62.40619866374248,26
3108,"We also perform a detailed study on MRPC and propose improvements to the dataset , showing that it improves generalizability of models trained on the dataset .",3,0.76586604,32.36570642699923,27
3108,"Lastly , we apply our metrics to filter the output of a paraphrase generation model and show how it can be used to generate specific forms of paraphrases for data augmentation or robustness testing of NLP models .",3,0.72278947,20.31277541872015,38
3109,"We introduce SummScreen , a summarization dataset comprised of pairs of TV series transcripts and human written recaps .",2,0.59061354,103.35298985758232,19
3109,The dataset provides a challenging testbed for abstractive summarization for several reasons .,0,0.46516615,38.040517734092006,13
3109,Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript .,0,0.73983335,43.15819474800132,20
3109,These details must be found and integrated to form the succinct plot descriptions in the recaps .,3,0.6233467,142.322016591827,17
3109,"Also , TV scripts contain content that does not directly pertain to the central plot but rather serves to develop characters or provide comic relief .",0,0.8667807,47.394478087028894,26
3109,This information is rarely contained in recaps .,0,0.8692388,144.9578214964068,8
3109,"Since characters are fundamental to TV series , we also propose two entity-centric evaluation metrics .",2,0.628273,97.88255755881076,17
3109,"Empirically , we characterize the dataset by evaluating several methods , including neural models and those based on nearest neighbors .",2,0.79249763,55.199026658499804,21
3109,"An oracle extractive approach outperforms all benchmarked models according to automatic metrics , showing that the neural models are unable to fully exploit the input transcripts .",3,0.8817349,39.04604208585806,27
3109,Human evaluation and qualitative analysis reveal that our non-oracle models are competitive with their oracle counterparts in terms of generating faithful plot events and can benefit from better content selectors .,3,0.9665704,55.10579772008455,31
3109,"Both oracle and non-oracle models generate unfaithful facts , suggesting future research directions .",3,0.8869141,56.65296864367337,14
3110,"We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process , thus focusing on the task-specific parts of an input .",2,0.4024791,24.471146881940147,37
3110,A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust trainable top-k operator .,3,0.82147074,70.28113374935482,20
3110,"Our experiments on a challenging long document summarization task show that even our simple baseline performs comparably to the current SOTA , and with trainable pooling we can retain its top quality , while being 1.8 × faster during training , 4.5 × faster during inference , and up to 13 × more computationally efficient in the decoder .",3,0.8910235,42.726981196899864,59
3111,In many natural language processing ( NLP ) tasks the same input ( e.g .,0,0.8850875,18.218450164591705,15
3111,source sentence ) can have multiple possible outputs ( e.g .,0,0.580253,91.08152545964086,11
3111,translations ) .,3,0.33934432,1156.0909258175766,3
3111,To analyze how this ambiguity ( also known as intrinsic uncertainty ) shapes the distribution learned by neural sequence models we measure sentence-level uncertainty by computing the degree of overlap between references in multi-reference test sets from two different NLP tasks : machine translation ( MT ) and grammatical error correction ( GEC ) .,2,0.78403735,43.294821311043684,57
3111,"At both the sentence-and the task-level , intrinsic uncertainty has major implications for various aspects of search such as the inductive biases in beam search and the complexity of exact search .",0,0.739141,51.25642679565761,36
3111,"In particular , we show that well-known pathologies such as a high number of beam search errors , the inadequacy of the mode , and the drop in system performance with large beam sizes apply to tasks with high level of ambiguity such as MT but not to less uncertain tasks such as GEC .",3,0.9074105,54.20127279538142,56
3111,"Furthermore , we propose a novel exact n-best search algorithm for neural sequence models , and show that intrinsic uncertainty affects model uncertainty as the model tends to overly spread out the probability mass for uncertain tasks and sentences .",3,0.6519733,84.57996435284912,40
3112,Most previous methods for text data augmentation are limited to simple tasks and weak baselines .,0,0.85553974,29.633119877867838,16
3112,"We explore data augmentation on hard tasks ( i.e. , few-shot natural language understanding ) and strong baselines ( i.e. , pretrained models with over one billion parameters ) .",2,0.80172044,37.171307748586365,30
3112,"Under this setting , we reproduced a large number of previous augmentation methods and found that these methods bring marginal gains at best and sometimes degrade the performance much .",3,0.87552845,46.46247424867327,30
3112,"To address this challenge , we propose a novel data augmentation method FlipDA that jointly uses a generative model and a classifier to generate label-flipped data .",1,0.44137606,22.223044038909364,29
3112,Central to the idea of FlipDA is the discovery that generating label-flipped data is more crucial to the performance than generating label-preserved data .,0,0.7450956,41.31121229955102,28
3112,Experiments show that FlipDA achieves a good tradeoff between effectiveness and robustness — it substantially improves many tasks while not negatively affecting the others .,3,0.9469524,47.89358639863763,25
3113,"Speech pre-training has primarily demonstrated efficacy on classification tasks , while its capability of generating novel speech , similar to how GPT-2 can generate coherent paragraphs , has barely been explored .",0,0.79431695,54.748575516947035,34
3113,"Generative Spoken Language Modeling ( GSLM ) ( CITATION ) is the only prior work addressing the generative aspect of speech pre-training , which builds a text-free language model using discovered units .",0,0.7927564,54.25201770253885,34
3113,"Unfortunately , because the units used in GSLM discard most prosodic information , GSLM fails to leverage prosody for better comprehension and does not generate expressive speech .",0,0.5812123,106.00537692922845,28
3113,"In this work , we present a prosody-aware generative spoken language model ( pGSLM ) .",1,0.79234314,44.36279090319181,18
3113,"It is composed of a multi-stream transformer language model ( MS-TLM ) of speech , represented as discovered unit and prosodic feature streams , and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms .",2,0.43831828,115.59867040650394,40
3113,"Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling , and also generate natural , meaningful , and coherent speech given a spoken prompt .",3,0.9825883,83.04836121320947,32
3113,Audio samples can be found at https://speechbot.github.io/pgslm .,3,0.5886509,24.289407499950787,8
3113,Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm .,3,0.4722767,34.889353338507526,8
3114,"As a broad and major category in machine reading comprehension ( MRC ) , the generalized goal of discriminative MRC is answer prediction from the given materials .",0,0.9241091,67.95549440288721,28
3114,"However , the focuses of various discriminative MRC tasks may be diverse enough : multi-choice MRC requires model to highlight and integrate all potential critical evidence globally ;",0,0.6993548,183.44817128975646,28
3114,while extractive MRC focuses on higher local boundary preciseness for answer extraction .,3,0.58986497,263.81415675343635,13
3114,"Among previous works , there lacks a unified design with pertinence for the overall discriminative MRC tasks .",0,0.88757735,96.81865928725803,18
3114,"To fill in above gap , we propose a lightweight POS-Enhanced Iterative Co-Attention Network ( POI-Net ) as the first attempt of unified modeling with pertinence , to handle diverse discriminative MRC tasks synchronously .",1,0.44675273,74.74349002638904,39
3114,"Nearly without introducing more parameters , our lite unified design brings model significant improvement with both encoder and decoder components .",3,0.9424403,160.3839960527284,21
3114,"The evaluation results on four discriminative MRC benchmarks consistently indicate the general effectiveness and applicability of our model , and the code is available at https://github.com/Yilin1111/poi-net .",3,0.97702515,22.214827760550005,27
3115,This work presents methods for learning cross-lingual sentence representations using paired or unpaired bilingual texts .,1,0.7960575,29.11947027370953,16
3115,"We hypothesize that the cross-lingual alignment strategy is transferable , and therefore a model trained to align only two languages can encode multilingually more aligned representations .",3,0.6741468,39.71179529995373,27
3115,We thus introduce dual-pivot transfer : training on one language pair and evaluating on other pairs .,2,0.70350343,59.51270069648772,17
3115,"To study this theory , we design unsupervised models trained on unpaired sentences and single-pair supervised models trained on bitexts , both based on the unsupervised language model XLM-R with its parameters frozen .",2,0.8377948,51.89772821092793,37
3115,"The experiments evaluate the models as universal sentence encoders on the task of unsupervised bitext mining on two datasets , where the unsupervised model reaches the state of the art of unsupervised retrieval , and the alternative single-pair supervised model approaches the performance of multilingually supervised models .",2,0.6882735,34.634675289456524,50
3115,The results suggest that bilingual training techniques as proposed can be applied to get sentence representations with multilingual alignment .,3,0.99010897,51.08311625846113,20
3116,Natural language spatial video grounding aims to detect the relevant objects in video frames with descriptive sentences as the query .,0,0.7419765,196.91291472955285,21
3116,"In spite of the great advances , most existing methods rely on dense video frame annotations , which require a tremendous amount of human effort .",0,0.9241164,58.474130463826555,26
3116,"To achieve effective grounding under a limited annotation budget , we investigate one-shot video grounding and learn to ground natural language in all video frames with solely one frame labeled , in an end-to-end manner .",2,0.6936125,50.97990481829578,39
3116,One major challenge of end-to-end one-shot video grounding is the existence of videos frames that are either irrelevant to the language query or the labeled frame .,0,0.90372914,49.86018763187989,30
3116,"Another challenge relates to the limited supervision , which might result in ineffective representation learning .",0,0.63229215,87.8208026398299,16
3116,"To address these challenges , we designed an end-to-end model via Information Tree for One-Shot video grounding ( IT-OS ) .",2,0.5160241,82.10716339641246,27
3116,"Its key module , the information tree , can eliminate the interference of irrelevant frames based on branch search and branch cropping techniques .",3,0.38421863,328.0236059334447,24
3116,"In addition , several self-supervised tasks are proposed based on the information tree to improve the representation learning under insufficient labeling .",2,0.5561627,67.39030696498749,22
3116,Experiments on the benchmark dataset demonstrate the effectiveness of our model .,3,0.82580525,8.325539042203763,12
3117,A release note is a technical document that describes the latest changes to a software product and is crucial in open source software development .,0,0.89731246,22.087892164133283,25
3117,"However , it still remains challenging to generate release notes automatically .",0,0.9478171,82.97105735039412,12
3117,"In this paper , we present a new dataset called RNSum , which contains approximately 82,000 English release notes and the associated commit messages derived from the online repositories in GitHub .",1,0.6762674,52.44368480411707,32
3117,"Then , we propose classwise extractive-then-abstractive / abstractive summarization approaches to this task , which can employ a modern transformer-based seq2seq network like BART and can be applied to various repositories without specific constraints .",2,0.5578512,56.179909339045466,41
3117,The experimental results on the RNSum dataset show that the proposed methods can generate less noisy release notes at higher coverage than the baselines .,3,0.97144115,36.41278393420642,25
3117,We also observe that there is a significant gap in the coverage of essential information when compared to human references .,3,0.98298,16.264262758164524,21
3117,Our dataset and the code are publicly available .,3,0.60859674,13.014872338078327,9
3118,"To perform well on a machine reading comprehension ( MRC ) task , machine readers usually require commonsense knowledge that is not explicitly mentioned in the given documents .",0,0.9427346,35.09469281616702,29
3118,This paper aims to extract a new kind of structured knowledge from scripts and use it to improve MRC .,1,0.9387519,34.871930914254186,20
3118,"We focus on scripts as they contain rich verbal and nonverbal messages , and two relevant messages originally conveyed by different modalities during a short time period may serve as arguments of a piece of commonsense knowledge as they function together in daily communications .",2,0.4665182,97.92816872007005,45
3118,"To save human efforts to name relations , we propose to represent relations implicitly by situating such an argument pair in a context and call it contextualized knowledge .",2,0.36872187,72.35691067034213,29
3118,"To use the extracted knowledge to improve MRC , we compare several fine-tuning strategies to use the weakly-labeled MRC data constructed based on contextualized knowledge and further design a teacher-student paradigm with multiple teachers to facilitate the transfer of knowledge in weakly-labeled MRC data .",2,0.7959593,21.519401952812206,50
3118,"Experimental results show that our paradigm outperforms other methods that use weakly-labeled data and improves a state-of-the-art baseline by 4.3 % in accuracy on a Chinese multiple-choice MRC dataset C3 , wherein most of the questions require unstated prior knowledge .",3,0.90877503,24.933987259844955,51
3118,"We also seek to transfer the knowledge to other tasks by simply adapting the resulting student reader , yielding a 2.9 % improvement in F1 on a relation extraction dataset DialogRE , demonstrating the potential usefulness of the knowledge for non-MRC tasks that require document comprehension .",3,0.8364458,64.0089735283926,48
3119,We introduce an argumentation annotation approach to model the structure of argumentative discourse in student-written business model pitches .,2,0.45735547,71.81440228459452,19
3119,"Additionally , the annotation scheme captures a series of persuasiveness scores such as the specificity , strength , evidence , and relevance of the pitch and the individual components .",2,0.60300887,71.97312611244968,30
3119,"Based on this scheme , we annotated a corpus of 200 business model pitches in German .",2,0.7937289,133.1775368548926,17
3119,"Moreover , we trained predictive models to detect argumentative discourse structures and embedded them in an adaptive writing support system for students that provides them with individual argumentation feedback independent of an instructor , time , and location .",2,0.6024334,89.5664503741237,39
3119,We evaluated our tool in a real-world writing exercise and found promising results for the measured self-efficacy and perceived ease-of-use .,3,0.90637594,19.778911208367457,24
3119,"Finally , we present our freely available corpus of persuasive business model pitches with 3,207 annotated sentences in German language and our annotation guidelines .",3,0.54758805,130.78935934944607,25
3120,Recent studies have shown the advantages of evaluating NLG systems using pairwise comparisons as opposed to direct assessment .,0,0.8754529,34.62038441942725,19
3120,"Given k systems , a naive approach for identifying the top-ranked system would be to uniformly obtain pairwise comparisons from all k \choose 2 pairs of systems .",0,0.52991325,64.00305256199536,28
3120,"However , this can be very expensive as the number of human annotations required would grow quadratically with k .",0,0.54622257,47.36659857108961,20
3120,"In this work , we introduce Active Evaluation , a framework to efficiently identify the top-ranked system by actively choosing system pairs for comparison using dueling bandit algorithms .",1,0.6871362,102.64937409923697,29
3120,We perform extensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation datasets spanning 5 tasks and show that the number of human annotations can be reduced by 80 % .,3,0.7630428,64.809521045384,32
3120,"To further reduce the number of human annotations , we propose model-based dueling bandit algorithms which combine automatic evaluation metrics with human evaluations .",2,0.57915723,43.813167772681524,26
3120,"Specifically , we eliminate sub-optimal systems even before the human annotation process and perform human evaluations only on test examples where the automatic metric is highly uncertain .",2,0.7450874,62.776016472665106,28
3120,This reduces the number of human annotations required further by 89 % .,3,0.7952628,61.67561521904493,13
3120,"In effect , we show that identifying the top-ranked system requires only a few hundred human annotations , which grow linearly with k .",3,0.9324907,59.15006256526217,24
3120,"Lastly , we provide practical recommendations and best practices to identify the top-ranked system efficiently .",3,0.6438662,54.72085779889488,16
3120,Our code has been made publicly available at https://github.com/akashkm99/duelnlg .,3,0.5896733,21.953310993133044,10
3121,An audience ’s prior beliefs and morals are strong indicators of how likely they will be affected by a given argument .,0,0.6300891,37.090376556052504,22
3121,Utilizing such knowledge can help focus on shared values to bring disagreeing parties towards agreement .,3,0.5793098,102.01921429239347,16
3121,"In argumentation technology , however , this is barely exploited so far .",0,0.88628453,280.39236829754407,13
3121,This paper studies the feasibility of automatically generating morally framed arguments as well as their effect on different audiences .,1,0.92356086,57.97734871485198,20
3121,"Following the moral foundation theory , we propose a system that effectively generates arguments focusing on different morals .",2,0.45418942,84.32887481892676,19
3121,"In an in-depth user study , we ask liberals and conservatives to evaluate the impact of these arguments .",2,0.7119551,30.269360140951893,20
3121,"Our results suggest that , particularly when prior beliefs are challenged , an audience becomes more affected by morally framed arguments .",3,0.9899503,138.49751914790758,22
3122,"Transformer-based language models such as BERT ( CITATION ) have achieved the state-of-the-art performance on various NLP tasks , but are computationally prohibitive .",0,0.8779435,8.566022835315978,32
3122,"A recent line of works use various heuristics to successively shorten sequence length while transforming tokens through encoders , in tasks such as classification and ranking that require a single token embedding for prediction .",0,0.78705627,80.90250140864289,35
3122,"We present a novel solution to this problem , called Pyramid-BERT where we replace previously used heuristics with a core-set based token selection method justified by theoretical results .",2,0.39628184,82.41362650561314,29
3122,"The core-set based token selection technique allows us to avoid expensive pre-training , gives a space-efficient fine tuning , and thus makes it suitable to handle longer sequence lengths .",3,0.70792013,71.93394395732726,31
3122,We provide extensive experiments establishing advantages of pyramid BERT over several baselines and existing works on the GLUE benchmarks and Long Range Arena ( CITATION ) datasets .,3,0.60780454,107.97405475441549,28
3123,A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations .,0,0.8403481,42.583394078379776,19
3123,"An encoding , however , might be spurious — i.e. , the model might not rely on it when making predictions .",0,0.5231241,81.01387356195384,22
3123,"In this paper , we try to find an encoding that the model actually uses , introducing a usage-based probing setup .",1,0.8737503,91.34880068625294,24
3123,We first choose a behavioral task which cannot be solved without using the linguistic property .,2,0.7587509,88.35626653615394,16
3123,"Then , we attempt to remove the property by intervening on the model ’s representations .",2,0.7568522,74.05211033957069,16
3123,"We contend that , if an encoding is used by the model , its removal should harm the performance on the chosen behavioral task .",3,0.76217973,132.7591959059758,25
3123,"As a case study , we focus on how BERT encodes grammatical number , and on how it uses this encoding to solve the number agreement task .",1,0.61844265,41.774594204393544,28
3123,"Experimentally , we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output .",3,0.890649,55.38363394262279,21
3123,We also find that BERT uses a separate encoding of grammatical number for nouns and verbs .,3,0.97931165,36.31722223116295,17
3123,"Finally , we identify in which layers information about grammatical number is transferred from a noun to its head verb .",3,0.6434728,71.00607292669336,21
3124,"We introduce BitFit , a sparse-finetuning method where only the bias-terms of the model ( or a subset of them ) are being modified .",2,0.69065446,66.21353657519293,26
3124,"We show that with small-to-medium training data , applying BitFit on pre-trained BERT models is competitive with ( and sometimes better than ) fine-tuning the entire model .",3,0.9506239,28.94471635820235,31
3124,"For larger data , the method is competitive with other sparse fine-tuning methods .",3,0.870229,41.14061999635046,14
3124,"Besides their practical utility , these findings are relevant for the question of understanding the commonly-used process of finetuning : they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training , rather than learning new task-specific linguistic knowledge .",3,0.98675007,33.9717124826727,48
3125,"Existing self-explaining models typically favor extracting the shortest possible rationales — snippets of an input text “ responsible for ” corresponding output — to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .",0,0.8888305,54.45068227691534,42
3125,"However , this assumption has yet to be validated .",0,0.8895572,32.992230260635836,10
3125,"To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .",2,0.5179763,59.11278737432635,24
3125,"Compared to existing baselines , LimitedInk achieves compatible end-task performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .",3,0.89025795,72.21930784278538,29
3125,"We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk-generated rationales with different lengths .",2,0.87080985,49.57109055626151,38
3125,"We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .",3,0.9719588,81.99205971408159,32
3126,Numerous analyses of reading time ( RT ) data have been undertaken in the effort to learn more about the internal processes that occur during reading comprehension .,0,0.94679797,31.81384496431772,28
3126,"However , data measured on words at the end of a sentence –or even clause –is often omitted due to the confounding factors introduced by so-called “ wrap-up effects , ” which manifests as a skewed distribution of RTs for these words .",0,0.84964657,61.37619384075923,45
3126,"Consequently , the understanding of the cognitive processes that might be involved in these effects is limited .",0,0.91679054,27.29850872166971,18
3126,"In this work , we attempt to learn more about these processes by looking for the existence –or absence –of a link between wrap-up effects and information theoretic quantities , such as word and context information content .",1,0.8552539,53.1455756547744,40
3126,"We find that the information distribution of prior context is often predictive of sentence-and clause-final RTs ( while not of sentence-medial RTs ) , which lends support to several prior hypotheses about the processes involved in wrap-up effects .",3,0.976275,77.23787709754129,45
3127,Argument pair extraction ( APE ) aims to automatically mine argument pairs from two interrelated argumentative documents .,0,0.8958732,59.43295560995599,18
3127,"Existing studies typically identify argument pairs indirectly by predicting sentence-level relations between two documents , neglecting the modeling of the holistic argument-level interactions .",0,0.8834542,82.88516951476035,27
3127,"Towards this issue , we propose to address APE via a machine reading comprehension ( MRC ) framework with two phases .",1,0.72859746,38.669253230091726,22
3127,The first phase employs an argument mining ( AM ) query to identify all arguments in two documents .,2,0.75347394,131.0096939842436,19
3127,"The second phase considers each identified argument as an APE query to extract its paired arguments from another document , allowing to better capture the argument-level interactions .",2,0.7851928,94.95326098323734,29
3127,"Also , this framework enables these two phases to be jointly trained in a single MRC model , thereby maximizing the mutual benefits of them .",3,0.50592774,45.15223402395741,26
3127,"Experimental results demonstrate that our approach achieves the best performance , outperforming the state-of-the-art method by 7.11 % in F1 score .",3,0.9561892,8.51509488999941,28
3128,"It has often been observed that mode-seeking decoding methods , i.e. , those that produce high-probability text under the model , lead to unnatural language .",0,0.9309462,69.9983212816305,28
3128,"On the other hand , the lower-probability text generated by stochastic methods is perceived as more human-like .",3,0.80134153,18.08434184720025,20
3128,"In this note , we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens .",1,0.8406284,34.45595740264882,22
3128,"Specifically , we posit that human-like language should contain an amount of information ( quantified as negative log-probability ) that is close to the entropy of the distribution over natural strings .",2,0.47995004,32.23585824300587,32
3128,"Further , we posit that language with substantially more ( or less ) information is undesirable .",3,0.7860866,74.5040851514545,17
3128,We provide preliminary empirical evidence in favor of this hypothesis ;,3,0.8498801,73.17702377203693,11
3128,quality ratings of both human and machine-generated text — covering multiple tasks and common decoding strategies — suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance .,3,0.7452612,50.15271645392005,38
3129,Discovering Out-of-Domain( OOD ) intents is essential for developing new skills in a task-oriented dialogue system .,0,0.92708313,27.9544505597067,19
3129,The key challenge is how to transfer prior IND knowledge to OOD clustering .,0,0.71674746,82.64770208151286,14
3129,"Different from existing work based on shared intent representation , we propose a novel disentangled knowledge transfer method via a unified multi-head contrastive learning framework .",2,0.68677616,26.30425745105329,26
3129,We aim to bridge the gap between IND pre-training and OOD clustering .,1,0.8475494,26.237482702465616,13
3129,Experiments and analysis on two benchmark datasets show the effectiveness of our method .,3,0.8231719,7.619535924487104,14
3130,Natural language applied to natural 2D images describes a fundamentally 3D world .,0,0.9297735,148.8143940592588,13
3130,"We present the Voxel-informed Language Grounder ( VLG ) , a language grounding model that leverages 3D geometric information in the form of voxel maps derived from the visual input using a volumetric reconstruction model .",2,0.3297534,46.78757944141215,37
3130,"We show that VLG significantly improves grounding accuracy on SNARE , an object reference game task .",3,0.9685772,176.8977564484552,17
3130,"At the time of writing , VLG holds the top place on the SNARE leaderboard , achieving SOTA results with a 2.0 % absolute improvement .",3,0.81555825,37.27081400923482,26
3131,"Prompt tuning , which only tunes continuous prompts with a frozen language model , substantially reduces per-task storage and memory usage at training .",3,0.763087,177.91355372308382,24
3131,"However , in the context of NLU , prior work reveals that prompt tuning does not perform well for normal-sized pretrained models .",0,0.8354788,47.61815118472258,23
3131,"We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks , indicating a lack of universality .",3,0.976409,83.51549126862375,22
3131,We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks .,3,0.8404524,93.13463860064721,26
3131,It matches the performance of finetuning while having only 0.1 %-3 % tuned parameters .,3,0.9265807,68.05150953955163,17
3131,Our method P-Tuning v2 is an implementation of Deep Prompt Tuning ( CITATION ) optimized and adapted for NLU .,2,0.72907335,106.31948740430458,20
3131,"Given the universality and simplicity of P-Tuning v2 , we believe it can serve as an alternative to finetuning and a strong baseline for future research .",3,0.960983,19.016803639978857,27
3132,"When tasked with supporting multiple languages for a given problem , two approaches have arisen : training a model for each language with the annotation budget divided equally among them , and training on a high-resource language followed by zero-shot transfer to the remaining languages .",0,0.82797134,30.0237860796388,47
3132,"In this work , we show that the strategy of joint learning across multiple languages using a single model performs substantially better than the aforementioned alternatives .",1,0.52729505,33.975454651955665,27
3132,"We also demonstrate that active learning provides additional , complementary benefits .",3,0.96101105,120.72206097703398,12
3132,We show that this simple approach enables the model to be data efficient by allowing it to arbitrate its annotation budget to query languages it is less certain on .,3,0.8464943,69.68404262090239,30
3132,"We illustrate the effectiveness of our proposed method on a diverse set of tasks : a classification task with 4 languages , a sequence tagging task with 4 languages and a dependency parsing task with 5 languages .",3,0.46669155,12.535662298412165,38
3132,"Our proposed method , whilst simple , substantially outperforms the other viable alternatives for building a model in a multilingual setting under constrained budgets .",3,0.92096686,88.81042821096638,25
3133,"In this work , we focus on the problem of distinguishing a human written news article from a news article that is created by manipulating entities in a human written news article ( e.g. , replacing entities with factually incorrect entities ) .",1,0.784609,25.197238364342606,43
3133,Such manipulated articles can mislead the reader by posing as a human written news article .,0,0.8592922,71.45379914078576,16
3133,We propose a neural network based detector that detects manipulated news articles by reasoning about the facts mentioned in the article .,1,0.41959938,30.092268093004883,22
3133,Our proposed detector exploits factual knowledge via graph convolutional neural network along with the textual information in the news article .,2,0.6298938,44.56070203121713,21
3133,"We also create challenging datasets for this task by considering various strategies to generate the new replacement entity ( e.g. , entity generation from GPT-2 ) .",2,0.72968125,62.77442999403543,29
3133,"In all the settings , our proposed model either matches or outperforms the state-of-the-art detector in terms of accuracy .",3,0.9348503,15.08161809974302,26
3133,Our code and data are available at https://github.com/UBC-NLP/manipulated_entity_detection .,3,0.6374861,8.775177308549043,9
3134,"The success of a natural language processing ( NLP ) system on a task does not amount to fully understanding the complexity of the task , typified by many deep learning models .",0,0.9526535,26.016095161333833,33
3134,Recent studies suggest that pre-trained BERT can capture lexico-semantic clues from words in the context .,0,0.8944504,18.233759155771814,16
3134,"However , to what extent BERT captures the transitive nature of some lexical relations is unclear .",0,0.8660393,30.696643855267787,17
3134,"From a probing perspective , we examine WordNet word senses and the IS-A relation , which is a transitive relation .",2,0.80353314,100.42165397283463,23
3134,"That is , for senses A , B , and C , A is-a B and B is-a C entail A is-a C .",3,0.51487476,36.58287694615284,30
3134,"We aim to quantify how much BERT agrees with the transitive property of IS-A relations , via a minimalist probing setting .",1,0.89289343,158.30638168940095,24
3134,Our investigation reveals that BERT ’s predictions do not fully obey the transitivity property of the IS-A relation .,3,0.98403984,54.9084999895908,21
3135,Pretrained language models such as BERT have achieved remarkable success in several NLP tasks .,0,0.8780196,5.097773364609635,15
3135,"With the wide adoption of BERT in real-world applications , researchers begin to investigate the implicit biases encoded in the BERT .",0,0.9317754,22.06520139474992,22
3135,"In this paper , we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT .",1,0.8560768,68.62256955274795,20
3135,We find some interesting patterns .,3,0.9575773,89.53574814570288,6
3135,"For example , the language models are overall more positive towards the stock market , but there are significant differences in preferences between a pair of industry sectors , or even within a sector .",3,0.96972895,65.55349804611276,35
3135,"Given the prevalence of NLP models in financial decision making systems , this work raises the awareness of their potential implicit preferences in the stock markets .",3,0.44800416,42.3877561494672,27
3135,Awareness of such problems can help practitioners improve robustness and accountability of their financial NLP pipelines .,3,0.6675365,40.326578009727974,17
3136,"We present Pixie , a manually annotated dataset for preference classification comprising 8,890 sentences drawn from app reviews .",2,0.6210396,96.64140298650558,19
3136,"Unlike previous studies on preference classification , Pixie contains implicit ( omitting an entity being compared ) and indirect ( lacking comparative linguistic cues ) comparisons .",0,0.63156515,408.88070670085085,27
3136,"We find that transformer-based pretrained models , finetuned on Pixie , achieve a weighted average F1 score of 83.34 % and outperform the existing state-of-the-art preference classification model ( 73.99 % ) .",3,0.9517343,29.792859513080668,41
3137,A key challenge facing natural language interfaces is enabling users to understand the capabilities of the underlying system .,0,0.9386295,22.363735813466956,19
3137,We propose a novel approach for generating explanations of a natural language interface based on semantic parsing .,1,0.52352655,20.925238629635583,18
3137,"We focus on counterfactual explanations , which are post-hoc explanations that describe to the user how they could have minimally modified their utterance to achieve their desired goal .",2,0.56395304,20.20896191928561,29
3137,"In particular , the user provides an utterance along with a demonstration of their desired goal ;",2,0.4145785,119.29615016477622,17
3137,"then , our algorithm synthesizes a paraphrase of their utterance that is guaranteed to achieve their goal .",2,0.66113573,37.77154472501694,18
3137,"In two user studies , we demonstrate that our approach substantially improves user performance , and that it generates explanations that more closely match the user ’s intent compared to two ablations .",3,0.80998075,28.738624946363256,33
3138,"Theory ( IRT ) has been extensively used to numerically characterize question difficulty and discrimination for human subjects in domains including cognitive psychology and education ( Primi et al. , 2014 ; Downing , 2003 ) .",0,0.8658744,119.17561524417582,37
3138,"More recently , IRT has been used to similarly characterize item difficulty and discrimination for natural language models across various datasets ( Lalor et al. , 2019 ; Vania et al. , 2021 ; Rodriguez et al. , 2021 ) .",0,0.81708443,59.11636725376122,41
3138,"In this work , we explore predictive models for directly estimating and explaining these traits for natural language questions in a question-answering context .",1,0.8537638,35.05772079508072,26
3138,We use HotpotQA for illustration .,2,0.6645385,78.12798924014103,6
3138,"Our experiments show that it is possible to predict both difficulty and discrimination parameters for new questions , and these traits are correlated with features of questions , answers , and associated contexts .",3,0.976157,69.58924139417867,34
3138,Our findings can have significant implications for the creation of new datasets and tests on the one hand and strategies such as active learning and curriculum learning on the other .,3,0.9894254,30.06904584625543,31
3139,"Several pre-training objectives , such as masked language modeling ( MLM ) , have been proposed to pre-train language models ( e.g .",0,0.8468875,20.284807322756272,23
3139,BERT ) with the aim of learning better language representations .,0,0.5283781,40.7181079742269,11
3139,"However , to the best of our knowledge , no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties .",0,0.8382351,26.315843321385547,28
3139,We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted .,1,0.5604015,33.94149054714117,45
3139,"To this end , we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones .",2,0.8073113,16.34243838836868,18
3139,We then probe for linguistic characteristics encoded in the representation of the resulting models .,2,0.7483899,65.93416913760552,15
3139,We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives .,3,0.98066795,29.279917310831316,25
3139,These surprising results question the dominant narrative of linguistically informed pre-training .,3,0.91929257,61.22232843605142,12
3140,Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks .,0,0.8889042,15.118022067296911,24
3140,"In this paper , we investigate prompt tuning for semantic parsing — the task of mapping natural language utterances onto formal meaning representations .",1,0.9105453,37.1319355564921,24
3140,"On the low-resource splits of Overnight and TOPv2 , we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart , as well as strong GPT-3 and BART baselines .",3,0.9670925,48.59422171744485,35
3140,"We also conduct ablation studies across different model scales and target representations , finding that , with increasing model scale , prompt tuned T5 models improve at generating target representations that are far from the pre-training distribution .",3,0.8487269,86.75665313367969,38
3141,"Pretrained language models are typically trained on massive web-based datasets , which are often “ contaminated ” with downstream test sets .",0,0.7994349,41.09675932242389,22
3141,It is not clear to what extent models exploit the contaminated data for downstream tasks .,0,0.9018071,19.070747977130214,16
3141,We present a principled method to study this question .,1,0.7733793,42.188067993780265,10
3141,"We pretrain BERT models on joint corpora of Wikipedia and labeled downstream datasets , and fine-tune them on the relevant task .",2,0.80747896,37.76062270810331,22
3141,Comparing performance between samples seen and unseen during pretraining enables us to define and quantify levels of memorization and exploitation .,3,0.39913347,67.44473213512275,21
3141,"Experiments with two models and three downstream tasks show that exploitation exists in some cases , but in others the models memorize the contaminated data , but do not exploit it .",3,0.8854989,57.81212475198468,32
3141,We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size .,3,0.940345,23.056902961524088,26
3141,Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation .,3,0.9890904,43.23882816131237,28
3142,Annotation errors that stem from various sources are usually unavoidable when performing large-scale annotation of linguistic data .,0,0.93174994,45.35715979702714,18
3142,"In this paper , we evaluate the feasibility of using the Transformer model to detect various types of annotator errors in morphological data sets that contain inflected word forms .",1,0.9356463,22.74405557276488,30
3142,"We evaluate our error detection model on four languages by introducing three different types of artificial errors in the data : ( 1 ) typographic errors , where single characters in the data are inserted , replaced , or deleted ;",2,0.83796954,77.09283003648054,41
3142,( 2 ) linguistic confusion errors where two inflected forms are systematically swapped ;,3,0.49026123,684.3311281579314,14
3142,"and ( 3 ) self-adversarial errors where the Transformer model itself is used to generate plausible-looking , but erroneous forms by retrieving high-scoring predictions from the search beam .",0,0.41259652,81.17160040763828,33
3142,"Results show that the Transformer model can with perfect , or near-perfect recall detect errors in all three scenarios , even when significant amounts of the annotated data ( 5 %-30 % ) are corrupted on all languages tested .",3,0.9835091,62.82380922019387,41
3142,"Precision varies across the languages and types of errors , but is high enough that the model can be very effectively used to flag suspicious entries in large data sets for further scrutiny by human annotators .",3,0.9421549,45.45637837371687,37
3143,Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language .,0,0.8750008,57.1606392052437,18
3143,"However , entropymust typically be estimated from observed data because researchers do not have access to the underlying probability distribution .",0,0.9246322,103.58077802468154,21
3143,"While entropy estimation is a well-studied problem in other fields , there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data .",0,0.9025661,24.803941903753863,31
3143,"In this work , we fill this void , studying the empirical effectiveness of different entropy estimators for linguistic distributions .",1,0.85988975,104.78691441852885,21
3143,"In a replication of two recent information-theoretic linguistic studies , we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators .",3,0.868281,25.8461522632074,30
3143,We end this paper with a concrete recommendation for the entropy estimators that should be used in future linguistic studies .,3,0.6325149,28.466791390381843,21
3144,"In recent years , a flurry of morphological datasets had emerged , most notably UniMorph , aa multi-lingual repository of inflection tables .",0,0.9426954,61.888905968804266,23
3144,"However , the flat structure of the current morphological annotation makes the treatment of some languages quirky , if not impossible , specifically in cases of polypersonal agreement .",0,0.8059095,177.0641756871988,29
3144,"In this paper we propose a general solution for such cases and expand the UniMorph annotation schema to naturally address this phenomenon , in which verbs agree with multiple arguments using true affixes .",1,0.80497175,98.62928231440091,34
3144,"We apply this extended schema to one such language , Georgian , and provide a human-verified , accurate and balanced morphological dataset for Georgian verbs .",2,0.52740866,198.5281190913886,26
3144,"The dataset has 4 times more tables and 6 times more verb forms compared to the existing UniMorph dataset , covering all possible variants of argument marking , demonstrating the adequacy of our proposed scheme .",3,0.94205916,64.99954711625927,36
3144,"Experiments on a reinflection task show that generalization is easy when the data is split at the form level , but extremely hard when splitting along lemma lines .",3,0.7952317,48.77317976455157,29
3144,"Expanding the other languages in UniMorph according to this schema is expected to improve both the coverage , consistency and interpretability of this benchmark .",3,0.8585674,76.32026704518006,25
3145,Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks .,0,0.5679337,6.644397594448495,24
3145,"However , such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency .",0,0.9372138,17.002719607648324,22
3145,"To alleviate this issue , we propose to jointly distill and quantize the model , where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model .",2,0.5319001,32.64236426016742,33
3145,"Empirical analyses show that , despite the challenging nature of generative tasks , we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets .",3,0.931858,40.498574080122964,40
3145,We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models .,3,0.6391306,44.42896707545994,26
3145,"To the best of our knowledge , this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks .",3,0.8881867,11.281544747262927,28
3146,"Comprehending a dialogue requires a model to capture diverse kinds of key information in the utterances , which are either scattered around or implicitly implied in different turns of conversations .",0,0.8302147,47.75345572300976,31
3146,"Therefore , dialogue comprehension requires diverse capabilities such as paraphrasing , summarizing , and commonsense reasoning .",0,0.8812126,66.6507770567213,17
3146,"Towards the objective of pre-training a zero-shot dialogue comprehension model , we develop a novel narrative-guided pre-training strategy that learns by narrating the key information from a dialogue input .",1,0.44237322,20.404552496921866,32
3146,"However , the dialogue-narrative parallel corpus for such a pre-training strategy is currently unavailable .",0,0.9047516,58.335328395844876,16
3146,"For this reason , we first construct a dialogue-narrative parallel corpus by automatically aligning movie subtitles and their synopses .",2,0.81420434,37.09480718113454,22
3146,We then pre-train a BART model on the data and evaluate its performance on four dialogue-based tasks that require comprehension .,2,0.8351459,22.658685438648444,23
3146,Experimental results show that our model not only achieves superior zero-shot performance but also exhibits stronger fine-grained dialogue comprehension capabilities .,3,0.9816067,15.29199870252558,23
3146,The data and code are available at https://github.com/zhaochaocs/Diana .,3,0.6039512,13.17457758405895,9
3147,GPT is an auto-regressive Transformer-based pre-trained language model which has attracted a lot of attention in the natural language processing ( NLP ) domain .,0,0.88776636,11.617573864240883,27
3147,The success of GPT is mostly attributed to its pre-training on huge amount of data and its large number of parameters .,0,0.7549083,15.676861281845355,22
3147,"Despite the superior performance of GPT , this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory .",3,0.6664835,38.3352171069333,29
3147,This problem can be mitigated using model compression techniques ;,0,0.6098499,123.40994545601536,10
3147,"however , compressing GPT models has not been investigated much in the literature .",0,0.8805085,56.27686072354212,14
3147,"In this work , we use Kronecker decomposition to compress the linear mappings of the GPT-2 model .",2,0.65115494,24.34348648995224,20
3147,Our Kronecker GPT-2 model ( KnGPT2 ) is initialized based on the Kronecker decomposed version of the GPT-2 model and then is undergone a very light pre-training on only a small portion of the training data with intermediate layer knowledge distillation ( ILKD ) .,2,0.61820185,29.97367024651103,49
3147,"Finally , our KnGPT2 is fine-tuned on downstream tasks using ILKD as well .",3,0.8058559,64.74882393586059,14
3147,"We evaluate our model on both language modeling and General Language Understanding Evaluation benchmark tasks and show that with more efficient pre-training and similar number of parameters , our KnGPT2 outperforms the existing DistilGPT2 model significantly .",3,0.8816324,57.45538314654465,37
3148,A key challenge in attribute value extraction ( AVE ) from e-commerce sites is how to handle a large number of attributes for diverse products .,0,0.9418935,44.55676066470691,26
3148,"Although this challenge is partially addressed by a question answering ( QA ) approach which finds a value in product data for a given query ( attribute ) , it does not work effectively for rare and ambiguous queries .",0,0.91726035,44.06925219019088,40
3148,We thus propose simple knowledge-driven query expansion based on possible answers ( values ) of a query ( attribute ) for QA-based AVE .,3,0.41850743,133.9247813142314,27
3148,We retrieve values of a query ( attribute ) from the training data to expand the query .,2,0.80365026,79.21704015403118,18
3148,"We train a model with two tricks , knowledge dropout and knowledge token mixing , which mimic the imperfection of the value knowledge in testing .",2,0.85319614,147.4627457906362,26
3148,"Experimental results on our cleaned version of AliExpress dataset show that our method improves the performance of AVE ( + 6.08 macro F1 ) , especially for rare and ambiguous attributes ( + 7.82 and + 6.86 macro F1 , respectively ) .",3,0.9393658,41.0011224613504,43
3149,"To understand a story with multiple events , it is important to capture the proper relations across these events .",0,0.88346994,47.458285343644185,20
3149,"However , existing event relation extraction ( ERE ) framework regards it as a multi-class classification task and do not guarantee any coherence between different relation types , such as anti-symmetry .",0,0.9049124,48.22379564615896,32
3149,"If a phone line “ died ” after “ storm ” , then it is obvious that the “ storm ” happened before the “ died ” .",0,0.6679451,27.141075427573217,28
3149,"Current framework of event relation extraction do not guarantee this coherence and thus enforces it via constraint loss function ( Wang et al. , 2020 ) .",0,0.7596427,137.28840439273614,27
3149,"In this work , we propose to modify the underlying ERE model to guarantee coherence by representing each event as a box representation ( BERE ) without applying explicit constraints .",1,0.5503382,85.55011988944302,31
3149,"From our experiments , BERE also shows stronger conjunctive constraint satisfaction while performing on par or better in F1 compared to previous models with constraint injection .",3,0.9820963,119.95378144633125,27
3150,End-to-end speech translation relies on data that pair source-language speech inputs with corresponding translations into a target language .,0,0.80968046,35.39566501609396,24
3150,"Such data are notoriously scarce , making synthetic data augmentation by back-translation or knowledge distillation a necessary ingredient of end-to-end training .",0,0.9153904,35.45587769099291,25
3150,"In this paper , we present a novel approach to data augmentation that leverages audio alignments , linguistic properties , and translation .",1,0.8962268,39.08669834029014,23
3150,"First , we augment a transcription by sampling from a suffix memory that stores text and audio data .",2,0.87778974,133.08263220899826,19
3150,"Second , we translate the augmented transcript .",2,0.86328983,250.00181117515433,8
3150,"Finally , we recombine concatenated audio segments and the generated translation .",2,0.7221497,82.45430984918187,12
3150,"Our method delivers consistent improvements of up to 0.9 and 1.1 BLEU points on top of augmentation with knowledge distillation on five language pairs on CoVoST 2 and on two language pairs on Europarl-ST , respectively .",3,0.85397303,29.468580356974265,39
3151,"Document-level text simplification often deletes some sentences besides performing lexical , grammatical or structural simplification to reduce text complexity .",0,0.83385044,66.1653104882511,22
3151,"In this work , we focus on sentence deletions for text simplification and use a news genre-specific functional discourse structure , which categorizes sentences based on their contents and their function roles in telling a news story , for predicting sentence deletion .",1,0.4342469,78.92050696275514,43
3151,"We incorporate sentence categories into a neural net model in two ways for predicting sentence deletions , either as additional features or by jointly predicting sentence deletions and sentence categories .",2,0.8671783,52.62645433201409,31
3151,"Experimental results using human-annotated data show that incorporating the functional structure improves the recall of sentence deletion prediction by 6.5 % and 10.7 % respectively using the two methods , and improves the overall F1-score by 3.6 % and 4.3 % respectively .",3,0.92957866,19.15527828673318,45
3152,We exploit the pre-trained seq2seq model mBART for multilingual text style transfer .,2,0.7910685,33.417794121138485,13
3152,Using machine translated data as well as gold aligned English sentences yields state-of-the-art results in the three target languages we consider .,3,0.7879969,33.59758288241166,28
3152,"Besides , in view of the general scarcity of parallel data , we propose a modular approach for multilingual formality transfer , which consists of two training strategies that target adaptation to both language and task .",2,0.5018912,39.87596851100579,37
3152,Our approach achieves competitive performance without monolingual task-specific parallel data and can be applied to other style transfer tasks as well as to other languages .,3,0.9004076,19.544211158975752,26
3153,"Transfer learning ( TL ) in natural language processing ( NLP ) has seen a surge of interest in recent years , as pre-trained models have shown an impressive ability to transfer to novel tasks .",0,0.9650163,19.458637214829828,36
3153,"Three main strategies have emerged for making use of multiple supervised datasets during fine-tuning : training on an intermediate task before training on the target task ( STILTs ) , using multi-task learning ( MTL ) to train jointly on a supplementary task and the target task ( pairwise MTL ) , or simply using MTL to train jointly on all available datasets ( MTL-ALL ) .",0,0.57431054,28.93053146517335,69
3153,"In this work , we compare all three TL methods in a comprehensive analysis on the GLUE dataset suite .",1,0.6130379,78.20421149898982,20
3153,We find that there is a simple heuristic for when to use one of these techniques over the other : pairwise MTL is better than STILTs when the target task has fewer instances than the supporting task and vice versa .,3,0.9769726,46.73842276300397,41
3153,We show that this holds true in more than 92 % of applicable cases on the GLUE dataset and validate this hypothesis with experiments varying dataset size .,3,0.9299884,55.73976349222014,28
3153,The simplicity and effectiveness of this heuristic is surprising and warrants additional exploration by the TL community .,3,0.8969411,50.642859841184375,18
3153,"Furthermore , we find that MTL-ALL is worse than the pairwise methods in almost every case .",3,0.97991425,54.68929444999396,19
3153,We hope this study will aid others as they choose between TL methods for NLP tasks .,3,0.9096545,59.78050745559595,17
3154,"Text-to-SQL aims to parse natural language questions into SQL queries , which is valuable in providing an easy interface to access large databases .",0,0.8417749,25.83149653925355,24
3154,Previous work has observed that leveraging lexico-logical alignments is very helpful to improve parsing performance .,0,0.9187835,31.0458633563076,16
3154,"However , current attention-based approaches can only model such alignments at the token level and have unsatisfactory generalization capability .",0,0.8782887,33.150628573083026,22
3154,"In this paper , we propose a new approach to leveraging explicit lexico-logical alignments .",1,0.91274226,24.350040019584014,15
3154,It first identifies possible phrase-level alignments and injects them as additional contexts to guide the parsing procedure .,2,0.55809563,34.74924796126392,18
3154,Experimental results on \textsc{ Squall } show that our approach can make better use of such alignments and obtains an absolute improvement of 3.4 % compared with the current state-of-the-art .,3,0.9484092,17.254627987974693,37
3155,A Temporal Knowledge Graph ( TKG ) is a sequence of KGs corresponding to different timestamps .,0,0.66871154,15.379201030085978,17
3155,TKG reasoning aims to predict potential facts in the future given the historical KG sequences .,0,0.8326007,87.14053361460658,16
3155,One key of this task is to mine and understand evolutional patterns of facts from these sequences .,0,0.9108764,80.58240449880894,18
3155,"The evolutional patterns are complex in two aspects , length-diversity and time-variability .",0,0.7391496,60.56550056457686,16
3155,"Existing models for TKG reasoning focus on modeling fact sequences of a fixed length , which cannot discover complex evolutional patterns that vary in length .",0,0.8033158,105.23040172995569,26
3155,"Furthermore , these models are all trained offline , which cannot well adapt to the changes of evolutional patterns from then on .",0,0.77052486,123.35076012632643,23
3155,"Thus , we propose a new model , called Complex Evolutional Network ( CEN ) , which uses a length-aware Convolutional Neural Network ( CNN ) to handle evolutional patterns of different lengths via an easy-to-difficult curriculum learning strategy .",2,0.38882235,31.027726573114542,45
3155,"Besides , we propose to learn the model under the online setting so that it can adapt to the changes of evolutional patterns over time .",2,0.4771644,38.22350868776497,26
3155,Extensive experiments demonstrate that CEN obtains substantial performance improvement under both the traditional offline and the proposed online settings .,3,0.8946455,38.72874608054633,20
3156,Dialogue state tracking ( DST ) aims to extract essential information from multi-turn dialog situations and take appropriate actions .,0,0.92082137,47.746431502744365,20
3156,"A belief state , one of the core pieces of information , refers to the subject and its specific content , and appears in the form of domain-slot-value .",0,0.8517995,97.75037266237159,29
3156,"The trained model predicts “ accumulated ” belief states in every turn , and joint goal accuracy and slot accuracy are mainly used to evaluate the prediction ;",2,0.5538238,286.5489547571393,28
3156,"however , we specify that the current evaluation metrics have a critical limitation when evaluating belief states accumulated as the dialogue proceeds , especially in the most used MultiWOZ dataset .",3,0.86219597,96.51245889162884,31
3156,"Additionally , we propose relative slot accuracy to complement existing metrics .",2,0.45858008,185.54502889841882,12
3156,"Relative slot accuracy does not depend on the number of predefined slots , and allows intuitive evaluation by assigning relative scores according to the turn of each dialog .",3,0.60849816,73.00272915772882,29
3156,"This study also encourages not solely the reporting of joint goal accuracy , but also various complementary metrics in DST tasks for the sake of a realistic evaluation .",3,0.9406314,117.05616094026877,29
3157,LM-BFF ( CITATION ) achieves significant few-shot performance by using auto-generated prompts and adding demonstrations similar to an input example .,3,0.6504725,82.52412756123181,23
3157,"To improve the approach of LM-BFF , this paper proposes LM-BFF-MS — better few-shot fine-tuning of language models with multiple soft demonstrations by making its further extensions , which include 1 ) prompts with multiple demonstrations based on automatic generation of multiple label words ;",1,0.4069108,120.85309187485211,51
3157,and 2 ) soft demonstration memory which consists of multiple sequences of globally shared word embeddings for a similar context .,2,0.5776895,137.1598928646213,21
3157,"Experiments conducted on eight NLP tasks show that LM-BFF-MS leads to improvements over LM-BFF on five tasks , particularly achieving 94.0 and 90.4 on SST-2 and MRPC , respectively .",3,0.94150364,32.629673395118736,38
3158,Dialogue State Tracking ( DST ) is primarily evaluated using Joint Goal Accuracy ( JGA ) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction .,0,0.81470764,59.62012262291054,34
3158,"Generally in DST , the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn .",0,0.75441843,92.34110007263236,26
3158,"Due to this cumulative nature of the belief state , it is difficult to get a correct prediction once a misprediction has occurred .",0,0.83741945,40.29544832404976,24
3158,"Thus , although being a useful metric , it can be harsh at times and underestimate the true potential of a DST model .",0,0.49362522,58.18384231379251,24
3158,"Moreover , an improvement in JGA can sometimes decrease the performance of turn-level or non-cumulative belief state prediction due to inconsistency in annotations .",3,0.7665826,115.35600395329753,26
3158,"So , using JGA as the only metric for model selection may not be ideal for all scenarios .",3,0.7104005,51.589055409182144,19
3158,"In this work , we discuss various evaluation metrics used for DST along with their shortcomings .",1,0.8433965,56.05227260192273,17
3158,"To address the existing issues , we propose a new evaluation metric named Flexible Goal Accuracy ( FGA ) .",1,0.35155693,43.750297429915754,20
3158,FGA is a generalized version of JGA .,0,0.36179116,115.25785996510724,8
3158,"But unlike JGA , it tries to give penalized rewards to mispredictions that are locally correct i.e .",3,0.42240408,144.10695721689473,18
3158,the root cause of the error is an earlier turn .,3,0.55938846,52.339371548029135,11
3158,"By doing so , FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics .",3,0.6527813,111.54418555894732,25
3158,We also show that FGA is a better discriminator of DST model performance .,3,0.97700477,47.092363905588726,14
3159,"As a recent development in few-shot learning , prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks .",0,0.8822368,17.707497749459275,24
3159,"However , despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks , existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context ( WiC ) dataset .",3,0.5147517,48.0814034264155,34
3159,"Specifically , none of the existing few-shot approaches ( including the in-context learning of GPT-3 ) can attain a performance that is meaningfully different from the random baseline .",3,0.8520241,40.94617252624585,32
3159,"Trying to fill this gap , we propose a new prompting technique , based on similarity metrics , which boosts few-shot performance to the level of fully supervised methods .",1,0.47248253,55.85563439312296,30
3159,"Our simple adaptation shows that the failure of existing prompt-based techniques in semantic distinction is due to their improper configuration , rather than lack of relevant knowledge in the representations .",3,0.93417346,81.79673025321725,32
3159,We also show that this approach can be effectively extended to other downstream tasks for which a single prompt is sufficient .,3,0.9347934,15.059165063536442,22
3160,"Abstract Meaning Representation ( AMR ) parsing aims to translate sentences to semantic representation with a hierarchical structure , and is recently empowered by pretrained sequence-to-sequence models .",0,0.95315886,31.02555915598035,30
3160,"However , there exists a gap between their flat training objective ( i.e. , equally treats all output tokens ) and the hierarchical AMR structure , which limits the model generalization .",0,0.768903,84.02222061679406,32
3160,"To bridge this gap , we propose a Hierarchical Curriculum Learning ( HCL ) framework with Structure-level ( SC ) and Instance-level Curricula ( IC ) .",1,0.46839303,25.14849873992642,29
3160,SC switches progressively from core to detail AMR semantic elements while IC transits from structure-simple to-complex AMR instances during training .,3,0.8058607,279.80309735227024,23
3160,"Through these two warming-up processes , HCL reduces the difficulty of learning complex structures , thus the flat model can better adapt to the AMR hierarchy .",3,0.9050085,120.13609682340172,29
3160,"Extensive experiments on AMR2.0 , AMR3.0 , structure-complex and out-of-distribution situations verify the effectiveness of HCL .",3,0.80905145,26.27362058749194,19
3161,Neural models for distantly supervised relation extraction ( DS-RE ) encode each sentence in an entity-pair bag separately .,0,0.4263498,54.64339056534497,23
3161,These are then aggregated for bag-level relation prediction .,2,0.61126184,47.59951312191231,9
3161,"Since , at encoding time , these approaches do not allow information to flow from other sentences in the bag , we believe that they do not utilize the available bag data to the fullest .",0,0.5287871,67.90593481505488,36
3161,"In response , we explore a simple baseline approach ( PARE ) in which all sentences of a bag are concatenated into a passage of sentences , and encoded jointly using BERT .",2,0.7349268,57.87779898037099,33
3161,The contextual embeddings of tokens are aggregated using attention with the candidate relation as query – this summary of whole passage predicts the candidate relation .,2,0.7323943,99.01289851713496,26
3161,We find that our simple baseline solution outperforms existing state-of-the-art DS-RE models in both monolingual and multilingual DS-RE datasets .,3,0.97506535,13.05115329419647,29
3162,We present a debiased dataset for the Person-centric Visual Grounding ( PCVG ) task first proposed by Cui et al .,2,0.3724698,43.3609137169833,21
3162,( 2021 ) in the Who ’s Waldo dataset .,2,0.6561863,141.3982708274431,10
3162,"Given an image and a caption , PCVG requires pairing up a person ’s name mentioned in a caption with a bounding box that points to the person in the image .",0,0.4901503,53.14318091002207,32
3162,We find that the original Who ’s Waldo dataset compiled for this task contains a large number of biased samples that are solvable simply by heuristic methods ;,3,0.9539101,72.9808018906855,28
3162,"for instance , in many cases the first name in the sentence corresponds to the largest bounding box , or the sequence of names in the sentence corresponds to an exact left-to-right order in the image .",0,0.6530487,24.465237372637898,39
3162,"Naturally , models trained on these biased data lead to over-estimation of performance on the benchmark .",0,0.6558007,27.58554696899458,17
3162,"To enforce models being correct for the correct reasons , we design automated tools to filter and debias the original dataset by ruling out all examples of insufficient context , such as those with no verb or with a long chain of conjunct names in their captions .",2,0.74653864,78.7942014797948,48
3162,Our experiments show that our new sub-sampled dataset contains less bias with much lowered heuristic performances and widened gaps between heuristic and supervised methods .,3,0.97883683,87.7978574681896,25
3162,We also demonstrate the same benchmark model trained on our debiased training set outperforms that trained on the original biased ( and larger ) training set on our debiased test set .,3,0.9305348,38.18105568032068,32
3162,We argue our debiased dataset offers the PCVG task a more practical baseline for reliable benchmarking and future improvements .,3,0.9813783,123.36399493711652,20
3163,Translate-train is a general training approach to multilingual tasks .,0,0.5590947,52.80481213287101,10
3163,The key idea is to use the translator of the target language to generate training data to mitigate the gap between the source and target languages .,0,0.64912534,13.50085052817503,27
3163,"However , its performance is often hampered by the artifacts in the translated texts ( translationese ) .",0,0.9144104,103.05427730908302,18
3163,"We discover that such artifacts have common patterns in different languages and can be modeled by deep learning , and subsequently propose an approach to conduct translate-train using Translationese Embracing the effect of Artifacts ( TEA ) .",3,0.5169011,114.88693371413163,39
3163,"TEA learns to mitigate such effect on the training data of a source language ( whose original and translationese are both available ) , and applies the learned module to facilitate the inference on the target language .",2,0.4739529,72.40184682571045,38
3163,Extensive experiments on the multilingual QA dataset TyDiQA demonstrate that TEA outperforms strong baselines .,3,0.78787756,19.18683921574064,15
3164,We consider the problem of pretraining a two-stage open-domain question answering ( QA ) system ( retriever + reader ) with strong transfer capabilities .,1,0.4095945,37.56427897979911,26
3164,The key challenge is how to construct a large amount of high-quality question-answer-context triplets without task-specific annotations .,0,0.84056926,22.893268699978673,22
3164,"Specifically , the triplets should align well with downstream tasks by : ( i ) covering a wide range of domains ( for open-domain applications ) , ( ii ) linking a question to its semantically relevant context with supporting evidence ( for training the retriever ) , and ( iii ) identifying the correct answer in the context ( for training the reader ) .",3,0.7926292,38.00107651250948,66
3164,Previous pretraining approaches generally fall short of one or more of these requirements .,0,0.8461676,31.430093547431937,14
3164,"In this work , we automatically construct a large-scale corpus that meets all three criteria by consulting millions of references cited within Wikipedia .",2,0.5472669,52.764402242362024,24
3164,The well-aligned pretraining signals benefit both the retriever and the reader significantly .,3,0.94558525,50.5841167378386,15
3164,Our pretrained retriever leads to 2 %-10 % absolute gains in top-20 accuracy .,3,0.91854316,65.67551901050257,17
3164,"And with our pretrained reader , the entire system improves by up to 4 % in exact match .",3,0.89812773,111.49802756986688,19
3165,"Since the inception of crowdsourcing , aggregation has been a common strategy for dealing with unreliable data .",0,0.96570057,37.97185061189082,18
3165,Aggregate ratings are more reliable than individual ones .,3,0.7343614,77.74736061615833,9
3165,"However , many Natural Language Processing ( NLP ) applications that rely on aggregate ratings only report the reliability of individual ratings , which is the incorrect unit of analysis .",0,0.9506363,61.75150826622318,31
3165,"In these instances , the data reliability is under-reported , and a proposed k-rater reliability ( kRR ) should be used as the correct data reliability for aggregated datasets .",0,0.6489,52.2208835510323,30
3165,It is a multi-rater generalization of inter-rater reliability ( IRR ) .,0,0.6688724,28.318502684511618,12
3165,"We conducted two replications of the WordSim-353 benchmark , and present empirical , analytical , and bootstrap-based methods for computing kRR on WordSim-353 .",2,0.8347244,83.78912633498433,29
3165,These methods produce very similar results .,3,0.79519826,44.32816503130365,7
3165,We hope this discussion will nudge researchers to report kRR in addition to IRR .,3,0.82635045,114.90961584952535,15
3166,"We introduce FLOTA ( Few Longest Token Approximation ) , a simple yet effective method to improve the tokenization of pretrained language models ( PLMs ) .",2,0.45959556,52.937850590925436,27
3166,FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization .,0,0.42983243,45.494271729052635,20
3166,"We evaluate FLOTA on morphological gold segmentations as well as a text classification task , using BERT , GPT-2 , and XLNet as example PLMs .",2,0.68418777,70.83043033584403,28
3166,"FLOTA leads to performance gains , makes inference more efficient , and enhances the robustness of PLMs with respect to whitespace noise .",3,0.870375,72.40495404256362,23
3167,"In this paper , we propose Self-Contrastive Decorrelation ( SCD ) , a self-supervised approach .",1,0.83451164,23.479130332938286,16
3167,"Given an input sentence , it optimizes a joint self-contrastive and decorrelation objective .",2,0.5157174,82.14123248209746,14
3167,Learning a representation is facilitated by leveraging the contrast arising from the instantiation of standard dropout at different rates .,3,0.36251122,132.78622968710766,20
3167,The proposed method is conceptually simple yet empirically powerful .,3,0.8419438,20.081572216974187,10
3167,It achieves comparable results with state-of-the-art methods on multiple benchmarks without using contrastive pairs .,3,0.89098465,17.02344290796205,21
3167,This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods .,3,0.97314113,25.883164280566458,19
3168,"Cosine similarity of contextual embeddings is used in many NLP tasks ( e.g. , QA , IR , MT ) and metrics ( e.g. , BERTScore ) .",0,0.7808064,38.72910619467238,28
3168,"Here , we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency .",3,0.3446525,88.04285937655403,27
3168,"We find that relative to human judgements , cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts , even after controlling for polysemy and other factors .",3,0.97122014,32.592073315087774,38
3168,We conjecture that this underestimation of similarity for high frequency words is due to differences in the representational geometry of high and low frequency words and provide a formal argument for the two-dimensional case .,3,0.7169653,33.634610802111474,36
3169,"Compositional generalization is a fundamental trait in humans , allowing us to effortlessly combine known phrases to form novel sentences .",0,0.9345003,74.83103862227003,21
3169,Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize .,0,0.9482357,40.40520589005228,16
3169,"In this paper , we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark .",1,0.74839514,61.410503706181395,19
3169,"We demonstrate that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance , thereby showing that their compositional generalization abilities were previously underestimated .",3,0.9363297,39.55132484641543,33
3169,We perform detailed empirical analysis of this phenomenon .,2,0.40592182,33.726820478884584,9
3169,Our results indicate that the generalization performance of models is highly sensitive to the characteristics of the training data which should be carefully considered while designing such benchmarks in future .,3,0.9897883,19.09064614411562,31
3170,Open-domain question answering is a challenging task with a wide variety of practical applications .,0,0.94465625,8.693343361432667,15
3170,Existing modern approaches mostly follow a standard two-stage paradigm : retriever then reader .,0,0.940832,153.03261866049473,15
3170,"In this article , we focus on improving the effectiveness of the reader module and propose a novel copy-augmented generative approach that integrates the merits of both extractive and generative readers .",1,0.9122075,28.806043610319637,33
3170,"In particular , our model is built upon the powerful generative model FiD ( CITATION ) .",2,0.6019527,78.70257910304979,17
3170,We enhance the original generative reader by incorporating a pointer network to encourage the model to directly copy words from the retrieved passages .,2,0.6778235,41.26854721782311,24
3170,"We conduct experiments on the two benchmark datasets , Natural Questions and TriviaQA , and the empirical results demonstrate the performance gains of our proposed approach .",3,0.5424523,20.105482449300233,27
3171,"Dense retrieval models , which aim at retrieving the most relevant document for an input query on a dense representation space , have gained considerable attention for their remarkable success .",0,0.94928354,40.98699939886897,31
3171,"Yet , dense models require a vast amount of labeled training data for notable performance , whereas it is often challenging to acquire query-document pairs annotated by humans .",0,0.88593084,61.798314758884324,31
3171,"To tackle this problem , we propose a simple but effective Document Augmentation for dense Retrieval ( DAR ) framework , which augments the representations of documents with their interpolation and perturbation .",2,0.32518384,34.20325236157756,33
3171,"We validate the performance of DAR on retrieval tasks with two benchmark datasets , showing that the proposed DAR significantly outperforms relevant baselines on the dense retrieval of both the labeled and unlabeled documents .",3,0.73469,32.11359714963209,35
3172,"Signed Language Processing ( SLP ) concerns the automated processing of signed languages , the main means of communication of Deaf and hearing impaired individuals .",0,0.9688825,97.67107290677387,26
3172,"SLP features many different tasks , ranging from sign recognition to translation and production of signed speech , but has been overlooked by the NLP community thus far .",0,0.91981107,55.19910562139731,29
3172,"In this paper , we bring to attention the task of modelling the phonology of sign languages .",1,0.9025187,34.893737342235006,18
3172,We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties .,2,0.81023574,27.91546833735956,21
3172,We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties .,2,0.3503072,16.234296369985106,26
3172,"We find that , despite the inherent challenges of the task , graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree .",3,0.9770254,50.82594506394335,38
3172,"Most importantly , we show that this performance pertains even on signs unobserved during training .",3,0.9658373,73.39403369830815,16
3173,Creating chatbots to behave like real people is important in terms of believability .,0,0.83278173,29.35203466037904,14
3173,"Errors in general chatbots and chatbots that follow a rough persona have been studied , but those in chatbots that behave like real people have not been thoroughly investigated .",0,0.9501979,31.717547576536603,30
3173,"We collected a large amount of user interactions of a generation-based chatbot trained from large-scale dialogue data of a specific character , i.e. , target person , and analyzed errors related to that person .",2,0.94401747,56.19019712952593,37
3173,"We found that person-specific errors can be divided into two types : errors in attributes and those in relations , each of which can be divided into two levels : self and other .",3,0.9824492,32.28599980498387,34
3173,"The correspondence with an existing taxonomy of errors was also investigated , and person-specific errors that should be addressed in the future were clarified .",3,0.68051934,57.0361318669138,25
3174,"This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis , directly predicting sentiment graphs from text .",1,0.62163174,25.913419983780766,27
3174,We advance the state of the art on 4 out of 5 standard benchmark sets .,2,0.56110424,15.843554872213444,16
3174,"We release the source code , models and predictions .",2,0.5713227,96.20470378890805,10
3175,"Transformer-based models are widely used in natural language understanding ( NLU ) tasks , and multimodal transformers have been effective in visual-language tasks .",0,0.92174596,20.141854989542832,28
3175,This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders .,1,0.8884143,23.72806773196359,15
3175,Our framework is inspired by cross-modal encoders ’ success in visual-language tasks while we alter the learning objective to cater to the language-heavy characteristics of NLU .,2,0.7031354,46.72498586500745,31
3175,"After training with a small number of extra adapting steps and finetuned , the proposed XDBERT ( cross-modal distilled BERT ) outperforms pretrained-BERT in general language understanding evaluation ( GLUE ) , situations with adversarial generations ( SWAG ) benchmarks , and readability benchmarks .",3,0.7568319,123.20174600457018,47
3175,We analyze the performance of XDBERT on GLUE to show that the improvement is likely visually grounded .,3,0.5976633,88.04693173509905,18
3176,Omission and addition of content is a typical issue in neural machine translation .,0,0.94116634,51.11245210558396,14
3176,We propose a method for detecting such phenomena with off-the-shelf translation models .,1,0.502173,16.589072444541404,14
3176,"Using contrastive conditioning , we compare the likelihood of a full sequence under a translation model to the likelihood of its parts , given the corresponding source or target sequence .",2,0.8388851,53.78986198813025,31
3176,This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation .,3,0.502189,30.274744336019864,24
3176,The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model .,3,0.8669793,30.595907279947735,19
3177,This work addresses the question of the localization of syntactic information encoded in the transformers representations .,1,0.82253796,36.56820942019152,17
3177,"We tackle this question from two perspectives , considering the object-past participle agreement in French , by identifying , first , in which part of the sentence and , second , in which part of the representation the syntactic information is encoded .",1,0.46192384,51.156133765388816,45
3177,"The results of our experiments , using probing , causal analysis and feature selection method , show that syntactic information is encoded locally in a way consistent with the French grammar .",3,0.968163,86.06964577703775,32
3178,Livonian is one of the most endangered languages in Europe with just a tiny handful of speakers and virtually no publicly available corpora .,0,0.9433424,24.227002417450148,24
3178,"In this paper we tackle the task of developing neural machine translation ( NMT ) between Livonian and English , with a two-fold aim : on one hand , preserving the language and on the other – enabling access to Livonian folklore , lifestories and other textual intangible heritage as well as making it easier to create further parallel corpora .",1,0.7813826,52.76095543116376,62
3178,We rely on Livonian ’s linguistic similarity to Estonian and Latvian and collect parallel and monolingual data for the four languages for translation experiments .,2,0.83017683,46.46320537055365,25
3178,"We combine different low-resource NMT techniques like zero-shot translation , cross-lingual transfer and synthetic data creation to reach the highest possible translation quality as well as to find which base languages are empirically more helpful for transfer to Livonian .",2,0.78344065,47.66253958773302,40
3178,"The resulting NMT systems and the collected monolingual and parallel data , including a manually translated and verified translation benchmark , are publicly released via OPUS and Huggingface repositories .",3,0.41803,140.067020917755,30
3179,Text-based games ( TGs ) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action spaces .,0,0.94137335,78.98774675369356,27
3179,"In these games , the agent learns to explore the environment via natural language interactions with the game simulator .",0,0.4425705,49.51936361188792,20
3179,A fundamental challenge in TGs is the efficient exploration of the large action space when the agent has not yet acquired enough knowledge about the environment .,0,0.8930601,35.456739942155224,27
3179,"We propose CommExpl , an exploration technique that injects external commonsense knowledge , via a pretrained language model ( LM ) , into the agent during training when the agent is the most uncertain about its next action .",2,0.51873,62.869329971690476,39
3179,Our method exhibits improvement on the collected game scores during the training in four out of nine games from Jericho .,3,0.8337539,111.62064349971887,21
3179,"Additionally , the produced trajectory of actions exhibit lower perplexity , when tested with a pretrained LM , indicating better closeness to human language .",3,0.9605589,186.84144549847522,25
3180,"Pre-trained language models ( PLMs ) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora , especially those rare entities .",0,0.9094889,58.376316422881125,24
3180,"In this paper , we propose to build a simple but effective Pluggable Entity Lookup Table ( PELT ) on demand by aggregating the entity ’s output representations of multiple occurrences in the corpora .",1,0.77646804,40.569276184949736,35
3180,PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs .,3,0.76353353,174.54853405355996,15
3180,"Compared to previous knowledge-enhanced PLMs , PELT only requires 0.2 %-5 % pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario .",3,0.8175811,39.348615675037784,32
3180,"The experiments on knowledge-related tasks demonstrate that our method , PELT , can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures .",3,0.9599779,55.99622570359602,28
3180,Our code and models are publicly available at https://github.com/thunlp/PELT .,3,0.50536674,5.164877302393879,10
3181,The emergence of multilingual pre-trained language models makes it possible to adapt to target languages with only few labeled examples .,0,0.8743518,13.278806165869112,21
3181,"However , vanilla fine-tuning tends to achieve degenerated and unstable results , owing to the Language Interference among different languages , and Parameter Overload under the few-sample transfer learning scenarios .",3,0.67872024,97.59132559469641,31
3181,"To address two problems elegantly , we propose S4-Tuning , a Simple Cross-lingual Sub-network Tuning method .",2,0.45241508,44.744697162514534,19
3181,"S4-Tuning first detects the most essential sub-network for each target language , and only updates it during fine-tuning .",2,0.4965254,53.62690471031774,21
3181,"In this way , the language sub-networks lower the scale of trainable parameters , and hence better suit the low-resource scenarios .",3,0.78037256,58.30590601172922,22
3181,"Meanwhile , the commonality and characteristics across languages are modeled by the overlapping and non-overlapping parts to ease the interference among languages .",2,0.37719625,45.55543384483358,23
3181,"Simple but effective , S4-Tuning gains consistent improvements over vanilla fine-tuning on three multi-lingual tasks involving 37 different languages in total ( XNLI , PAWS-X , and Tatoeba ) .",3,0.87847716,56.73739696511957,33
3182,Certainty calibration is an important goal on the path to interpretability and trustworthy AI .,0,0.7612413,74.60543955720867,15
3182,"Particularly in the context of human-in-the-loop systems , high-quality low to mid-range certainty estimates are essential .",0,0.7924079,24.06796585486368,21
3182,"In the presence of a dominant high-certainty class , for instance the non-entity class in NER problems , existing calibration error measures are completely insensitive to potentially large errors in this certainty region of interest .",0,0.68902487,119.2304094135358,36
3182,We introduce a region-balanced calibration error metric that weights all certainty regions equally .,2,0.7800878,261.8472768993543,14
3182,"When low and mid certainty estimates are taken into account , calibration error is typically larger than previously reported .",3,0.7849961,71.20808770653458,20
3182,"We introduce a simple extension of temperature scaling , requiring no additional computation , that can reduce both traditional and region-balanced notions of calibration error over existing baselines .",2,0.42091426,123.30877106125945,30
3183,Reasoning using negation is known to be difficult for transformer-based language models .,0,0.8299991,20.801639287518572,15
3183,"While previous studies have used the tools of psycholinguistics to probe a transformer ’s ability to reason over negation , none have focused on the types of negation studied in developmental psychology .",0,0.8875444,33.43493644323677,33
3183,"We explore how well transformers can process such categories of negation , by framing the problem as a natural language inference ( NLI ) task .",1,0.513762,54.450773151371266,26
3183,We curate a set of diagnostic questions for our target categories from popular NLI datasets and evaluate how well a suite of models reason over them .,2,0.7582704,46.883957618944805,27
3183,"We find that models perform consistently better only on certain categories , suggesting clear distinctions in how they are processed .",3,0.9823073,83.1432983405557,21
3184,"Natural Language Understanding ( NLU ) models can be trained on sensitive information such as phone numbers , zip-codes etc .",0,0.8183776,39.3564311914887,21
3184,Recent literature has focused on Model Inversion Attacks ( ModIvA ) that can extract training data from model parameters .,0,0.9204147,91.67585574469624,20
3184,"In this work , we present a version of such an attack by extracting canaries inserted in NLU training data .",1,0.7823815,88.76483100342715,21
3184,"In the attack , an adversary with open-box access to the model reconstructs the canaries contained in the model ’s training set .",2,0.61449146,90.30448975017919,24
3184,"We evaluate our approach by performing text completion on canaries and demonstrate that by using the prefix ( non-sensitive ) tokens of the canary , we can generate the full canary .",3,0.5984458,72.55162808540786,32
3184,"As an example , our attack is able to reconstruct a four digit code in the training dataset of the NLU model with a probability of 0.5 in its best configuration .",3,0.85471344,53.914660702294434,32
3184,"As countermeasures , we identify several defense mechanisms that , when combined , effectively eliminate the risk of ModIvA in our experiments .",3,0.82855284,149.3655669367208,23
3185,Multiple metrics have been introduced to measure fairness in various natural language processing tasks .,0,0.8992271,14.718911554624826,15
3185,These metrics can be roughly categorized into two categories : 1 ) extrinsic metrics for evaluating fairness in downstream applications and 2 ) intrinsic metrics for estimating fairness in upstream contextualized language representation models .,0,0.6192273,35.32039347547531,35
3185,"In this paper , we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models .",1,0.8561166,57.969138507235314,24
3185,"We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting , even when correcting for metric misalignments , noise in evaluation datasets , and confounding factors such as experiment configuration for extrinsic metrics .",3,0.9740447,57.37872312147586,39
3186,AMR parsing is the task that maps a sentence to an AMR semantic graph automatically .,0,0.8876181,43.8764213675399,16
3186,The difficulty comes from generating the complex graph structure .,0,0.84643394,92.71148676658811,10
3186,"The previous state-of-the-art method translates the AMR graph into a sequence , then directly fine-tunes a pretrained sequence-to-sequence Transformer model ( BART ) .",0,0.5038842,20.96341405291178,28
3186,"However , purely treating the graph as a sequence does not take advantage of structural information about the graph .",0,0.81686103,30.957853788543257,20
3186,"In this paper , we design several strategies to add the important ancestor information into the Transformer Decoder .",1,0.7606886,81.79723730315408,19
3186,Our experiments show that we can improve the performance for both AMR 2.0 and AMR 3.0 dataset and achieve new state-of-the-art results .,3,0.96724117,5.7410413270570855,29
3187,"Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze ( 2019 ) , but only between related languages .",0,0.8088068,20.587840646234998,39
3187,"However , source and training languages are rarely related , when parsing truly low-resource languages .",0,0.72550625,128.27159070006985,16
3187,"To close this gap , we adopt a method from multi-task learning , which relies on automated curriculum learning , to dynamically optimize for parsing performance on outlier languages .",2,0.60566455,66.58118000525079,30
3187,We show that this approach is significantly better than uniform and size-proportional sampling in the zero-shot setting .,3,0.9369142,23.93965751655688,18
3188,Recent advances in Automatic Speech Recognition ( ASR ) have made it possible to reliably produce automatic transcripts of clinician-patient conversations .,0,0.9661769,20.348353742546553,24
3188,"However , access to clinical datasets is heavily restricted due to patient privacy , thus slowing down normal research practices .",0,0.93968284,90.28274679735178,21
3188,"We detail the development of a public access , high quality dataset comprising of 57 mocked primary care consultations , including audio recordings , their manual utterance-level transcriptions , and the associated consultation notes .",2,0.63618916,141.73599552807818,37
3188,Our work illustrates how the dataset can be used as a benchmark for conversational medical ASR as well as consultation note generation from transcripts .,3,0.95628494,52.162138143761645,25
3189,The goal-oriented document-grounded dialogue aims at responding to the user query based on the dialogue context and supporting document .,0,0.5250534,47.65651724170703,22
3189,Existing studies tackle this problem by decomposing it into two sub-tasks : knowledge identification and response generation .,0,0.9116313,19.771108330763926,18
3189,"However , such pipeline methods would unavoidably suffer from the error propagation issue .",0,0.8027814,50.05853333549141,14
3189,This paper proposes to unify these two sub-tasks via sequentially generating the grounding knowledge and the response .,1,0.82829744,36.73606794961937,18
3189,We further develop a prompt-connected multi-task learning strategy to model the characteristics and connections of different tasks and introduce linear temperature scheduling to reduce the negative effect of irrelevant document information .,2,0.48121202,66.24401160316819,32
3189,Experimental results demonstrate the effectiveness of our framework .,3,0.9554168,7.937308287098241,9
3190,"Interpolation-based regularisation methods such as Mixup , which generate virtual training samples , have proven to be effective for various tasks and modalities .",0,0.80547786,43.87078327558977,26
3190,"We extend Mixup and propose DMix , an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space .",2,0.5876992,90.0583902491295,26
3190,DMix leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation .,2,0.42733604,80.63933155540121,18
3190,"DMix achieves state-of-the-art results on sentence classification over existing data augmentation methods on 8 benchmark datasets across English , Arabic , Turkish , and Hindi languages while achieving benchmark F1 scores in 3 times less number of iterations .",3,0.81421167,30.85217730417484,44
3190,We probe the effectiveness of DMix in conjunction with various similarity measures and qualitatively analyze the different components .,2,0.49772835,77.97584188516419,19
3190,"DMix being generalizable , can be applied to various tasks , models and modalities .",3,0.57811874,136.282648341758,15
3191,"We leverage embedding duplication between aligned sub-words to extend the Parent-Child transfer learning method , so as to improve low-resource machine translation .",2,0.7103501,71.28632824583723,25
3191,"We conduct experiments on benchmark datasets of My-En , Id-En and Tr-En translation scenarios .",2,0.84142005,62.13430240719288,19
3191,"The test results show that our method produces substantial improvements , achieving the BLEU scores of 22.5 , 28.0 and 18.1 respectively .",3,0.977193,17.20726893893087,23
3191,"In addition , the method is computationally efficient which reduces the consumption of training time by 63.8 % , reaching the duration of 1.6 hours when training on a Tesla 16GB P100 GPU .",3,0.8669194,53.180951884944314,34
3191,All the models and source codes in the experiments will be made publicly available to support reproducible research .,3,0.62442285,22.686821612601506,19
3192,"Analyzing the temporal sequence of texts from sources such as social media , news , and parliamentary debates is a challenging problem as it exhibits time-varying scale-free properties and fine-grained timing irregularities .",0,0.9519772,42.758062560097386,38
3192,"We propose a Hyperbolic Hawkes Attention Network ( HYPHEN ) , which learns a data-driven hyperbolic space and models irregular powerlaw excitations using a hyperbolic Hawkes process .",1,0.3991519,57.59195417798898,28
3192,"Through quantitative and exploratory experiments over financial NLP , suicide ideation detection , and political debate analysis we demonstrate HYPHEN ’s practical applicability for modeling online text sequences in a geometry agnostic manner .",3,0.6543097,156.50831960872006,34
3193,Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings .,0,0.9419355,24.21876700264383,23
3193,"With advances in Natural Language Processing strategies , it is now possible to design automated systems to assess suicide risk .",0,0.93568355,45.3367583882679,21
3193,"However , such systems may generate uncertain predictions , leading to severe consequences .",0,0.9164655,112.13619560719593,14
3193,We hence reformulate suicide risk assessment as a selective prioritized prediction problem over the Columbia Suicide Severity Risk Scale ( C-SSRS ) .,2,0.5653206,73.92221004914975,23
3193,"We propose SASI , a risk-averse and self-aware transformer-based hierarchical attention classifier , augmented to refrain from making uncertain predictions .",1,0.44448122,97.98412625748283,25
3193,We show that SASI is able to refrain from 83 % of incorrect predictions on real-world Reddit data .,3,0.93617946,74.53325796704858,20
3193,"Furthermore , we discuss the qualitative , practical , and ethical aspects of SASI for suicide risk assessment as a human-in-the-loop framework .",1,0.47308046,36.16892521610861,27
3194,"Because meaning can often be inferred from lexical semantics alone , word order is often a redundant cue in natural language .",0,0.8993748,49.23575243824583,22
3194,"For example , the words chopped , chef , and onion are more likely used to convey “ The chef chopped the onion , ” not “ The onion chopped the chef .",3,0.81493574,62.37600201342458,33
3194,"Recent work has shown large language models to be surprisingly word order invariant , but crucially has largely considered natural prototypical inputs , where compositional meaning mostly matches lexical expectations .",0,0.8818787,129.16068914845968,31
3194,"To overcome this confound , we probe grammatical role representation in English BERT and GPT-2 , on instances where lexical expectations are not sufficient , and word order knowledge is necessary for correct classification .",2,0.66349155,68.57621843580854,37
3194,"Such non-prototypical instances are naturally occurring English sentences with inanimate subjects or animate objects , or sentences where we systematically swap the arguments to make sentences like “ The onion chopped the chef ” .",0,0.64382744,112.8573110421578,35
3194,"We find that , while early layer embeddings are largely lexical , word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions .",3,0.97579116,61.96515011620406,32
3194,"Our experiments isolate the effect of word order on the contextualization process , and highlight how models use context in the uncommon , but critical , instances where it matters .",3,0.929454,118.08494830574095,31
3195,"Triangular machine translation is a special case of low-resource machine translation where the language pair of interest has limited parallel data , but both languages have abundant parallel data with a pivot language .",0,0.87063843,45.848092522569694,34
3195,"Naturally , the key to triangular machine translation is the successful exploitation of such auxiliary data .",0,0.88211596,88.49026210740428,17
3195,"In this work , we propose a transfer-learning-based approach that utilizes all types of auxiliary data .",1,0.7441199,26.11145645545179,19
3195,"As we train auxiliary source-pivot and pivot-target translation models , we initialize some parameters of the pivot side with a pre-trained language model and freeze them to encourage both translation models to work in the same pivot language space , so that they can be smoothly transferred to the source-target translation model .",2,0.71836877,36.18775487522175,54
3195,Experiments show that our approach can outperform previous ones .,3,0.93351835,12.113056428921983,10
3196,Cognitively plausible visual dialogue models should keep a mental scoreboard of shared established facts in the dialogue context .,3,0.5367207,180.74156881965118,19
3196,We propose a theory-based evaluation method for investigating to what degree models pretrained on the VisDial dataset incrementally build representations that appropriately do scorekeeping .,1,0.41115174,112.20037881422158,27
3196,"Our conclusion is that the ability to make the distinction between shared and privately known statements along the dialogue is moderately present in the analysed models , but not always incrementally consistent , which may partially be due to the limited need for grounding interactions in the original task .",3,0.9900566,99.58957888516439,50
3197,Label smoothing and vocabulary sharing are two widely used techniques in neural machine translation models .,0,0.7243535,30.618039902394,16
3197,"However , we argue that simply applying both techniques can be conflicting and even leads to sub-optimal performance .",3,0.7665133,47.206609319845725,19
3197,"When allocating smoothed probability , original label smoothing treats the source-side words that would never appear in the target language equally to the real target-side words , which could bias the translation model .",3,0.66979796,66.35491365541455,36
3197,"To address this issue , we propose Masked Label Smoothing ( MLS ) , a new mechanism that masks the soft label probability of source-side words to zero .",0,0.43226767,73.60459716012758,29
3197,"Simple yet effective , MLS manages to better integrate label smoothing with vocabulary sharing .",3,0.72164744,298.6449669784383,15
3197,"Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets , including bilingual and multilingual translation from both translation quality and model ’s calibration .",3,0.9695879,82.05354301987511,31
3197,Our code is released at https://github.com/PKUnlp-icler/MLS .,3,0.55232865,32.69304458998128,7
3198,Multi-Label Text Classification ( MLTC ) is a fundamental and challenging task in natural language processing .,0,0.9533537,17.267451425964204,17
3198,Previous studies mainly focus on learning text representation and modeling label correlation but neglect the rich knowledge from the existing similar instances when predicting labels of a specific text .,0,0.8753201,92.11777774694619,30
3198,"To make up for this oversight , we propose a k nearest neighbor ( kNN ) mechanism which retrieves several neighbor instances and interpolates the model output with their labels .",2,0.51552975,59.219233305188176,31
3198,"Moreover , we design a multi-label contrastive learning objective that makes the model aware of the kNN classification process and improves the quality of the retrieved neighbors while inference .",2,0.6841081,44.19746526150295,30
3198,Extensive experiments show that our method can bring consistent and significant performance improvement to multiple MLTC models including the state-of-the-art pretrained and non-pretrained ones .,3,0.9518965,19.23088723074482,31
3199,Effectively finetuning pretrained language models ( PLMs ) is critical for their success in downstream tasks .,0,0.92333454,29.560340098606538,17
3199,"However , PLMs may have risks in overfitting the pretraining tasks and data , which usually have gap with the target downstream tasks .",0,0.6924011,120.15099193901523,24
3199,Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance .,3,0.54209983,22.62650009506202,18
3199,"In this paper , we propose a very simple yet effective method named NoisyTune to help better finetune PLMs on downstream tasks by adding some noise to the parameters of PLMs before fine-tuning .",1,0.8635015,20.131633716166665,34
3199,"More specifically , we propose a matrix-wise perturbing method which adds different uniform noises to different parameter matrices based on their standard deviations .",2,0.7286118,66.47738029950682,25
3199,"In this way , the varied characteristics of different types of parameters in PLMs can be considered .",3,0.6860298,42.07413951182513,18
3199,Extensive experiments on both GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can consistently empower the finetuning of different PLMs on different downstream tasks .,3,0.87382174,34.980607436325975,26
3200,Modern writing assistance applications are always equipped with a Grammatical Error Correction ( GEC ) model to correct errors in user-entered sentences .,0,0.9549179,27.536067288471127,23
3200,"Different scenarios have varying requirements for correction behavior , e.g. , performing more precise corrections ( high precision ) or providing more candidates for users ( high recall ) .",0,0.51504016,111.02452569912936,30
3200,"However , previous works adjust such trade-off only for sequence labeling approaches .",0,0.896369,113.80040137835392,15
3200,"In this paper , we propose a simple yet effective counterpart – Align-and-Predict Decoding ( APD ) for the most popular sequence-to-sequence models to offer more flexibility for the precision-recall trade-off .",1,0.83438987,29.60878358895516,40
3200,"During inference , APD aligns the already generated sequence with input and adjusts scores of the following tokens .",2,0.5070077,233.5793220736225,19
3200,"Experiments in both English and Chinese GEC benchmarks show that our approach not only adapts a single model to precision-oriented and recall-oriented inference , but also maximizes its potential to achieve state-of-the-art results .",3,0.94530815,19.164450966499697,41
3200,Our code is available at https://github.com/AutoTemp/Align-and-Predict .,3,0.63466537,10.551725349093463,7
3201,Injecting desired geometric properties into text representations has attracted a lot of attention .,0,0.9584988,24.267538778628108,14
3201,"A property that has been argued for , due to its better utilisation of representation space , is isotropy .",0,0.7953521,73.67872508862638,20
3201,"In parallel , VAEs have been successful in areas of NLP , but are known for their sub-optimal utilisation of the representation space .",0,0.84741265,41.625046621201996,24
3201,"To address an aspect of this , we investigate the impact of injecting isotropy during training of VAEs .",1,0.85815513,85.5200196006607,19
3201,We achieve this by using an isotropic Gaussian posterior ( IGP ) instead of the ellipsoidal Gaussian posterior .,2,0.67870736,26.896264724164972,19
3201,"We illustrate that IGP effectively encourages isotropy in the representations , inducing a more discriminative latent space .",3,0.91449684,102.92129177356331,18
3201,"Compared to vanilla VAE , this translates into a much better classification performance , robustness to input perturbation , and generative behavior .",3,0.85889804,55.14096688427411,23
3201,"Additionally , we offer insights about the representational properties encouraged by IGP .",3,0.61829543,126.7582008738644,13
3202,Several methods have been proposed for classifying long textual documents using Transformers .,0,0.8805783,32.14331802979682,13
3202,"However , there is a lack of consensus on a benchmark to enable a fair comparison among different approaches .",0,0.91125965,27.850585529610356,20
3202,"In this paper , we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets — both in terms of accuracy as well as time and space overheads .",1,0.84523165,27.6237523460208,34
3202,"Our datasets cover binary , multi-class , and multi-label classification tasks and represent various ways information is organized in a long text ( e.g .",2,0.64925426,48.19098159901232,25
3202,information that is critical to making the classification decision is at the beginning or towards the end of the document ) .,3,0.5033635,56.8709054243777,22
3202,Our results show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets .,3,0.98741746,20.574616423814877,20
3202,These findings emphasize the need for future studies to consider comprehensive baselines and datasets that better represent the task of long document classification to develop robust models .,3,0.9888297,41.77108848803506,28
3203,A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning ( RL ) .,0,0.85322934,81.82356921506278,23
3203,Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards .,3,0.7980215,80.31208018119446,19
3203,"However , metrics such as BERTScore greedily align candidate and reference tokens , which can allow system outputs to receive excess credit relative to a reference .",0,0.6501277,152.15496114221838,27
3203,"Furthermore , past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting .",0,0.80698717,234.70830923174762,15
3203,We address these issues by proposing metrics that replace the greedy alignments in BERTScore with optimized ones .,2,0.3971047,70.63578516599594,18
3203,We compute them on a model ’s trained token embeddings to prevent domain mismatch .,2,0.8089243,151.67433177511668,15
3203,Our model optimizing discrete alignment metrics consistently outperforms cross-entropy and BLEU reward baselines on AMR-to-text generation .,3,0.84645134,40.92160811726455,21
3203,"In addition , we find that this approach enjoys stable training compared to a non-RL setting .",3,0.97183955,70.10531206016279,18
3204,This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks .,1,0.76758206,45.699191769855425,15
3204,"We show that these corpora have few negations compared to general-purpose English , and that the few negations in them are often unimportant .",3,0.9437829,36.27945531295411,26
3204,"Indeed , one can often ignore negations and still make the right predictions .",0,0.8678468,90.6473842502209,14
3204,"Additionally , experimental results show that state-of-the-art transformers trained with these corpora obtain substantially worse results with instances that contain negation , especially if the negations are important .",3,0.9728424,28.83495821872089,35
3204,We conclude that new corpora accounting for negation are needed to solve natural language understanding tasks when negation is present .,3,0.9866035,40.407884050599925,21
3205,"In this paper , we challenge the ACL community to reckon with historical and ongoing colonialism by adopting a set of ethical obligations and best practices drawn from the Indigenous studies literature .",1,0.9075763,45.81204520950073,33
3205,"While the vast majority of NLP research focuses on a very small number of very high resource languages ( English , Chinese , etc ) , some work has begun to engage with Indigenous languages .",0,0.91631347,24.855551337191265,36
3205,No research involving Indigenous language data can be considered ethical without first acknowledging that Indigenous languages are not merely very low resource languages .,0,0.7912105,57.870541086475434,24
3205,The toxic legacy of colonialism permeates every aspect of interaction between Indigenous communities and outside researchers .,0,0.9281745,38.97573644464982,17
3205,"To this end , we propose that the ACL draft and adopt an ethical framework for NLP researchers and computational linguists wishing to engage in research involving Indigenous languages .",1,0.60745627,57.975385899529854,30
3206,Pre-trained models have shown very good performances on a number of question answering benchmarks especially when fine-tuned on multiple question answering datasets at once .,0,0.6321206,12.939659983404415,25
3206,"In this work , we propose an approach for generating a fine-tuning dataset thanks to a rule-based algorithm that generates questions and answers from unannotated sentences .",1,0.76247996,23.475363274788688,28
3206,"We show that the state-of-the-art model UnifiedQA can greatly benefit from such a system on a multiple-choice benchmark about physics , biology and chemistry it has never been trained on .",3,0.87666047,31.326636331253063,36
3206,"We further show that improved performances may be obtained by selecting the most challenging distractors ( wrong answers ) , with a dedicated ranker based on a pretrained RoBERTa model .",3,0.94259423,62.14979975433167,31
3207,Deep learning sequence models have been successful with morphological inflection generation .,0,0.8161838,71.03909247743991,12
3207,"The SIGMORPHON shared task results in the past several years indicate that such models can perform well , but only if the training data covers a good amount of different lemmata , or if the lemmata to be inflected at test time have also been seen in training , as has indeed been largely the case in these tasks .",0,0.64351773,43.04900487928551,60
3207,"Surprisingly , we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata — i.e .",3,0.97077054,32.08901395575864,36
3207,under “ wug test ”-like circumstances .,3,0.5338225,527.9479341185057,9
3207,This is true even though the actual number of training examples is very large .,3,0.5941866,16.381998987429217,15
3207,"While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand , our experiment results show that , to be more effective , the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters .",3,0.978215,64.37914058395685,62
3208,This paper introduces an adversarial method to stress-test trained metrics for the evaluation of conversational dialogue systems .,1,0.8317631,37.687824034185155,18
3208,The method leverages Reinforcement Learning to find response strategies that elicit optimal scores from the trained metrics .,2,0.7204615,65.84698121825602,18
3208,We apply our method to test recently proposed trained metrics .,2,0.6317731,140.26692981909466,11
3208,We find that they all are susceptible to giving high scores to responses generated by rather simple and obviously flawed strategies that our method converges on .,3,0.97136474,107.91362705727481,27
3208,"For instance , simply copying parts of the conversation context to form a response yields competitive scores or even outperforms responses written by humans .",0,0.54711366,92.20615364855765,25
3209,Distinct is a widely used automatic metric for evaluating diversity in language generation tasks .,0,0.77965784,27.941863473097907,15
3209,"However , we observed that the original approach to calculating distinct scores has evident biases that tend to assign higher penalties to longer sequences .",3,0.9694735,103.66625999151937,25
3209,We refine the calculation of distinct scores by scaling the number of distinct tokens based on their expectations .,2,0.7522167,66.16537358843843,19
3209,We provide both empirical and theoretical evidence to show that our method effectively removes the biases existing in the original distinct score .,3,0.76478106,43.33927124858485,23
3209,"Our experiments show that our proposed metric , Expectation-Adjusted Distinct ( EAD ) , correlates better with human judgment in evaluating response diversity .",3,0.97624844,49.039298850247356,26
3209,"To assist future research , we provide an example implementation at https://github.com/lsy641/Expectation-Adjusted-Distinct .",3,0.7267117,40.941184280368056,13
3210,"As privacy gains traction in the NLP community , researchers have started adopting various approaches to privacy-preserving methods .",0,0.9464454,33.473093904222026,21
3210,"One of the favorite privacy frameworks , differential privacy ( DP ) , is perhaps the most compelling thanks to its fundamental theoretical guarantees .",0,0.92905915,146.33025237401256,25
3210,"Despite the apparent simplicity of the general concept of differential privacy , it seems non-trivial to get it right when applying it to NLP .",0,0.641396,28.079883882450442,25
3210,"In this short paper , we formally analyze several recent NLP papers proposing text representation learning using DPText ( Beigi et al. , 2019a , b ; Alnasser et al. , 2021 ; Beigi et al. , 2021 ) and reveal their false claims of being differentially private .",1,0.7000871,68.16465751758983,49
3210,"Furthermore , we also show a simple yet general empirical sanity check to determine whether a given implementation of a DP mechanism almost certainly violates the privacy loss guarantees .",3,0.6697889,86.31793313152839,30
3210,Our main goal is to raise awareness and help the community understand potential pitfalls of applying differential privacy to text representation learning .,1,0.82726395,45.5000316805304,23
3211,"We consider the task of document-level entity linking ( EL ) , where it is important to make consistent decisions for entity mentions over the full document jointly .",0,0.37034553,50.18837395250747,30
3211,We aim to leverage explicit “ connections ” among mentions within the document itself : we propose to join EL and coreference resolution ( coref ) in a single structured prediction task over directed trees and use a globally normalized model to solve it .,2,0.60373837,171.54595806006014,45
3211,This contrasts with related works where two separate models are trained for each of the tasks and additional logic is required to merge the outputs .,0,0.47264838,35.06529433427342,26
3211,"Experimental results on two datasets show a boost of up to + 5 % F1-score on both coref and EL tasks , compared to their standalone counterparts .",3,0.92839605,44.52652679161969,30
3211,"For a subset of hard cases , with individual mentions lacking the correct EL in their candidate entity list , we obtain a + 50 % increase in accuracy .",3,0.9146453,196.02021516538218,30
3212,We present an efficient BERT-based multi-task ( MT ) framework that is particularly suitable for iterative and incremental development of the tasks .,1,0.5378399,33.5879799197686,25
3212,"The proposed framework is based on the idea of partial fine-tuning , i.e .",2,0.46441704,15.11808694694825,14
3212,only fine-tune some top layers of BERT while keep the other layers frozen .,3,0.6141222,44.57911739940165,16
3212,"For each task , we train independently a single-task ( ST ) model using partial fine-tuning .",2,0.8592077,67.74661775869848,18
3212,Then we compress the task-specific layers in each ST model using knowledge distillation .,2,0.8537645,56.651239758657596,14
3212,Those compressed ST models are finally merged into one MT model so that the frozen layers of the former are shared across the tasks .,2,0.6122167,83.68287677832521,25
3212,"We exemplify our approach on eight GLUE tasks , demonstrating that it is able to achieve 99.6 % of the performance of the full fine-tuning method , while reducing up to two thirds of its overhead .",3,0.7636294,28.31553886212103,38
3213,We present a new dataset containing 10 K human-annotated games of Go and show how these natural language annotations can be used as a tool for model interpretability .,3,0.5676588,32.517724093108626,29
3213,"Given a board state and its associated comment , our approach uses linear probing to predict mentions of domain-specific terms ( e.g. , ko , atari ) from the intermediate state representations of game-playing agents like AlphaGo Zero .",2,0.7886198,117.60758535453581,41
3213,"We find these game concepts are nontrivially encoded in two distinct policy networks , one trained via imitation learning and another trained via reinforcement learning .",3,0.9072677,75.10561627779279,26
3213,"Furthermore , mentions of domain-specific terms are most easily predicted from the later layers of both models , suggesting that these policy networks encode high-level abstractions similar to those used in the natural language annotations .",3,0.9703254,60.35006528200731,38
3214,Automatic ICD coding is defined as assigning disease codes to electronic medical records ( EMRs ) .,0,0.8503847,65.6830040871652,17
3214,Existing methods usually apply label attention with code representations to match related text snippets .,0,0.8263389,179.44212176971317,15
3214,"Unlike these works that model the label with the code hierarchy or description , we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD .",3,0.7222617,81.4035068710499,42
3214,"By aligning codes to concepts in UMLS , we collect synonyms of every code .",2,0.6685975,104.07353665988965,15
3214,"Then , we propose a multiple synonyms matching network to leverage synonyms for better code representation learning , and finally help the code classification .",2,0.5275961,101.54455864243775,25
3214,Experiments on the MIMIC-III dataset show that our proposed method outperforms previous state-of-the-art methods .,3,0.9402453,5.488024159295022,20
3215,"Pretrained language models ( PLMs ) have achieved superhuman performance on many benchmarks , creating a need for harder tasks .",0,0.9417184,45.45875187996737,21
3215,"We introduce CoDA21 ( Context Definition Alignment ) , a challenging benchmark that measures natural language understanding ( NLU ) capabilities of PLMs : Given a definition and a context each for k words , but not the words themselves , the task is to align the k definitions with the k contexts .",2,0.5466142,73.79929384431414,54
3215,"CoDA21 requires a deep understanding of contexts and definitions , including complex inference and world knowledge .",0,0.6524002,203.57613019235902,17
3215,"We find that there is a large gap between human and PLM performance , suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks .",3,0.98329955,41.486794653359865,31
3216,Recent active learning ( AL ) approaches in Natural Language Processing ( NLP ) proposed using off-the-shelf pretrained language models ( LMs ) .,0,0.913054,18.479054426208013,26
3216,"In this paper , we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue .",1,0.8632534,41.11636045725387,28
3216,We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL .,3,0.85057235,46.897261333912205,28
3216,We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL .,3,0.66281164,30.117008951832997,28
3216,"Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach , suggesting that a poor training strategy can be catastrophic for AL .",3,0.9685648,34.00487986519917,30
3217,"In this paper , we leverage large language models ( LLMs ) to perform zero-shot text style transfer .",1,0.48009667,37.435381665378,19
3217,"We present a prompting method that we call augmented zero-shot learning , which frames style transfer as a sentence rewriting task and requires only a natural language instruction , without model fine-tuning or exemplars in the target style .",2,0.5098043,71.35590975347188,39
3217,"Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment , but also on arbitrary transformations such as ‘ make this melodramatic ’ or ‘ insert a metaphor .’ .",3,0.8681574,60.03778061558046,39
3218,Our goal is to study the novel task of distant supervision for multilingual relation extraction ( Multi DS-RE ) .,1,0.9119064,56.976710315580256,22
3218,Research in Multi DS-RE has remained limited due to the absence of a reliable benchmarking dataset .,0,0.93805736,46.946741770626765,19
3218,"The only available dataset for this task , RELX-Distant ( Köksal and Özgür , 2020 ) , displays several unrealistic characteristics , leading to a systematic overestimation of model performance .",0,0.5800809,87.17818771861802,33
3218,"To alleviate these concerns , we release a new benchmark dataset for the task , named DiS-ReX .",2,0.60947347,51.84765255903008,20
3218,We also modify the widely-used bag attention models using an mBERT encoder and provide the first baseline results on the proposed task .,3,0.5013715,55.592287173931425,25
3218,"We show that DiS-ReX serves as a more challenging dataset than RELX-Distant , leaving ample room for future research in this domain .",3,0.9794074,59.6426427142116,27
3219,"In the domain of Morphology , Inflection is a fundamental and important task that gained a lot of traction in recent years , mostly via SIGMORPHON ’s shared-tasks .",0,0.91256386,59.99365208772706,31
3219,"With average accuracy above 0.9 over the scores of all languages , the task is considered mostly solved using relatively generic neural seq2seq models , even with little data provided .",3,0.84220225,75.17071696682831,31
3219,"In this work , we propose to re-evaluate morphological inflection models by employing harder train-test splits that will challenge the generalization capacity of the models .",1,0.8249722,45.731617488894976,26
3219,"In particular , as opposed to the naïve split-by-form , we propose a split-by-lemma method to challenge the performance on existing benchmarks .",2,0.66848236,51.0373552899123,27
3219,Our experiments with the three top-ranked systems on the SIGMORPHON ’s 2020 shared-task show that the lemma-split presents an average drop of 30 percentage points in macro-average for the 90 languages included .,3,0.9535729,95.87026877725464,35
3219,"The effect is most significant for low-resourced languages with a drop as high as 95 points , but even high-resourced languages lose about 10 points on average .",3,0.9517231,26.065838852830318,29
3219,"Our results clearly show that generalizing inflection to unseen lemmas is far from being solved , presenting a simple yet effective means to promote more sophisticated models .",3,0.9903832,40.9722755464713,28
3220,"Before entering the neural network , a token needs to be converted to its one-hot representation , which is a discrete distribution of the vocabulary .",0,0.39400145,49.99500848224786,27
3220,"Smoothed representation is the probability of candidate tokens obtained from the pre-trained masked language model , which can be seen as a more informative augmented substitution to the one-hot representation .",3,0.4452588,48.60947097674224,31
3220,"We propose an efficient data augmentation method , dub as text smoothing , by converting a sentence from its one-hot representation to controllable smoothed representation .",2,0.5140501,65.34060008972963,27
3220,We evaluate text smoothing on different datasets in a low-resource regime .,2,0.7374298,44.633813249397136,12
3220,Experimental results show that text smoothing outperforms various mainstream data augmentation methods by a substantial margin .,3,0.96363926,15.117124594019288,17
3220,"Moreover , text smoothing can be combined with these data augmentation methods to achieve better performance .",3,0.78412575,27.750057047763068,17
