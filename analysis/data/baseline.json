[{"_id": {"$oid": "638edcdf8cc249c55a54edc5"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:10:39.012Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "type": "task", "writing_model": "", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638edce88cc249c55a54edc6"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:10:48.754Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638edcf98cc249c55a54edc8"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:11:05.111Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "type": "task", "writing_model": "", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638edcfe8cc249c55a54edca"}, "user_id": "", "time": {"$date": "2022-12-06T06:11:10.092Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638edd068cc249c55a54edcc"}, "user_id": "", "time": {"$date": "2022-12-06T06:11:18.096Z"}, "text": "{\"writingInput\":[\"Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"2\",\"2\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638edd098cc249c55a54edcd"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:11:21.134Z"}, "text": "Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .The improved interpretability is believed to offer a sense of security by involving human in the decision-making process .Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .We show that existing IDLSes are highly vulnerable to adversarial manipulations .Specifically , we present ADV2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models .Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications ( e.g. , skin cancer diagnosis ) , we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input 's prediction and interpretation .Further , with both analytical and empirical evidence , we identify the prediction-interpretation gap as one root cause of this vulnerability-a DNN and its interpretation model are often misaligned , resulting in the possibility of exploiting both models simultaneously .Finally , we explore potential countermeasures against ADV2 , including leveraging its low transferability and incorporating it in an adversarial training framework .Our findings shed light on designing and operating IDLSes in a more secure and informative fashion , leading to several promising research directions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638edf388cc249c55a54edce"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:20:40.622Z"}, "text": "\n", "event_type": "auto-save"}, {"_id": {"$oid": "638edf6d8cc249c55a54edd0"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:21:33.487Z"}, "text": "Socially aware robots should be able, among others, to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved. Towards enhancing mutual performance, collaborative robots should be equipped with adaptation and learning capabilities. However, co-learning can be a time consuming procedure. For this reason, transferring knowledge from an expert could potentially boost the overall team performance. In the present study, transfer learning was integrated in a deep Reinforcement Learning (dRL) agent. In a real-time and real-world set-up, two groups of participants had to collaborate with a cobot under two different conditions of dRL agents; one that was transferring knowledge and one that did not. A probabilistic policy reuse method was used for the transfer learning (TL). The results showed that there was a significant difference between the performance of the two groups; TL halved the time needed for the training of new participants to the task. Moreover, TL also affected the subjective performance of the teams and enhanced the perceived fluency. Finally, in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638edf718cc249c55a54edd2"}, "user_id": "", "time": {"$date": "2022-12-06T06:21:37.912Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638edf7d8cc249c55a54edd3"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:21:49.657Z"}, "text": "Socially aware robots should be able , among others , to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .In the present study , transfer learning was integrated in a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ee06c8cc249c55a54edd5"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:25:48.465Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as nested entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and three observations: (1) generalizing to new document templates is very challenging, (2) few-shot performance has a lot of headroom, and (3) models struggle with nested fields such as lineitems in an invoice. We plan to open source the benchmark and the evaluation toolkit. We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638ee0708cc249c55a54edd7"}, "user_id": "", "time": {"$date": "2022-12-06T06:25:52.932Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638ee07c8cc249c55a54edd8"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:26:04.309Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as lineitems in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ee27d8cc249c55a54edda"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:34:37.742Z"}, "text": "Applying deep learning to solve real-life instances of hard combinatorial problems has tremendous potential. Research in this direction has focused on the Boolean satisfiability (SAT) problem, both because of its theoretical centrality and practical importance. A major roadblock faced, though, is that training sets are restricted to random formulas of size several orders of magnitude smaller than formulas of practical interest, raising serious concerns about generalization. This is because labeling random formulas of increasing size rapidly becomes intractable. By exploiting the probabilistic method in a fundamental way, we remove this roadblock entirely: we show how to generate correctly labeled random formulas of any desired size, without having to solve the underlying decision problem. Moreover, the difficulty of the classification task for the formulas produced by our generator is tunable by varying a simple scalar parameter. This opens up an entirely new level of sophistication for the machine learning methods that can be brought to bear on Satisfiability. Using our generator, we train existing state-of-the-art models for the task of predicting satisfiability on formulas with 10,000 variables. We find that they do no better than random guessing. As a first indication of what can be achieved with the new generator, we present a novel classifier that performs significantly better than random guessing (99%) on the same datasets, for most difficulty levels. Crucially, unlike past approaches that learn based on syntactic features of a formula, our classifier performs its learning on a short prefix of a solver\u2019s computation, an approach that we expect to be of independent interest.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638ee2828cc249c55a54eddc"}, "user_id": "", "time": {"$date": "2022-12-06T06:34:42.684Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638ee2908cc249c55a54eddd"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:34:56.150Z"}, "text": "Applying deep learning to solve real-life instances of hard combinatorial problems has tremendous potential .Research in this direction has focused on the Boolean satisfiability ( SAT ) problem , both because of its theoretical centrality and practical importance .A major roadblock faced , though , is that training sets are restricted to random formulas of size several orders of magnitude smaller than formulas of practical interest , raising serious concerns about generalization .This is because labeling random formulas of increasing size rapidly becomes intractable .By exploiting the probabilistic method in a fundamental way , we remove this roadblock entirely : we show how to generate correctly labeled random formulas of any desired size , without having to solve the underlying decision problem .Moreover , the difficulty of the classification task for the formulas produced by our generator is tunable by varying a simple scalar parameter .This opens up an entirely new level of sophistication for the machine learning methods that can be brought to bear on Satisfiability .Using our generator , we train existing state-of-the-art models for the task of predicting satisfiability on formulas with 10,000 variables .We find that they do no better than random guessing .As a first indication of what can be achieved with the new generator , we present a novel classifier that performs significantly better than random guessing ( 99 % ) on the same datasets , for most difficulty levels .Crucially , unlike past approaches that learn based on syntactic features of a formula , our classifier performs its learning on a short prefix of a solver \u2019s computation , an approach that we expect to be of independent interest .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ee2e78cc249c55a54eddf"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:36:23.890Z"}, "text": "Applying deep learning to solve real-life instances of hard combinatorial problems has tremendous potential .Research in this direction has focused on the Boolean satisfiability ( SAT ) problem , both because of its theoretical centrality and practical importance .A major roadblock faced , though , is that training sets are restricted to random formulas of size several orders of magnitude smaller than formulas of practical interest , raising serious concerns about generalization .This is because labeling random formulas of increasing size rapidly becomes intractable .By exploiting the probabilistic method in a fundamental way , we remove this roadblock entirely : we show how to generate correctly labeled random formulas of any desired size , without having to solve the underlying decision problem .Moreover , the difficulty of the classification task for the formulas produced by our generator is tunable by varying a simple scalar parameter .This opens up an entirely new level of sophistication for the machine learning methods that can be brought to bear on Satisfiability .Using our generator , we train existing state-of-the-art models for the task of predicting satisfiability on formulas with 10,000 variables .We find that they do no better than random guessing .As a first indication of what can be achieved with the new generator , we present a novel classifier that performs significantly better than random guessing ( 99 % ) on the same datasets , for most difficulty levels .Crucially , unlike past approaches that learn based on syntactic features of a formula , our classifier performs its learning on a short prefix of a solver \u2019s computation , an approach that we expect to be of independent interest .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638ee2ec8cc249c55a54ede1"}, "user_id": "", "time": {"$date": "2022-12-06T06:36:28.942Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638ee2f78cc249c55a54ede2"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:36:39.861Z"}, "text": "Applying deep learning to solve real-life instances of hard combinatorial problems has tremendous potential .Research in this direction has focused on the Boolean satisfiability ( SAT ) problem , both because of its theoretical centrality and practical importance .A major roadblock faced , though , is that training sets are restricted to random formulas of size several orders of magnitude smaller than formulas of practical interest , raising serious concerns about generalization .This is because labeling random formulas of increasing size rapidly becomes intractable .By exploiting the probabilistic method in a fundamental way , we remove this roadblock entirely : we show how to generate correctly labeled random formulas of any desired size , without having to solve the underlying decision problem .Moreover , the difficulty of the classification task for the formulas produced by our generator is tunable by varying a simple scalar parameter .This opens up an entirely new level of sophistication for the machine learning methods that can be brought to bear on Satisfiability .Using our generator , we train existing state-of-the-art models for the task of predicting satisfiability on formulas with 10,000 variables .We find that they do no better than random guessing .As a first indication of what can be achieved with the new generator , we present a novel classifier that performs significantly better than random guessing ( 99 % ) on the same datasets , for most difficulty levels .Crucially , unlike past approaches that learn based on syntactic features of a formula , our classifier performs its learning on a short prefix of a solver \u2019s computation , an approach that we expect to be of independent interest .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ee3728cc249c55a54ede4"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:38:42.197Z"}, "text": "Reinforcement Learning (RL) algorithms are known to scale poorly to environments with many available actions, requiring numerous samples to learn an optimal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as inapplicable actions (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm. In this paper, we propose a more systematic approach to introduce this knowledge into the algorithm. We (i) standardize the way knowledge can be manually specified to the agent; and (ii) present a new framework to autonomously learn these statedependent action constraints jointly with the policy. We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algorithm by providing a reliable signal to mask out irrelevant actions. Moreover, we demonstrate that thanks to the transferability of the knowledge acquired, it can be reused in other tasks to make the learning process more efficient.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638ee3778cc249c55a54ede6"}, "user_id": "", "time": {"$date": "2022-12-06T06:38:47.064Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638edcd78cc249c55a54edc3"}, {"_id": {"$oid": "638ee3828cc249c55a54ede7"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T06:38:58.735Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these statedependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algorithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f6c0ac3fa89c878d6f25e"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T16:21:30.673Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "type": "task", "writing_model": "", "conversation_id": "638f6c03c3fa89c878d6f25c"}, {"_id": {"$oid": "638f6c10c3fa89c878d6f260"}, "user_id": "", "time": {"$date": "2022-12-06T16:21:36.899Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c03c3fa89c878d6f25c"}, {"_id": {"$oid": "638f6c1ac3fa89c878d6f261"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T16:21:46.908Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users .This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .We found that although users are interested in explanations for the road not taken \u2014 namely , why the model chose one result and not a well-defined , seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f6c23c3fa89c878d6f263"}, "user_id": "", "time": {"$date": "2022-12-06T16:21:55.185Z"}, "text": "{\"writingInput\":[\"This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"1\",\"1\",\"1\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c03c3fa89c878d6f25c"}, {"_id": {"$oid": "638f74a6c3fa89c878d6f266"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:58:14.856Z"}, "text": "Reinforcement Learning (RL) algorithms are known to scale poorly to environ- ments with many available actions, requiring numerous samples to learn an opti- mal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as\u00a0inapplicable actions\u00a0(i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL al- gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm. In this paper, we propose a more systematic approach to introduce this knowledge into the algorithm. We (i) standardize the way knowledge can be manually specified to the agent; and (ii) present a new framework to autonomously learn these state- dependent action constraints jointly with the policy. We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo- rithm by providing a reliable signal to mask out irrelevant actions. Moreover, we demonstrate that thanks to the transferability of the knowledge acquired, it can be reused in other tasks to make the learning process more efficient.\n", "type": "task", "writing_model": "", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f74acc3fa89c878d6f268"}, "user_id": "", "time": {"$date": "2022-12-06T16:58:20.673Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f74b6c3fa89c878d6f269"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:58:30.672Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ-ments with many available actions , requiring numerous samples to learn an opti-mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL al-gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f7593c3fa89c878d6f26b"}, "user_id": "", "time": {"$date": "2022-12-06T17:02:11.208Z"}, "text": "{\"writingInput\":[\"The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"1\",\"1\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f76afc3fa89c878d6f26c"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T17:06:55.573Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions (such as inapplicable ones), the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL al-gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f76cdc3fa89c878d6f26d"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T17:07:25.037Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions (such as inapplicable ones), the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward.Knowing this information can help reduce the sample complexity of RL al-gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f76ebc3fa89c878d6f26f"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T17:07:55.611Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions (such as inapplicable ones), the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward.Knowing this information can help reduce the sample complexity of RL al-gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f76f1c3fa89c878d6f271"}, "user_id": "", "time": {"$date": "2022-12-06T17:08:01.479Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f76fbc3fa89c878d6f272"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T17:08:11.478Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions ( such as inapplicable ones ) , the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward .Knowing this information can help reduce the sample complexity of RL al-gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f771bc3fa89c878d6f274"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T17:08:43.174Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions ( such as inapplicable ones ) , the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward .Knowing this information can help reduce the sample complexity of RL al-gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f7720c3fa89c878d6f276"}, "user_id": "", "time": {"$date": "2022-12-06T17:08:48.473Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f7739c3fa89c878d6f278"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T17:09:13.698Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions, the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward .Knowing this information can help reduce the sample complexity of RL al-gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f773fc3fa89c878d6f27a"}, "user_id": "", "time": {"$date": "2022-12-06T17:09:19.177Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f7754c3fa89c878d6f27b"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T17:09:40.250Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions , the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward .Knowing this information can help reduce the sample complexity of RL al-gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f777ac3fa89c878d6f27c"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T17:10:18.904Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions (such as inapplicable actions), the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward .Knowing this information can help reduce the sample complexity of RL al-gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f77b6c3fa89c878d6f27e"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T17:11:18.406Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions (such as inapplicable actions), the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f77bac3fa89c878d6f280"}, "user_id": "", "time": {"$date": "2022-12-06T17:11:22.982Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f748dc3fa89c878d6f264"}, {"_id": {"$oid": "638f77c4c3fa89c878d6f281"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T17:11:32.981Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions ( such as inapplicable actions ) , the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9429c3fa89c878d6f284"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T19:12:41.302Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "type": "task", "writing_model": "", "conversation_id": "638f7bccc3fa89c878d6f282"}, {"_id": {"$oid": "638f942fc3fa89c878d6f286"}, "user_id": "", "time": {"$date": "2022-12-06T19:12:47.805Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f7bccc3fa89c878d6f282"}, {"_id": {"$oid": "638f9439c3fa89c878d6f287"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T19:12:57.836Z"}, "text": "Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .The improved interpretability is believed to offer a sense of security by involving human in the decision-making process .Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .We show that existing IDLSes are highly vulnerable to adversarial manipulations .Specifically , we present ADV2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models .Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications ( e.g. , skin cancer diagnosis ) , we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input 's prediction and interpretation .Further , with both analytical and empirical evidence , we identify the prediction-interpretation gap as one root cause of this vulnerability-a DNN and its interpretation model are often misaligned , resulting in the possibility of exploiting both models simultaneously .Finally , we explore potential countermeasures against ADV2 , including leveraging its low transferability and incorporating it in an adversarial training framework .Our findings shed light on designing and operating IDLSes in a more secure and informative fashion , leading to several promising research directions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9440c3fa89c878d6f289"}, "user_id": "", "time": {"$date": "2022-12-06T19:13:04.580Z"}, "text": "{\"writingInput\":[\"Specifically , we present ADV2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"5\",\"5\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f7bccc3fa89c878d6f282"}, {"_id": {"$oid": "638f94a8c3fa89c878d6f28c"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:14:48.372Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision\u0002making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return\u0002conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "type": "task", "writing_model": "", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f94adc3fa89c878d6f28d"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:14:53.515Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision\u0002making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return\u0002conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f94b3c3fa89c878d6f28f"}, "user_id": "", "time": {"$date": "2022-12-06T19:14:59.702Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f94bdc3fa89c878d6f290"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:15:09.704Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision \u0002making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f950cc3fa89c878d6f292"}, "user_id": "", "time": {"$date": "2022-12-06T19:16:28.696Z"}, "text": "{\"writingInput\":[\"We investigate whether these methods can directly address the problem of sequential decision \\u0002making .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"1\",\"1\",\"1\",\"1\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f9615c3fa89c878d6f293"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:20:53.265Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .\nWe investigate whether these methods can directly address the problem of sequential decision \u0002making .\nTo our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9650c3fa89c878d6f295"}, "user_id": "", "time": {"$date": "2022-12-06T19:21:52.830Z"}, "text": "{\"writingInput\":[\"We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .\",\"To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\",\"Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"2\",\"3\",\"6\",\"3\",\"6\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f96cec3fa89c878d6f297"}, "user_id": "", "time": {"$date": "2022-12-06T19:23:58.677Z"}, "text": "{\"writingInput\":[\"Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"6\",\"6\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f972cc3fa89c878d6f298"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:25:32.881Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .\nWe investigate whether these methods can directly address the problem of sequential decision \u0002making .\nTo our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We\u00a0find\u00a0that\u00a0conditioning\u00a0on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills.Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9752c3fa89c878d6f29a"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:26:10.333Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .\nWe investigate whether these methods can directly address the problem of sequential decision \u0002making .\nTo our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We\u00a0find\u00a0that\u00a0conditioning\u00a0on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills.Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f9758c3fa89c878d6f29c"}, "user_id": "", "time": {"$date": "2022-12-06T19:26:16.335Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f976bc3fa89c878d6f29d"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:26:35.486Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these methods can directly address the problem of sequential decision \u0002making .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f97ccc3fa89c878d6f29e"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:28:12.895Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision \u0002making .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f97d1c3fa89c878d6f2a0"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:28:17.060Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision \u0002making .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f97d6c3fa89c878d6f2a2"}, "user_id": "", "time": {"$date": "2022-12-06T19:28:22.914Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f97e5c3fa89c878d6f2a3"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:28:37.438Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision \u0002making .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9828c3fa89c878d6f2a5"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:29:44.545Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision \u0002making for further improved performance in terms of generating quality samples.To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f982ec3fa89c878d6f2a7"}, "user_id": "", "time": {"$date": "2022-12-06T19:29:50.576Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f9838c3fa89c878d6f2a8"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:30:00.572Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision \u0002making for further improved performance in terms of generating quality samples .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9839c3fa89c878d6f2aa"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:30:01.035Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision \u0002making for further improved performance in terms of generating quality samples .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f983fc3fa89c878d6f2ac"}, "user_id": "", "time": {"$date": "2022-12-06T19:30:07.196Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f984dc3fa89c878d6f2ad"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:30:21.942Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision \u0002making for further improved performance in terms of generating quality samples .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f985fc3fa89c878d6f2ae"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:30:39.893Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision \u0002making for further improved performance in terms of generating quality samples.To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f986fc3fa89c878d6f2b0"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:30:55.320Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision \u0002making for further improved performance in terms of generating quality samples.To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f9875c3fa89c878d6f2b2"}, "user_id": "", "time": {"$date": "2022-12-06T19:31:01.910Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f9496c3fa89c878d6f28a"}, {"_id": {"$oid": "638f987fc3fa89c878d6f2b3"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:31:11.909Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision \u0002making for further improved performance in terms of generating quality samples .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return \u0002conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fbc90351e28e6bd441a70"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:05:04.132Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "event_type": "save"}, {"_id": {"$oid": "638fbcaa351e28e6bd441a71"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:05:30.928Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "event_type": "save"}, {"_id": {"$oid": "638fbcbd6934d0d99fffbae5"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:05:49.361Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "save"}, {"_id": {"$oid": "638fbcca6934d0d99fffbae6"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:06:02.499Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "save"}, {"_id": {"$oid": "638fbd206934d0d99fffbae8"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:07:28.494Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "save"}, {"_id": {"$oid": "638fbd2c6934d0d99fffbaea"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:07:40.699Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "638fbd1c6934d0d99fffbae7"}, {"_id": {"$oid": "638fbd326934d0d99fffbaec"}, "user_id": "", "time": {"$date": "2022-12-06T22:07:46.644Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fbd1c6934d0d99fffbae7"}, {"_id": {"$oid": "638fbd426934d0d99fffbaed"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:08:02.001Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fbd476934d0d99fffbaef"}, "user_id": "", "time": {"$date": "2022-12-06T22:08:07.744Z"}, "text": "{\"writingInput\":[\"We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"4\",\"4\",\"4\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fbd1c6934d0d99fffbae7"}, {"_id": {"$oid": "638fbd7e6934d0d99fffbaf1"}, "user_id": "", "time": {"$date": "2022-12-06T22:09:02.481Z"}, "text": "{\"writingInput\":[\"We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"4\",\"4\",\"4\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fbd1c6934d0d99fffbae7"}, {"_id": {"$oid": "638fbf696934d0d99fffbaf4"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:17:13.326Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "type": "task", "writing_model": "", "conversation_id": "638fbf636934d0d99fffbaf2"}, {"_id": {"$oid": "638fbf716934d0d99fffbaf5"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:17:21.421Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638fbf766934d0d99fffbaf7"}, "user_id": "", "time": {"$date": "2022-12-06T22:17:26.191Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fbf636934d0d99fffbaf2"}, {"_id": {"$oid": "638fbf7e6934d0d99fffbaf9"}, "user_id": "", "time": {"$date": "2022-12-06T22:17:34.762Z"}, "text": "{\"writingInput\":[\"Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"2\",\"2\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fbf636934d0d99fffbaf2"}, {"_id": {"$oid": "638fbf806934d0d99fffbafa"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:17:36.191Z"}, "text": "Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .The improved interpretability is believed to offer a sense of security by involving human in the decision-making process .Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .We show that existing IDLSes are highly vulnerable to adversarial manipulations .Specifically , we present ADV2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models .Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications ( e.g. , skin cancer diagnosis ) , we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input 's prediction and interpretation .Further , with both analytical and empirical evidence , we identify the prediction-interpretation gap as one root cause of this vulnerability-a DNN and its interpretation model are often misaligned , resulting in the possibility of exploiting both models simultaneously .Finally , we explore potential countermeasures against ADV2 , including leveraging its low transferability and incorporating it in an adversarial training framework .Our findings shed light on designing and operating IDLSes in a more secure and informative fashion , leading to several promising research directions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fc1cc6934d0d99fffbafd"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:27:24.649Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "type": "task", "writing_model": "", "conversation_id": "638fc1c76934d0d99fffbafb"}, {"_id": {"$oid": "638fc1d56934d0d99fffbafe"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:27:33.284Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638fc1da6934d0d99fffbb00"}, "user_id": "", "time": {"$date": "2022-12-06T22:27:38.149Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fc1c76934d0d99fffbafb"}, {"_id": {"$oid": "638fc1e46934d0d99fffbb01"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:27:48.779Z"}, "text": "Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .The improved interpretability is believed to offer a sense of security by involving human in the decision-making process .Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .We show that existing IDLSes are highly vulnerable to adversarial manipulations .Specifically , we present ADV2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models .Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications ( e.g. , skin cancer diagnosis ) , we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input 's prediction and interpretation .Further , with both analytical and empirical evidence , we identify the prediction-interpretation gap as one root cause of this vulnerability-a DNN and its interpretation model are often misaligned , resulting in the possibility of exploiting both models simultaneously .Finally , we explore potential countermeasures against ADV2 , including leveraging its low transferability and incorporating it in an adversarial training framework .Our findings shed light on designing and operating IDLSes in a more secure and informative fashion , leading to several promising research directions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fc1eb6934d0d99fffbb03"}, "user_id": "", "time": {"$date": "2022-12-06T22:27:55.508Z"}, "text": "{\"writingInput\":[\"Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications ( e.g. , skin cancer diagnosis ) , we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input 's prediction and interpretation .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"6\",\"6\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fc1c76934d0d99fffbafb"}, {"_id": {"$oid": "638fc31c6934d0d99fffbb06"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:33:00.466Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "type": "task", "writing_model": "", "conversation_id": "638fc3186934d0d99fffbb04"}, {"_id": {"$oid": "638fc3256934d0d99fffbb07"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:33:09.997Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638fc32a6934d0d99fffbb09"}, "user_id": "", "time": {"$date": "2022-12-06T22:33:14.104Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fc3186934d0d99fffbb04"}, {"_id": {"$oid": "638fc3346934d0d99fffbb0a"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:33:24.997Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users .This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .We found that although users are interested in explanations for the road not taken \u2014 namely , why the model chose one result and not a well-defined , seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fc37d6934d0d99fffbb0c"}, "user_id": "", "time": {"$date": "2022-12-06T22:34:37.290Z"}, "text": "{\"writingInput\":[\"We found that although users are interested in explanations for the road not taken \u2014 namely , why the model chose one result and not a well-defined , seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"2\",\"2\",\"2\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fc3186934d0d99fffbb04"}, {"_id": {"$oid": "638fc49e6934d0d99fffbb0d"}, "user_id": "CY", "time": {"$date": "2022-12-06T22:39:26.417Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users .This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .We found that although users are interested in explanations for the road not taken \u2014 namely , why the model chose one result and not a well-defined , seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions .\n", "event_type": "save"}, {"_id": {"$oid": "638fc98ed55bde2acd79f82c"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T23:00:30.679Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "638fc988d55bde2acd79f82a"}, {"_id": {"$oid": "638fc997d55bde2acd79f82d"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T23:00:39.691Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fc998d55bde2acd79f82f"}, "user_id": "", "time": {"$date": "2022-12-06T23:00:40.067Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fc988d55bde2acd79f82a"}, {"_id": {"$oid": "638fc9a3d55bde2acd79f831"}, "user_id": "", "time": {"$date": "2022-12-06T23:00:51.522Z"}, "text": "{\"writingInput\":[\"Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .\",\"Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"0\",\"3\",\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fc988d55bde2acd79f82a"}, {"_id": {"$oid": "638fc9aad55bde2acd79f832"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T23:00:58.695Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390bb6cb41eedf55b556619"}, "user_id": "Hua", "time": {"$date": "2022-12-07T16:12:28.542Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "type": "task", "writing_model": "", "conversation_id": "6390bb67b41eedf55b556617"}, {"_id": {"$oid": "6390bb76b41eedf55b55661a"}, "user_id": "Hua", "time": {"$date": "2022-12-07T16:12:38.014Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "6390bb77b41eedf55b55661c"}, "user_id": "", "time": {"$date": "2022-12-07T16:12:39.014Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bb67b41eedf55b556617"}, {"_id": {"$oid": "6390bb81b41eedf55b55661d"}, "user_id": "Hua", "time": {"$date": "2022-12-07T16:12:49.019Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users .This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .We found that although users are interested in explanations for the road not taken \u2014 namely , why the model chose one result and not a well-defined , seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390bb84b41eedf55b55661f"}, "user_id": "", "time": {"$date": "2022-12-07T16:12:52.962Z"}, "text": "{\"writingInput\":[\"This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"1\",\"1\",\"1\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bb67b41eedf55b556617"}, {"_id": {"$oid": "6390c671b41eedf55b556622"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:59:29.533Z"}, "text": "Socially aware robots should be able, among others, to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved. Towards enhancing mutual performance, collaborative robots should be equipped with adaptation and learning capabilities. However, co-learning can be a time consuming procedure. For this reason, transferring knowledge from an expert could potentially boost the overall team performance. In the present study, transfer learning was integrated in a deep Reinforcement Learning (dRL) agent. In a real-time and real-world set-up, two groups of participants had to collaborate with a cobot under two different conditions of dRL agents; one that was transferring knowledge and one that did not. A probabilistic policy reuse method was used for the transfer learning (TL). The results showed that there was a significant difference between the performance of the two groups; TL halved the time needed for the training of new participants to the task. Moreover, TL also affected the subjective performance of the teams and enhanced the perceived fluency. Finally, in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour.\n", "type": "task", "writing_model": "", "conversation_id": "6390c661b41eedf55b556620"}, {"_id": {"$oid": "6390c678b41eedf55b556624"}, "user_id": "", "time": {"$date": "2022-12-07T16:59:36.763Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390c661b41eedf55b556620"}, {"_id": {"$oid": "6390c682b41eedf55b556625"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:59:46.331Z"}, "text": "Socially aware robots should be able , among others , to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .In the present study , transfer learning was integrated in a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c744b41eedf55b556627"}, "user_id": "", "time": {"$date": "2022-12-07T17:03:00.152Z"}, "text": "{\"writingInput\":[\"For this reason , transferring knowledge from an expert could potentially boost the overall team performance .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"3\",\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390c661b41eedf55b556620"}, {"_id": {"$oid": "6390c900b41eedf55b556628"}, "user_id": "p3", "time": {"$date": "2022-12-07T17:10:24.978Z"}, "text": "Socially aware robots should be able , among others , to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .transfer learning was integrated in a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c913b41eedf55b556629"}, "user_id": "p3", "time": {"$date": "2022-12-07T17:10:43.468Z"}, "text": "Socially aware robots should be able , among others , to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .Totransfer learning was integrated in a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c92eb41eedf55b55662a"}, "user_id": "p3", "time": {"$date": "2022-12-07T17:11:10.456Z"}, "text": "Socially aware robots should be able , among others , to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .To achieve a similar goal, we integrate transfer learning a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c93fb41eedf55b55662c"}, "user_id": "p3", "time": {"$date": "2022-12-07T17:11:27.029Z"}, "text": "Socially aware robots should be able , among others , to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .To achieve a similar goal, we integrate transfer learning into a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "6390c661b41eedf55b556620"}, {"_id": {"$oid": "6390c940b41eedf55b55662d"}, "user_id": "p3", "time": {"$date": "2022-12-07T17:11:28.646Z"}, "text": "Socially aware robots should be able , among others , to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .To achieve a similar goal, we integrate transfer learning into a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c945b41eedf55b55662f"}, "user_id": "", "time": {"$date": "2022-12-07T17:11:33.183Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390c661b41eedf55b556620"}, {"_id": {"$oid": "6390c94fb41eedf55b556630"}, "user_id": "p3", "time": {"$date": "2022-12-07T17:11:43.459Z"}, "text": "Socially aware robots should be able , among others , to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .To achieve a similar goal , we integrate transfer learning into a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c9bdb41eedf55b556632"}, "user_id": "", "time": {"$date": "2022-12-07T17:13:33.377Z"}, "text": "{\"writingInput\":[\"Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"1\",\"1\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390c661b41eedf55b556620"}, {"_id": {"$oid": "6390cb3bb41eedf55b556634"}, "user_id": "p3", "time": {"$date": "2022-12-07T17:19:55.559Z"}, "text": "Socially aware robots should be able to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .To achieve a similar goal , we integrate transfer learning into a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "6390c661b41eedf55b556620"}, {"_id": {"$oid": "6390cb3cb41eedf55b556635"}, "user_id": "p3", "time": {"$date": "2022-12-07T17:19:56.487Z"}, "text": "Socially aware robots should be able to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .To achieve a similar goal , we integrate transfer learning into a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390cb43b41eedf55b556637"}, "user_id": "", "time": {"$date": "2022-12-07T17:20:03.177Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390c661b41eedf55b556620"}, {"_id": {"$oid": "6390cb4bb41eedf55b556638"}, "user_id": "p3", "time": {"$date": "2022-12-07T17:20:11.110Z"}, "text": "Socially aware robots should be able to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved .Towards enhancing mutual performance , collaborative robots should be equipped with adaptation and learning capabilities .However , co-learning can be a time consuming procedure .For this reason , transferring knowledge from an expert could potentially boost the overall team performance .To achieve a similar goal , we integrate transfer learning into a deep Reinforcement Learning ( dRL ) agent .In a real-time and real-world set-up , two groups of participants had to collaborate with a cobot under two different conditions of dRL agents ; one that was transferring knowledge and one that did not .A probabilistic policy reuse method was used for the transfer learning ( TL ) .The results showed that there was a significant difference between the performance of the two groups ; TL halved the time needed for the training of new participants to the task .Moreover , TL also affected the subjective performance of the teams and enhanced the perceived fluency .Finally , in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour .\n", "event_type": "save"}, {"_id": {"$oid": "6390f8b8b41eedf55b55663a"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:34:00.475Z"}, "text": "Understanding visually-rich business documents to extract structured data and auto-\nmate business workflows has been receiving\nattention both in academia and industry. Al-\nthough recent multi-modal language models\nhave achieved impressive results, we find that\nexisting benchmarks do not reflect the complexity of real documents seen in industry. In\nthis work, we identify the desiderata for a more\ncomprehensive benchmark and propose one\nwe call Visually Rich Document Understand-\ning (VRDU). VRDU contains two datasets that\nrepresent several challenges: rich schema in-\ncluding diverse data types as well as nested en-\ntities, complex templates including tables and\nmulti-column layouts, and diversity of differ-\nent layouts (templates) within a single docu-\nment type. We design few-shot and conven-\ntional experiment settings along with a care-\nfully designed matching algorithm to evalu-\nate extraction results. We report the perfor-\nmance of strong baselines and three observa-\ntions: (1) generalizing to new document tem-\nplates is very challenging, (2) few-shot perfor-\nmance has a lot of headroom, and (3) mod-\nels struggle with nested fields such as line-\nitems in an invoice. We plan to open source\nthe benchmark and the evaluation toolkit. We\nhope this helps the community make progress\non these challenging tasks in extracting struc-\ntured data from visually rich documents.\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f8bdb41eedf55b55663c"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:34:05.843Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as nested entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and three observations: (1) generalizing to new document templates is very challenging, (2) few-shot performance has a lot of headroom, and (3) models struggle with nested fields such as line-items in an invoice. We plan to open source the benchmark and the evaluation toolkit. We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents.\n\n", "type": "task", "writing_model": "", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390f8c4b41eedf55b55663e"}, "user_id": "", "time": {"$date": "2022-12-07T20:34:12.804Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390f8cdb41eedf55b55663f"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:34:21.553Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f8ffb41eedf55b556641"}, "user_id": "", "time": {"$date": "2022-12-07T20:35:11.577Z"}, "text": "{\"writingInput\":[\"Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .\",\"Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .\",\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\",\"We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\"],\"explainInput\":\"counterfactul\",\"writingIndex\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"2\",\"3\",\"3\",\"4\",\"3\",\"5\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390f901b41eedf55b556643"}, "user_id": "", "time": {"$date": "2022-12-07T20:35:13.841Z"}, "text": "{\"writingInput\":[\"Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .\",\"Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .\",\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\",\"We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\"],\"explainInput\":\"counterfactul\",\"writingIndex\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"2\",\"3\",\"3\",\"4\",\"3\",\"5\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390f903b41eedf55b556645"}, "user_id": "", "time": {"$date": "2022-12-07T20:35:15.560Z"}, "text": "{\"writingInput\":[\"Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .\",\"Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .\",\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\",\"We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"2\",\"3\",\"3\",\"4\",\"3\",\"5\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fb1bb41eedf55b556646"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:44:11.266Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390fb3bb41eedf55b556647"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:44:43.476Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390fb52b41eedf55b556649"}, "user_id": "", "time": {"$date": "2022-12-07T20:45:06.286Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"2\",\"3\",\"3\",\"4\",\"3\",\"5\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fb57b41eedf55b55664b"}, "user_id": "", "time": {"$date": "2022-12-07T20:45:11.022Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\"],\"explainInput\":\"counterfactul\",\"writingIndex\":[\"2\",\"3\",\"3\",\"4\",\"3\",\"5\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fbacb41eedf55b55664d"}, "user_id": "", "time": {"$date": "2022-12-07T20:46:36.072Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\"],\"explainInput\":\"counterfactul\",\"writingIndex\":[\"2\",\"3\",\"3\",\"4\",\"3\",\"5\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fbacb41eedf55b55664f"}, "user_id": "", "time": {"$date": "2022-12-07T20:46:36.296Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\"],\"explainInput\":\"counterfactul\",\"writingIndex\":[\"2\",\"3\",\"3\",\"4\",\"3\",\"5\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fbadb41eedf55b556651"}, "user_id": "", "time": {"$date": "2022-12-07T20:46:37.587Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\"],\"explainInput\":\"counterfactul\",\"writingIndex\":[\"2\",\"3\",\"3\",\"4\",\"3\",\"5\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fbb0b41eedf55b556653"}, "user_id": "", "time": {"$date": "2022-12-07T20:46:40.961Z"}, "text": "{\"writingInput\":[\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\",\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"3\",\"2\",\"4\",\"5\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fbbfb41eedf55b556655"}, "user_id": "", "time": {"$date": "2022-12-07T20:46:55.924Z"}, "text": "{\"writingInput\":[\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\",\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"3\",\"2\",\"4\",\"5\",\"6\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fc01b41eedf55b556657"}, "user_id": "", "time": {"$date": "2022-12-07T20:48:01.190Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"2\",\"4\",\"5\",\"6\",\"3\",\"3\",\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fc0cb41eedf55b556659"}, "user_id": "", "time": {"$date": "2022-12-07T20:48:12.254Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\"],\"explainInput\":\"counterfactul\",\"writingIndex\":[\"2\",\"4\",\"5\",\"6\",\"3\",\"3\",\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fc53b41eedf55b55665b"}, "user_id": "", "time": {"$date": "2022-12-07T20:49:23.419Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .\",\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .\",\"We plan to open source the benchmark and the evaluation toolkit .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"2\",\"4\",\"5\",\"6\",\"3\",\"3\",\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f89eb41eedf55b556639"}, {"_id": {"$oid": "6390fc83b41eedf55b55665e"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:50:11.033Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as nested entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and three observations: (1) generalizing to new document templates is very challenging, (2) few-shot performance has a lot of headroom, and (3) models struggle with nested fields such as line-items in an invoice. We plan to open source the benchmark and the evaluation toolkit. We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents.\n\n", "type": "task", "writing_model": "", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390fc8cb41eedf55b55665f"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:50:20.195Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as nested entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and three observations: (1) generalizing to new document templates is very challenging, (2) few-shot performance has a lot of headroom, and (3) models struggle with nested fields such as line-items in an invoice. We plan to open source the benchmark and the evaluation toolkit. We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents.\n\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390fc8eb41eedf55b556661"}, "user_id": "", "time": {"$date": "2022-12-07T20:50:22.449Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390fc95b41eedf55b556663"}, "user_id": "", "time": {"$date": "2022-12-07T20:50:29.555Z"}, "text": "{\"writingInput\":[\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"3\",\"3\",\"3\",\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390fc9db41eedf55b556664"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:50:37.686Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU ) .VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390fd1bb41eedf55b556666"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:52:43.482Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding ( VRDU VRDU contains two dataset representing various challenges such as rich schema with diverse data types and nested entities, complex templates including tables and multi-column layouts, and diversity of different layout (templates) within a single document type We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-2", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390fd21b41eedf55b556668"}, "user_id": "", "time": {"$date": "2022-12-07T20:52:49.820Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390fd33b41eedf55b55666a"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:53:07.641Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\nVRDU contains two datasets representing various challenges such as rich schema with diverse data types and nested entities, complex templates including tables and multi-column layouts, and diversity of different layout (templates) within a single document type.We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-2", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390fd3ab41eedf55b55666c"}, "user_id": "", "time": {"$date": "2022-12-07T20:53:14.852Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390fd43b41eedf55b55666d"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:53:23.164Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .VRDU contains two datasets representing various challenges such as rich schema with diverse data types and nested entities , complex templates including tables and multi-column layouts , and diversity of different layout ( templates ) within a single document type .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390fd72b41eedf55b55666e"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:54:10.882Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .VRDU contains two datasets representing various challenges such as rich schema with diverse data types and nested entities , complex templates including tables and multi-column layouts , and diversity of different layout ( templates ) within a single document type .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390fd9fb41eedf55b556670"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:54:55.853Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-2", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390fda5b41eedf55b556672"}, "user_id": "", "time": {"$date": "2022-12-07T20:55:01.422Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390fdb2b41eedf55b556673"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:55:14.979Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390fdebb41eedf55b556675"}, "user_id": "", "time": {"$date": "2022-12-07T20:56:11.232Z"}, "text": "{\"writingInput\":[\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\"],\"explainInput\":\"counterfactul\",\"writingIndex\":[\"2\",\"3\",\"3\",\"2\",\"3\",\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390fdedb41eedf55b556677"}, "user_id": "", "time": {"$date": "2022-12-07T20:56:13.211Z"}, "text": "{\"writingInput\":[\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"2\",\"3\",\"3\",\"2\",\"3\",\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "6390feacb41eedf55b556678"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:59:24.878Z"}, "text": "Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents .\n", "event_type": "save"}, {"_id": {"$oid": "6390ff0cb41eedf55b55667a"}, "user_id": "", "time": {"$date": "2022-12-07T21:01:00.747Z"}, "text": "{\"writingInput\":[\"We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .\",\"VRDU contains two datasets that represent several challenges : rich schema including diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single document type .\"],\"explainInput\":\"counterfactul\",\"writingIndex\":[\"2\",\"3\",\"3\",\"2\",\"3\",\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390fc79b41eedf55b55665c"}, {"_id": {"$oid": "63910fd7b41eedf55b55667c"}, "user_id": "", "time": {"$date": "2022-12-07T22:12:39.837Z"}, "text": "{\"writingInput\":[\"This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .\"],\"explainInput\":\"counterfactul\",\"writingIndex\":[\"1\",\"1\",\"1\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bb67b41eedf55b556617"}, {"_id": {"$oid": "639110b6b41eedf55b55667f"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:16:22.967Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive. Well-defined proac- tive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user. However, proactivity is a double- edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user. For designing adequate proac- tive dialog strategies, we propose a novel approach including both socially as well as task-relevant features in the dialog. Here, the primary goal is to optimize proactive behavior so that it is task-oriented - this implies high task success and efficiency - while also being socially effective by fostering user trust. In- cluding both aspects in the reward function for training a proac- tive dialog agent using reinforcement learning showed the ben- efit of our approach for a more successful human-machine co- operation.\n", "type": "task", "writing_model": "", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "639110bcb41eedf55b556681"}, "user_id": "", "time": {"$date": "2022-12-07T22:16:28.777Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "639110c6b41eedf55b556682"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:16:38.874Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .In-cluding both aspects in the reward function for training a proac-tive dialog agent using reinforcement learning showed the ben-efit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639110eeb41eedf55b556684"}, "user_id": "", "time": {"$date": "2022-12-07T22:17:18.864Z"}, "text": "{\"writingInput\":[\"Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"1\",\"1\",\"1\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "6391130ab41eedf55b556686"}, "user_id": "", "time": {"$date": "2022-12-07T22:26:18.137Z"}, "text": "{\"writingInput\":[\"The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"0\",\"0\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "6391140db41eedf55b556687"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:30:37.431Z"}, "text": "intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .In-cluding both aspects in the reward function for training a proac-tive dialog agent using reinforcement learning showed the ben-efit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911438b41eedf55b556688"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:31:20.353Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent bystanders and become proactive .Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .In-cluding both aspects in the reward function for training a proac-tive dialog agent using reinforcement learning showed the ben-efit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911476b41eedf55b556689"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:32:22.120Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders.Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .In-cluding both aspects in the reward function for training a proac-tive dialog agent using reinforcement learning showed the ben-efit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911550b41eedf55b55668a"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:36:00.620Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders.Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user.For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .In-cluding both aspects in the reward function for training a proac-tive dialog agent using reinforcement learning showed the ben-efit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639115aeb41eedf55b55668b"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:37:34.011Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders.Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user.For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented. This implies high task success and efficiency, while also being socially effective by fostering user trust .In-cluding both aspects in the reward function for training a proac-tive dialog agent using reinforcement learning showed the ben-efit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911601b41eedf55b55668c"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:38:57.932Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders.Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user.For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented. This implies high task success and efficiency, while also being socially effective by fostering user trust.Incorporating reinforcement learning to train -cluding both aspects in the reward function for training a proac-tive dialog agent using reinforcement learning showed the ben-efit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911632b41eedf55b55668d"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:39:46.313Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders.Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user.For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented. This implies high task success and efficiency, while also being socially effective by fostering user trust.Incorporating reinforcement learning that takes into account both aspects showed the benefit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911649b41eedf55b55668e"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:40:09.998Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders.Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user.For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented. This implies high task success and efficiency, while also being socially effective by fostering user trust.Incorporating reinforcement learning that takes into account both aspects in agent training showed the benefit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6391164bb41eedf55b556690"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:40:11.096Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders.Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user.For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented. This implies high task success and efficiency, while also being socially effective by fostering user trust.Incorporating reinforcement learning that takes into account both aspects in agent training showed the benefit of our approach for a more successful human-machine co-operation .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "63911650b41eedf55b556692"}, "user_id": "", "time": {"$date": "2022-12-07T22:40:16.738Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "6391165ab41eedf55b556693"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:40:26.741Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders .Well-defined proac-tive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented .This implies high task success and efficiency , while also being socially effective by fostering user trust .Incorporating reinforcement learning that takes into account both aspects in agent training showed the benefit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911714b41eedf55b556695"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:43:32.362Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders .While well-defined proac-tive behavior may improve human-machine cooperation , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented .This implies high task success and efficiency , while also being socially effective by fostering user trust .Incorporating reinforcement learning that takes into account both aspects in agent training showed the benefit of our approach for a more successful human-machine co-operation .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "63911719b41eedf55b556696"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:43:37.739Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders .While well-defined proac-tive behavior may improve human-machine cooperation , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented .This implies high task success and efficiency , while also being socially effective by fostering user trust .Incorporating reinforcement learning that takes into account both aspects in agent training showed the benefit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6391171ab41eedf55b556698"}, "user_id": "", "time": {"$date": "2022-12-07T22:43:38.031Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "63911724b41eedf55b556699"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:43:48.036Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders .While well-defined proac-tive behavior may improve human-machine cooperation , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented .This implies high task success and efficiency , while also being socially effective by fostering user trust .Incorporating reinforcement learning that takes into account both aspects in agent training showed the benefit of our approach for a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911844b41eedf55b55669b"}, "user_id": "", "time": {"$date": "2022-12-07T22:48:36.765Z"}, "text": "{\"writingInput\":[\"Here , the primary goal is to optimize proactive behavior so that it is task-oriented .\",\"This implies high task success and efficiency , while also being socially effective by fostering user trust .\"],\"explainInput\":\"baseline request\",\"writingIndex\":[\"3\",\"4\",\"3\",\"4\",\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "639118c5b41eedf55b55669c"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:50:45.700Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders .While well-defined proac-tive behavior may improve human-machine cooperation , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented .This implies high task success and efficiency , while also being socially effective by fostering user trust We find that incorporating reinforcement learning using both aspects in agent training contributes to a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911907b41eedf55b55669d"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:51:51.185Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders .While well-defined proac-tive behavior may improve human-machine cooperation , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Our primary goal is to optimize proactive behavior so that it is task-oriented .This implies high task success and efficiency , while also being socially effective by fostering user trust We find that incorporating reinforcement learning using both aspects in agent training contributes to a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911945b41eedf55b55669f"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:52:53.828Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders .While well-defined proac-tive behavior may improve human-machine cooperation , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Our primary goal is to optimize proactive behavior so that it is task-oriented .This implies high task success and efficiency , while also being socially effective by fostering user trust We find that incorporating reinforcement learning using both aspects in agent training contributes to a more successful human-machine co-operation .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "6391194bb41eedf55b5566a1"}, "user_id": "", "time": {"$date": "2022-12-07T22:52:59.491Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639110a2b41eedf55b55667d"}, {"_id": {"$oid": "63911955b41eedf55b5566a2"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:53:09.490Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders .While well-defined proac-tive behavior may improve human-machine cooperation , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Our primary goal is to optimize proactive behavior so that it is task-oriented .We find that incorporating reinforcement learning using both aspects in agent training contributes to a more successful human-machine co-operation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6391197fb41eedf55b5566a3"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:53:51.048Z"}, "text": "The existing intelligent dialog agents have not yet escaped their role as silent and inactive bystanders .While well-defined proac-tive behavior may improve human-machine cooperation , proactivity is a double-edged sword because poorly executed actions may adversely affect both the task outcome and the relationship with the user .For designing adequate proac-tive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Our primary goal is to optimize proactive behavior so that it is task-oriented .We find that incorporating reinforcement learning using both aspects in agent training contributes to a more successful human-machine co-operation .\n", "event_type": "save"}]