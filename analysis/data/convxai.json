[{"_id": {"$oid": "638ed86c21f98f4a479b7339"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T05:51:40.916Z"}, "text": "Considering the spectral properties of images, we propose a new self-attention mechanism with highly reduced computational complexity, up to a linear rate. To better preserve edges while promoting similarity within objects, we propose individualized processes over different frequency bands. In particular, we study a case where the process is merely over low-frequency components. By ablation study, we show that low frequency self-attention can achieve very close or better performance relative to full frequency even without retraining the network. Accordingly, we design and embed novel plug-and-play modules to the head of a CNN network that we refer to as FsaNet. The frequency self-attention 1) takes low frequency coefficients as input, 2) can be mathematically equivalent to spatial domain selfattention with linear structures, 3) simplifies token mapping (1\u00d71 convolution) stage and token mixing stage simultaneously. We show that the frequency self-attention requires 87.29% \u223c 90.04% less memory, 96.13% \u223c 98.07% less FLOPs, and 97.56% \u223c 98.18% in run time than the regular self-attention. Compared to other ResNet101-based self-attention networks, FsaNet achieves a new state-of-the-art result (83.0% mIoU) on Cityscape test dataset and competitive results on ADE20k and VOCaug. Code is accessible at https://github.com/zfy-csu/FsaNet.\n", "type": "task", "writing_model": "", "conversation_id": "638ed85c21f98f4a479b7337"}, {"_id": {"$oid": "638ed87121f98f4a479b733b"}, "user_id": "", "time": {"$date": "2022-12-06T05:51:45.616Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ed85c21f98f4a479b7337"}, {"_id": {"$oid": "638ed87c21f98f4a479b733c"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T05:51:56.474Z"}, "text": "Considering the spectral properties of images , we propose a new self-attention mechanism with highly reduced computational complexity , up to a linear rate .To better preserve edges while promoting similarity within objects , we propose individualized processes over different frequency bands .In particular , we study a case where the process is merely over low-frequency components .By ablation study , we show that low frequency self-attention can achieve very close or better performance relative to full frequency even without retraining the network .Accordingly , we design and embed novel plug-and-play modules to the head of a CNN network that we refer to as FsaNet .The frequency self-attention 1 ) takes low frequency coefficients as input , 2 ) can be mathematically equivalent to spatial domain selfattention with linear structures , 3 ) simplifies token mapping ( 1\u00d71 convolution ) stage and token mixing stage simultaneously .We show that the frequency self-attention requires 87.29 % \u223c 90.04 % less memory , 96.13 % \u223c 98.07 % less FLOPs , and 97.56 % \u223c 98.18 % in run time than the regular self-attention .Compared to other ResNet101-based self-attention networks , FsaNet achieves a new state-of-the-art result ( 83.0 % mIoU ) on Cityscape test dataset and competitive results on ADE20 k and VOCaug .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ed89321f98f4a479b733e"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T05:52:19.524Z"}, "text": "Federated embodied agent learning [39] protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Results on two VLN datasets (R2R [2] and RxR [19]) show that NAW can easily navigate the deployed VLN agent regardless of the language instruction, without affecting its performance on normal test sets. Then, we propose a new Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated VLN, which provides the server with a \u201cprompt\u201d of the vision-and-language alignment variance between the benign and malicious clients so that they can be distinguished during training. We validate the effectiveness of the PBA method on protecting the global model from the NAW attack, which outperforms other state-of-the-art defense methods by a large margin in the defense metrics on R2R and RxR.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638ed85c21f98f4a479b7337"}, {"_id": {"$oid": "638ed89721f98f4a479b7340"}, "user_id": "", "time": {"$date": "2022-12-06T05:52:23.796Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ed85c21f98f4a479b7337"}, {"_id": {"$oid": "638ed8a221f98f4a479b7341"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T05:52:34.151Z"}, "text": "Federated embodied agent learning [ 39 ] protects the data privacy of individual visual environments by keeping data locally at each client ( the individual environment ) during training .However , since the local data is inaccessible to the server under federated learning , attackers may easily poison the training data of the local client to build a backdoor in the agent without notice .Deploying such an agent raises the risk of potential harm to humans , as the attackers may easily navigate and control the agent as they wish via the backdoor .Towards Byzantine-robust federated embodied agent learning , in this paper , we study the attack and defense for the task of vision-and-language navigation ( VLN ) , where the agent is required to follow natural language instructions to navigate indoor environments .First , we introduce a simple but effective attack strategy , Navigation as Wish ( NAW ) , in which the malicious client manipulates local trajectory data to implant a backdoor into the global model .Results on two VLN datasets ( R2R [ 2 ] and RxR [ 19 ] ) show that NAW can easily navigate the deployed VLN agent regardless of the language instruction , without affecting its performance on normal test sets .Then , we propose a new Prompt-Based Aggregation ( PBA ) to defend against the NAW attack in federated VLN , which provides the server with a \u201c prompt \u201d of the vision-and-language alignment variance between the benign and malicious clients so that they can be distinguished during training .We validate the effectiveness of the PBA method on protecting the global model from the NAW attack , which outperforms other state-of-the-art defense methods by a large margin in the defense metrics on R2R and RxR .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ed8ce21f98f4a479b7343"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T05:53:18.306Z"}, "text": "Federated embodied agent learning [ 39 ] protects the data privacy of individual visual environments by keeping data locally at each client ( the individual environment ) during training .However , since the local data is inaccessible to the server under federated learning , attackers may easily poison the training data of the local client to build a backdoor in the agent without notice .Deploying such an agent raises the risk of potential harm to humans , as the attackers may easily navigate and control the agent as they wish via the backdoor .Towards Byzantine-robust federated embodied agent learning , in this paper , we study the attack and defense for the task of vision-and-language navigation ( VLN ) , where the agent is required to follow natural language instructions to navigate indoor environments .First , we introduce a simple but effective attack strategy , Navigation as Wish ( NAW ) , in which the malicious client manipulates local trajectory data to implant a backdoor into the global model .Then , we propose a new Prompt-Based Aggregation ( PBA ) to defend against the NAW attack in federated VLN , which provides the server with a \u201c prompt \u201d of the vision-and-language alignment variance between the benign and malicious clients so that they can be distinguished during training .\nResults on two VLN datasets ( R2R [ 2 ] and RxR [ 19 ] ) show that NAW can easily navigate the deployed VLN agent regardless of the language instruction , without affecting its performance on normal test sets .We validate the effectiveness of the PBA method on protecting the global model from the NAW attack , which outperforms other state-of-the-art defense methods by a large margin in the defense metrics on R2R and RxR .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638ed85c21f98f4a479b7337"}, {"_id": {"$oid": "638ed8d221f98f4a479b7345"}, "user_id": "", "time": {"$date": "2022-12-06T05:53:22.739Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ed85c21f98f4a479b7337"}, {"_id": {"$oid": "638ed8dd21f98f4a479b7346"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T05:53:33.547Z"}, "text": "Federated embodied agent learning [ 39 ] protects the data privacy of individual visual environments by keeping data locally at each client ( the individual environment ) during training .However , since the local data is inaccessible to the server under federated learning , attackers may easily poison the training data of the local client to build a backdoor in the agent without notice .Deploying such an agent raises the risk of potential harm to humans , as the attackers may easily navigate and control the agent as they wish via the backdoor .Towards Byzantine-robust federated embodied agent learning , in this paper , we study the attack and defense for the task of vision-and-language navigation ( VLN ) , where the agent is required to follow natural language instructions to navigate indoor environments .First , we introduce a simple but effective attack strategy , Navigation as Wish ( NAW ) , in which the malicious client manipulates local trajectory data to implant a backdoor into the global model .Then , we propose a new Prompt-Based Aggregation ( PBA ) to defend against the NAW attack in federated VLN , which provides the server with a \u201c prompt \u201d of the vision-and-language alignment variance between the benign and malicious clients so that they can be distinguished during training .Results on two VLN datasets ( R2R [ 2 ] and RxR [ 19 ] ) show that NAW can easily navigate the deployed VLN agent regardless of the language instruction , without affecting its performance on normal test sets .We validate the effectiveness of the PBA method on protecting the global model from the NAW attack , which outperforms other state-of-the-art defense methods by a large margin in the defense metrics on R2R and RxR .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ed8fe21f98f4a479b7348"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T05:54:06.994Z"}, "text": "Proactive robot assistance enables a robot to anticipate and provide for a user\u2019s needs without being explicitly asked. We formulate proactive assistance as the problem of the robot anticipating temporal patterns of object movements associated with everyday user routines, and proactively assisting the user by placing objects to adapt the environment to their needs. We introduce a generative graph neural network to learn a unified spatio-temporal predictive model of object dynamics from temporal sequences of object arrangements. We additionally contribute the Household Object Movements from Everyday Routines (HOMER) dataset, which tracks household objects associated with human activities of daily living across 50+ days for five simulated households. Our model outperforms the leading baseline in predicting object movement, correctly predicting locations for 11.1% more objects and wrongly predicting locations for 11.5% fewer objects used by the human user.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638ed85c21f98f4a479b7337"}, {"_id": {"$oid": "638ed90221f98f4a479b734a"}, "user_id": "", "time": {"$date": "2022-12-06T05:54:10.841Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ed85c21f98f4a479b7337"}, {"_id": {"$oid": "638ed90d21f98f4a479b734b"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T05:54:21.740Z"}, "text": "Proactive robot assistance enables a robot to anticipate and provide for a user \u2019s needs without being explicitly asked .We formulate proactive assistance as the problem of the robot anticipating temporal patterns of object movements associated with everyday user routines , and proactively assisting the user by placing objects to adapt the environment to their needs .We introduce a generative graph neural network to learn a unified spatio-temporal predictive model of object dynamics from temporal sequences of object arrangements .We additionally contribute the Household Object Movements from Everyday Routines ( HOMER ) dataset , which tracks household objects associated with human activities of daily living across 50 + days for five simulated households .Our model outperforms the leading baseline in predicting object movement , correctly predicting locations for 11.1 % more objects and wrongly predicting locations for 11.5 % fewer objects used by the human user .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ed92721f98f4a479b734d"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T05:54:47.422Z"}, "text": "Multi-view attributed graph clustering is an important approach to partition multi-view data based on the attribute feature and adjacent matrices from different views. Some attempts have been made in utilizing Graph Neural Network (GNN), which have achieved promising clustering performance. Despite this, few of them pay attention to the inherent specific information embedded in multiple views. Meanwhile, they are incapable of recovering the latent high-level representation from the low-level ones, greatly limiting the downstream clustering performance. To fill these gaps, a novel Dual Information enhanced multi-view Attributed Graph Clustering (DIAGC) method is proposed in this paper. Specifically, the proposed method introduces the Specific Information Reconstruction (SIR) module to disentangle the explorations of the consensus and specific information from multiple views, which enables GCN to capture the more essential low-level representations. Besides, the Mutual Information Maximization (MIM) module maximizes the agreement between the latent high-level representation and lowlevel ones, and enables the high-level representation to satisfy the desired clustering structure with the help of the Self-supervised Clustering (SC) module. Extensive experiments on several realworld benchmarks demonstrate the effectiveness of the proposed DIAGC method compared with the state-of-the-art baselines\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638ed85c21f98f4a479b7337"}, {"_id": {"$oid": "638ed92b21f98f4a479b734f"}, "user_id": "", "time": {"$date": "2022-12-06T05:54:51.415Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ed85c21f98f4a479b7337"}, {"_id": {"$oid": "638ed93e21f98f4a479b7350"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T05:55:10.787Z"}, "text": "Multi-view attributed graph clustering is an important approach to partition multi-view data based on the attribute feature and adjacent matrices from different views .Some attempts have been made in utilizing Graph Neural Network ( GNN ) , which have achieved promising clustering performance .Despite this , few of them pay attention to the inherent specific information embedded in multiple views .Meanwhile , they are incapable of recovering the latent high-level representation from the low-level ones , greatly limiting the downstream clustering performance .To fill these gaps , a novel Dual Information enhanced multi-view Attributed Graph Clustering ( DIAGC ) method is proposed in this paper .Specifically , the proposed method introduces the Specific Information Reconstruction ( SIR ) module to disentangle the explorations of the consensus and specific information from multiple views , which enables GCN to capture the more essential low-level representations .Besides , the Mutual Information Maximization ( MIM ) module maximizes the agreement between the latent high-level representation and lowlevel ones , and enables the high-level representation to satisfy the desired clustering structure with the help of the Self-supervised Clustering ( SC ) module .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638edf3621f98f4a479b7353"}, "user_id": "Hua", "time": {"$date": "2022-12-06T06:20:38.998Z"}, "text": "An electroencephalogram is an effective approach that provides a bidirectional pathway between the user and computer in a non-invasive way. In this study, we adopted the visual imagery data for controlling the BCI-based robotic arm. Visual imagery increases the power of the alpha frequency range of the visual cortex over time as the user performs the task. We proposed a deep learning architecture to decode the visual imagery data using only two channels and also we investigated the combination of two EEG channels that has significant classification performance. When using the proposed method, the highest classification performance using two channels in the offline experiment was 0.661. Also, the highest success rate in the online experiment using two channels (AF3\u2013Oz) was 0.78. Our results provide the possibility of controlling the BCI-based robotic arm using visual imagery data.\n", "type": "task", "writing_model": "", "conversation_id": "638eddb021f98f4a479b7351"}, {"_id": {"$oid": "638edf3b21f98f4a479b7355"}, "user_id": "", "time": {"$date": "2022-12-06T06:20:43.584Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638eddb021f98f4a479b7351"}, {"_id": {"$oid": "638edf4721f98f4a479b7356"}, "user_id": "Hua", "time": {"$date": "2022-12-06T06:20:55.324Z"}, "text": "An electroencephalogram is an effective approach that provides a bidirectional pathway between the user and computer in a non-invasive way .In this study , we adopted the visual imagery data for controlling the BCI-based robotic arm .Visual imagery increases the power of the alpha frequency range of the visual cortex over time as the user performs the task .We proposed a deep learning architecture to decode the visual imagery data using only two channels and also we investigated the combination of two EEG channels that has significant classification performance .When using the proposed method , the highest classification performance using two channels in the offline experiment was 0.661 .Also , the highest success rate in the online experiment using two channels ( AF3\u2013 Oz ) was 0.78 .Our results provide the possibility of controlling the BCI-based robotic arm using visual imagery data .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638edf5f21f98f4a479b7358"}, "user_id": "Hua", "time": {"$date": "2022-12-06T06:21:19.418Z"}, "text": "Students\u2019 ability to ask curious questions is a crucial skill that improves their learning processes. To train this skill, previous research has used a conversational agent that propose specific cues to prompt children\u2019s curiosity during learning. Despite showing pedagogical efficiency, this method is still limited since it relies on generating the said prompts by hand for each educational resource, which can be a very long and costly process. In this context, we leverage the advances in the natural language processing field and explore using a large language model (GPT-3) to automate the generation of this agent\u2019s curiosity-prompting cues to help children ask more and deeper questions. We then used this study to investigate a different curiosity-prompting behavior for the agent. The study was conducted with 75 students aged between 9 and 10. They either interacted with a hand-crafted conversational agent that proposes \u201dclosed\u201d manually-extracted cues leading to predefined questions, a GPT-3-driven one that proposes the same type of cues, or a GPT-3- driven one that proposes \u201dopen\u201d cues that can lead to several possible questions. Results showed a similar question-asking performance between children who had the two \u201dclosed\u201d agents, but a significantly better one for participants with the \u201dopen\u201d agent. Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies. In a second step, we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638eddb021f98f4a479b7351"}, {"_id": {"$oid": "638edf6521f98f4a479b735a"}, "user_id": "", "time": {"$date": "2022-12-06T06:21:25.598Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638eddb021f98f4a479b7351"}, {"_id": {"$oid": "638edf7021f98f4a479b735b"}, "user_id": "Hua", "time": {"$date": "2022-12-06T06:21:36.421Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .In this context , we leverage the advances in the natural language processing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that proposes \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ee06321f98f4a479b735e"}, "user_id": "Hua", "time": {"$date": "2022-12-06T06:25:39.177Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive. Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user. However, proactivity is a doubleedged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user. For designing adequate proactive dialog strategies, we propose a novel approach including both socially as well as task-relevant features in the dialog. Here, the primary goal is to optimize proactive behavior so that it is task-oriented - this implies high task success and efficiency - while also being socially effective by fostering user trust. Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation.\n", "type": "task", "writing_model": "", "conversation_id": "638ee05c21f98f4a479b735c"}, {"_id": {"$oid": "638ee06c21f98f4a479b735f"}, "user_id": "Hua", "time": {"$date": "2022-12-06T06:25:48.478Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive. Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user. However, proactivity is a doubleedged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user. For designing adequate proactive dialog strategies, we propose a novel approach including both socially as well as task-relevant features in the dialog. Here, the primary goal is to optimize proactive behavior so that it is task-oriented - this implies high task success and efficiency - while also being socially effective by fostering user trust. Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation.\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ee06c21f98f4a479b7361"}, "user_id": "", "time": {"$date": "2022-12-06T06:25:48.698Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ee05c21f98f4a479b735c"}, {"_id": {"$oid": "638ee07621f98f4a479b7362"}, "user_id": "Hua", "time": {"$date": "2022-12-06T06:25:58.798Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a doubleedged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ee29121f98f4a479b7364"}, "user_id": "Hua", "time": {"$date": "2022-12-06T06:34:57.244Z"}, "text": "Learned Bloom Filters, i.e., models induced from data via machine learning techniques and solving the approximate set membership problem, have recently been introduced with the aim of enhancing the performance of standard Bloom Filters, with special focus on space occupancy. Unlike in the classical case, the \u201ccomplexity\u201d of the data used to build the filter might heavily impact on its performance. Therefore, here we propose the first in-depth analysis, to the best of our knowledge, for the performance assessment of a given Learned Bloom Filter, in conjunction with a given classifier, on a dataset of a given classification complexity. Indeed, we propose a novel methodology, supported by software, for designing, analyzing and implementing Learned Bloom Filters in function of specific constraints on their multi-criteria nature (that is, constraints involving space efficiency, false positive rate, and reject time). Our experiments show that the proposed methodology and the supporting software are valid and useful: we find out that only two classifiers have desirable properties in relation to problems with different data complexity, and, interestingly, none of them has been considered so far in the literature. We also experimentally show that the Sandwiched variant of Learned Bloom filters is the most robust to data complexity and classifier performance variability, as well as those usually having smaller reject times. The software can be readily used to test new Learned Bloom Filter proposals, which can be compared with the best ones identified here.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638ee05c21f98f4a479b735c"}, {"_id": {"$oid": "638ee29721f98f4a479b7366"}, "user_id": "", "time": {"$date": "2022-12-06T06:35:03.050Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ee05c21f98f4a479b735c"}, {"_id": {"$oid": "638ee2a221f98f4a479b7367"}, "user_id": "Hua", "time": {"$date": "2022-12-06T06:35:14.635Z"}, "text": "Learned Bloom Filters , i.e. , models induced from data via machine learning techniques and solving the approximate set membership problem , have recently been introduced with the aim of enhancing the performance of standard Bloom Filters , with special focus on space occupancy .Unlike in the classical case , the \u201c complexity \u201d of the data used to build the filter might heavily impact on its performance .Therefore , here we propose the first in-depth analysis , to the best of our knowledge , for the performance assessment of a given Learned Bloom Filter , in conjunction with a given classifier , on a dataset of a given classification complexity .Indeed , we propose a novel methodology , supported by software , for designing , analyzing and implementing Learned Bloom Filters in function of specific constraints on their multi-criteria nature ( that is , constraints involving space efficiency , false positive rate , and reject time ) .Our experiments show that the proposed methodology and the supporting software are valid and useful : we find out that only two classifiers have desirable properties in relation to problems with different data complexity , and , interestingly , none of them has been considered so far in the literature .We also experimentally show that the Sandwiched variant of Learned Bloom filters is the most robust to data complexity and classifier performance variability , as well as those usually having smaller reject times .The software can be readily used to test new Learned Bloom Filter proposals , which can be compared with the best ones identified here .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ee2e021f98f4a479b736a"}, "user_id": "gya", "time": {"$date": "2022-12-06T06:36:16.943Z"}, "text": "Learned Bloom Filters, i.e., models induced from data via machine learning techniques and solving the approximate set membership problem, have recently been introduced with the aim of enhancing the performance of standard Bloom Filters, with special focus on space occupancy. Unlike in the classical case, the \u201ccomplexity\u201d of the data used to build the filter might heavily impact on its performance. Therefore, here we propose the first in-depth analysis, to the best of our knowledge, for the performance assessment of a given Learned Bloom Filter, in conjunction with a given classifier, on a dataset of a given classification complexity. Indeed, we propose a novel methodology, supported by software, for designing, analyzing and implementing Learned Bloom Filters in function of specific constraints on their multi-criteria nature (that is, constraints involving space efficiency, false positive rate, and reject time). Our experiments show that the proposed methodology and the supporting software are valid and useful: we find out that only two classifiers have desirable properties in relation to problems with different data complexity, and, interestingly, none of them has been considered so far in the literature. We also experimentally show that the Sandwiched variant of Learned Bloom filters is the most robust to data complexity and classifier performance variability, as well as those usually having smaller reject times. The software can be readily used\n", "type": "task", "writing_model": "", "conversation_id": "638ee2d521f98f4a479b7368"}, {"_id": {"$oid": "638ee2e621f98f4a479b736c"}, "user_id": "", "time": {"$date": "2022-12-06T06:36:22.610Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ee2d521f98f4a479b7368"}, {"_id": {"$oid": "638ee2f221f98f4a479b736d"}, "user_id": "gya", "time": {"$date": "2022-12-06T06:36:34.318Z"}, "text": "Learned Bloom Filters , i.e. , models induced from data via machine learning techniques and solving the approximate set membership problem , have recently been introduced with the aim of enhancing the performance of standard Bloom Filters , with special focus on space occupancy .Unlike in the classical case , the \u201c complexity \u201d of the data used to build the filter might heavily impact on its performance .Therefore , here we propose the first in-depth analysis , to the best of our knowledge , for the performance assessment of a given Learned Bloom Filter , in conjunction with a given classifier , on a dataset of a given classification complexity .Indeed , we propose a novel methodology , supported by software , for designing , analyzing and implementing Learned Bloom Filters in function of specific constraints on their multi-criteria nature ( that is , constraints involving space efficiency , false positive rate , and reject time ) .Our experiments show that the proposed methodology and the supporting software are valid and useful : we find out that only two classifiers have desirable properties in relation to problems with different data complexity , and , interestingly , none of them has been considered so far in the literature .We also experimentally show that the Sandwiched variant of Learned Bloom filters is the most robust to data complexity and classifier performance variability , as well as those usually having smaller reject times .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ee36521f98f4a479b736f"}, "user_id": "gya", "time": {"$date": "2022-12-06T06:38:29.941Z"}, "text": "We consider partially observable Markov decision processes (POMDPs) modeling an agent that needs a supply of a certain resource (e.g., electricity stored in batteries) to operate correctly. The resource is consumed by agent\u2019s actions and can be replenished only in certain states. The agent aims to minimize the expected cost of reaching some goal while preventing resource exhaustion, a problem we call resourceconstrained goal optimization (RSGO). We take a two-step approach to the RSGO problem. First, using formal methods techniques, we design an algorithm computing a shield for a given scenario: a procedure that observes the agent and prevents it from using actions that might eventually lead to resource exhaustion. Second, we augment the POMCP heuristic search algorithm for POMDP planning with our shields to obtain an algorithm solving the RSGO problem. We implement our algorithm and present experiments showing its applicability to benchmarks from the literature.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638ee2d521f98f4a479b7368"}, {"_id": {"$oid": "638ee36b21f98f4a479b7371"}, "user_id": "", "time": {"$date": "2022-12-06T06:38:35.716Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ee2d521f98f4a479b7368"}, {"_id": {"$oid": "638ee37621f98f4a479b7372"}, "user_id": "gya", "time": {"$date": "2022-12-06T06:38:46.589Z"}, "text": "We consider partially observable Markov decision processes ( POMDPs ) modeling an agent that needs a supply of a certain resource ( e.g. , electricity stored in batteries ) to operate correctly .The resource is consumed by agent \u2019s actions and can be replenished only in certain states .The agent aims to minimize the expected cost of reaching some goal while preventing resource exhaustion , a problem we call resourceconstrained goal optimization ( RSGO ) .We take a two-step approach to the RSGO problem .First , using formal methods techniques , we design an algorithm computing a shield for a given scenario : a procedure that observes the agent and prevents it from using actions that might eventually lead to resource exhaustion .Second , we augment the POMCP heuristic search algorithm for POMDP planning with our shields to obtain an algorithm solving the RSGO problem .We implement our algorithm and present experiments showing its applicability to benchmarks from the literature .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ee40321f98f4a479b7374"}, "user_id": "gya", "time": {"$date": "2022-12-06T06:41:07.023Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decisionmaking. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a returnconditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638ee2d521f98f4a479b7368"}, {"_id": {"$oid": "638ee40921f98f4a479b7376"}, "user_id": "", "time": {"$date": "2022-12-06T06:41:13.067Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ee2d521f98f4a479b7368"}, {"_id": {"$oid": "638ee41321f98f4a479b7377"}, "user_id": "gya", "time": {"$date": "2022-12-06T06:41:23.959Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decisionmaking .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a returnconditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638eea5221f98f4a479b737a"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T07:08:02.059Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decisionmaking. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a returnconditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "type": "task", "writing_model": "", "conversation_id": "638eea3e21f98f4a479b7378"}, {"_id": {"$oid": "638eea5821f98f4a479b737c"}, "user_id": "", "time": {"$date": "2022-12-06T07:08:08.029Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638eea3e21f98f4a479b7378"}, {"_id": {"$oid": "638eea6221f98f4a479b737d"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T07:08:18.983Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decisionmaking .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a returnconditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638eeac821f98f4a479b737f"}, "user_id": "", "time": {"$date": "2022-12-06T07:10:00.505Z"}, "text": "{\"writingInput\":[\"We investigate whether these methods can directly address the problem of sequential decisionmaking .\"],\"explainInput\":\"Can you explain this sentence review?\",\"writingIndex\":[\"1\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638eea3e21f98f4a479b7378"}, {"_id": {"$oid": "638eeb2721f98f4a479b7381"}, "user_id": "", "time": {"$date": "2022-12-06T07:11:35.742Z"}, "text": "{\"writingInput\":[\"We investigate whether these methods can directly address the problem of sequential decisionmaking .\"],\"explainInput\":\"show me model and data\\n\",\"writingIndex\":[\"1\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638eea3e21f98f4a479b7378"}, {"_id": {"$oid": "638eeb3a21f98f4a479b7383"}, "user_id": "", "time": {"$date": "2022-12-06T07:11:54.901Z"}, "text": "{\"writingInput\":[\"We investigate whether these methods can directly address the problem of sequential decisionmaking .\"],\"explainInput\":\"explain\\n\",\"writingIndex\":[\"1\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638eea3e21f98f4a479b7378"}, {"_id": {"$oid": "638ef00421f98f4a479b7385"}, "user_id": "", "time": {"$date": "2022-12-06T07:32:20.089Z"}, "text": "{\"writingInput\":[\"We investigate whether these methods can directly address the problem of sequential decisionmaking .\"],\"explainInput\":\"Can you explain this sentence review?\",\"writingIndex\":[\"1\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638eea3e21f98f4a479b7378"}, {"_id": {"$oid": "638ef11c21f98f4a479b7387"}, "user_id": "", "time": {"$date": "2022-12-06T07:37:00.979Z"}, "text": "{\"writingInput\":[\"We investigate whether these methods can directly address the problem of sequential decisionmaking .\"],\"explainInput\":\"example\\n\",\"writingIndex\":[\"1\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638eea3e21f98f4a479b7378"}, {"_id": {"$oid": "638ef5f7e88d054ed6cc46f8"}, "user_id": "Hua", "time": {"$date": "2022-12-06T07:57:43.001Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "type": "task", "writing_model": "", "conversation_id": "638ef5d5e88d054ed6cc46f6"}, {"_id": {"$oid": "638ef5fae88d054ed6cc46fa"}, "user_id": "", "time": {"$date": "2022-12-06T07:57:46.555Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ef5d5e88d054ed6cc46f6"}, {"_id": {"$oid": "638ef604e88d054ed6cc46fb"}, "user_id": "Hua", "time": {"$date": "2022-12-06T07:57:56.554Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users .This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .We found that although users are interested in explanations for the road not taken \u2014 namely , why the model chose one result and not a well-defined , seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ef60ae88d054ed6cc46fd"}, "user_id": "", "time": {"$date": "2022-12-06T07:58:02.921Z"}, "text": "{\"writingInput\":[\"This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .\"],\"explainInput\":\"How to improve the sentence?\",\"writingIndex\":[\"1\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ef5d5e88d054ed6cc46f6"}, {"_id": {"$oid": "638ef721e88d054ed6cc46ff"}, "user_id": "", "time": {"$date": "2022-12-06T08:02:41.951Z"}, "text": "{\"writingInput\":[\"This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .\"],\"explainInput\":\"quality\\n\",\"writingIndex\":[\"1\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ef5d5e88d054ed6cc46f6"}, {"_id": {"$oid": "638ef7f5a16c2832a79b1515"}, "user_id": "hua", "time": {"$date": "2022-12-06T08:06:13.140Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "type": "task", "writing_model": "", "conversation_id": "638ef7eea16c2832a79b1513"}, {"_id": {"$oid": "638ef7fea16c2832a79b1516"}, "user_id": "hua", "time": {"$date": "2022-12-06T08:06:22.290Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638ef807a16c2832a79b1518"}, "user_id": "hua", "time": {"$date": "2022-12-06T08:06:31.745Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "type": "task", "writing_model": "", "conversation_id": "638ef7eea16c2832a79b1513"}, {"_id": {"$oid": "638ef80ba16c2832a79b151a"}, "user_id": "", "time": {"$date": "2022-12-06T08:06:35.280Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ef7eea16c2832a79b1513"}, {"_id": {"$oid": "638ef815a16c2832a79b151b"}, "user_id": "hua", "time": {"$date": "2022-12-06T08:06:45.282Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users .This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .We found that although users are interested in explanations for the road not taken \u2014 namely , why the model chose one result and not a well-defined , seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ef9c5b48e3a4aade8f5c7"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:13:57.023Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "type": "task", "writing_model": "", "conversation_id": "638ef9c0b48e3a4aade8f5c5"}, {"_id": {"$oid": "638ef9ceb48e3a4aade8f5c8"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:14:06.485Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638ef9d3b48e3a4aade8f5ca"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:14:11.730Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "type": "task", "writing_model": "", "conversation_id": "638ef9c0b48e3a4aade8f5c5"}, {"_id": {"$oid": "638ef9d7b48e3a4aade8f5cc"}, "user_id": "", "time": {"$date": "2022-12-06T08:14:15.582Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ef9c0b48e3a4aade8f5c5"}, {"_id": {"$oid": "638ef9e1b48e3a4aade8f5cd"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:14:25.584Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users .This paper summarizes the common forms of explanations ( such as feature attribution , decision rules , or probes ) used in over 200 recent papers about natural language processing ( NLP ) , and compares them against user questions collected in the XAI Question Bank .We found that although users are interested in explanations for the road not taken \u2014 namely , why the model chose one result and not a well-defined , seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638ef9f9b48e3a4aade8f5cf"}, "user_id": "", "time": {"$date": "2022-12-06T08:14:49.888Z"}, "text": "{\"writingInput\":[\"We found that although users are interested in explanations for the road not taken \u2014 namely , why the model chose one result and not a well-defined , seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"2\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ef9c0b48e3a4aade8f5c5"}, {"_id": {"$oid": "638efbb3b48e3a4aade8f5d1"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:22:11.619Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638ef9c0b48e3a4aade8f5c5"}, {"_id": {"$oid": "638efbb8b48e3a4aade8f5d3"}, "user_id": "", "time": {"$date": "2022-12-06T08:22:16.284Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638ef9c0b48e3a4aade8f5c5"}, {"_id": {"$oid": "638efbc2b48e3a4aade8f5d4"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:22:26.944Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638efc923651cb267904c958"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:25:54.106Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "638efc8b3651cb267904c956"}, {"_id": {"$oid": "638efc9b3651cb267904c959"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:26:03.489Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638efcb63651cb267904c95b"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:26:30.322Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "638efc8b3651cb267904c956"}, {"_id": {"$oid": "638efcba3651cb267904c95d"}, "user_id": "", "time": {"$date": "2022-12-06T08:26:34.404Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638efc8b3651cb267904c956"}, {"_id": {"$oid": "638efcc53651cb267904c95e"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:26:45.250Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638efcc53651cb267904c960"}, "user_id": "", "time": {"$date": "2022-12-06T08:26:45.472Z"}, "text": "{\"writingInput\":[\"We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638efc8b3651cb267904c956"}, {"_id": {"$oid": "638efd4195f0af48a9e1a6b1"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:28:49.800Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "638efd3995f0af48a9e1a6af"}, {"_id": {"$oid": "638efd4a95f0af48a9e1a6b2"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:28:58.659Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638efd6895f0af48a9e1a6b5"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:29:28.323Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "638efd6195f0af48a9e1a6b3"}, {"_id": {"$oid": "638efd7095f0af48a9e1a6b6"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:29:36.905Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638efd7295f0af48a9e1a6b8"}, "user_id": "", "time": {"$date": "2022-12-06T08:29:38.007Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638efd6195f0af48a9e1a6b3"}, {"_id": {"$oid": "638efd7c95f0af48a9e1a6b9"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T08:29:48.863Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638efd7d95f0af48a9e1a6bb"}, "user_id": "", "time": {"$date": "2022-12-06T08:29:49.165Z"}, "text": "{\"writingInput\":[\"We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638efd6195f0af48a9e1a6b3"}, {"_id": {"$oid": "638efda195f0af48a9e1a6bd"}, "user_id": "", "time": {"$date": "2022-12-06T08:30:25.355Z"}, "text": "{\"writingInput\":[\"Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .\"],\"explainInput\":\"how to improve?\\n\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638efd6195f0af48a9e1a6b3"}, {"_id": {"$oid": "638efdae95f0af48a9e1a6bf"}, "user_id": "", "time": {"$date": "2022-12-06T08:30:38.117Z"}, "text": "{\"writingInput\":[\"We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .\"],\"explainInput\":\"improve?\\n\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638efd6195f0af48a9e1a6b3"}, {"_id": {"$oid": "638f6a038a37977af6a11ba7"}, "user_id": "11111", "time": {"$date": "2022-12-06T16:12:51.257Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decisionmaking. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a returnconditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "type": "task", "writing_model": "", "conversation_id": "638f69d18a37977af6a11ba5"}, {"_id": {"$oid": "638f6a078a37977af6a11ba9"}, "user_id": "", "time": {"$date": "2022-12-06T16:12:55.950Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f69d18a37977af6a11ba5"}, {"_id": {"$oid": "638f6a1b8a37977af6a11baa"}, "user_id": "11111", "time": {"$date": "2022-12-06T16:13:15.614Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decisionmaking .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a returnconditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f6a508a37977af6a11bac"}, "user_id": "", "time": {"$date": "2022-12-06T16:14:08.134Z"}, "text": "{\"writingInput\":[\"Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\"],\"explainInput\":\"how to improve\\n\",\"writingIndex\":[\"6\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f69d18a37977af6a11ba5"}, {"_id": {"$oid": "638f6adb8a37977af6a11bae"}, "user_id": "", "time": {"$date": "2022-12-06T16:16:27.353Z"}, "text": "{\"writingInput\":[\"Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\"],\"explainInput\":\"counterfactual\\n\",\"writingIndex\":[\"6\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f69d18a37977af6a11ba5"}, {"_id": {"$oid": "638f6afd8a37977af6a11bb0"}, "user_id": "", "time": {"$date": "2022-12-06T16:17:01.424Z"}, "text": "{\"writingInput\":[\"Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\"],\"explainInput\":\"method\",\"writingIndex\":[\"6\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f69d18a37977af6a11ba5"}, {"_id": {"$oid": "638f6b4a8a37977af6a11bb2"}, "user_id": "11111", "time": {"$date": "2022-12-06T16:18:18.710Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decisionmaking .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a returnconditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .\nWe\u00a0investigate\u00a0how\u00a0conditioning\u00a0on a single constraint or skill during training\u00a0affects\u00a0behaviors at test-time that can satisfy\u00a0multiple\u00a0constraints together or demonstrate a composition of skills.Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f69d18a37977af6a11ba5"}, {"_id": {"$oid": "638f6b4e8a37977af6a11bb4"}, "user_id": "", "time": {"$date": "2022-12-06T16:18:22.762Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f69d18a37977af6a11ba5"}, {"_id": {"$oid": "638f6b5f8a37977af6a11bb5"}, "user_id": "11111", "time": {"$date": "2022-12-06T16:18:39.200Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decisionmaking .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a returnconditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We investigate how conditioning on a single constraint or skill during training affects behaviors at test-time that can satisfy multiple constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f6b6d8a37977af6a11bb7"}, "user_id": "", "time": {"$date": "2022-12-06T16:18:53.555Z"}, "text": "{\"writingInput\":[\"Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\"],\"explainInput\":\"example\\n\",\"writingIndex\":[\"7\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f69d18a37977af6a11ba5"}, {"_id": {"$oid": "638f6bb88a37977af6a11bb9"}, "user_id": "", "time": {"$date": "2022-12-06T16:20:08.801Z"}, "text": "{\"writingInput\":[\"Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\"],\"explainInput\":\"label:background, keyword: answer, rank:quality_score\",\"writingIndex\":[\"7\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f69d18a37977af6a11ba5"}, {"_id": {"$oid": "638f6be28a37977af6a11bbb"}, "user_id": "", "time": {"$date": "2022-12-06T16:20:50.931Z"}, "text": "{\"writingInput\":[\"Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\"],\"explainInput\":\"what's the important words\\n\",\"writingIndex\":[\"7\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f69d18a37977af6a11ba5"}, {"_id": {"$oid": "638f6cac8a37977af6a11bbe"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:24:12.496Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision- making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return- conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "type": "task", "writing_model": "", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6cb08a37977af6a11bbf"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:24:16.462Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision- making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return- conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f6cb18a37977af6a11bc1"}, "user_id": "", "time": {"$date": "2022-12-06T16:24:17.789Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6cbb8a37977af6a11bc2"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:24:27.770Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f6e668a37977af6a11bc4"}, "user_id": "", "time": {"$date": "2022-12-06T16:31:34.670Z"}, "text": "{\"writingInput\":[\"To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\"],\"explainInput\":\"How confident is this prediction?\",\"writingIndex\":[\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6eb78a37977af6a11bc5"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:32:55.305Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\nTo our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\nOur results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f6edd8a37977af6a11bc7"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:33:33.238Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\nTo our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\nOur results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6ee28a37977af6a11bc9"}, "user_id": "", "time": {"$date": "2022-12-06T16:33:38.583Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6ef38a37977af6a11bca"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:33:55.394Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f6f528a37977af6a11bcc"}, "user_id": "", "time": {"$date": "2022-12-06T16:35:30.691Z"}, "text": "{\"writingInput\":[\"Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\"],\"explainInput\":\"How confident is this prediction?\\n\",\"writingIndex\":[\"5\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6f8f8a37977af6a11bce"}, "user_id": "", "time": {"$date": "2022-12-06T16:36:31.020Z"}, "text": "{\"writingInput\":[\"Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\"],\"explainInput\":\"How can I revise the input to get a different prediction?\\n\",\"writingIndex\":[\"5\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6fb78a37977af6a11bd0"}, "user_id": "", "time": {"$date": "2022-12-06T16:37:11.009Z"}, "text": "{\"writingInput\":[\"Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\"],\"explainInput\":\"No\\n\",\"writingIndex\":[\"5\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6fcc8a37977af6a11bd1"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:37:32.942Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\nOur\u00a0results\u00a0show\u00a0that\u00a0conditioning\u00a0on a single constraint or skill during training leads to behaviors at test-time that can satisfy\u00a0multiple\u00a0constraints together or demonstrate a\u00a0combination\u00a0of skills\nTo our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f6fdb8a37977af6a11bd3"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:37:47.486Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our\u00a0results\u00a0show\u00a0that\u00a0conditioning\u00a0on a single constraint or skill during training leads to behaviors at test-time that can satisfy\u00a0multiple\u00a0constraints together or demonstrate a\u00a0combination\u00a0of skills\nTo our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6fe08a37977af6a11bd5"}, "user_id": "", "time": {"$date": "2022-12-06T16:37:52.726Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6fe58a37977af6a11bd7"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:37:57.620Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6fea8a37977af6a11bd9"}, "user_id": "", "time": {"$date": "2022-12-06T16:38:02.382Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6ff58a37977af6a11bdb"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:38:13.921Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .\nOur\u00a0results\u00a0show\u00a0that\u00a0conditioning\u00a0on a single constraint or skill during training leads to behaviors at test-time that can satisfy\u00a0multiple\u00a0constraints together or demonstrate a\u00a0combination\u00a0of skills.\nTo our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f6ffa8a37977af6a11bdd"}, "user_id": "", "time": {"$date": "2022-12-06T16:38:18.998Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f700b8a37977af6a11bde"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:38:35.658Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f70598a37977af6a11be0"}, "user_id": "", "time": {"$date": "2022-12-06T16:39:53.295Z"}, "text": "{\"writingInput\":[\"Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\"],\"explainInput\":\"How can I revise the input to get a different prediction?\\n\",\"writingIndex\":[\"7\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f70a08a37977af6a11be2"}, "user_id": "", "time": {"$date": "2022-12-06T16:41:04.587Z"}, "text": "{\"writingInput\":[\"Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\"],\"explainInput\":\"What are some published sentences that look similar to mine semantically?\\n\",\"writingIndex\":[\"7\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f71318a37977af6a11be3"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:43:29.731Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling is more appropriate approach for decision-making in terms of accuracy.\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f714c8a37977af6a11be5"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:43:56.295Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f714e8a37977af6a11be6"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:43:58.594Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy.\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f71518a37977af6a11be8"}, "user_id": "", "time": {"$date": "2022-12-06T16:44:01.406Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f71618a37977af6a11be9"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:44:17.124Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f71b48a37977af6a11beb"}, "user_id": "", "time": {"$date": "2022-12-06T16:45:40.652Z"}, "text": "{\"writingInput\":[\"Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .\"],\"explainInput\":\"How confident is this prediction?\\n\",\"writingIndex\":[\"0\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f71de8a37977af6a11bed"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:46:22.626Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f71e38a37977af6a11bef"}, "user_id": "", "time": {"$date": "2022-12-06T16:46:27.532Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f72018a37977af6a11bf0"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:46:57.317Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f726c8a37977af6a11bf2"}, "user_id": "", "time": {"$date": "2022-12-06T16:48:44.217Z"}, "text": "{\"writingInput\":[\"By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .\"],\"explainInput\":\"How can I revise the input to get a different prediction?\\n\",\"writingIndex\":[\"3\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f72be8a37977af6a11bf3"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:50:06.182Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .\nWe\u00a0present\u00a0a\u00a0method\u00a0to\u00a0simplify\u00a0offline reinforcement\u00a0learning\u00a0(RL)\u00a0by\u00a0modeling a policy as a return-conditional diffusion\u00a0model,\u00a0allowing\u00a0us\u00a0tobypass\u00a0the need for dynamic programming and\u00a0reducemany of the complexities\u00a0associated\u00a0with traditional offline RL.\nWe further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f72db8a37977af6a11bf5"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:50:35.650Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We\u00a0present\u00a0a\u00a0method\u00a0to\u00a0simplify\u00a0offline reinforcement\u00a0learning\u00a0(RL)\u00a0by\u00a0modeling a policy as a return-conditional diffusion\u00a0model,\u00a0allowing\u00a0us\u00a0to bypass\u00a0the need for dynamic programming and\u00a0reduce many of the complexities\u00a0associated\u00a0with traditional offline RL.\nWe further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f72e08a37977af6a11bf7"}, "user_id": "", "time": {"$date": "2022-12-06T16:50:40.743Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f72f08a37977af6a11bf8"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:50:56.892Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model , allowing us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f73238a37977af6a11bf9"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:51:47.714Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model. This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f73268a37977af6a11bfb"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:51:50.999Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model. This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f732b8a37977af6a11bfd"}, "user_id": "", "time": {"$date": "2022-12-06T16:51:55.857Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f73468a37977af6a11bfe"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:52:22.360Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f736f8a37977af6a11c00"}, "user_id": "", "time": {"$date": "2022-12-06T16:53:03.559Z"}, "text": "{\"writingInput\":[\"We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .\"],\"explainInput\":\"How can I revise the input to get a different prediction?\\n\",\"writingIndex\":[\"5\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f73848a37977af6a11c02"}, "user_id": "", "time": {"$date": "2022-12-06T16:53:24.092Z"}, "text": "{\"writingInput\":[\"We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .\"],\"explainInput\":\"finding\\n\",\"writingIndex\":[\"5\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f73d98a37977af6a11c03"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:54:49.132Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We\u00a0find\u00a0that\u00a0modeling policies as conditional diffusion models\u00a0offers\u00a0advantages\u00a0when\u00a0considering two other conditioning\u00a0variables,\u00a0such\u00a0as\u00a0constraints and skills.\nOur results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f73da8a37977af6a11c05"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:54:50.768Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We\u00a0find\u00a0that\u00a0modeling policies as conditional diffusion models\u00a0offers\u00a0advantages\u00a0when\u00a0considering two other conditioning\u00a0variables,\u00a0such\u00a0as\u00a0constraints and skills.\nOur results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f73e08a37977af6a11c07"}, "user_id": "", "time": {"$date": "2022-12-06T16:54:56.121Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f73f28a37977af6a11c08"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:55:14.178Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We find that modeling policies as conditional diffusion models offers advantages when considering two other conditioning variables , such as constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .Our results illustrate that conditional generative modeling outperforms several baseline models for decision-making in terms of accuracy .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f742e8a37977af6a11c0a"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:56:14.459Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We find that modeling policies as conditional diffusion models offers advantages when considering two other conditioning variables , such as constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f74338a37977af6a11c0c"}, "user_id": "", "time": {"$date": "2022-12-06T16:56:19.701Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f743d8a37977af6a11c0d"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:56:29.694Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We find that modeling policies as conditional diffusion models offers advantages when considering two other conditioning variables , such as constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f746a8a37977af6a11c0e"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:57:14.447Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .\nWe investigate whether these methods can directly address the problem of sequential decision-making .\nWe view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .\nWe present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .\nThis approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .\nWe find that modeling policies as conditional diffusion models offers advantages when considering two other conditioning variables , such as constraints and skills .\nOur results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .\nTo our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\n\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f74748a37977af6a11c10"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:57:24.994Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .\nWe investigate whether these methods can directly address the problem of sequential decision-making .\nWe view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .\nWe present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .\nThis approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .\nWe find that modeling policies as conditional diffusion models offers advantages when considering two other conditioning variables , such as constraints and skills .\nOur results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .\nWe also find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\n\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f747a8a37977af6a11c12"}, "user_id": "", "time": {"$date": "2022-12-06T16:57:30.216Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f6c928a37977af6a11bbc"}, {"_id": {"$oid": "638f74848a37977af6a11c13"}, "user_id": "Maryam Tabar", "time": {"$date": "2022-12-06T16:57:40.206Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We find that modeling policies as conditional diffusion models offers advantages when considering two other conditioning variables , such as constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .We also find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f92428a37977af6a11c16"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T19:04:34.966Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "638f92368a37977af6a11c14"}, {"_id": {"$oid": "638f92488a37977af6a11c18"}, "user_id": "", "time": {"$date": "2022-12-06T19:04:40.660Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f92368a37977af6a11c14"}, {"_id": {"$oid": "638f92528a37977af6a11c19"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T19:04:50.655Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f92858a37977af6a11c1b"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T19:05:41.306Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f92368a37977af6a11c14"}, {"_id": {"$oid": "638f928b8a37977af6a11c1d"}, "user_id": "", "time": {"$date": "2022-12-06T19:05:47.286Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f92368a37977af6a11c14"}, {"_id": {"$oid": "638f92958a37977af6a11c1e"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T19:05:57.273Z"}, "text": "Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .The improved interpretability is believed to offer a sense of security by involving human in the decision-making process .Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .We show that existing IDLSes are highly vulnerable to adversarial manipulations .Specifically , we present ADV2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models .Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications ( e.g. , skin cancer diagnosis ) , we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input 's prediction and interpretation .Further , with both analytical and empirical evidence , we identify the prediction-interpretation gap as one root cause of this vulnerability-a DNN and its interpretation model are often misaligned , resulting in the possibility of exploiting both models simultaneously .Finally , we explore potential countermeasures against ADV2 , including leveraging its low transferability and incorporating it in an adversarial training framework .Our findings shed light on designing and operating IDLSes in a more secure and informative fashion , leading to several promising research directions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f929f8a37977af6a11c20"}, "user_id": "", "time": {"$date": "2022-12-06T19:06:07.198Z"}, "text": "{\"writingInput\":[\"Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .\"],\"explainInput\":\"how to improve\\n\",\"writingIndex\":[\"0\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f92368a37977af6a11c14"}, {"_id": {"$oid": "638f93408a37977af6a11c22"}, "user_id": "", "time": {"$date": "2022-12-06T19:08:48.666Z"}, "text": "{\"writingInput\":[\"Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .\",\"Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"0\",\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f92368a37977af6a11c14"}, {"_id": {"$oid": "638f93628a37977af6a11c24"}, "user_id": "", "time": {"$date": "2022-12-06T19:09:22.027Z"}, "text": "{\"writingInput\":[\"Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .\"],\"explainInput\":\"similar examples\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f92368a37977af6a11c14"}, {"_id": {"$oid": "638f93d48a37977af6a11c26"}, "user_id": "", "time": {"$date": "2022-12-06T19:11:16.020Z"}, "text": "{\"writingInput\":[\"Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .\"],\"explainInput\":\"label:purpose, keyword: answer, rank:quality_score\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f92368a37977af6a11c14"}, {"_id": {"$oid": "638f93ff8a37977af6a11c28"}, "user_id": "", "time": {"$date": "2022-12-06T19:11:59.553Z"}, "text": "{\"writingInput\":[\"Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .\"],\"explainInput\":\"counterfactual prediction\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f92368a37977af6a11c14"}, {"_id": {"$oid": "638f989d8a37977af6a11c2b"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:31:41.073Z"}, "text": "Reinforcement Learning (RL) algorithms are known to scale poorly to environ\u0002ments with many available actions, requiring numerous samples to learn an opti\u0002mal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as inapplicable actions (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL al\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm. In this paper, we propose a more systematic approach to introduce this knowledge into the algorithm. We (i) standardize the way knowledge can be manually specified to the agent; and (ii) present a new framework to autonomously learn these state\u0002dependent action constraints jointly with the policy. We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions. Moreover, we demonstrate that thanks to the transferability of the knowledge acquired, it can be reused in other tasks to make the learning process more efficient.\n", "type": "task", "writing_model": "", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f98a58a37977af6a11c2c"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:31:49.054Z"}, "text": "Reinforcement Learning (RL) algorithms are known to scale poorly to environ\u0002ments with many available actions, requiring numerous samples to learn an opti\u0002mal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as inapplicable actions (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL al\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm. In this paper, we propose a more systematic approach to introduce this knowledge into the algorithm. We (i) standardize the way knowledge can be manually specified to the agent; and (ii) present a new framework to autonomously learn these state\u0002dependent action constraints jointly with the policy. We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions. Moreover, we demonstrate that thanks to the transferability of the knowledge acquired, it can be reused in other tasks to make the learning process more efficient.\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f98a68a37977af6a11c2e"}, "user_id": "", "time": {"$date": "2022-12-06T19:31:50.366Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f98b78a37977af6a11c2f"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:32:07.367Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL al\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f98ef8a37977af6a11c31"}, "user_id": "", "time": {"$date": "2022-12-06T19:33:03.718Z"}, "text": "{\"writingInput\":[],\"explainInput\":\"How can I revise the input to get a different prediction?\",\"writingIndex\":[],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f98fa8a37977af6a11c33"}, "user_id": "", "time": {"$date": "2022-12-06T19:33:14.445Z"}, "text": "{\"writingInput\":[\"Knowing this information can help reduce the sample complexity of RL al\\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .\"],\"explainInput\":\"How can I revise the input to get a different prediction?\",\"writingIndex\":[\"2\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f99188a37977af6a11c34"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:33:44.383Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f99278a37977af6a11c36"}, "user_id": "", "time": {"$date": "2022-12-06T19:33:59.226Z"}, "text": "{\"writingInput\":[\"Knowing this information can help reduce the sample complexity of RL al\\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .\"],\"explainInput\":\"purpose\\n\",\"writingIndex\":[\"2\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f997d8a37977af6a11c38"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:35:25.417Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .\nIn\u00a0this\u00a0work,\u00a0we\u00a0aim\u00a0to\u00a0reduce the sample complexity of\u00a0existing\u00a0reinforcement\u00a0learning\u00a0(RL)\u00a0algorithms\u00a0by\u00a0utilizing\u00a0information\u00a0to\u00a0mask\u00a0the inapplicable actions from the policy distribution\u00a0and\u00a0focus\u00a0on\u00a0exploring\u00a0those\u00a0relevant to finding an optimal policy.This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f997e8a37977af6a11c39"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:35:26.430Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .\nIn\u00a0this\u00a0work,\u00a0we\u00a0aim\u00a0to\u00a0reduce the sample complexity of\u00a0existing\u00a0reinforcement\u00a0learning\u00a0(RL)\u00a0algorithms\u00a0by\u00a0utilizing\u00a0information\u00a0to\u00a0mask\u00a0the inapplicable actions from the policy distribution\u00a0and\u00a0focus\u00a0on\u00a0exploring\u00a0those\u00a0relevant to finding an optimal policy.This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f99838a37977af6a11c3b"}, "user_id": "", "time": {"$date": "2022-12-06T19:35:31.755Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f998d8a37977af6a11c3c"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:35:41.739Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by utilizing information to mask the inapplicable actions from the policy distribution and focus on exploring those relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f99b98a37977af6a11c3e"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:36:25.548Z"}, "text": "Reinforcement Learning (RL) algorithms are known to scale poorly to environ\u0002ments with many available actions, requiring numerous samples to learn an opti\u0002mal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as inapplicable actions (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL al\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm. In this paper, we propose a more systematic approach to introduce this knowledge into the algorithm. We (i) standardize the way knowledge can be manually specified to the agent; and (ii) present a new framework to autonomously learn these state\u0002dependent action constraints jointly with the policy. We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions. Moreover, we demonstrate that thanks to the transferability of the knowledge acquired, it can be reused in other tasks to make the learning process more efficient.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f99bf8a37977af6a11c40"}, "user_id": "", "time": {"$date": "2022-12-06T19:36:31.694Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f99ce8a37977af6a11c41"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:36:46.570Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL al\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f99d48a37977af6a11c43"}, "user_id": "", "time": {"$date": "2022-12-06T19:36:52.177Z"}, "text": "{\"writingInput\":[\"Knowing this information can help reduce the sample complexity of RL al\\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .\"],\"explainInput\":\"How can I revise the input to get a different prediction?\",\"writingIndex\":[\"2\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f99f78a37977af6a11c44"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:37:27.263Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL\u00a0algorithms\u00a0by\u00a0preventing\u00a0the\u00a0agent\u00a0from\u00a0exploring\u00a0inapplicable actions\u00a0and\u00a0instead\u00a0focusing\u00a0on\u00a0only\u00a0those\u00a0that\u00a0are\u00a0relevant to finding an optimal policy.This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9a078a37977af6a11c46"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:37:43.848Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL\u00a0algorithms\u00a0by\u00a0preventing\u00a0the\u00a0agent\u00a0from\u00a0exploring\u00a0inapplicable actions\u00a0and\u00a0instead\u00a0focusing\u00a0on\u00a0only\u00a0those\u00a0that\u00a0are\u00a0relevant to finding an optimal policy.This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9a0d8a37977af6a11c48"}, "user_id": "", "time": {"$date": "2022-12-06T19:37:49.989Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9a178a37977af6a11c49"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:37:59.989Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL algorithms by preventing the agent from exploring inapplicable actions and instead focusing on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9a388a37977af6a11c4b"}, "user_id": "", "time": {"$date": "2022-12-06T19:38:32.917Z"}, "text": "{\"writingInput\":[],\"explainInput\":\"How can I revise the input to get a different prediction?\",\"writingIndex\":[],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9a3e8a37977af6a11c4d"}, "user_id": "", "time": {"$date": "2022-12-06T19:38:38.794Z"}, "text": "{\"writingInput\":[\"Knowing this information can help reduce the sample complexity of RL algorithms by preventing the agent from exploring inapplicable actions and instead focusing on only those that are relevant to finding an optimal policy .\"],\"explainInput\":\"How can I revise the input to get a different prediction?\",\"writingIndex\":[\"2\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9a598a37977af6a11c4f"}, "user_id": "", "time": {"$date": "2022-12-06T19:39:05.376Z"}, "text": "{\"writingInput\":[\"Knowing this information can help reduce the sample complexity of RL algorithms by preventing the agent from exploring inapplicable actions and instead focusing on only those that are relevant to finding an optimal policy .\"],\"explainInput\":\"purpose\\n\",\"writingIndex\":[\"2\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9a758a37977af6a11c51"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:39:33.839Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .In\u00a0this\u00a0work,\u00a0we\u00a0aim\u00a0to\u00a0reduce the sample complexity of\u00a0existing\u00a0reinforcement\u00a0learning\u00a0(RL)\u00a0algorithms by\u00a0providing\u00a0information\u00a0about\u00a0the\u00a0task\u00a0environment\u00a0that\u00a0helps\u00a0prevent\u00a0the agent from exploring inapplicable actions and instead\u00a0focuses\u00a0on only those that are relevant to finding an optimal policy.This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9a7c8a37977af6a11c53"}, "user_id": "", "time": {"$date": "2022-12-06T19:39:40.024Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9a858a37977af6a11c54"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:39:49.988Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9ab18a37977af6a11c56"}, "user_id": "", "time": {"$date": "2022-12-06T19:40:33.807Z"}, "text": "{\"writingInput\":[\"The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"1\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9acf8a37977af6a11c58"}, "user_id": "", "time": {"$date": "2022-12-06T19:41:03.378Z"}, "text": "{\"writingInput\":[\"The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .\"],\"explainInput\":\"Which words in this sentence are most important for this prediction?\",\"writingIndex\":[\"1\"],\"ButtonID\":\"[Important Words]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9b7a8a37977af6a11c59"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:43:54.270Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9b9c8a37977af6a11c5a"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:44:28.052Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions ( i.e. actions that have no effect on the environment when performed in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9ba38a37977af6a11c5c"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:44:35.282Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions ( i.e. actions that have no effect on the environment in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9ba98a37977af6a11c5e"}, "user_id": "", "time": {"$date": "2022-12-06T19:44:41.316Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9bb38a37977af6a11c5f"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:44:51.305Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions ( i.e. actions that have no effect on the environment in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9bef8a37977af6a11c61"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:45:51.654Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward.In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9bf48a37977af6a11c62"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:45:56.620Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward.In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9bf58a37977af6a11c64"}, "user_id": "", "time": {"$date": "2022-12-06T19:45:57.992Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9bff8a37977af6a11c65"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:46:07.986Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9c348a37977af6a11c67"}, "user_id": "", "time": {"$date": "2022-12-06T19:47:00.132Z"}, "text": "{\"writingInput\":[\"Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\\u0002ments with many available actions , requiring numerous samples to learn an opti\\u0002mal policy .\"],\"explainInput\":\"How can I revise the input to get a different prediction?\",\"writingIndex\":[\"0\"],\"ButtonID\":\"[Important Words]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9c4e8a37977af6a11c69"}, "user_id": "", "time": {"$date": "2022-12-06T19:47:26.535Z"}, "text": "{\"writingInput\":[\"Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\\u0002ments with many available actions , requiring numerous samples to learn an opti\\u0002mal policy .\"],\"explainInput\":\"background\\n\",\"writingIndex\":[\"0\"],\"ButtonID\":\"[Important Words]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9c778a37977af6a11c6b"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:48:07.446Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to\u00a0have\u00a0difficulty\u00a0generalizing\u00a0to\u00a0environments\u00a0with\u00a0a\u00a0high-dimensional\u00a0observation\u00a0space,\u00a0and\u00a0require\u00a0a\u00a0large\u00a0number\u00a0of\u00a0samples\u00a0for\u00a0efficient\u00a0learning\u00a0of\u00a0an\u00a0optimal\u00a0policy.The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9c7a8a37977af6a11c6c"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:48:10.281Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to\u00a0have\u00a0difficulty\u00a0generalizing\u00a0to\u00a0environments\u00a0with\u00a0a\u00a0high-dimensional\u00a0observation\u00a0space,\u00a0and\u00a0require\u00a0a\u00a0large\u00a0number\u00a0of\u00a0samples\u00a0for\u00a0efficient\u00a0learning\u00a0of\u00a0an\u00a0optimal\u00a0policy.The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9c7d8a37977af6a11c6e"}, "user_id": "", "time": {"$date": "2022-12-06T19:48:13.813Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9c8f8a37977af6a11c6f"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:48:31.303Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to have difficulty generalizing to environments with a high-dimensional observation space , and require a large number of samples for efficient learning of an optimal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638f9ccd8a37977af6a11c71"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:49:33.166Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to have difficulty generalizing to environments with a high-dimensional observation space , and require a large number of samples for efficient learning of an optimal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9cd38a37977af6a11c73"}, "user_id": "", "time": {"$date": "2022-12-06T19:49:39.126Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638f98948a37977af6a11c29"}, {"_id": {"$oid": "638f9cdd8a37977af6a11c74"}, "user_id": "p2", "time": {"$date": "2022-12-06T19:49:49.822Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to have difficulty generalizing to environments with a high-dimensional observation space , and require a large number of samples for efficient learning of an optimal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638facee750731de207f3807"}, "user_id": "CY", "time": {"$date": "2022-12-06T20:58:22.792Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638fadd5750731de207f380d"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:02:13.424Z"}, "text": "\n", "type": "task", "writing_model": "", "conversation_id": "638fadc1750731de207f380b"}, {"_id": {"$oid": "638faf39750731de207f3813"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:08:09.555Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb0e08a37977af6a11c75"}, "user_id": "11111", "time": {"$date": "2022-12-06T21:15:12.401Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decisionmaking .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a returnconditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We investigate how conditioning on a single constraint or skill during training affects behaviors at test-time that can satisfy multiple constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb65e750731de207f3816"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:38:38.632Z"}, "text": "what is this\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb65e750731de207f3818"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:38:38.637Z"}, "text": "what is this\n", "type": "task", "writing_model": "", "conversation_id": "638fb657750731de207f3815"}, {"_id": {"$oid": "638fb663750731de207f381a"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:38:43.442Z"}, "text": "what is this\n", "type": "task", "writing_model": "", "conversation_id": "638fb657750731de207f3815"}, {"_id": {"$oid": "638fb673750731de207f381d"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:38:59.638Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "type": "task", "writing_model": "", "conversation_id": "638fb66d750731de207f381b"}, {"_id": {"$oid": "638fb67b750731de207f381e"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:39:07.030Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb682750731de207f3820"}, "user_id": "", "time": {"$date": "2022-12-06T21:39:14.676Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fb66d750731de207f381b"}, {"_id": {"$oid": "638fb68c750731de207f3821"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:39:24.653Z"}, "text": "Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .The improved interpretability is believed to offer a sense of security by involving human in the decision-making process .Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .We show that existing IDLSes are highly vulnerable to adversarial manipulations .Specifically , we present ADV2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models .Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications ( e.g. , skin cancer diagnosis ) , we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input 's prediction and interpretation .Further , with both analytical and empirical evidence , we identify the prediction-interpretation gap as one root cause of this vulnerability-a DNN and its interpretation model are often misaligned , resulting in the possibility of exploiting both models simultaneously .Finally , we explore potential countermeasures against ADV2 , including leveraging its low transferability and incorporating it in an adversarial training framework .Our findings shed light on designing and operating IDLSes in a more secure and informative fashion , leading to several promising research directions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb68c750731de207f3822"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:39:24.794Z"}, "text": "Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .The improved interpretability is believed to offer a sense of security by involving human in the decision-making process .Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .We show that existing IDLSes are highly vulnerable to adversarial manipulations .Specifically , we present ADV2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models .Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications ( e.g. , skin cancer diagnosis ) , we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input 's prediction and interpretation .Further , with both analytical and empirical evidence , we identify the prediction-interpretation gap as one root cause of this vulnerability-a DNN and its interpretation model are often misaligned , resulting in the possibility of exploiting both models simultaneously .Finally , we explore potential countermeasures against ADV2 , including leveraging its low transferability and incorporating it in an adversarial training framework .Our findings shed light on designing and operating IDLSes in a more secure and informative fashion , leading to several promising research directions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb68c750731de207f3824"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:39:24.797Z"}, "text": "Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .The improved interpretability is believed to offer a sense of security by involving human in the decision-making process .Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .We show that existing IDLSes are highly vulnerable to adversarial manipulations .Specifically , we present ADV2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models .Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications ( e.g. , skin cancer diagnosis ) , we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input 's prediction and interpretation .Further , with both analytical and empirical evidence , we identify the prediction-interpretation gap as one root cause of this vulnerability-a DNN and its interpretation model are often misaligned , resulting in the possibility of exploiting both models simultaneously .Finally , we explore potential countermeasures against ADV2 , including leveraging its low transferability and incorporating it in an adversarial training framework .Our findings shed light on designing and operating IDLSes in a more secure and informative fashion , leading to several promising research directions .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fb66d750731de207f381b"}, {"_id": {"$oid": "638fb696750731de207f3826"}, "user_id": "", "time": {"$date": "2022-12-06T21:39:34.344Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fb66d750731de207f381b"}, {"_id": {"$oid": "638fb6a0750731de207f3827"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:39:44.359Z"}, "text": "Providing explanations for deep neural network ( DNN ) models is crucial for their use in security-sensitive domains .The improved interpretability is believed to offer a sense of security by involving human in the decision-making process .Yet , due to its data-driven nature , the interpretability itself is potentially susceptible to malicious manipulations , about which little is known thus far .Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems ( IDLSes ) .We show that existing IDLSes are highly vulnerable to adversarial manipulations .Specifically , we present ADV2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models .Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications ( e.g. , skin cancer diagnosis ) , we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input 's prediction and interpretation .Further , with both analytical and empirical evidence , we identify the prediction-interpretation gap as one root cause of this vulnerability-a DNN and its interpretation model are often misaligned , resulting in the possibility of exploiting both models simultaneously .Finally , we explore potential countermeasures against ADV2 , including leveraging its low transferability and incorporating it in an adversarial training framework .Our findings shed light on designing and operating IDLSes in a more secure and informative fashion , leading to several promising research directions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb6b8750731de207f3828"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:40:08.412Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb735750731de207f382a"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:42:13.732Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb763750731de207f382d"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:42:59.705Z"}, "text": "\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb764750731de207f382e"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:00.172Z"}, "text": "\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb764750731de207f382f"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:00.342Z"}, "text": "\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb764750731de207f3830"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:00.510Z"}, "text": "\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb766750731de207f3831"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:02.217Z"}, "text": "\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb766750731de207f3832"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:02.387Z"}, "text": "\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb77b750731de207f3834"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:23.416Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb77c750731de207f3835"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:24.600Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb77c750731de207f3836"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:24.775Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb77c750731de207f3837"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:24.953Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb77d750731de207f3838"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:25.446Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb77d750731de207f3839"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:25.630Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb77d750731de207f383a"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:25.808Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb77e750731de207f383b"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:26.025Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb784750731de207f383c"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:32.605Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb78a750731de207f383d"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:38.053Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb78a750731de207f383e"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:38.232Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb78a750731de207f383f"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:38.383Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb78a750731de207f3840"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:38.545Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb78a750731de207f3841"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:38.851Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb78b750731de207f3842"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:39.069Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb78b750731de207f3843"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:39.231Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb78f750731de207f3845"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:43:43.241Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "638fb777750731de207f3833"}, {"_id": {"$oid": "638fb798750731de207f3847"}, "user_id": "", "time": {"$date": "2022-12-06T21:43:52.676Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fb777750731de207f3833"}, {"_id": {"$oid": "638fb7a4750731de207f3848"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:44:04.686Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb7af750731de207f3849"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:44:15.061Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb7b4750731de207f384a"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:44:20.724Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb7e4750731de207f384c"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:45:08.265Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fb85f12860f9b7208578b"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:47:11.082Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "save"}, {"_id": {"$oid": "638fb8c812860f9b7208578d"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:48:56.239Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "save"}, {"_id": {"$oid": "638fb8ff12860f9b7208578f"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:49:51.384Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "save"}, {"_id": {"$oid": "638fb95012860f9b72085791"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:51:12.768Z"}, "text": "sdf\n", "event_type": "save"}, {"_id": {"$oid": "638fb97712860f9b72085793"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:51:51.720Z"}, "text": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing IDLSes are highly vulnerable to adversarial manipulations. Specifically, we present ADV2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability - a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions. \n", "event_type": "save"}, {"_id": {"$oid": "638fba2612860f9b72085796"}, "user_id": "CY", "time": {"$date": "2022-12-06T21:54:46.604Z"}, "text": "\n", "event_type": "save"}, {"_id": {"$oid": "638fc96958bc1a83a15166b7"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T22:59:53.131Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "638fc95f58bc1a83a15166b5"}, {"_id": {"$oid": "638fc96e58bc1a83a15166b9"}, "user_id": "", "time": {"$date": "2022-12-06T22:59:58.113Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fc95f58bc1a83a15166b5"}, {"_id": {"$oid": "638fc97a58bc1a83a15166ba"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T23:00:10.324Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\nSave it!!!\n", "event_type": "save"}, {"_id": {"$oid": "638fd6b358bc1a83a15166bd"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T23:56:35.020Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. \n\nWe investigate whether these methods can directly address the problem of sequential decision- making. \n\nWe view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. \n\nTo our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. \n\nBy modeling a policy as a return- conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. \n\nWe further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. \n\nConditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. \n\nOur results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "type": "task", "writing_model": "", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd6ba58bc1a83a15166be"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T23:56:42.679Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. \n\nWe investigate whether these methods can directly address the problem of sequential decision- making. \n\nWe view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. \n\nTo our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. \n\nBy modeling a policy as a return- conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. \n\nWe further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. \n\nConditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. \n\nOur results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd6bc58bc1a83a15166c0"}, "user_id": "", "time": {"$date": "2022-12-06T23:56:44.370Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd6c658bc1a83a15166c1"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T23:56:54.382Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return-conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd6fa58bc1a83a15166c3"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T23:57:46.194Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decisionmaking. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a returnconditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.\n\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd6ff58bc1a83a15166c5"}, "user_id": "", "time": {"$date": "2022-12-06T23:57:51.611Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd70958bc1a83a15166c6"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-06T23:58:01.623Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decisionmaking .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a returnconditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd78458bc1a83a15166c8"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:00:04.638Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. \n\nWe investigate whether these methods can directly address the problem of sequential decision- making. \n\nWe view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. \n\nTo our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. \n\nBy modeling a policy as a returnconditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. \n\nWe further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. \n\nConditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. \n\nOur results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd78958bc1a83a15166ca"}, "user_id": "", "time": {"$date": "2022-12-07T00:00:09.434Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd7a858bc1a83a15166cc"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:00:40.170Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone.We investigate whether these methods can directly address the problem of sequential decision- making. \nWe view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. \nTo our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. \nBy modeling a policy as a returnconditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. \nWe further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. \nConditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. \nOur results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd7ad58bc1a83a15166ce"}, "user_id": "", "time": {"$date": "2022-12-07T00:00:45.120Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd7b758bc1a83a15166cf"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:00:55.103Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a returnconditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd7c658bc1a83a15166d1"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:01:10.749Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decisionmaking. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a returnconditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd7cc58bc1a83a15166d3"}, "user_id": "", "time": {"$date": "2022-12-07T00:01:16.003Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd7d658bc1a83a15166d4"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:01:26.013Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decisionmaking .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a returnconditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd7e358bc1a83a15166d6"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:01:39.229Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .\n\nWe investigate whether these methods can directly address the problem of sequential decision-making .\n\nWe view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .\n\nWe present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .\n\nThis approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .\n\nWe find that modeling policies as conditional diffusion models offers advantages when considering two other conditioning variables , such as constraints and skills .\n\nOur results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .\n\nWe also find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd7e858bc1a83a15166d8"}, "user_id": "", "time": {"$date": "2022-12-07T00:01:44.065Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd7f258bc1a83a15166d9"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:01:54.196Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We find that modeling policies as conditional diffusion models offers advantages when considering two other conditioning variables , such as constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .We also find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd81258bc1a83a15166db"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:02:26.194Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .\nWe investigate whether these methods can directly address the problem of sequential decision-making .\nWe view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling.\nWe present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .\nThis approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .\nWe find that modeling policies as conditional diffusion models offers advantages when considering two other conditioning variables , such as constraints and skills .\nOur results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .\nWe also find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd81758bc1a83a15166dd"}, "user_id": "", "time": {"$date": "2022-12-07T00:02:31.482Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd82158bc1a83a15166de"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:02:41.475Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We investigate whether these methods can directly address the problem of sequential decision-making .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We present a method to simplify offline reinforcement learning ( RL ) by modeling a policy as a return-conditional diffusion model .This approach allows us to bypass the need for dynamic programming and reduce many of the complexities associated with traditional offline RL .We find that modeling policies as conditional diffusion models offers advantages when considering two other conditioning variables , such as constraints and skills .Our results show that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy multiple constraints together or demonstrate a combination of skills .We also find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd83458bc1a83a15166e0"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:03:00.404Z"}, "text": "Reinforcement Learning (RL) algorithms are known to scale poorly to environments with many available actions, requiring numerous samples to learn an optimal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as inapplicable actions (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm. In this paper, we propose a more systematic approach to introduce this knowledge into the algorithm. We (i) standardize the way knowledge can be manually specified to the agent; and (ii) present a new framework to autonomously learn these statedependent action constraints jointly with the policy. We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algorithm by providing a reliable signal to mask out irrelevant actions. Moreover, we demonstrate that thanks to the transferability of the knowledge acq\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd83a58bc1a83a15166e2"}, "user_id": "", "time": {"$date": "2022-12-07T00:03:06.065Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd84458bc1a83a15166e3"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:03:16.132Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these statedependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algorithm by providing a reliable signal to mask out irrelevant actions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd86558bc1a83a15166e5"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:03:49.084Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .\n\nTo ignore irrelevant actions (such as inapplicable actions), the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward .\n\nThis is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .\n\nIn this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .\n\nWe ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .\n\nWe show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .\n\nMoreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd86958bc1a83a15166e7"}, "user_id": "", "time": {"$date": "2022-12-07T00:03:53.646Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd87358bc1a83a15166e8"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:04:03.659Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environments with many available actions , requiring numerous samples to learn an optimal policy .To ignore irrelevant actions ( such as inapplicable actions ) , the traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo-rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd89358bc1a83a15166ea"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:04:35.766Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .\n\nWe view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .\n\nWe investigate whether these generative modeling methods can directly address the problem of sequential decision making for further improved performance in terms of generating quality samples.\n\nTo our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .\n\nBy modeling a policy as a return conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .\n\nWe further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .\n\nWe find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .\n\nOur results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd89858bc1a83a15166ec"}, "user_id": "", "time": {"$date": "2022-12-07T00:04:40.898Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd8a258bc1a83a15166ed"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:04:50.916Z"}, "text": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone .We view decision-making not through the lens of reinforcement learning ( RL ) , but rather through conditional generative modeling .We investigate whether these generative modeling methods can directly address the problem of sequential decision making for further improved performance in terms of generating quality samples .To our surprise , we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks .By modeling a policy as a return conditional diffusion model , we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL .We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables : constraints and skills .We find that conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills .Our results illustrate that conditional generative modeling is a powerful tool for decision-making .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd8ba58bc1a83a15166ef"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:05:14.003Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to have difficulty generalizing to environments with a high-dimensional observation space , and require a large number of samples for efficient learning of an optimal policy .\n\nThe traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .\n\nIn this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .\n\nThis is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .\n\nIn this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .\n\nWe ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .\n\nWe show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\\x02rithm by providing a reliable signal to mask out irrelevant actions .\n\nMoreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd8be58bc1a83a15166f1"}, "user_id": "", "time": {"$date": "2022-12-07T00:05:18.995Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd8c958bc1a83a15166f2"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:05:29Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to have difficulty generalizing to environments with a high-dimensional observation space , and require a large number of samples for efficient learning of an optimal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo \\x02 rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "638fd9a958bc1a83a15166f4"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:09:13.184Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to have difficulty generalizing to environments with a high-dimensional observation space , and require a large number of samples for efficient learning of an optimal policy .\n\nThe traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .\n\nIn this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .\n\nThis is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .\n\nIn this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .\n\nWe ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .\n\nWe show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\\x02rithm by providing a reliable signal to mask out irrelevant actions .\n\nMoreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd9ae58bc1a83a15166f6"}, "user_id": "", "time": {"$date": "2022-12-07T00:09:18.665Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ICLR\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "638fd6ab58bc1a83a15166bb"}, {"_id": {"$oid": "638fd9b858bc1a83a15166f7"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-07T00:09:28.791Z"}, "text": "Reinforcement Learning ( RL ) algorithms are known to have difficulty generalizing to environments with a high-dimensional observation space , and require a large number of samples for efficient learning of an optimal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state-dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo \\x02 rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390b9ca8c15db04700de4b6"}, "user_id": "Hua", "time": {"$date": "2022-12-07T16:05:30.386Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390b9da8c15db04700de4b8"}, "user_id": "Hua", "time": {"$date": "2022-12-07T16:05:46.963Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "6390b9b98c15db04700de4b5"}, {"_id": {"$oid": "6390b9df8c15db04700de4ba"}, "user_id": "", "time": {"$date": "2022-12-07T16:05:51.762Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390b9b98c15db04700de4b5"}, {"_id": {"$oid": "6390b9ea8c15db04700de4bb"}, "user_id": "Hua", "time": {"$date": "2022-12-07T16:06:02.755Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390ba3a8c15db04700de4bd"}, "user_id": "", "time": {"$date": "2022-12-07T16:07:22.689Z"}, "text": "{\"writingInput\":[\"We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390b9b98c15db04700de4b5"}, {"_id": {"$oid": "6390ba9a8c15db04700de4bf"}, "user_id": "", "time": {"$date": "2022-12-07T16:08:58.279Z"}, "text": "{\"writingInput\":[\"We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .\"],\"explainInput\":\"similar examples\\n\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390b9b98c15db04700de4b5"}, {"_id": {"$oid": "6390bac58c15db04700de4c1"}, "user_id": "", "time": {"$date": "2022-12-07T16:09:41.870Z"}, "text": "{\"writingInput\":[\"We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .\"],\"explainInput\":\"label:background, keyword:answer, rank:quality_score, count:6\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390b9b98c15db04700de4b5"}, {"_id": {"$oid": "6390bafc8c15db04700de4c3"}, "user_id": "", "time": {"$date": "2022-12-07T16:10:36.496Z"}, "text": "{\"writingInput\":[\"We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .\"],\"explainInput\":\"counterfactual prediction\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390b9b98c15db04700de4b5"}, {"_id": {"$oid": "6390bb3e8c15db04700de4c4"}, "user_id": "Hua", "time": {"$date": "2022-12-07T16:11:42.354Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .\nWe\u00a0found\u00a0that\u00a0LimitedInk\u00a0can\u00a0effectively\u00a0conduct a user study on the impact of rationale\u00a0length,\u00a0as\u00a0human judges\u00a0are\u00a0able\u00a0to predict the sentiment label of documents based\u00a0solely\u00a0on\u00a0LimitedInk-generated\u00a0rationales\u00a0of\u00a0varying\u00a0lengths\u00a0.We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390bc498c15db04700de4c7"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:16:09.650Z"}, "text": "Students\u2019 ability to ask curious questions is a crucial skill that improves their learning processes. To train this skill, previous research has used a conversational agent that propose specific cues to prompt children\u2019s curiosity during learning. Despite showing pedagogical efficiency, this method is still limited since it relies on generating the said prompts by hand for each educational resource, which can be a very long and costly process.\nIn this context, we leverage the advances in the natural language pro- cessing field and explore using a large language model (GPT-3) to automate the generation of this agent\u2019s curiosity-prompting cues to help children ask more and deeper questions. We then used this study to investigate a different curiosity-prompting behavior for the agent.\nThe study was conducted with 75 students aged between 9 and 10. They either interacted with a hand-crafted conversational agent that pro- poses \u201dclosed\u201d manually-extracted cues leading to predefined questions, a GPT-3-driven one that proposes the same type of cues, or a GPT-3- driven one that proposes \u201dopen\u201d cues that can lead to several possible questions. Results showed a similar question-asking performance between children who had the two \u201dclosed\u201d agents, but a significantly better one for participants with the \u201dopen\u201d agent.\nOur first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies. In a second step, we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity.\n", "type": "task", "writing_model": "", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390bc4e8c15db04700de4c8"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:16:14.106Z"}, "text": "Students\u2019 ability to ask curious questions is a crucial skill that improves their learning processes. To train this skill, previous research has used a conversational agent that propose specific cues to prompt children\u2019s curiosity during learning. Despite showing pedagogical efficiency, this method is still limited since it relies on generating the said prompts by hand for each educational resource, which can be a very long and costly process.\nIn this context, we leverage the advances in the natural language pro- cessing field and explore using a large language model (GPT-3) to automate the generation of this agent\u2019s curiosity-prompting cues to help children ask more and deeper questions. We then used this study to investigate a different curiosity-prompting behavior for the agent.\nThe study was conducted with 75 students aged between 9 and 10. They either interacted with a hand-crafted conversational agent that pro- poses \u201dclosed\u201d manually-extracted cues leading to predefined questions, a GPT-3-driven one that proposes the same type of cues, or a GPT-3- driven one that proposes \u201dopen\u201d cues that can lead to several possible questions. Results showed a similar question-asking performance between children who had the two \u201dclosed\u201d agents, but a significantly better one for participants with the \u201dopen\u201d agent.\nOur first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies. In a second step, we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity.\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390bc508c15db04700de4ca"}, "user_id": "", "time": {"$date": "2022-12-07T16:16:16.718Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390bc5b8c15db04700de4cb"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:16:27.065Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .In this context , we leverage the advances in the natural language pro-cessing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390be0b8c15db04700de4cc"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:23:39.972Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the saprompts by hand for each educational resource , which can be a very long and costly process .\nIn this context , we leverage the advances in the natural language pro-cessing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390be368c15db04700de4cd"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:24:22.842Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .\nTo help children ask more and deeper questions, we leverage the advances in the natural language processing and explore using a\nIn this context , we leverage the advances in the natural language pro-cessing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390be7b8c15db04700de4cf"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:25:31.360Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .\nTo help children ask more and deeper questions, we leverage the advances in the natural language processing and explore using a large language model (GPT-3) to automate the generation of the agent's curiosity prompting cues. \nWe then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390be7d8c15db04700de4d0"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:25:33.128Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .\nTo help children ask more and deeper questions, we leverage the advances in the natural language processing and explore using a large language model (GPT-3) to automate the generation of the agent's curiosity prompting cues. \nWe then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390be848c15db04700de4d2"}, "user_id": "", "time": {"$date": "2022-12-07T16:25:40.186Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390be8d8c15db04700de4d3"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:25:49.401Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390beb18c15db04700de4d5"}, "user_id": "", "time": {"$date": "2022-12-07T16:26:25.527Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390bed18c15db04700de4d7"}, "user_id": "", "time": {"$date": "2022-12-07T16:26:57.149Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"How are the structure labels distributed?\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Aspect Distribution]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390bf338c15db04700de4d9"}, "user_id": "", "time": {"$date": "2022-12-07T16:28:35.404Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"How can I revise the input to get a different prediction label?\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390bf488c15db04700de4db"}, "user_id": "", "time": {"$date": "2022-12-07T16:28:56.853Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"How can I revise the input to get a different prediction label?\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390bf698c15db04700de4dd"}, "user_id": "", "time": {"$date": "2022-12-07T16:29:29.353Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"Counterfactual explanation \",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390bf978c15db04700de4df"}, "user_id": "", "time": {"$date": "2022-12-07T16:30:15.322Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"purpose\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c0188c15db04700de4e1"}, "user_id": "", "time": {"$date": "2022-12-07T16:32:24.334Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"method\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c0248c15db04700de4e3"}, "user_id": "", "time": {"$date": "2022-12-07T16:32:36.514Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"Counterfactual explanation\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c02e8c15db04700de4e5"}, "user_id": "", "time": {"$date": "2022-12-07T16:32:46.980Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"method\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c0b88c15db04700de4e7"}, "user_id": "", "time": {"$date": "2022-12-07T16:35:04.403Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"What are some published sentences that look similar to mine semantically?\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c0f58c15db04700de4e9"}, "user_id": "", "time": {"$date": "2022-12-07T16:36:05.221Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"label: purpose\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c12c8c15db04700de4eb"}, "user_id": "", "time": {"$date": "2022-12-07T16:37:00.309Z"}, "text": "{\"writingInput\":[\"To help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"label: purpose, rank: long\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c23c8c15db04700de4ec"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:41:32.138Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .\n\n\n\nTo help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c2e48c15db04700de4ed"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:44:20.659Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .\n\nTo \n\nTo help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c33a8c15db04700de4ee"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:45:46.629Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .\n\nTo automate the generation of the agent's curiosity promoting cues, we le\n\nTo help children ask more and deeper questions , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c3a58c15db04700de4f0"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:47:33.406Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .\nTo address this issue, we leverage the advances in the natural language processing and explore using a large language model (GPT-3) to automate the generation of the agent's curiosity prompting cues. \nWe then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c3ac8c15db04700de4f2"}, "user_id": "", "time": {"$date": "2022-12-07T16:47:40.885Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c3b68c15db04700de4f3"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:47:50.784Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .To address this issue , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c4358c15db04700de4f5"}, "user_id": "", "time": {"$date": "2022-12-07T16:49:57.062Z"}, "text": "{\"writingInput\":[\"To address this issue , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c4458c15db04700de4f7"}, "user_id": "", "time": {"$date": "2022-12-07T16:50:13.649Z"}, "text": "{\"writingInput\":[\"To address this issue , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"What are some published sentences that look similar to mine semantically?\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c4638c15db04700de4f9"}, "user_id": "", "time": {"$date": "2022-12-07T16:50:43.089Z"}, "text": "{\"writingInput\":[\"To address this issue , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"label: purpose\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c4a98c15db04700de4fb"}, "user_id": "", "time": {"$date": "2022-12-07T16:51:53.743Z"}, "text": "{\"writingInput\":[\"To address this issue , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"confidence\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c50a8c15db04700de4fd"}, "user_id": "", "time": {"$date": "2022-12-07T16:53:30.860Z"}, "text": "{\"writingInput\":[\"To address this issue , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"label: purpose, rank: short\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c51c8c15db04700de4ff"}, "user_id": "", "time": {"$date": "2022-12-07T16:53:48.641Z"}, "text": "{\"writingInput\":[\"To address this issue , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"example\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c52c8c15db04700de501"}, "user_id": "", "time": {"$date": "2022-12-07T16:54:04.327Z"}, "text": "{\"writingInput\":[\"To address this issue , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"label: purpose, rank: short\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c5588c15db04700de503"}, "user_id": "", "time": {"$date": "2022-12-07T16:54:48.700Z"}, "text": "{\"writingInput\":[\"To address this issue , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"cont: 5\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c55e8c15db04700de505"}, "user_id": "", "time": {"$date": "2022-12-07T16:54:54.249Z"}, "text": "{\"writingInput\":[\"To address this issue , we leverage the advances in the natural language processing and explore using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .\"],\"explainInput\":\"count: 5\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Similar Examples]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c5948c15db04700de507"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:55:48.982Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .To address this issue , we leverage the advances in the natural language processing by using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c59d8c15db04700de509"}, "user_id": "", "time": {"$date": "2022-12-07T16:55:57.072Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c5a68c15db04700de50a"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:56:06.285Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .To address this issue , we leverage the advances in the natural language processing by using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c5c18c15db04700de50b"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:56:33.366Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .To address this issue , we leverage the advances in the natural language processing by using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "save"}, {"_id": {"$oid": "6390c5ca8c15db04700de50d"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:56:42.073Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .To address this issue , we leverage the advances in the natural language processing by using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c5d18c15db04700de50f"}, "user_id": "", "time": {"$date": "2022-12-07T16:56:49.172Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390bc038c15db04700de4c5"}, {"_id": {"$oid": "6390c5db8c15db04700de510"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:56:59.564Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .To address this issue , we leverage the advances in the natural language processing by using a large language model ( GPT-3 ) to automate the generation of the agent 's curiosity prompting cues .We then used this study to investigate a different curiosity-prompting behavior for the agent .The study was conducted with 75 students aged between 9 and 10 .They either interacted with a hand-crafted conversational agent that pro-poses \u201d closed \u201d manually-extracted cues leading to predefined questions , a GPT-3-driven one that proposes the same type of cues , or a GPT-3-driven one that proposes \u201d open \u201d cues that can lead to several possible questions .Results showed a similar question-asking performance between children who had the two \u201d closed \u201d agents , but a significantly better one for participants with the \u201d open \u201d agent .Our first results suggest the validity of using GPT-3 to facilitate the implementation of curiosity-stimulating learning technologies .In a second step , we also show that GPT-3 can be efficient in proposing the relevant open cues that leave children with more autonomy to express their curiosity .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c5f08c15db04700de511"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:57:20.144Z"}, "text": "It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken \u2014 namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart \u2014 most model interpretations cannot answer these questions. \n", "event_type": "auto-save"}, {"_id": {"$oid": "6390c6048c15db04700de512"}, "user_id": "p3", "time": {"$date": "2022-12-07T16:57:40.320Z"}, "text": "Socially aware robots should be able, among others, to support fluent human-robot collaboration in tasks that require interdependent actions in order to be solved. Towards enhancing mutual performance, collaborative robots should be equipped with adaptation and learning capabilities. However, co-learning can be a time consuming procedure. For this reason, transferring knowledge from an expert could potentially boost the overall team performance. In the present study, transfer learning was integrated in a deep Reinforcement Learning (dRL) agent. In a real-time and real-world set-up, two groups of participants had to collaborate with a cobot under two different conditions of dRL agents; one that was transferring knowledge and one that did not. A probabilistic policy reuse method was used for the transfer learning (TL). The results showed that there was a significant difference between the performance of the two groups; TL halved the time needed for the training of new participants to the task. Moreover, TL also affected the subjective performance of the teams and enhanced the perceived fluency. Finally, in many cases the objective performance metrics did not correlate with the subjective ones providing interesting insights about the design of transparent and explainable cobot behaviour.\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f36e8c15db04700de515"}, "user_id": "Hua", "time": {"$date": "2022-12-07T20:11:26.599Z"}, "text": "Students\u2019 ability to ask curious questions is a crucial skill that improves their learning processes. To train this skill, previous research has used a conversational agent that propose specific cues to prompt children\u2019s curiosity during learning. Despite showing pedagogical efficiency, this method is still limited since it relies on generating the said prompts by hand for each educational resource, which can be a very long and costly process. In this context, we leverage the advances in the natural language processing field and explore using a large language model (GPT-3) to automate the generation of this agent\u2019s curiosity-prompting cues to help children ask more and deeper questions. We then used this study to in\n", "type": "task", "writing_model": "", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "6390f3738c15db04700de517"}, "user_id": "", "time": {"$date": "2022-12-07T20:11:31.334Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "6390f38d8c15db04700de518"}, "user_id": "Hua", "time": {"$date": "2022-12-07T20:11:57.666Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .In this context , we leverage the advances in the natural language processing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f3be8c15db04700de51b"}, "user_id": "", "time": {"$date": "2022-12-07T20:12:46.510Z"}, "text": "{\"writingInput\":[\"In this context , we leverage the advances in the natural language processing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "6390f3d68c15db04700de51d"}, "user_id": "", "time": {"$date": "2022-12-07T20:13:10.865Z"}, "text": "{\"writingInput\":[\"In this context , we leverage the advances in the natural language processing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .\"],\"explainInput\":\"similar example\\n\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "6390f41b8c15db04700de51f"}, "user_id": "", "time": {"$date": "2022-12-07T20:14:19.333Z"}, "text": "{\"writingInput\":[\"In this context , we leverage the advances in the natural language processing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .\"],\"explainInput\":\"label:finding, rank: quality_score, count: 10\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "6390f4288c15db04700de521"}, "user_id": "", "time": {"$date": "2022-12-07T20:14:32.337Z"}, "text": "{\"writingInput\":[\"In this context , we leverage the advances in the natural language processing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .\"],\"explainInput\":\"counterfactual\\n\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "6390f4468c15db04700de522"}, "user_id": "Hua", "time": {"$date": "2022-12-07T20:15:02.733Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .In this context , we leverage the advances in the natural language processing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .\n\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f4b88c15db04700de525"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:16:56.195Z"}, "text": "The next step for intelligent dialog agents is to escape their role\nas silent bystanders and become proactive. Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off\nresponsibility from the user. However, proactivity is a double-\nedged sword because poorly executed preemptive actions may\nhave a devastating effect not only on the task outcome but also\non the relationship with the user. For designing adequate proactive dialog strategies, we propose a novel approach including\nboth socially as well as task-relevant features in the dialog.\nHere, the primary goal is to optimize proactive behavior so that\nit is task-oriented - this implies high task success and efficiency\n- while also being socially effective by fostering user trust. Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation.\n", "type": "task", "writing_model": "", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f4be8c15db04700de527"}, "user_id": "", "time": {"$date": "2022-12-07T20:17:02.310Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f4c98c15db04700de528"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:17:13.126Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f4e98c15db04700de52a"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:17:45.346Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation .\n", "type": "task", "writing_model": "model-writing-2", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f4f08c15db04700de52c"}, "user_id": "", "time": {"$date": "2022-12-07T20:17:52.918Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f4f98c15db04700de52d"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:18:01.932Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f4fe8c15db04700de52f"}, "user_id": "", "time": {"$date": "2022-12-07T20:18:06.285Z"}, "text": "{\"writingInput\":[\"Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation .\"],\"explainInput\":\"\",\"writingIndex\":[\"5\"],\"ButtonID\":\"\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f5178c15db04700de531"}, "user_id": "", "time": {"$date": "2022-12-07T20:18:31.238Z"}, "text": "{\"writingInput\":[\"Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"5\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f5388c15db04700de533"}, "user_id": "", "time": {"$date": "2022-12-07T20:19:04.149Z"}, "text": "{\"writingInput\":[\"Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation .\"],\"explainInput\":\"How can I revise the input to get a different prediction label?\",\"writingIndex\":[\"5\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f5638c15db04700de535"}, "user_id": "", "time": {"$date": "2022-12-07T20:19:47.090Z"}, "text": "{\"writingInput\":[\"Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for a more successful human-machine cooperation .\"],\"explainInput\":\"Counterfactual Explanation\",\"writingIndex\":[\"5\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f58e8c15db04700de537"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:20:30.161Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning, which demonstrates the benefit of our approach in achieving more successful human-machine cooperation \n", "type": "task", "writing_model": "model-writing-2", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f5948c15db04700de539"}, "user_id": "", "time": {"$date": "2022-12-07T20:20:36.506Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f5a48c15db04700de53b"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:20:52.024Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust. We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning, which demonstrates the benefit of our approach in achieving more successful human-machine cooperation.\n", "type": "task", "writing_model": "model-writing-2", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f5a98c15db04700de53d"}, "user_id": "", "time": {"$date": "2022-12-07T20:20:57.500Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f5b58c15db04700de53e"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:21:09.772Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning , which demonstrates the benefit of our approach in achieving more successful human-machine cooperation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f5ce8c15db04700de540"}, "user_id": "", "time": {"$date": "2022-12-07T20:21:34.419Z"}, "text": "{\"writingInput\":[\"Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f5d58c15db04700de542"}, "user_id": "", "time": {"$date": "2022-12-07T20:21:41.424Z"}, "text": "{\"writingInput\":[\"Here , the primary goal is to optimize proactive behavior so that it is task-oriented-this implies high task success and efficiency-while also being socially effective by fostering user trust .\"],\"explainInput\":\"Counterfactual Explanation\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f5f88c15db04700de544"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:22:16.974Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog.The core assumption is that users act in accordance with what is best for them , given the limits imposed by their cognitive architecture and their experience of the task environment Optimizing proactive behavior so as to be task-oriented, which would imply high task success and efficiency, while also being socially effective by fostering user trust, is the primary goal of this work.We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning , which demonstrates the benefit of our approach in achieving more successful human-machine cooperation .\n", "type": "task", "writing_model": "model-writing-2", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f5ff8c15db04700de545"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:22:23.516Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog.The core assumption is that users act in accordance with what is best for them , given the limits imposed by their cognitive architecture and their experience of the task environment Optimizing proactive behavior so as to be task-oriented, which would imply high task success and efficiency, while also being socially effective by fostering user trust, is the primary goal of this work.We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning , which demonstrates the benefit of our approach in achieving more successful human-machine cooperation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f5ff8c15db04700de547"}, "user_id": "", "time": {"$date": "2022-12-07T20:22:23.932Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f6128c15db04700de548"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:22:42.868Z"}, "text": "The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .The core assumption is that users act in accordance with what is best for them , given the limits imposed by their cognitive architecture and their experience of the task environment Optimizing proactive behavior so as to be task-oriented , which would imply high task success and efficiency , while also being socially effective by fostering user trust , is the primary goal of this work .We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning , which demonstrates the benefit of our approach in achieving more successful human-machine cooperation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f6198c15db04700de54a"}, "user_id": "", "time": {"$date": "2022-12-07T20:22:49.414Z"}, "text": "{\"writingInput\":[\"The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"0\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f6228c15db04700de54c"}, "user_id": "", "time": {"$date": "2022-12-07T20:22:58.913Z"}, "text": "{\"writingInput\":[\"The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .\"],\"explainInput\":\"Similar Examples\",\"writingIndex\":[\"0\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f6538c15db04700de54e"}, "user_id": "", "time": {"$date": "2022-12-07T20:23:47.828Z"}, "text": "{\"writingInput\":[\"The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive .\"],\"explainInput\":\"Counterfactual Explanation\",\"writingIndex\":[\"0\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f6788c15db04700de550"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:24:24.337Z"}, "text": "The next step for intelligent dialog agents is to demonstrate the ability to escape their role as silent bystanders and become proactive in conversations, showing that they can take control of the conversation to lead it down meaningful paths.Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .The core assumption is that users act in accordance with what is best for them , given the limits imposed by their cognitive architecture and their experience of the task environment Optimizing proactive behavior so as to be task-oriented , which would imply high task success and efficiency , while also being socially effective by fostering user trust , is the primary goal of this work .We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning , which demonstrates the benefit of our approach in achieving more successful human-machine cooperation .\n", "type": "task", "writing_model": "model-writing-2", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f67e8c15db04700de552"}, "user_id": "", "time": {"$date": "2022-12-07T20:24:30.832Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f6888c15db04700de553"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:24:40.877Z"}, "text": "The next step for intelligent dialog agents is to demonstrate the ability to escape their role as silent bystanders and become proactive in conversations , showing that they can take control of the conversation to lead it down meaningful paths .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .The core assumption is that users act in accordance with what is best for them , given the limits imposed by their cognitive architecture and their experience of the task environment Optimizing proactive behavior so as to be task-oriented , which would imply high task success and efficiency , while also being socially effective by fostering user trust , is the primary goal of this work .We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning , which demonstrates the benefit of our approach in achieving more successful human-machine cooperation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f7168c15db04700de554"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:27:02.400Z"}, "text": "The next step for intelligent dialog agents is to demonstrate the ability to escape their role as silent bystanders and become proactive in conversations , showing that they can take control of the conversation to lead it down meaningful paths .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .The core assumption is that users act in accordance with what is best for them , given the limits imposed by their cognitive architecture and their experience of the task environment. Optimizing proactive behavior so as to be task-oriented , which would imply high task success and efficiency , while also being socially effective by fostering user trust , is the primary goal of this work .We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning , which demonstrates the benefit of our approach in achieving more successful human-machine cooperation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f7818c15db04700de556"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:28:49.724Z"}, "text": "The next step for intelligent dialog agents is to demonstrate the ability to escape their role as silent bystanders and become proactive in conversations , showing that they can take control of the conversation to lead it down meaningful paths .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Optimizing proactive behavior so as to be task-oriented , which would imply high task success and efficiency , while also being socially effective by fostering user trust , is the primary goal of this work .We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning , which demonstrates the benefit of our approach in achieving more successful human-machine cooperation .\n", "type": "task", "writing_model": "model-writing-2", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f7878c15db04700de558"}, "user_id": "", "time": {"$date": "2022-12-07T20:28:55.605Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f45d8c15db04700de523"}, {"_id": {"$oid": "6390f7968c15db04700de559"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:29:10.646Z"}, "text": "The next step for intelligent dialog agents is to demonstrate the ability to escape their role as silent bystanders and become proactive in conversations , showing that they can take control of the conversation to lead it down meaningful paths .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Optimizing proactive behavior so as to be task-oriented , which would imply high task success and efficiency , while also being socially effective by fostering user trust , is the primary goal of this work .We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning , which demonstrates the benefit of our approach in achieving more successful human-machine cooperation .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6390f8758c15db04700de55a"}, "user_id": "p4", "time": {"$date": "2022-12-07T20:32:53.794Z"}, "text": "The next step for intelligent dialog agents is to demonstrate the ability to escape their role as silent bystanders and become proactive in conversations , showing that they can take control of the conversation to lead it down meaningful paths .Well-defined proactive behavior may improve human-machine cooperation , as the agent takes a more active role during interaction and takes off responsibility from the user .However , proactivity is a double-edged sword because poorly executed preemptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user .For designing adequate proactive dialog strategies , we propose a novel approach including both socially as well as task-relevant features in the dialog .Optimizing proactive behavior so as to be task-oriented , which would imply high task success and efficiency , while also being socially effective by fostering user trust , is the primary goal of this work .We propose a reward function that combines both aspects to train a proactive dialog agent using reinforcement learning , which demonstrates the benefit of our approach in achieving more successful human-machine cooperation .\n", "event_type": "save"}, {"_id": {"$oid": "63910e788c15db04700de55d"}, "user_id": "Hua", "time": {"$date": "2022-12-07T22:06:48.727Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "63910e678c15db04700de55b"}, {"_id": {"$oid": "63910e7e8c15db04700de55f"}, "user_id": "", "time": {"$date": "2022-12-07T22:06:54.672Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "63910e678c15db04700de55b"}, {"_id": {"$oid": "63910e898c15db04700de560"}, "user_id": "Hua", "time": {"$date": "2022-12-07T22:07:05.569Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63910ef28c15db04700de562"}, "user_id": "", "time": {"$date": "2022-12-07T22:08:50.651Z"}, "text": "{\"writingInput\":[\"In this context , we leverage the advances in the natural language processing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .\"],\"explainInput\":\"counterfactual\\n\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "63910f098c15db04700de564"}, "user_id": "", "time": {"$date": "2022-12-07T22:09:13.883Z"}, "text": "{\"writingInput\":[\"In this context , we leverage the advances in the natural language processing field and explore using a large language model ( GPT-3 ) to automate the generation of this agent \u2019s curiosity-prompting cues to help children ask more and deeper questions .\"],\"explainInput\":\"finding\\n\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "63910f378c15db04700de566"}, "user_id": "Hua", "time": {"$date": "2022-12-07T22:09:59.089Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .In this\u00a0context,\u00a0we\u00a0explore\u00a0the\u00a0use\u00a0of\u00a0a\u00a0large\u00a0language\u00a0model\u00a0(GPT-3)\u00a0to\u00a0leverage the advances in natural language processing and automate the generation of curiosity-prompting cues\u00a0for\u00a0an\u00a0agent\u00a0to help children ask more and deeper questions .\n\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "63910f3c8c15db04700de568"}, "user_id": "", "time": {"$date": "2022-12-07T22:10:04.831Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "63910f4c8c15db04700de56a"}, "user_id": "", "time": {"$date": "2022-12-07T22:10:20.764Z"}, "text": "{\"writingInput\":[\"In this context , we explore the use of a large language model ( GPT-3 ) to leverage the advances in natural language processing and automate the generation of curiosity-prompting cues for an agent to help children ask more and deeper questions .\"],\"explainInput\":\"similar example\\n\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "63910f528c15db04700de56b"}, "user_id": "Hua", "time": {"$date": "2022-12-07T22:10:26.823Z"}, "text": "Students \u2019 ability to ask curious questions is a crucial skill that improves their learning processes .To train this skill , previous research has used a conversational agent that propose specific cues to prompt children \u2019s curiosity during learning .Despite showing pedagogical efficiency , this method is still limited since it relies on generating the said prompts by hand for each educational resource , which can be a very long and costly process .In this context , we explore the use of a large language model ( GPT-3 ) to leverage the advances in natural language processing and automate the generation of curiosity-prompting cues for an agent to help children ask more and desueper questions .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63910f8a8c15db04700de56d"}, "user_id": "", "time": {"$date": "2022-12-07T22:11:22.249Z"}, "text": "{\"writingInput\":[\"In this context , we explore the use of a large language model ( GPT-3 ) to leverage the advances in natural language processing and automate the generation of curiosity-prompting cues for an agent to help children ask more and deeper questions .\"],\"explainInput\":\"label:finding, rank: long, count: 10\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "6390f3508c15db04700de513"}, {"_id": {"$oid": "639119b58c15db04700de570"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:54:45.908Z"}, "text": "Understanding visually-rich business docu- ments to extract structured data and auto- mate business workflows has been receiving attention both in academia and industry. Al- though recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the com- plexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understand- ing (VRDU). VRDU contains two datasets that represent several challenges: rich schema in- cluding diverse data types as well as nested en- tities, complex templates including tables and multi-column layouts, and diversity of differ- ent layouts (templates) within a single docu- ment type. We design few-shot and conven- tional experiment settings along with a care- fully designed matching algorithm to evalu- ate extraction results. We report the perfor- mance of strong baselines and three observa- tions: (1) generalizing to new document tem- plates is very challenging, (2) few-shot perfor- mance has a lot of headroom, and (3) mod- els struggle with nested fields such as line- items in an invoice. We plan to open source the benchmark and the evaluation toolkit. We hope this helps the community make progress on these challenging tasks in extracting struc- tured data from visually rich documents.\n", "type": "task", "writing_model": "", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639119bd8c15db04700de572"}, "user_id": "", "time": {"$date": "2022-12-07T22:54:53.714Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639119c78c15db04700de573"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:55:03.661Z"}, "text": "Understanding visually-rich business docu-ments to extract structured data and auto-mate business workflows has been receiving attention both in academia and industry .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understand-ing ( VRDU ) .VRDU contains two datasets that represent several challenges : rich schema in-cluding diverse data types as well as nested en-tities , complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639119ea8c15db04700de575"}, "user_id": "", "time": {"$date": "2022-12-07T22:55:38.094Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understand-ing ( VRDU ) .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"2\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911a1a8c15db04700de577"}, "user_id": "", "time": {"$date": "2022-12-07T22:56:26.178Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understand-ing ( VRDU ) .\"],\"explainInput\":\"How can I revise the input to get a different prediction label?\",\"writingIndex\":[\"2\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911a2d8c15db04700de579"}, "user_id": "", "time": {"$date": "2022-12-07T22:56:45.166Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understand-ing ( VRDU ) .\"],\"explainInput\":\"counterfactual explanation\",\"writingIndex\":[\"2\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911a3e8c15db04700de57b"}, "user_id": "", "time": {"$date": "2022-12-07T22:57:02.114Z"}, "text": "{\"writingInput\":[\"In this work , we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understand-ing ( VRDU ) .\"],\"explainInput\":\"method\",\"writingIndex\":[\"2\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911a7e8c15db04700de57c"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:58:06.768Z"}, "text": "Understanding visually-rich business docu-ments to extract structured data and auto-mate business workflows has been receiving attention both in academia and industry .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In\u00a0this\u00a0work,\u00a0we\u00a0propose\u00a0a\u00a0comprehensive\u00a0benchmark called\u00a0Visually\u00a0Rich\u00a0Document\u00a0Understanding\u00a0(VRDU) and\u00a0identify the desiderata for\u00a0it.VRDU contains two datasets that represent several challenges : rich schema in-cluding diverse data types as well as nested en-tities , complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911a8e8c15db04700de57e"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:58:22.272Z"}, "text": "Understanding visually-rich business docu-ments to extract structured data and auto-mate business workflows has been receiving attention both in academia and industry .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In\u00a0this\u00a0work,\u00a0we\u00a0propose\u00a0a\u00a0comprehensive\u00a0benchmark called\u00a0Visually\u00a0Rich\u00a0Document\u00a0Understanding\u00a0(VRDU) and\u00a0identify the desiderata for\u00a0it.VRDU contains two datasets that represent several challenges : rich schema in-cluding diverse data types as well as nested en-tities , complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911a958c15db04700de580"}, "user_id": "", "time": {"$date": "2022-12-07T22:58:29.209Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911a9f8c15db04700de581"}, "user_id": "P5", "time": {"$date": "2022-12-07T22:58:39.196Z"}, "text": "Understanding visually-rich business docu-ments to extract structured data and auto-mate business workflows has been receiving attention both in academia and industry .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .VRDU contains two datasets that represent several challenges : rich schema in-cluding diverse data types as well as nested en-tities , complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911abb8c15db04700de583"}, "user_id": "", "time": {"$date": "2022-12-07T22:59:07.177Z"}, "text": "{\"writingInput\":[\"Understanding visually-rich business docu-ments to extract structured data and auto-mate business workflows has been receiving attention both in academia and industry .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"0\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911ace8c15db04700de585"}, "user_id": "", "time": {"$date": "2022-12-07T22:59:26.945Z"}, "text": "{\"writingInput\":[\"Understanding visually-rich business docu-ments to extract structured data and auto-mate business workflows has been receiving attention both in academia and industry .\"],\"explainInput\":\"How can I revise the input to get a different prediction label?\",\"writingIndex\":[\"0\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911ada8c15db04700de587"}, "user_id": "", "time": {"$date": "2022-12-07T22:59:38.566Z"}, "text": "{\"writingInput\":[\"Understanding visually-rich business docu-ments to extract structured data and auto-mate business workflows has been receiving attention both in academia and industry .\"],\"explainInput\":\"counterfactual explanation\",\"writingIndex\":[\"0\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911ae88c15db04700de589"}, "user_id": "", "time": {"$date": "2022-12-07T22:59:52.874Z"}, "text": "{\"writingInput\":[\"Understanding visually-rich business docu-ments to extract structured data and auto-mate business workflows has been receiving attention both in academia and industry .\"],\"explainInput\":\"background\",\"writingIndex\":[\"0\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911b1c8c15db04700de58a"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:00:44.275Z"}, "text": "Recent\u00a0research\u00a0and\u00a0industry\u00a0efforts\u00a0have\u00a0focused\u00a0onunderstanding\u00a0visually-rich business\u00a0documents\u00a0to extract structured data and\u00a0automate\u00a0business workflows.Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .VRDU contains two datasets that represent several challenges : rich schema in-cluding diverse data types as well as nested en-tities , complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911b658c15db04700de58c"}, "user_id": "", "time": {"$date": "2022-12-07T23:01:57.919Z"}, "text": "{\"writingInput\":[\"VRDU contains two datasets that represent several challenges : rich schema in-cluding diverse data types as well as nested en-tities , complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .\"],\"explainInput\":\"similar examples\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911bc98c15db04700de58e"}, "user_id": "", "time": {"$date": "2022-12-07T23:03:37.431Z"}, "text": "{\"writingInput\":[\"VRDU contains two datasets that represent several challenges : rich schema in-cluding diverse data types as well as nested en-tities , complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .\"],\"explainInput\":\"label:background, rank:short, count:10\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911c3c8c15db04700de590"}, "user_id": "", "time": {"$date": "2022-12-07T23:05:32.192Z"}, "text": "{\"writingInput\":[\"VRDU contains two datasets that represent several challenges : rich schema in-cluding diverse data types as well as nested en-tities , complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .\"],\"explainInput\":\"counterfactual explanation\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911c4d8c15db04700de592"}, "user_id": "", "time": {"$date": "2022-12-07T23:05:49.750Z"}, "text": "{\"writingInput\":[\"VRDU contains two datasets that represent several challenges : rich schema in-cluding diverse data types as well as nested en-tities , complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .\"],\"explainInput\":\"background\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911ca28c15db04700de593"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:07:14.498Z"}, "text": "Recent\u00a0research\u00a0and\u00a0industry\u00a0efforts\u00a0have\u00a0focused\u00a0onunderstanding\u00a0visually-rich business\u00a0documents\u00a0to extract structured data and\u00a0automate\u00a0business workflows.Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .Multimodal\u00a0pre-training\u00a0with\u00a0text,\u00a0layout,\u00a0and\u00a0imagehas\u00a0made\u00a0significant\u00a0progress\u00a0for\u00a0Visually\u00a0RichDocument\u00a0Understanding\u00a0(VRDU),\u00a0especially\u00a0the\u00a0fixed-layout\u00a0documents\u00a0such\u00a0as\u00a0scanned\u00a0document\u00a0images. VRDU contains two datasets that represent several\u00a0challenges,\u00a0including\u00a0rich schema\u00a0with\u00a0diverse data types as well as nested\u00a0entities,\u00a0complex templates including tables and multi-column\u00a0layouts,\u00a0and diversity of\u00a0different\u00a0layouts within a single\u00a0document\u00a0typeWe design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911cb48c15db04700de594"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:07:32.194Z"}, "text": "Recent\u00a0research\u00a0and\u00a0industry\u00a0efforts\u00a0have\u00a0focused\u00a0onunderstanding\u00a0visually-rich business\u00a0documents\u00a0to extract structured data and\u00a0automate\u00a0business workflows.Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .Multimodal\u00a0pre-training\u00a0with\u00a0text,\u00a0layout,\u00a0and\u00a0imagehas\u00a0made\u00a0significant\u00a0progress\u00a0for\u00a0Visually\u00a0RichDocument\u00a0Understanding\u00a0(VRDU),\u00a0especially\u00a0the\u00a0fixed-layout\u00a0documents\u00a0such\u00a0as\u00a0scanned\u00a0document\u00a0images. VRDU contains two datasets that represent several\u00a0challenges,\u00a0including\u00a0rich schema\u00a0with\u00a0diverse data types as well as nested\u00a0entities,\u00a0complex templates including tables and multi-column\u00a0layouts,\u00a0and diversity of\u00a0different\u00a0layouts within a single\u00a0document\u00a0type.We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911cb68c15db04700de596"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:07:34.928Z"}, "text": "Recent\u00a0research\u00a0and\u00a0industry\u00a0efforts\u00a0have\u00a0focused\u00a0onunderstanding\u00a0visually-rich business\u00a0documents\u00a0to extract structured data and\u00a0automate\u00a0business workflows.Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .Multimodal\u00a0pre-training\u00a0with\u00a0text,\u00a0layout,\u00a0and\u00a0imagehas\u00a0made\u00a0significant\u00a0progress\u00a0for\u00a0Visually\u00a0RichDocument\u00a0Understanding\u00a0(VRDU),\u00a0especially\u00a0the\u00a0fixed-layout\u00a0documents\u00a0such\u00a0as\u00a0scanned\u00a0document\u00a0images. VRDU contains two datasets that represent several\u00a0challenges,\u00a0including\u00a0rich schema\u00a0with\u00a0diverse data types as well as nested\u00a0entities,\u00a0complex templates including tables and multi-column\u00a0layouts,\u00a0and diversity of\u00a0different\u00a0layouts within a single\u00a0document\u00a0type.We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911cbe8c15db04700de598"}, "user_id": "", "time": {"$date": "2022-12-07T23:07:42.538Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911cc88c15db04700de599"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:07:52.519Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .Multimodal pre-training with text , layout , and imagehas made significant progress for Visually RichDocument Understanding ( VRDU ) , especially the fixed-layout documents such as scanned document images .VRDU contains two datasets that represent several challenges , including rich schema with diverse data types as well as nested entities , complex templates including tables and multi-column layouts , and diversity of different layouts within a single document type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911d308c15db04700de59a"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:09:36.947Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .VRDU contains two datasets that represent several challenges: rich schema in- cluding diverse data types as well as nested en- tities, complex templates including tables and multi-column layouts, and diversity of differ- ent layouts (templates) within a single docu- ment type.We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911d878c15db04700de59b"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:11:03.858Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .VRDU contains two datasets that represent several challenges. One is rich schema including diverse data types as well as nested entities, and the other is complex templates including tables and multi-column layouts, and diversity of differ- ent layouts (templates) within a single docu- ment type.We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911d908c15db04700de59d"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:11:12.267Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .VRDU contains two datasets that represent several challenges. One is rich schema including diverse data types as well as nested entities, and the other is complex templates including tables and multi-column layouts, and diversity of differ- ent layouts (templates) within a single docu- ment type.We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911d978c15db04700de59f"}, "user_id": "", "time": {"$date": "2022-12-07T23:11:19.519Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911da18c15db04700de5a0"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:11:29.515Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .VRDU contains two datasets that represent several challenges .One is rich schema including diverse data types as well as nested entities , and the other is complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911dc78c15db04700de5a2"}, "user_id": "", "time": {"$date": "2022-12-07T23:12:07.652Z"}, "text": "{\"writingInput\":[\"VRDU contains two datasets that represent several challenges .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911dd88c15db04700de5a4"}, "user_id": "", "time": {"$date": "2022-12-07T23:12:24.572Z"}, "text": "{\"writingInput\":[\"VRDU contains two datasets that represent several challenges .\"],\"explainInput\":\"counterfactual explanation\",\"writingIndex\":[\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911e598c15db04700de5a5"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:14:33.185Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .VRDU contains two datasets:  rich schema including diverse data types as well as nested entities , and the other is complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911e668c15db04700de5a6"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:14:46.419Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We contruct VRDU contains two datasets:  rich schema including diverse data types as well as nested entities , and the other is complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911e958c15db04700de5a8"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:15:33.450Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We\u00a0construct\u00a0two datasets\u00a0representing\u00a0several challenges\u00a0for\u00a0VRDU. One is rich schema including diverse data types as well as nested entities , and the other is complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911e9b8c15db04700de5a9"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:15:39.854Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We\u00a0construct\u00a0two datasets\u00a0representing\u00a0several challenges\u00a0for\u00a0VRDU. One is rich schema including diverse data types as well as nested entities , and the other is complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911e9c8c15db04700de5ab"}, "user_id": "", "time": {"$date": "2022-12-07T23:15:40.589Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911ea68c15db04700de5ac"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:15:50.583Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU .One is rich schema including diverse data types as well as nested entities , and the other is complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911ec38c15db04700de5ae"}, "user_id": "", "time": {"$date": "2022-12-07T23:16:19.679Z"}, "text": "{\"writingInput\":[\"One is rich schema including diverse data types as well as nested entities , and the other is complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911ed08c15db04700de5b0"}, "user_id": "", "time": {"$date": "2022-12-07T23:16:32.571Z"}, "text": "{\"writingInput\":[\"One is rich schema including diverse data types as well as nested entities , and the other is complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .\"],\"explainInput\":\"counterfactual explanation\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911edf8c15db04700de5b2"}, "user_id": "", "time": {"$date": "2022-12-07T23:16:47.719Z"}, "text": "{\"writingInput\":[\"One is rich schema including diverse data types as well as nested entities , and the other is complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .\"],\"explainInput\":\"method\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911f538c15db04700de5b3"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:18:43.541Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU: 1) rich schema including diverse data types as well as nested entities rich schema including diverse data types as well as nested entities , and the other is complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911f7a8c15db04700de5b5"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:19:22.097Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU: 1) rich schema including diverse data types as well as nested entities, and 2) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911f7b8c15db04700de5b6"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:19:23.404Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU: 1) rich schema including diverse data types as well as nested entities, and 2) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911f818c15db04700de5b8"}, "user_id": "", "time": {"$date": "2022-12-07T23:19:29.639Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911f8b8c15db04700de5b9"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:19:39.639Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63911fc18c15db04700de5bb"}, "user_id": "", "time": {"$date": "2022-12-07T23:20:33.677Z"}, "text": "{\"writingInput\":[\"We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911fca8c15db04700de5bd"}, "user_id": "", "time": {"$date": "2022-12-07T23:20:42.823Z"}, "text": "{\"writingInput\":[\"We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .\"],\"explainInput\":\"How can I revise the input to get a different prediction label?\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911fdb8c15db04700de5bf"}, "user_id": "", "time": {"$date": "2022-12-07T23:20:59.763Z"}, "text": "{\"writingInput\":[\"We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .\"],\"explainInput\":\"counterfactual explanation\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63911fea8c15db04700de5c1"}, "user_id": "", "time": {"$date": "2022-12-07T23:21:14.817Z"}, "text": "{\"writingInput\":[\"We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .\"],\"explainInput\":\"method\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "6391201f8c15db04700de5c2"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:22:07.418Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conven-tional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639120398c15db04700de5c3"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:22:33.316Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6391204e8c15db04700de5c5"}, "user_id": "", "time": {"$date": "2022-12-07T23:22:54.488Z"}, "text": "{\"writingInput\":[\"We design few-shot and conven-tional experiment settings along with a care-fully designed matching algorithm to evalu-ate extraction results .\"],\"explainInput\":\"similar examples\",\"writingIndex\":[\"4\"],\"ButtonID\":\"[Counterfactual Explanation]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639120848c15db04700de5c7"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:23:48.455Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "6391208b8c15db04700de5c9"}, "user_id": "", "time": {"$date": "2022-12-07T23:23:55.848Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639120958c15db04700de5ca"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:24:05.841Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639120e88c15db04700de5cb"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:25:28.434Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639120ff8c15db04700de5cd"}, "user_id": "", "time": {"$date": "2022-12-07T23:25:51.331Z"}, "text": "{\"writingInput\":[\"We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .\"],\"explainInput\":\"counterfactual explanation\",\"writingIndex\":[\"5\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "6391210f8c15db04700de5cf"}, "user_id": "", "time": {"$date": "2022-12-07T23:26:07.336Z"}, "text": "{\"writingInput\":[\"We report the perfor-mance of strong baselines and three observa-tions : ( 1 ) generalizing to new document tem-plates is very challenging , ( 2 ) few-shot perfor-mance has a lot of headroom , and ( 3 ) mod-els struggle with nested fields such as line-items in an invoice .\"],\"explainInput\":\"finding\",\"writingIndex\":[\"5\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639121418c15db04700de5d1"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:26:57.635Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639121458c15db04700de5d2"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:27:01.261Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639121488c15db04700de5d4"}, "user_id": "", "time": {"$date": "2022-12-07T23:27:04.931Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639121528c15db04700de5d5"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:27:14.917Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We report the performance of strong baselines and three observations : ( 1 ) generalizing to new document templates is very challenging , ( 2 ) few-shot performance has a lot of headroom , and ( 3 ) models struggle with nested fields such as line-items in an invoice .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6391223e8c15db04700de5d6"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:31:10.332Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We find that generalizing to new document templates is very challenging, and models struggle with nested fields such as line-items in an invoice.\nAlso, few-shot performance has a lot of headroom.We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639122788c15db04700de5d7"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:32:08.045Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We find that generalizing to new document templates is very challenging, models struggle with nested fields such as line-items in an invoice, and few-shot performance has a lot of headroom.We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639122958c15db04700de5d8"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:32:37.644Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We find that generalizing to new document templates is very challenging, and models struggle with nested fields such as line-items in an invoice. Also, few-shot performance has a lot of headroom.We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "6391229b8c15db04700de5da"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:32:43.358Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We find that generalizing to new document templates is very challenging, and models struggle with nested fields such as line-items in an invoice. Also, few-shot performance has a lot of headroom.We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639122a28c15db04700de5dc"}, "user_id": "", "time": {"$date": "2022-12-07T23:32:50.793Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639122ac8c15db04700de5dd"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:33:00.821Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We find that generalizing to new document templates is very challenging , and models struggle with nested fields such as line-items in an invoice .Also , few-shot performance has a lot of headroom .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639122d88c15db04700de5df"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:33:44.933Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We find that generalizing to new document templates is very challenging , models struggle with nested fields such as line-items in an invoice, and few-shot performance has a lot of headroom .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639122df8c15db04700de5e1"}, "user_id": "", "time": {"$date": "2022-12-07T23:33:51.763Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639122e98c15db04700de5e2"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:34:01.754Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .We find that generalizing to new document templates is very challenging , models struggle with nested fields such as line-items in an invoice , and few-shot performance has a lot of headroom .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639123258c15db04700de5e4"}, "user_id": "", "time": {"$date": "2022-12-07T23:35:01.748Z"}, "text": "{\"writingInput\":[\"We find that generalizing to new document templates is very challenging , models struggle with nested fields such as line-items in an invoice , and few-shot performance has a lot of headroom .\"],\"explainInput\":\"counterfactual explanation\",\"writingIndex\":[\"5\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639123358c15db04700de5e6"}, "user_id": "", "time": {"$date": "2022-12-07T23:35:17.534Z"}, "text": "{\"writingInput\":[\"We find that generalizing to new document templates is very challenging , models struggle with nested fields such as line-items in an invoice , and few-shot performance has a lot of headroom .\"],\"explainInput\":\"finding\",\"writingIndex\":[\"5\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639123618c15db04700de5e8"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:36:01.387Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .Our\u00a0findings\u00a0show\u00a0that generalizing to new document templates is very\u00a0challenging,\u00a0models\u00a0have\u00a0difficulty dealing\u00a0with nested fields such as line-items in an\u00a0invoice,and\u00a0there\u00a0is\u00a0plenty\u00a0of\u00a0room\u00a0for\u00a0improvement\u00a0in\u00a0few-shot performance.We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "type": "task", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639123658c15db04700de5e9"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:36:05.656Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .Our\u00a0findings\u00a0show\u00a0that generalizing to new document templates is very\u00a0challenging,\u00a0models\u00a0have\u00a0difficulty dealing\u00a0with nested fields such as line-items in an\u00a0invoice,and\u00a0there\u00a0is\u00a0plenty\u00a0of\u00a0room\u00a0for\u00a0improvement\u00a0in\u00a0few-shot performance.We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639123688c15db04700de5eb"}, "user_id": "", "time": {"$date": "2022-12-07T23:36:08.087Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"ACL\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "639123728c15db04700de5ec"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:36:18.084Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Al-though recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the com-plexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .Our findings show that generalizing to new document templates is very challenging , models have difficulty dealing with nested fields such as line-items in an invoice , and there is plenty of room for improvement in few-shot performance .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639123a78c15db04700de5ed"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:37:11.771Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .Our findings show that generalizing to new document templates is very challenging , models have difficulty dealing with nested fields such as line-items in an invoice , and there is plenty of room for improvement in few-shot performance .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639123c88c15db04700de5ee"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:37:44.280Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .Our findings show that generalizing to new document templates is very challenging , models have difficulty dealing with nested fields such as line-items in an invoice , and there is plenty of room for improvement in few-shot performance .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "auto-save"}, {"_id": {"$oid": "639123cd8c15db04700de5ef"}, "user_id": "P5", "time": {"$date": "2022-12-07T23:37:49.228Z"}, "text": "Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .Although recent multi-modal language models have achieved impressive results , we find that existing benchmarks do not reflect the complexity of real documents seen in industry .In this work , we propose a comprehensive benchmark called Visually Rich Document Understanding ( VRDU ) and identify the desiderata for it .We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of different layouts ( templates ) within a single docu-ment type .The evaluation of extraction results was conducted in few-shot and conventional experiment settings along with a carefully designed matching algorithm .Our findings show that generalizing to new document templates is very challenging , models have difficulty dealing with nested fields such as line-items in an invoice , and there is plenty of room for improvement in few-shot performance .We plan to open source the benchmark and the evaluation toolkit .We hope this helps the community make progress on these challenging tasks in extracting struc-tured data from visually rich documents .\n", "event_type": "save"}, {"_id": {"$oid": "6391246e8c15db04700de5f1"}, "user_id": "", "time": {"$date": "2022-12-07T23:40:30.203Z"}, "text": "{\"writingInput\":[\"Recent research and industry efforts have focused onunderstanding visually-rich business documents to extract structured data and automate business workflows .\",\"We construct two datasets representing several challenges for VRDU : 1 ) rich schema including diverse data types as well as nested entities , and 2 ) complex templates including tables and multi-column layouts , and diversity of differ-ent layouts ( templates ) within a single docu-ment type .\"],\"explainInput\":\"How should I use XAI to improve this sentence writing?\",\"writingIndex\":[\"0\",\"3\"],\"ButtonID\":\"[InstanceWhy]\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "639119a08c15db04700de56e"}, {"_id": {"$oid": "63915b538c15db04700de5f4"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-08T03:34:43.697Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "type": "task", "writing_model": "", "conversation_id": "63915b4d8c15db04700de5f2"}, {"_id": {"$oid": "63915b5b8c15db04700de5f5"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-08T03:34:51.560Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans . However , this assumption has yet to be validated . To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length . Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models . We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths . We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63915b608c15db04700de5f7"}, "user_id": "", "time": {"$date": "2022-12-08T03:34:56.153Z"}, "text": "{\"writingInput\":[\"\"],\"explainInput\":\"CHI\",\"writingIndex\":[0],\"ButtonID\":\"None\"}", "type": "conv", "writing_model": "model-writing-1", "conversation_id": "63915b4d8c15db04700de5f2"}, {"_id": {"$oid": "63915b6a8c15db04700de5f8"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-08T03:35:06.148Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}, {"_id": {"$oid": "63915ba58c15db04700de5f9"}, "user_id": "Hua Shen", "time": {"$date": "2022-12-08T03:36:05.259Z"}, "text": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201c responsible for \u201d corresponding output \u2014 to explain the model prediction , with the assumption that shorter rationales are more intuitive to humans .However , this assumption has yet to be validated .To answer this question , we design a self-explaining model , LimitedInk , which allows users to extract rationales at any target length .Compared to existing baselines , LimitedInk achieves compatible endtask performance and human-annotated rationale agreement , making it a suitable representation of the recent class of self-explaining models .We use LimitedInk to conduct a user study on the impact of rationale length , where we ask human judges to predict the sentiment label of documents based only on LimitedInk generated rationales with different lengths .We show rationales that are too short do not help humans predict labels better than randomly masked text , suggesting the need for more careful design of the best human rationales .\n", "event_type": "auto-save"}]