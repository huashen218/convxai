[
  {
    "_id": "638f9cdd8a37977af6a11c74",
    "user_id": "P2",
    "time": "2022-12-06 19:49:49.822000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to have difficulty generalizing to environments with a high-dimensional observation space , and require a large number of samples for efficient learning of an optimal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f9ccd8a37977af6a11c71",
    "user_id": "P2",
    "time": "2022-12-06 19:49:33.166000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to have difficulty generalizing to environments with a high-dimensional observation space , and require a large number of samples for efficient learning of an optimal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "type": "task",
    "writing_model": "model-writing-1",
    "conversation_id": "638f98948a37977af6a11c29"
  },
  {
    "_id": "638f9c8f8a37977af6a11c6f",
    "user_id": "P2",
    "time": "2022-12-06 19:48:31.303000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to have difficulty generalizing to environments with a high-dimensional observation space , and require a large number of samples for efficient learning of an optimal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f9c7a8a37977af6a11c6c",
    "user_id": "P2",
    "time": "2022-12-06 19:48:10.281000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to\u00a0have\u00a0difficulty\u00a0generalizing\u00a0to\u00a0environments\u00a0with\u00a0a\u00a0high-dimensional\u00a0observation\u00a0space,\u00a0and\u00a0require\u00a0a\u00a0large\u00a0number\u00a0of\u00a0samples\u00a0for\u00a0efficient\u00a0learning\u00a0of\u00a0an\u00a0optimal\u00a0policy.The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f9c778a37977af6a11c6b",
    "user_id": "P2",
    "time": "2022-12-06 19:48:07.446000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to\u00a0have\u00a0difficulty\u00a0generalizing\u00a0to\u00a0environments\u00a0with\u00a0a\u00a0high-dimensional\u00a0observation\u00a0space,\u00a0and\u00a0require\u00a0a\u00a0large\u00a0number\u00a0of\u00a0samples\u00a0for\u00a0efficient\u00a0learning\u00a0of\u00a0an\u00a0optimal\u00a0policy.The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "type": "task",
    "writing_model": "model-writing-1",
    "conversation_id": "638f98948a37977af6a11c29"
  },
  {
    "_id": "638f9bff8a37977af6a11c65",
    "user_id": "P2",
    "time": "2022-12-06 19:46:07.986000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f9bf48a37977af6a11c62",
    "user_id": "P2",
    "time": "2022-12-06 19:45:56.620000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward.In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f9bef8a37977af6a11c61",
    "user_id": "P2",
    "time": "2022-12-06 19:45:51.654000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand to ignore irrelevant actions while also learning to maximize its reward.In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "type": "task",
    "writing_model": "model-writing-1",
    "conversation_id": "638f98948a37977af6a11c29"
  },
  {
    "_id": "638f9bb38a37977af6a11c5f",
    "user_id": "P2",
    "time": "2022-12-06 19:44:51.305000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions ( i.e. actions that have no effect on the environment in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f9ba38a37977af6a11c5c",
    "user_id": "P2",
    "time": "2022-12-06 19:44:35.282000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions ( i.e. actions that have no effect on the environment in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "type": "task",
    "writing_model": "model-writing-1",
    "conversation_id": "638f98948a37977af6a11c29"
  },
  {
    "_id": "638f9b9c8a37977af6a11c5a",
    "user_id": "P2",
    "time": "2022-12-06 19:44:28.052000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions ( i.e. actions that have no effect on the environment when performed in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f9b7a8a37977af6a11c59",
    "user_id": "P2",
    "time": "2022-12-06 19:43:54.270000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f9a858a37977af6a11c54",
    "user_id": "P2",
    "time": "2022-12-06 19:39:49.988000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by providing information about the task environment that helps prevent the agent from exploring inapplicable actions and instead focuses on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f9a758a37977af6a11c51",
    "user_id": "P2",
    "time": "2022-12-06 19:39:33.839000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .In\u00a0this\u00a0work,\u00a0we\u00a0aim\u00a0to\u00a0reduce the sample complexity of\u00a0existing\u00a0reinforcement\u00a0learning\u00a0(RL)\u00a0algorithms by\u00a0providing\u00a0information\u00a0about\u00a0the\u00a0task\u00a0environment\u00a0that\u00a0helps\u00a0prevent\u00a0the agent from exploring inapplicable actions and instead\u00a0focuses\u00a0on only those that are relevant to finding an optimal policy.This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "type": "task",
    "writing_model": "model-writing-1",
    "conversation_id": "638f98948a37977af6a11c29"
  },
  {
    "_id": "638f9a178a37977af6a11c49",
    "user_id": "P2",
    "time": "2022-12-06 19:37:59.989000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL algorithms by preventing the agent from exploring inapplicable actions and instead focusing on only those that are relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f9a078a37977af6a11c46",
    "user_id": "P2",
    "time": "2022-12-06 19:37:43.848000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL\u00a0algorithms\u00a0by\u00a0preventing\u00a0the\u00a0agent\u00a0from\u00a0exploring\u00a0inapplicable actions\u00a0and\u00a0instead\u00a0focusing\u00a0on\u00a0only\u00a0those\u00a0that\u00a0are\u00a0relevant to finding an optimal policy.This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "type": "task",
    "writing_model": "model-writing-1",
    "conversation_id": "638f98948a37977af6a11c29"
  },
  {
    "_id": "638f99f78a37977af6a11c44",
    "user_id": "P2",
    "time": "2022-12-06 19:37:27.263000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL\u00a0algorithms\u00a0by\u00a0preventing\u00a0the\u00a0agent\u00a0from\u00a0exploring\u00a0inapplicable actions\u00a0and\u00a0instead\u00a0focusing\u00a0on\u00a0only\u00a0those\u00a0that\u00a0are\u00a0relevant to finding an optimal policy.This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f99ce8a37977af6a11c41",
    "user_id": "P2",
    "time": "2022-12-06 19:36:46.570000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL al\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f99b98a37977af6a11c3e",
    "user_id": "P2",
    "time": "2022-12-06 19:36:25.548000",
    "text": "Reinforcement Learning (RL) algorithms are known to scale poorly to environ\u0002ments with many available actions, requiring numerous samples to learn an opti\u0002mal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as inapplicable actions (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL al\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm. In this paper, we propose a more systematic approach to introduce this knowledge into the algorithm. We (i) standardize the way knowledge can be manually specified to the agent; and (ii) present a new framework to autonomously learn these state\u0002dependent action constraints jointly with the policy. We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions. Moreover, we demonstrate that thanks to the transferability of the knowledge acquired, it can be reused in other tasks to make the learning process more efficient.\n",
    "type": "task",
    "writing_model": "model-writing-1",
    "conversation_id": "638f98948a37977af6a11c29"
  },
  {
    "_id": "638f998d8a37977af6a11c3c",
    "user_id": "P2",
    "time": "2022-12-06 19:35:41.739000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .In this work , we aim to reduce the sample complexity of existing reinforcement learning ( RL ) algorithms by utilizing information to mask the inapplicable actions from the policy distribution and focus on exploring those relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f997e8a37977af6a11c39",
    "user_id": "P2",
    "time": "2022-12-06 19:35:26.430000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .\nIn\u00a0this\u00a0work,\u00a0we\u00a0aim\u00a0to\u00a0reduce the sample complexity of\u00a0existing\u00a0reinforcement\u00a0learning\u00a0(RL)\u00a0algorithms\u00a0by\u00a0utilizing\u00a0information\u00a0to\u00a0mask\u00a0the inapplicable actions from the policy distribution\u00a0and\u00a0focus\u00a0on\u00a0exploring\u00a0those\u00a0relevant to finding an optimal policy.This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f997d8a37977af6a11c38",
    "user_id": "P2",
    "time": "2022-12-06 19:35:25.417000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .\nIn\u00a0this\u00a0work,\u00a0we\u00a0aim\u00a0to\u00a0reduce the sample complexity of\u00a0existing\u00a0reinforcement\u00a0learning\u00a0(RL)\u00a0algorithms\u00a0by\u00a0utilizing\u00a0information\u00a0to\u00a0mask\u00a0the inapplicable actions from the policy distribution\u00a0and\u00a0focus\u00a0on\u00a0exploring\u00a0those\u00a0relevant to finding an optimal policy.This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "type": "task",
    "writing_model": "model-writing-1",
    "conversation_id": "638f98948a37977af6a11c29"
  },
  {
    "_id": "638f99188a37977af6a11c34",
    "user_id": "P2",
    "time": "2022-12-06 19:33:44.383000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f98b78a37977af6a11c2f",
    "user_id": "P2",
    "time": "2022-12-06 19:32:07.367000",
    "text": "Reinforcement Learning ( RL ) algorithms are known to scale poorly to environ\u0002ments with many available actions , requiring numerous samples to learn an opti\u0002mal policy .The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand , while also learning to maximize its reward , to ignore irrelevant actions such as inapplicable actions ( i.e. actions that have no effect on the environment when performed in a given state ) .Knowing this information can help reduce the sample complexity of RL al\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy .This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm .In this paper , we propose a more systematic approach to introduce this knowledge into the algorithm .We ( i ) standardize the way knowledge can be manually specified to the agent ; and ( ii ) present a new framework to autonomously learn these state \u0002dependent action constraints jointly with the policy .We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions .Moreover , we demonstrate that thanks to the transferability of the knowledge acquired , it can be reused in other tasks to make the learning process more efficient .\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f98a58a37977af6a11c2c",
    "user_id": "P2",
    "time": "2022-12-06 19:31:49.054000",
    "text": "Reinforcement Learning (RL) algorithms are known to scale poorly to environ\u0002ments with many available actions, requiring numerous samples to learn an opti\u0002mal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as inapplicable actions (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL al\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm. In this paper, we propose a more systematic approach to introduce this knowledge into the algorithm. We (i) standardize the way knowledge can be manually specified to the agent; and (ii) present a new framework to autonomously learn these state\u0002dependent action constraints jointly with the policy. We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions. Moreover, we demonstrate that thanks to the transferability of the knowledge acquired, it can be reused in other tasks to make the learning process more efficient.\n",
    "event_type": "auto-save"
  },
  {
    "_id": "638f989d8a37977af6a11c2b",
    "user_id": "P2",
    "time": "2022-12-06 19:31:41.073000",
    "text": "Reinforcement Learning (RL) algorithms are known to scale poorly to environ\u0002ments with many available actions, requiring numerous samples to learn an opti\u0002mal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as inapplicable actions (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL al\u0002gorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm. In this paper, we propose a more systematic approach to introduce this knowledge into the algorithm. We (i) standardize the way knowledge can be manually specified to the agent; and (ii) present a new framework to autonomously learn these state\u0002dependent action constraints jointly with the policy. We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algo\u0002rithm by providing a reliable signal to mask out irrelevant actions. Moreover, we demonstrate that thanks to the transferability of the knowledge acquired, it can be reused in other tasks to make the learning process more efficient.\n",
    "type": "task",
    "writing_model": "",
    "conversation_id": "638f98948a37977af6a11c29"
  }
]